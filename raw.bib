@misc{2019PointingDecodersMultiObjective,
  title = {Pointing {{Decoders}} with {{Multi-Objective Training}} for {{Diversity}} in {{Web Search Recommendation}}},
  date = {2019},
  keywords = {review},
  annotation = {EMNLP 2019 review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KJVLSIXZ/Anonymous - 2019 - Pointing Decoders with Multi-Objective Training for Diversity in Web Search Recommendation(2).pdf}
}

@misc{2019TemporalOrderEstimation,
  title = {Temporal {{Order Estimation}} in {{Recurrent Neural Networks}}},
  date = {2019},
  keywords = {review},
  annotation = {NIPS 2019 review (secondary for Hongyuan)},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/34I9HXMQ/Anonymous - 2019 - Temporal Order Estimation in Recurrent Neural Networks(2).pdf}
}

@misc{2020CausalCorpusLexical,
  title = {A {{Causal Corpus}}, {{Lexical Knowledge Base}}, and {{Sentential Generator}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N57ULVXQ/Anonymous - 2020 - A Causal Corpus, Lexical Knowledge Base, and Sentential Generator(2).pdf}
}

@misc{2020CausalInferenceScript,
  title = {Causal {{Inference}} of {{Script Knowledge}}},
  date = {2020},
  eprint = {25246403},
  eprinttype = {pmid},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  archiveprefix = {arXiv},
  isbn = {9788578110796},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W8U5BZR3/Anonymous - 2020 - Causal Inference of Script Knowledge(2).pdf}
}

@misc{2020ChartdrivenSequenceLabeling,
  title = {Chart-Driven {{Sequence Labeling}}},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KWJ3IQGQ/Anonymous - 2020 - Chart-driven Sequence Labeling(2).pdf}
}

@misc{2020DynamicKnowledgeIntegration,
  title = {Dynamic {{Knowledge Integration}} for {{Deep Text Matching}}},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/73Z99KX6/Submission 6020.pdf}
}

@misc{2020KnowledgeenhancedSequencetoTreeSolver,
  title = {Knowledge-Enhanced {{Sequence-to-Tree}} Solver for {{Math Word Problems}}},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B5LU8JVH/Submission 6848.pdf}
}

@misc{2020LearningTopicPreserving,
  title = {Learning {{Topic Preserving Embeddings}} with {{Seq2Seq}}},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9WVUDPIH/Anonymous - 2020 - Learning Topic Preserving Embeddings with Seq2Seq(2).pdf}
}

@misc{2020ROMaControllableText,
  title = {{{ROMa}} : {{Controllable Text Generation}} with {{Random Observation Machine}}},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T9USZZYP/Anonymous - 2020 - ROMa Controllable Text Generation with Random Observation Machine(2).pdf}
}

@misc{2020SemanticAnalysisAnalytic,
  title = {Semantic {{Analysis}} via {{Analytic Truth}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SN9R97H7/Anonymous - 2020 - Semantic Analysis via Analytic Truth(2).pdf}
}

@misc{2020StatutoryLegalReasoning,
  title = {Statutory {{Legal Reasoning}} : {{Challenging Natural Language Systems}} with {{Understanding Prescriptive Rules}}},
  date = {2020},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ELQLV77Q/Anonymous - 2020 - Statutory Legal Reasoning Challenging Natural Language Systems with Understanding Prescriptive Rules(2).pdf}
}

@misc{2020TrackingProgressLanguage,
  title = {Tracking the Progress of {{Language Models}} by Extracting Their Underlying {{Knowledge Graphs}}},
  date = {2020},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PXMZZ989/Tracking the Progress of Language Models by Extracting Their Underlaying Knowledge Graphs.pdf}
}

@misc{2020UnifiedMultiIntent,
  title = {Unified {{Multi Intent Order}} and {{Slot Prediction}} Using {{Selective Learning Propagation}} 051},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZJ88SK3/Anonymous - 2020 - Unified Multi Intent Order and Slot Prediction using Selective Learning Propagation 051(2).pdf}
}

@misc{2021AnatomyCatastrophicForgetting,
  title = {Anatomy of {{Catastrophic Forgetting}}: {{Hidden Representation}} and {{Task Semantics}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E6J7SRTR/document.pdf}
}

@misc{2021AvoidingRepetitionNeural,
  title = {Avoiding {{Repetition}} in {{Neural Open Information Extraction}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZDKBFZD/3336_file_Paper.pdf}
}

@misc{2021BETTERProgramEvaluation,
  title = {{{BETTER Program Evaluation Plan}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/86EZSBJY/BETTER-Eval-Plan_v20.pdf}
}

@misc{2021ClearingPathTruly,
  title = {Clearing the {{Path}} for {{Truly Semantic Representation Learning}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W9EBW9GB/document.pdf}
}

@misc{2021ConnectingMatrixSequential,
  title = {Connecting the {{Matrix}} and {{Sequential Representations}} for {{Joint Entity}} and {{Relation Extraction}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UPEYCT3B/4723_file_Paper.pdf}
}

@misc{2021ContrastiveRepresentationDistillation,
  title = {Contrastive {{Representation Distillation}} for {{Semi-Supervised Relation Extraction}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DBD5HAFB/2717_file_Paper.pdf}
}

@misc{2021CoreLMCoreferenceawareLanguage,
  title = {{{CoreLM}}: {{Coreference-aware Language Model Fine-Tuning}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SPUYANKG/838_file_Paper.pdf}
}

@misc{2021Doc2DictInformationExtraction,
  title = {{{Doc2Dict}}: {{Information Extraction}} as {{Text Generation}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V3FM436N/654_file_Paper.pdf}
}

@misc{2021EndtoEndLearningCoherent,
  title = {End-to-{{End Learning}} of {{Coherent Probabilistic Forecasts}} for {{Hierarchical Time Series}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LQDWKH98/Supplementary 6359.pdf;/home/hiaoxui/.local/share/zotero_files/storage/XSVT9RHN/Submission 6359.pdf}
}

@misc{2021EnhancingKnowledgeGraph,
  title = {Enhancing {{Knowledge Graph Extraction}} with {{Multi-Label Attributes}}, {{Applied}} to {{Scientific Claims}} and {{Aspect-Based Sentiment}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MFE6JTFG/3562_file_Paper.pdf}
}

@misc{2021ExploringOutputConstraints,
  title = {Exploring {{Output Constraints}} in {{Autoregressive Structured Prediction}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R5N9KSCI/3725_file_Paper.pdf}
}

@misc{2021ExploringPatentFeatures,
  title = {Exploring {{Patent Features}} for {{Relation Extraction}} with {{Contrastive Student-Teacher Learning}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DC4CD7TT/1906_file_Paper.pdf}
}

@misc{2021HySPAHybridSpan,
  title = {{{HySPA}}: {{Hybrid Span Generation}} for {{Scalable Text-to-Graph Extraction}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/48WZVLJF/3212_file_Paper.pdf}
}

@misc{2021MultilingualGenerativeLanguage,
  title = {Multilingual {{Generative Language Models}} for {{Zero-Shot Cross-Lingual Event Argument Extraction}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CSALZ3ML/multilingual_generative_langua.pdf}
}

@misc{2021MultiplexNetFullySatisfied,
  title = {{{MultiplexNet}}: {{Towards Fully Satisfied Logical Constraints}} in {{Neural Networks}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8EXRP74A/multiplexnet_towards_fully_satisfied_logical_constraints_in_neural_networks.pdf}
}

@misc{2021NeuralOpenInformation,
  title = {Neural {{Open Information Extraction}} with {{Pointer-Generator Networks}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/THH4VVZ7/2377_file_Paper.pdf}
}

@misc{2021RelationExtractionTables,
  title = {Relation {{Extraction}} from {{Tables}} Using {{Artificially Generated Metadata}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3KIBD4QZ/679_file_Paper.pdf}
}

@misc{2021SpanLevelEmotionCause,
  title = {Span-{{Level Emotion Cause Analysis}} by {{Neural Sequence Tagging}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q76IMYUK/3195 Submission.pdf}
}

@misc{2021WeaklySupervisedNeuroSymbolic,
  title = {Weakly {{Supervised Neuro-Symbolic Module Networks}} for {{Numerical Reasoning}}},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QEBWMS56/weakly_supervised_neuro_symbolic_module_networks_for_numerical_reasoning.pdf}
}

@misc{2022AskingRightQuestions,
  title = {Asking the {{Right Questions}}: {{Human-Written Questions}} for {{Low-Resource Information Extraction}}},
  date = {2022},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5SJTACYL/template_as_qa.pdf}
}

@misc{2022ContrastiveFrameworkKeyphrase,
  title = {A {{Contrastive Framework}} for {{Keyphrase Prediction}}},
  date = {2022},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AWU48DGH/1057_file_Paper.pdf}
}

@misc{2022ExplainMySurprise,
  title = {Explain {{My Surprise}}: {{Learning Efficient Long-Term Memory}} by {{Predicting Uncertain Outcome}}},
  date = {2022},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZVTKECLX/explain_my_surprise_learning_e.pdf}
}

@misc{2022ExploringTimeInformation,
  title = {Exploring {{Time Information}} for {{Entity Alignment}}},
  date = {2022},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/955FBFUG/3978_file_Paper.pdf}
}

@misc{2022HierarchicalGraphTransformer,
  title = {Hierarchical {{Graph Transformer}} with {{Adaptive Node Sampling}}},
  date = {2022},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TCKJ6ESB/hierarchical_graph_transformer.pdf}
}

@misc{2022ImprovedInductionNarrative,
  title = {Improved {{Induction}} of {{Narrative Chains}} via {{Cross-Document Relations}}},
  date = {2022},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GC5BKD8M/ImprovedNarrativeChains.pdf}
}

@misc{2022JointInformationExtraction,
  title = {Joint {{Information Extraction}} as {{High-Order Span-Relation Inference}}},
  date = {2022},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FVRASIYZ/2377_file_Paper.pdf}
}

@misc{2022ModelRevisionContent,
  title = {On {{Model Revision}}: {{Content Re-mapping}} and {{Avoidance}}},
  date = {2022},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E8LPXWNY/3666_file_Paper.pdf}
}

@misc{2022PredictiveQueryingAutoregressive,
  title = {Predictive {{Querying}} for {{Autoregressive Neural Sequence Models}}},
  date = {2022},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LHY4J6N7/predictive_querying_for_autore.pdf}
}

@misc{2022ProbabilisticAttentiontoInfluenceNeural,
  title = {Probabilistic {{Attention-to-Influence Neural Models}} for {{Event Sequences}}},
  date = {2022},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JBJI86P6/probabilistic_attention_to_inf.pdf}
}

@misc{2022RetrievalPromptTuning,
  title = {Retrieval of {{Prompt Tuning Enhances Zero-Shot Task Generalization}}},
  date = {2022},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/48NSHFYC/2162_file_Paper.pdf}
}

@misc{2023CertifiablyRobustTransformers,
  title = {Certifiably {{Robust Transformers}} with 1-{{Lipschitz Self-Attention}}},
  date = {2023},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/37M9HNY7/certifiably_robust_transformer.pdf}
}

@misc{2023LiftingCurseCapacity,
  title = {Lifting the {{Curse}} of {{Capacity Gap}} in {{Distilling Large Language Models}}},
  date = {2023},
  keywords = {distillation,review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C397HUWR/2023 - Lifting the Curse of Capacity Gap in Distilling La.pdf}
}

@misc{2023TransformerPatcherOneMistake,
  title = {Transformer-{{Patcher}}: {{One Mistake Worth One Neuron}}},
  date = {2023},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3M5G344Y/transformer_patcher_one_mistak.pdf}
}

@inproceedings{abend2013UniversalConceptualCognitive,
  title = {Universal {{Conceptual Cognitive Annotation}} ( {{UCCA}} )},
  booktitle = {{{ACL}}},
  author = {Abend, O. and Rappoport, A.},
  date = {2013},
  pages = {228--238},
  url = {http://www.aclweb.org/anthology/P13-1023},
  isbn = {978-1-937284-50-3},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EBEUE4KU/Abend, Rappoport - 2013 - Universal Conceptual Cognitive Annotation ( UCCA )(2).pdf}
}

@inproceedings{abend2017StateArtSemantic,
  title = {The State of the Art in Semantic Representation},
  booktitle = {{{ACL}}},
  author = {Abend, O. and Rappoport, A.},
  date = {2017},
  pages = {77--89},
  doi = {10.18653/v1/P17-1008},
  abstract = {© 2017 Association for Computational Linguistics. Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.},
  isbn = {978-1-945626-75-3},
  annotation = {49 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RGTSTCBQ/Abend, Rappoport - 2017 - The state of the art in semantic representation(2).pdf}
}

@inproceedings{abnar2021ExploringLimitsLarge,
  title = {Exploring the {{Limits}} of {{Large Scale Pre-training}}},
  booktitle = {{{ICLR}}},
  author = {Abnar, Samira and Dehghani, Mostafa and Neyshabur, Behnam and Sedghi, Hanie},
  date = {2021-10-05},
  eprint = {2110.02095},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.02095},
  urldate = {2022-02-24},
  abstract = {Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might observe that improvements in pre-training would transfer favorably to most downstream tasks. In this work, we systematically study this phenomena and establish that, as we increase the upstream accuracy, the performance of downstream tasks saturates. In particular, we investigate more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten million to ten billion, trained on the largest scale of available image data (JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition tasks. We propose a model for downstream performance that reflects the saturation phenomena and captures the nonlinear relationship in performance of upstream and downstream tasks. Delving deeper to understand the reasons that give rise to these phenomena, we show that the saturation behavior we observe is closely related to the way that representations evolve through the layers of the models. We showcase an even more extreme scenario where performance on upstream and downstream are at odds with each other. That is, to have a better downstream performance, we need to hurt upstream accuracy.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6NEZ7IR5/Abnar et al. - 2021 - Exploring the Limits of Large Scale Pre-training.pdf;/home/hiaoxui/.local/share/zotero_files/storage/T3Z7KLD3/2110.html}
}

@incollection{abney1991ParsingChunks,
  title = {Parsing {{By Chunks}}},
  booktitle = {Principle-{{Based Parsing}}},
  author = {Abney, Steven P.},
  editor = {Berwick, Robert C. and Abney, Steven P. and Tenny, Carol},
  date = {1991},
  series = {Studies in {{Linguistics}} and {{Philosophy}}},
  volume = {44},
  pages = {257--278},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-011-3474-3_10},
  url = {http://link.springer.com/10.1007/978-94-011-3474-3_10},
  urldate = {2020-08-14},
  editorb = {Chierchia, Gennaro and Jacobson, Pauline and Pelletier, Francis J.},
  editorbtype = {redactor},
  isbn = {978-0-7923-1637-4 978-94-011-3474-3},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RCE7MH5G/Abney - 1991 - Parsing By Chunks.pdf}
}

@article{abney1996PartialParsingFinitestate,
  title = {Partial Parsing via Finite-State Cascades},
  author = {Abney, Steven},
  date = {1996-12},
  journaltitle = {Natural Language Engineering},
  shortjournal = {Nat. Lang. Eng.},
  volume = {2},
  number = {4},
  pages = {337--344},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324997001599},
  url = {https://www.cambridge.org/core/product/identifier/S1351324997001599/type/journal_article},
  urldate = {2020-08-14},
  abstract = {Finite state cascades represent an attractive architecture for parsing unrestricted text. Deterministic parsers specified by finite state cascades are fast and reliable. They can be extended at modest cost to construct parse trees with finite feature structures. Finally, such deterministic parsers do not necessarily involve trading off accuracy against speed—they may in fact be more accurate than exhaustive search stochastic context free parsers.},
  langid = {english},
  keywords = {unread},
  annotation = {594 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H4UQMLY4/Abney - 1996 - Partial parsing via finite-state cascades.pdf}
}

@incollection{abney1997PartofSpeechTaggingPartial,
  title = {Part-of-{{Speech Tagging}} and {{Partial Parsing}}},
  booktitle = {Corpus-{{Based Methods}} in {{Language}} and {{Speech Processing}}},
  author = {Abney, S.},
  editor = {Young, Steve and Bloothooft, Gerrit},
  date = {1997},
  series = {Text, {{Speech}} and {{Language Technology}}},
  volume = {2},
  pages = {118--136},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-017-1183-8_4},
  url = {http://link.springer.com/10.1007/978-94-017-1183-8_4},
  urldate = {2020-08-14},
  editorb = {Ide, Nancy and Véronis, Jean},
  editorbtype = {redactor},
  isbn = {978-90-481-4813-4 978-94-017-1183-8},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IMYJJV3P/Abney - 1997 - Part-of-Speech Tagging and Partial Parsing.pdf}
}

@misc{aghajanyan2020ConversationalSemanticParsing,
  title = {Conversational {{Semantic Parsing}}},
  author = {Aghajanyan, Armen and Maillard, Jean and Shrivastava, Akshat and Diedrick, Keith and Haeger, Mike and Li, Haoran and Mehdad, Yashar and Stoyanov, Ves and Kumar, Anuj and Lewis, Mike and Gupta, Sonal},
  date = {2020-09-28},
  eprint = {2009.13655},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.13655},
  urldate = {2020-10-21},
  abstract = {The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which achieve better or comparable performance to the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KJ57UCDT/Aghajanyan et al. - 2020 - Conversational Semantic Parsing.pdf;/home/hiaoxui/.local/share/zotero_files/storage/P78XTAVF/2009.html}
}

@misc{aghajanyan2021MuppetMassiveMultitask,
  title = {Muppet: {{Massive Multi-task Representations}} with {{Pre-Finetuning}}},
  shorttitle = {Muppet},
  author = {Aghajanyan, Armen and Gupta, Anchit and Shrivastava, Akshat and Chen, Xilun and Zettlemoyer, Luke and Gupta, Sonal},
  date = {2021-01-26},
  eprint = {2101.11038},
  eprinttype = {arxiv},
  url = {11},
  urldate = {2021-01-29},
  abstract = {We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g.\textasciitilde RoBERTa) and generation models (e.g.\textasciitilde BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ED5SNQRT/Aghajanyan et al. - 2021 - Muppet Massive Multi-task Representations with Pr.pdf;/home/hiaoxui/.local/share/zotero_files/storage/2MF35IWG/2101.html}
}

@article{agrawal2004Primes,
  title = {Primes Is in {{P}}},
  author = {Agrawal, M. and Kayal, N. and Saxena, N.},
  date = {2004},
  journaltitle = {Annals of mathematics},
  volume = {160},
  number = {2},
  pages = {781--793},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EYTFGCW3/Agrawal, Kayal, Saxena - 2004 - Primes is in P(2).pdf}
}

@inproceedings{ainslie2020ETCEncodingLong,
  title = {{{ETC}}: {{Encoding Long}} and {{Structured Inputs}} in {{Transformers}}},
  shorttitle = {{{ETC}}},
  booktitle = {{{EMNLP}}},
  author = {Ainslie, Joshua and Ontanon, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li},
  date = {2020-10-27},
  eprint = {2004.08483},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.08483},
  urldate = {2021-03-10},
  abstract = {Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.},
  archiveprefix = {arXiv},
  annotation = {18 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/65PNYPMF/Ainslie et al. - 2020 - ETC Encoding Long and Structured Inputs in Transf.pdf;/home/hiaoxui/.local/share/zotero_files/storage/2KBE5VQN/2004.html}
}

@inproceedings{aitchison2020WhyBiggerNot,
  title = {Why Bigger Is Not Always Better : On Finite and Infinite Neural Networks},
  booktitle = {{{ICML}}},
  author = {Aitchison, L.},
  date = {2020},
  eprint = {1910.08013v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/58WSYQ6P/Aitchison - 2019 - Why bigger is not always better on finite and infinite neural networks(6).pdf;/home/hiaoxui/.local/share/zotero_files/storage/6I3R6EU6/Aitchison - 2019 - Why bigger is not always better on finite and infinite neural networks(4).pdf;/home/hiaoxui/.local/share/zotero_files/storage/KUZ7VUSL/Aitchison - 2019 - Why bigger is not always better on finite and infinite neural networks(5).pdf}
}

@inproceedings{al-shedivat2018ContinuousAdaptationMetaLearning,
  title = {Continuous {{Adaptation}} via {{Meta-Learning}} in {{Nonstationary}} and {{Competitive Environments}}},
  booktitle = {{{ICLR}}},
  author = {Al-Shedivat, M. and Bansal, T. and Burda, Y. and Sutskever, I. and Mordatch, I. and Abbeel, P.},
  date = {2018},
  pages = {1--23},
  abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of ex-ponential moving averages of squared past gradients. It has been empirically ob-served that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous anal-ysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with " long-term memory " of past gradi-ents, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E5WEB49D/Al-Shedivat et al. - 2018 - Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments(2).pdf}
}

@inproceedings{alajrami2022HowDoesPretraining,
  title = {How Does the Pre-Training Objective Affect What Large Language Models Learn about Linguistic Properties?},
  booktitle = {{{ACL}}},
  author = {Alajrami, Ahmed and Aletras, Nikolaos},
  date = {2022-03-19},
  eprint = {2203.10415},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.10415},
  urldate = {2022-03-23},
  abstract = {Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B2C7Y2ZL/Alajrami and Aletras - 2022 - How does the pre-training objective affect what la.pdf;/home/hiaoxui/.local/share/zotero_files/storage/5MI7R2ND/2203.html}
}

@article{albert1993BayesianAnalysisBinary,
  title = {Bayesian {{Analysis}} of {{Binary}} and {{Polychotomous Response Data}}},
  author = {Albert, J. H. and Chib, S.},
  date = {1993},
  journaltitle = {JASA},
  volume = {88},
  number = {422},
  pages = {669--679},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7CFJEXGU/Albert, Chib - 1993 - Bayesian Analysis of Binary and Polychotomous Response Data(2).pdf}
}

@misc{alberti2019BERTBaselineNatural,
  title = {A {{BERT Baseline}} for the {{Natural Questions}}},
  author = {Alberti, C. and Lee, K. and Collins, M.},
  date = {2019},
  eprint = {1901.08634},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.08634},
  abstract = {This technical note describes a new baseline for the Natural Questions. Our model is based on BERT and reduces the gap between the model F1 scores reported in the original dataset paper and the human upper bound by 30\% and 50\% relative for the long and short answer tasks respectively. This baseline has been submitted to the official NQ leaderboard at ai.google.com/research/NaturalQuestions. Code, preprocessed data and pretrained model are available at https://github.com/google-research/language/tree/master/language/question\_answering/bert\_joint.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {57 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3SMS6JRY/Alberti, Lee, Collins - 2019 - A BERT Baseline for the Natural Questions(2).pdf}
}

@inproceedings{alizadeh2020CorpusVisualQuestion,
  title = {A {{Corpus}} for {{Visual Question Answering Annotated}} with {{Frame Semantic Information}}},
  booktitle = {{{LREC}}},
  author = {Alizadeh, Mehrdad and Eugenio, Barbara Di},
  date = {2020},
  pages = {8},
  abstract = {Visual Question Answering (VQA) has been widely explored as a computer vision problem, however enhancing VQA systems with linguistic information is necessary for tackling the complexity of the task. The language understanding part can play a major role especially for questions asking about events or actions expressed via verbs. We hypothesize that if the question focuses on events described by verbs, then the model should be aware of or trained with verb semantics, as expressed via semantic role labels, argument types, and/or frame elements. Unfortunately, no VQA dataset exists that includes verb semantic information. We created a new VQA dataset annotated with verb semantic information called imSituVQA. imSituVQA is built by taking advantage of the imSitu dataset annotations. The imSitu dataset consists of images manually labeled with semantic frame elements, mostly taken from FrameNet.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MW2BVBCH/Alizadeh and Eugenio - A Corpus for Visual Question Answering Annotated w.pdf}
}

@inproceedings{alonso2017ParsingUniversalDependencies,
  title = {Parsing Universal Dependencies without Training},
  booktitle = {{{EACL}}},
  author = {Alonso, H. M. and Agić, Ž. and Plank, B. and Søgaard, A.},
  date = {2017},
  pages = {230--240},
  abstract = {We propose UDP, the first training-free parser for Universal Dependencies (UD). Our algorithm is based on PageRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages.},
  isbn = {978-1-5108-3860-4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZI6T7FFI/Alonso et al. - 2017 - Parsing universal dependencies without training(2).pdf}
}

@inproceedings{alvarez-melis2017TreestructureDecodingDoublyRecurrent,
  title = {Tree-Structure Decoding with {{Doubly-Recurrent Neural Networks}}},
  booktitle = {{{ICLR}}},
  author = {Alvarez-Melis, D. and Jaakkola, T. S.},
  date = {2017},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F3BAF5EB/Alvarez-Melis, Jaakkola - 2017 - Tree-structure decoding with Doubly-Recurrent Neural Networks(2).pdf}
}

@inproceedings{ammar2014ConditionalRandomField,
  title = {Conditional {{Random Field Autoencoders}} for {{Unsupervised Structured Prediction}}},
  booktitle = {{{NeurIPS}}},
  author = {Ammar, W. and Dyer, C. and Smith, N. A.},
  date = {2014},
  eprint = {1411.1147},
  eprinttype = {arxiv},
  issn = {10495258},
  url = {http://arxiv.org/abs/1411.1147},
  abstract = {We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observable data using a feature-rich conditional random field. Then a reconstruction of the input is (re)generated, conditional on the latent structure, using models for which maximum likelihood estimation has a closed-form. Our autoencoder formulation enables efficient learning without making unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. We show competitive results with instantiations of the model for two canonical NLP tasks: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {68 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7PGKLRY2/Ammar, Dyer, Smith - 2014 - Conditional Random Field Autoencoders for Unsupervised Structured Prediction(2).pdf}
}

@misc{andersen2018FourierDomainJerk,
  title = {A {{Fourier Domain}} "{{Jerk}}" {{Search}} for {{Binary Pulsars}}},
  author = {Andersen, B. C. and Ransom, S. M.},
  date = {2018},
  eprint = {1807.07900},
  eprinttype = {arxiv},
  issn = {20418213},
  doi = {10.3847/2041-8213/aad59f},
  url = {http://arxiv.org/abs/1807.07900},
  abstract = {While binary pulsar systems are fantastic laboratories for a wide array of astrophysics, they are particularly difficult to detect. The orbital motion of the pulsar changes its apparent spin frequency over the course of an observation, essentially "smearing" the response of the time series in the Fourier domain. We review the Fourier domain acceleration search (FDAS), which uses a matched filtering algorithm to correct for this smearing by assuming constant acceleration for a small enough portion of the orbit. We discuss the theory and implementation of a Fourier domain "jerk" search, developed as part of the \textbackslash textsc\{PRESTO\} software package, which extends the FDAS to account for a linearly changing acceleration, or constant orbital jerk, of the pulsar. We test the performance of our algorithm on archival Green Bank Telescope observations of the globular cluster Terzan\textasciitilde 5, and show that while the jerk search has a significantly longer runtime, it improves search sensitivity to binaries when the observation duration is \$5\$ to \$15\textbackslash\%\$ of the orbital period. Finally, we present the jerk-search-enabled detection of Ter5am (PSR\textasciitilde J1748\$-\$2446am), a new highly-accelerated pulsar in a compact, eccentric, and relativistic orbit, with a likely pulsar mass of 1.649\$\^\{+0.037\}\_\{-0.11\}\$\textbackslash,\textbackslash msun.},
  archiveprefix = {arXiv},
  annotation = {14 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6CCW9Q34/Andersen, Ransom - 2018 - A Fourier Domain Jerk Search for Binary Pulsars(2).pdf}
}

@article{anderson2019MoreDifferent,
  title = {More Is {{Different}}},
  author = {Anderson, P. W.},
  date = {2019},
  journaltitle = {Science},
  volume = {177},
  number = {4047},
  pages = {393--396},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S5WFUZAZ/Anderson - 2019 - More is Different(2).pdf}
}

@inproceedings{anderson2019OptimizationAbstractionSynergistic,
  title = {Optimization and Abstraction: {{A}} Synergistic Approach for Analyzing Neural Network Robustness},
  booktitle = {Programming {{Language Design}} and {{Implementation}}},
  author = {Anderson, G. and Dillig, I. and Pailoor, S. and Chaudhuri, S.},
  date = {2019},
  eprint = {1904.09959},
  eprinttype = {arxiv},
  pages = {731--744},
  doi = {10.1145/3314221.3314614},
  abstract = {In recent years, the notion of local robustness (or robustness for short) has emerged as a desirable property of deep neural networks. Intuitively, robustness means that small perturbations to an input do not cause the network to perform misclas-sifications. In this paper, we present a novel algorithm for verifying robustness properties of neural networks. Our method synergistically combines gradient-based optimization methods for counterexample search with abstraction-based proof search to obtain a sound and (δ-)complete decision procedure. Our method also employs a data-driven approach to learn a verification policy that guides abstract interpretation during proof search. We have implemented the proposed approach in a tool called Charon and experimentally evaluated it on hundreds of benchmarks. Our experiments show that the proposed approach significantly outperforms three state-of-the-art tools, namely AI2, Reluplex, and Reluval.},
  archiveprefix = {arXiv},
  isbn = {978-1-4503-6712-7},
  keywords = {unread},
  annotation = {25 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LEJSSUHX/Anderson et al. - 2019 - Optimization and abstraction A synergistic approach for analyzing neural network robustness(2).pdf}
}

@article{ando2005FrameworkLearningPredictive,
  title = {A {{Framework}} for {{Learning Predictive Structures}} from {{Multiple Tasks}} and {{Unlabeled Data}}},
  author = {Ando, R. K. and Zhang, T.},
  date = {2005},
  journaltitle = {JMLR},
  volume = {6},
  pages = {1817--1853},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9NPPMQM4/Ando, Zhang - 2005 - A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data(2).pdf}
}

@inproceedings{ando2005HighperformanceSemisupervisedLearning,
  title = {A High-Performance Semi-Supervised Learning Method for Text Chunking},
  booktitle = {{{ACL}}},
  author = {Ando, R. K. and Zhang, T.},
  date = {2005},
  pages = {1--9},
  abstract = {In machine learning, whether one can\textbackslash nbuild a more accurate classifier by using\textbackslash nunlabeled data (semi-supervised learning)\textbackslash nis an important issue. Although a num-\textbackslash nber of semi-supervised methods have been\textbackslash nproposed, their effectiveness on NLP tasks\textbackslash nis not always clear. This paper presents\textbackslash na novel semi-supervised method that em-\textbackslash nploys a learning paradigm which we call\textbackslash nstructural learning. The idea is to find\textbackslash n“what good classifiers are like” by learn-\textbackslash ning from thousands of automatically gen-\textbackslash nerated auxiliary classification problems on\textbackslash nunlabeled data. By doing so, the common\textbackslash npredictive structure shared by the multiple\textbackslash nclassification problems can be discovered,\textbackslash nwhich can then be used to improve perfor-\textbackslash nmance on the target problem. The method\textbackslash nproduces performance higher than the pre-\textbackslash nvious best results on CoNLL’00 syntac-\textbackslash ntic chunking and CoNLL’03 named entity\textbackslash nchunking (English and German).},
  isbn = {1-932432-51-5},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ULX9QC65/Ando, Zhang - 2005 - A high-performance semi-supervised learning method for text chunking(2).pdf}
}

@inproceedings{andor2016GloballyNormalizedTransitionBased,
  title = {Globally {{Normalized Transition-Based Neural Networks}}},
  booktitle = {{{ACL}}},
  author = {Andor, D. and Alberti, C. and Weiss, D. and Severyn, A. and Presta, A. and Ganchev, K. and Petrov, S. and Collins, M.},
  date = {2016},
  eprint = {1603.06042},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1603.06042},
  abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {473 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JRYHFTMH/Andor et al. - 2016 - Globally Normalized Transition-Based Neural Networks(2).pdf}
}

@inproceedings{andor2019GivingBERTCalculator,
  title = {Giving {{BERT}} a {{Calculator}}: {{Finding Operations}} and {{Arguments}} with {{Reading Comprehension}}},
  booktitle = {{{EMNLP}}},
  author = {Andor, D. and He, L. and Lee, K. and Pitler, E.},
  date = {2019},
  volume = {2},
  eprint = {1909.00109},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.00109},
  abstract = {Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers. We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning. We augment the model with a predefined set of executable 'programs' which encompass simple arithmetic as well as extraction. Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it. On the recent Discrete Reasoning Over Passages (DROP) dataset, designed to challenge reading comprehension models, we show a 33\% absolute improvement by adding shallow programs. The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {23 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3QRSCK93/Andor et al. - 2019 - Giving BERT a Calculator Finding Operations and Arguments with Reading Comprehension(2).pdf}
}

@misc{andreas2016LearningComposeNeural,
  title = {Learning to {{Compose Neural Networks}} for {{Question Answering}}},
  author = {Andreas, J. and Rohrbach, M. and Darrell, T. and Klein, D.},
  date = {2016},
  eprint = {1601.01705},
  eprinttype = {arxiv},
  doi = {10.18653/v1/N16-1181},
  url = {http://arxiv.org/abs/1601.01705},
  abstract = {We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.},
  archiveprefix = {arXiv},
  isbn = {9781941643914},
  annotation = {417 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WT9AM6US/Andreas et al. - 2016 - Learning to Compose Neural Networks for Question Answering(2).pdf}
}

@inproceedings{andreas2017AnalogsLinguisticStructure,
  title = {Analogs of {{Linguistic Structure}} in {{Deep Representations}}},
  booktitle = {{{EMNLP}}},
  author = {Andreas, J. and Klein, D.},
  date = {2017},
  eprint = {1707.08139},
  eprinttype = {arxiv},
  issn = {18734197},
  doi = {10.18653/v1/D17-1311},
  url = {http://arxiv.org/abs/1707.08139},
  abstract = {We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a "syntax" with functional analogues to qualitative properties of natural language.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {9 citations (Semantic Scholar/DOI) [2021-03-26] 9 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7J7R3R9M/Andreas, Klein - 2017 - Analogs of Linguistic Structure in Deep Representations(2).pdf}
}

@inproceedings{andrews2011TransformationProcessPriors,
  title = {Transformation {{Process Priors}}},
  booktitle = {{{NeurIPS}}},
  author = {Andrews, N. and Eisner, J. M.},
  date = {2011},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QGWF8GE9/Andrews, Eisner - 2011 - Transformation Process Priors(2).pdf}
}

@inproceedings{andrews2014RobustEntityClustering,
  title = {Robust Entity Clustering via Phylogenetic Inference},
  booktitle = {{{ACL}}},
  author = {Andrews, N. and Eisner, J. M. and Dredze, M.},
  date = {2014},
  doi = {10.3115/v1/p14-1073},
  abstract = {Entity clustering must determine when two named-entity mentions refer to the same entity. Typical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity. In this paper, we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data. The generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context. Clustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection process. We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets. © 2014 Association for Computational Linguistics.},
  isbn = {978-1-937284-72-5},
  keywords = {unread},
  annotation = {17 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7VXIIC7P/Andrews, Eisner, Dredze - 2014 - Robust entity clustering via phylogenetic inference(2).pdf}
}

@inproceedings{andrews2017BayesianModelingLexical,
  title = {Bayesian Modeling of Lexical Resources for Low-Resource Settings},
  booktitle = {{{ACL}}},
  author = {Andrews, N. and Dredze, M. and Van Durme, B. and Eisner, J. M.},
  date = {2017},
  pages = {1029--1039},
  doi = {10.18653/v1/P17-1095},
  abstract = {Lexical resources such as dictionaries and gazetteers are often used as auxiliary data for tasks such as part-of-speech induction and named-entity recognition. However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of features which generalize better. In this paper, we investigate a more robust approach: we stipulate that the lexicon is the result of an assumed generative process. Practically, this means that we may treat the lexical resources as observations under the proposed generative model. The lexical resources provide training data for the generative model without requiring separate data to estimate lexical feature weights. We evaluate the proposed approach in two settings: part-of-speech induction and low-resource named-entity recognition.},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HXNI9GG7/Andrews et al. - 2017 - Bayesian modeling of lexical resources for low-resource settings(2).pdf}
}

@article{andrieu2010ParticleMarkovChain,
  title = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Andrieu, C. and Doucet, A. and Holenstein, R.},
  date = {2010},
  journaltitle = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
  volume = {72},
  number = {3},
  pages = {269--342},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2009.00736.x},
  abstract = {Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a Lévy-driven stochastic volatility model. © 2010 Royal Statistical Society.},
  keywords = {unread},
  annotation = {1481 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D7UT6DGS/Andrieu, Doucet, Holenstein - 2010 - Particle Markov chain Monte Carlo methods(2).pdf}
}

@inproceedings{angeli2010SimpleDomainIndependentProbabilistic,
  title = {A {{Simple Domain-Independent Probabilistic Approach}} to {{Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Angeli, G. and Liang, P. and Klein, D.},
  date = {2010},
  pages = {502--512},
  url = {http://www.aclweb.org/anthology/D10-1049},
  abstract = {We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework. In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domainsRobocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-of-the-art domain-specific systems both in terms of BLEU scores and human evaluation.},
  isbn = {1-932432-86-8},
  issue = {October},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GAYR4XYH/Angeli, Liang, Klein - 2010 - A Simple Domain-Independent Probabilistic Approach to Generation(2).pdf}
}

@inproceedings{angermueller2020PopulationBasedBlackBoxOptimization,
  title = {Population-{{Based Black-Box Optimization}} for {{Biological Sequence Design}}},
  booktitle = {{{ICML}}},
  author = {Angermueller, Christof and Belanger, David and Gane, Andreea and Mariet, Zelda and Dohan, David and Murphy, Kevin and Colwell, Lucy and Sculley, D.},
  date = {2020},
  volume = {168},
  eprint = {2006.03227},
  eprinttype = {arxiv},
  pages = {757--768},
  doi = {10.1111/j.1365-246X.2006.03227.x},
  url = {http://arxiv.org/abs/2006.03227},
  urldate = {2021-01-04},
  abstract = {The use of black-box optimization for the design of new biological sequences is an emerging research area with potentially revolutionary impact. The cost and latency of wet-lab experiments requires methods that find good sequences in few experimental rounds of large batches of sequences--a setting that off-the-shelf black-box optimization methods are ill-equipped to handle. We find that the performance of existing methods varies drastically across optimization tasks, posing a significant obstacle to real-world applications. To improve robustness, we propose Population-Based Black-Box Optimization (P3BO), which generates batches of sequences by sampling from an ensemble of methods. The number of sequences sampled from any method is proportional to the quality of sequences it previously proposed, allowing P3BO to combine the strengths of individual methods while hedging against their innate brittleness. Adapting the hyper-parameters of each of the methods online using evolutionary optimization further improves performance. Through extensive experiments on in-silico optimization tasks, we show that P3BO outperforms any single method in its population, proposing higher quality sequences as well as more diverse batches. As such, P3BO and Adaptive-P3BO are a crucial step towards deploying ML to real-world sequence design.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {64 citations (Semantic Scholar/DOI) [2021-03-26] 16 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZMK78P64/Angermueller et al. - 2007 - Population-Based Black-Box Optimization for Biolog.pdf;/home/hiaoxui/.local/share/zotero_files/storage/S89CSH3I/2006.html}
}

@inproceedings{anonymousMultiCQAZeroShotTransfer2020,
  title = {{{MultiCQA}} : {{Zero-Shot Transfer}} of {{Self-Supervised Text Matching Models}} on a {{Massive Scale}}},
  booktitle = {{{EMNLP}}},
  author = {Ruckle, A. and Pfeiffer, J. and Gurevych, I.},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9UW9Z5J7/Anonymous - 2020 - MultiCQA Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale(2).pdf}
}

@inproceedings{arnaud2017IdentifyingCognateSets,
  title = {Identifying {{Cognate Sets Across Dictionaries}} of {{Related Languages}}},
  booktitle = {{{EMNLP}}},
  author = {Arnaud, A. S. and Beck, D.},
  date = {2017},
  pages = {2519--2528},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZVNLVPDS/Arnaud, Beck - 2017 - Identifying Cognate Sets Across Dictionaries of Related Languages(2).pdf}
}

@inproceedings{artetxe2018UnsupervisedStatisticalMachine,
  title = {Unsupervised {{Statistical Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  date = {2018},
  pages = {3632--3642},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1399},
  url = {http://aclweb.org/anthology/D18-1399},
  urldate = {2020-10-23},
  abstract = {While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at https:// github.com/artetxem/monoses.},
  eventtitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  langid = {english},
  keywords = {unread},
  annotation = {135 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8DQNZEXL/Artetxe et al. - 2018 - Unsupervised Statistical Machine Translation.pdf}
}

@inproceedings{artetxe2019EffectiveApproachUnsupervised,
  title = {An {{Effective Approach}} to {{Unsupervised Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  date = {2019},
  pages = {194--203},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1019},
  url = {https://www.aclweb.org/anthology/P19-1019},
  urldate = {2020-10-23},
  abstract = {While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through onthe-fly back-translation. Together, we obtain large improvements over the previous stateof-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014.},
  eventtitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {unread},
  annotation = {60 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZIYBN6QU/Artetxe et al. - 2019 - An Effective Approach to Unsupervised Machine Tran.pdf}
}

@inproceedings{arthur2016IncorporatingDiscreteTranslation,
  title = {Incorporating {{Discrete Translation Lexicons}} into {{Neural Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Arthur, P. and Neubig, G. and Nakamura, S.},
  date = {2016},
  eprint = {1606.02006},
  eprinttype = {arxiv},
  pages = {1557--1567},
  abstract = {Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CP637YFH/Arthur, Neubig, Nakamura - 2016 - Incorporating Discrete Translation Lexicons into Neural Machine Translation(2).pdf}
}

@article{artzi2013WeaklySupervisedLearning,
  title = {Weakly {{Supervised Learning}} of {{Semantic Parsers}} for {{Mapping Instructions}} to {{Actions}}},
  author = {Artzi, Y. and Zettlemoyer, L. S.},
  date = {2013},
  journaltitle = {TACL},
  volume = {1},
  pages = {49--62},
  issn = {2307-387X},
  abstract = {The context in which language is used pro- vides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCGsemantic parsing approach that learns a joint model of mean- ing and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to di- rectly influence learning. It also enables algo- rithms that learn while executing instructions, for example by trying to replicate human ac- tions. Experiments on a benchmark naviga- tional dataset demonstrate strong performance under differing forms of supervision, includ- ing correctly executing 60\% more instruction sets relative to the previous state of the art.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UZ6HJV4K/Artzi, Zettlemoyer - 2013 - Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions(2).pdf}
}

@inproceedings{artzi2015BroadcoverageCCGSemantic,
  title = {Broad-Coverage {{CCG Semantic Parsing}} with {{AMR}}},
  booktitle = {{{EMNLP}}},
  author = {Artzi, Y. and Lee, K. and Zettlemoyer, L. S.},
  date = {2015},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/233HWHKL/Artzi, Lee, Zettlemoyer - 2015 - Broad-coverage CCG Semantic Parsing with AMR(2).pdf}
}

@article{arulampalam2002TutorialParticleFilters,
  title = {A Tutorial on Particle Filters for Online Nonlinear/Nongaussian Bayesian Tracking},
  author = {Arulampalam, M. S. and Maskell, S. and Gordon, N. and Clapp, T.},
  date = {2002},
  journaltitle = {TSP},
  volume = {50},
  number = {2},
  eprint = {978374},
  eprinttype = {pmid},
  pages = {723--737},
  issn = {1053587X},
  doi = {10.1109/9780470544198.ch73},
  abstract = {Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or "particle") representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example},
  archiveprefix = {arXiv},
  isbn = {9780470544198},
  annotation = {501 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VZCLFS2J/Arulampalam et al. - 2002 - A tutorial on particle filters for online nonlinearnongaussian bayesian tracking(2).pdf}
}

@inproceedings{arutiunian2020ReproducibilityChallengeReformer,
  title = {Reproducibility {{Challenge}}: {{Reformer}}},
  booktitle = {{{NeurIPS}}},
  author = {Arutiunian, Artashes and McGuire, Morgan and Gisnås, Hallvar and Imran, Sheik Mohamed and Pleban, Dean and Negi, Priyank and Lozano, David Arnoldo Ortiz},
  date = {2020},
  pages = {10},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GL8XS69B/Arutiunian et al. - Reproducibility Challenge Reformer.pdf}
}

@inproceedings{athalye2018ObfuscatedGradientsGive,
  title = {Obfuscated {{Gradients Give}} a {{False Sense}} of {{Security}}: {{Circumventing Defenses}} to {{Adversarial Examples}}},
  booktitle = {{{ICML}}},
  author = {Athalye, A. and Carlini, N. and Wagner, D.},
  date = {2018},
  eprint = {1802.00420},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.00420},
  abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. For each of the three types of obfuscated gradients we discover, we describe characteristic behaviors of defenses exhibiting this effect and develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 8 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely and 1 partially.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {1380 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KCDNE9HQ/Athalye, Carlini, Wagner - 2018 - Obfuscated Gradients Give a False Sense of Security Circumventing Defenses to Adversarial Examples(2).pdf}
}

@inproceedings{auli2013JointLanguageTranslation,
  title = {Joint {{Language}} and {{Translation Modeling}} with {{Recurrent Neural Networks}}},
  booktitle = {{{EMNLP}}},
  author = {Auli, M. and Galley, M. and Quirk, C. and Zweig, G.},
  date = {2013},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3LJFAVJK/Auli et al. - 2013 - Joint Language and Translation Modeling with Recurrent Neural Networks(2).pdf}
}

@misc{ba2016LayerNormalization,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016-07-21},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2020-08-04},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  annotation = {1897 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JHCWX8C4/Ba et al. - 2016 - Layer Normalization.pdf;/home/hiaoxui/.local/share/zotero_files/storage/CEGELCJQ/1607.html}
}

@inproceedings{baevski2020ClozedrivenPretrainingSelfattention,
  title = {Cloze-Driven Pretraining of Self-Attention Networks},
  booktitle = {{{EMNLP}}},
  author = {Baevski, A. and Edunov, S. and Liu, Y. and Zettlemoyer, L. S. and Auli, M.},
  date = {2020},
  eprint = {1903.07785},
  eprinttype = {arxiv},
  pages = {5360--5369},
  doi = {10.18653/v1/d19-1539},
  abstract = {We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.},
  archiveprefix = {arXiv},
  isbn = {978-1-950737-90-1},
  keywords = {unread},
  annotation = {102 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8AKM2NF5/Baevski et al. - 2020 - Cloze-driven pretraining of self-attention networks(2).pdf}
}

@article{bagchi2013DetectabilityEccentricBinary,
  title = {On the Detectability of Eccentric Binary Pulsars},
  author = {Bagchi, M. and Lorimer, D. R. and Wolfe, S.},
  date = {2013},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  volume = {432},
  number = {2},
  eprint = {1302.4914v2},
  eprinttype = {arxiv},
  pages = {1303--1314},
  issn = {00358711},
  doi = {10.1093/mnras/stt559},
  abstract = {By generalizing earlier work of Johnston \& Kulkarni, we present a detailed description of the reduction in the signal-to-noise ratio for observations of binary pulsars. We present analytical expressions, and provide software, to calculate the sensitivity reduction for orbits of arbitrary eccentricity. We find that this reduction can be quite significant, especially in the case of a massive companion like another neutron star or a black hole. On the other hand, the reduction is less for highly eccentric orbits. We also demonstrate that this loss of sensitivity can be recovered by employing "acceleration search" or "acceleration-jerk search" algorithms.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {19 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7XRXPPMG/Bagchi, Lorimer, Wolfe - 2013 - On the detectability of eccentric binary pulsars(2).pdf}
}

@misc{bagga1998AlgorithmsScoringCoreference,
  title = {Algorithms for {{Scoring Coreference Chains}}},
  author = {Bagga, Amit and Baldwin, Breck},
  date = {1998},
  abstract = {Scoring the performance of a system is an extremely important aspect of coreference algorithm performance. The score for a particular run is the single strongest measure of how well the system is performing and it can strongly determine directions for further improvements. In this paper, we present several di erent scoring algorithms and detail their respective strengths and weaknesses for varying classes of processing. We also demonstrate that tasks like information extraction have very di erent needs from information retrieval in terms of how to score the performance of coreference annotation.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SM66VS8P/Bagga and Baldwin - Algorithms for Scoring Coreference Chains.pdf}
}

@inproceedings{bahdanau2019LearningUnderstandGoal,
  title = {Learning to {{Understand Goal Specifications}} by {{Modelling Reward}}},
  booktitle = {{{ICLR}}},
  author = {Bahdanau, D. and Hill, F. and Leike, J. and Hughes, E. and Kohli, P. and Grefenstette, E.},
  date = {2019},
  eprint = {1803.00781v2},
  eprinttype = {arxiv},
  pages = {1--26},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L578ZLWV/Bahdanau et al. - 2019 - Learning to Understand Goal Specifications by Modelling Reward(2).pdf}
}

@inproceedings{bai2022GraphPretrainingAMR,
  title = {Graph {{Pre-training}} for {{AMR Parsing}} and {{Generation}}},
  booktitle = {{{ACL}}},
  author = {Bai, Xuefeng and Chen, Yulong and Zhang, Yue},
  date = {2022-03-22},
  eprint = {2203.07836},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.07836},
  urldate = {2022-03-23},
  abstract = {Abstract meaning representation (AMR) highlights the core semantic information of text in a graph structure. Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively. However, PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge. To this end, we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs. In particular, we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training. We further design a unified framework to bridge the gap between pre-training and fine-tuning tasks. Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model. To our knowledge, we are the first to consider pre-training on semantic graphs.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JCPAUZNM/Bai et al. - 2022 - Graph Pre-training for AMR Parsing and Generation.pdf;/home/hiaoxui/.local/share/zotero_files/storage/L3IGGEY8/2203.html}
}

@misc{bai2022LearningExpressiveTaskRelated,
  title = {Learning for {{Expressive Task-Related Sentence Representations}}},
  author = {Bai, Xueying and Shang, Jinghuan and Sun, Yifan and Balasubramanian, Niranjan},
  date = {2022-05-24},
  number = {arXiv:2205.12186},
  eprint = {2205.12186},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.12186},
  urldate = {2022-05-29},
  abstract = {NLP models learn sentence representations for downstream tasks by tuning a model which is pre-trained by masked language modeling. However, after tuning, the learned sentence representations may be skewed heavily toward label space and thus are not expressive enough to represent whole samples, which should contain task-related information of both sentence inputs and labels. In this work, we learn expressive sentence representations for supervised tasks which (1). contain task-related information in the sentence inputs, and (2). enable correct label predictions. To achieve this goal, we first propose a new objective which explicitly points out the label token space in the input, and predicts categories of labels via an added [MASK] token. This objective encourages fusing the semantic information of both the label and sentence. Then we develop a neighbor attention module, added on a frozen pre-trained model, to build connections between label/sentence tokens via their neighbors. The propagation can be further guided by the regularization on neighborhood representations to encourage expressiveness. Experimental results show that, despite tuning only 5\% additional parameters over a frozen pre-trained model, our model can achieve classification results comparable to the SOTA while maintaining strong expressiveness as well.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PCBPGVE5/Bai et al. - 2022 - Learning for Expressive Task-Related Sentence Repr.pdf;/home/hiaoxui/.local/share/zotero_files/storage/T9A56262/2205.html}
}

@inproceedings{baker2007FrameSemanticStructure,
  title = {Frame Semantic Structure Extraction},
  booktitle = {{{SemEval}}},
  author = {Baker, C. and Ellsworth, M. and Erk, K.},
  date = {2007},
  pages = {99--104},
  abstract = {This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http: //framenet.icsi.Berkeley.edu), and their semantic dependents, which are usually, but not always, their syntactic dependents (including subjects). The training data was FN annotated sentences. In testing, participants automatically annotated three previously unseen texts to match gold standard (human) annotation, including predicting previously unseen frames and roles. Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UC2QJ4FB/Baker, Ellsworth, Erk - 2007 - Frame semantic structure extraction(2).pdf}
}

@inproceedings{ballesteros2015ImprovedTransitionBasedParsing,
  title = {Improved {{Transition-Based Parsing}} by {{Modeling Characters}} Instead of {{Words}} with {{LSTMs}}},
  booktitle = {{{EMNLP}}},
  author = {Ballesteros, M. and Dyer, C. and Smith, N. A.},
  date = {2015},
  eprint = {1508.00657},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1508.00657},
  abstract = {We present extensions to a continuous-state dependency parsing method that makes it applicable to morphologically rich languages. Starting with a high-performance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.},
  archiveprefix = {arXiv},
  annotation = {270 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2Z68FTXI/Ballesteros, Dyer, Smith - 2015 - Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs(2).pdf}
}

@inproceedings{bamman2019AnnotatedDatasetLiterary,
  title = {An Annotated Dataset of Literary Entities},
  booktitle = {{{NAACL}}},
  author = {Bamman, David and Popat, Sejal and Shen, Sheng},
  date = {2019},
  pages = {2138--2144},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1220},
  url = {http://aclweb.org/anthology/N19-1220},
  urldate = {2021-08-06},
  abstract = {We present a new dataset comprised of 210,532 tokens evenly drawn from 100 different Englishlanguage literary texts annotated for ACE entity categories (person, location, geo-political entity, facility, organization, and vehicle). These categories include non-named entities (such as “the boy”, “the kitchen”) and nested structure (such as [[the cook]’s sister]). In contrast to existing datasets built primarily on news (focused on geopolitical entities and organizations), literary texts offer strikingly different distributions of entity categories, with much stronger emphasis on people and description of settings. We present empirical results demonstrating the performance of nested entity recognition models in this domain; training natively on in-domain literary data yields an improvement of over 20 absolute points in F-score (from 45.7 to 68.3), and mitigates a disparate impact in performance for male and female entities present in models trained on news data.},
  eventtitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  langid = {english},
  keywords = {unread},
  annotation = {16 citations (Semantic Scholar/DOI) [2021-08-06]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4TBLW6P7/Bamman et al. - 2019 - An annotated dataset of literary entities.pdf}
}

@inproceedings{banarescu2013AbstractMeaningRepresentation,
  title = {Abstract Meaning Representation for Sembanking},
  booktitle = {Linguistic {{Annotation Workshop}} \& {{Interoperability}} with {{Discourse}}},
  author = {Banarescu, L. and Bonial, C. and Cai, S. and Georgescu, M. and Griffitt, K. and Hermjakob, U. and Knight, K. and Koehn, P. and Palmer, M. and Schneider, N.},
  date = {2013},
  pages = {178--186},
  url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Abstract+Meaning+Representation+for+Sembanking#0},
  abstract = {We describe Abstract Meaning Represen- tation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sen- tences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural lan- guage understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KLYFLLSX/Banarescu et al. - 2013 - Abstract meaning representation for sembanking(2).pdf}
}

@inproceedings{bansal2014TailoringContinuousWord,
  title = {Tailoring {{Continuous Word Representations}} for {{Dependency Parsing}}},
  booktitle = {{{ACL}}},
  author = {Bansal, M. and Gimpel, K. and Livescu, K.},
  date = {2014},
  pages = {809--815},
  doi = {10.3115/v1/p14-2131},
  abstract = {Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the use of continuous word representations as features for dependency parsing. We com- pare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of oth- ers. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representa- tions achieves the best results, suggesting their complementarity.},
  keywords = {unread},
  annotation = {276 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XFIXB6PV/Bansal, Gimpel, Livescu - 2014 - Tailoring Continuous Word Representations for Dependency Parsing(2).pdf}
}

@inproceedings{bao2021GTransformerDocumentlevelMachine,
  title = {G-{{Transformer}} for {{Document-level Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Bao, Guangsheng and Zhang, Yue and Teng, Zhiyang and Chen, Boxing and Luo, Weihua},
  date = {2021-05-31},
  eprint = {2105.14761},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.14761},
  urldate = {2021-11-10},
  abstract = {Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both non-pretraining and pre-training settings on three benchmark datasets.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JGX5FZ5S/Bao et al. - 2021 - G-Transformer for Document-level Machine Translati.pdf;/home/hiaoxui/.local/share/zotero_files/storage/HAVEY7BE/2105.html}
}

@inproceedings{bao2022BEiTBERTPreTraining,
  title = {{{BEiT}}: {{BERT Pre-Training}} of {{Image Transformers}}},
  booktitle = {{{ICLR}}},
  author = {Bao, H. and Dong, L. and Piao, S. and Wei, F.},
  date = {2022},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3WJTZPBB/beit_bert_pre_training_of_imag.pdf}
}

@inproceedings{bara2021MindCraftTheoryMind,
  title = {{{MindCraft}}: {{Theory}} of {{Mind Modeling}} for {{Situated Dialogue}} in {{Collaborative Tasks}}},
  shorttitle = {{{MindCraft}}},
  booktitle = {{{EMNLP}}},
  author = {Bara, Cristian-Paul and CH-Wang, Sky and Chai, Joyce},
  date = {2021-09-13},
  eprint = {2109.06275},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.06275},
  urldate = {2021-11-25},
  abstract = {An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners’ beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S2ECXX8G/2109.06275.pdf}
}

@article{baran2008OptimallyAdaptiveIntegration,
  title = {Optimally Adaptive Integration of Univariate Lipschitz Functions},
  author = {Baran, I. and Demaine, E. D. and Katz, D. A.},
  date = {2008},
  journaltitle = {Algorithmica},
  volume = {50},
  number = {2},
  pages = {255--278},
  issn = {01784617},
  doi = {10.1007/s00453-007-9093-7},
  annotation = {10 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZSB8PDW/Baran, Demaine, Katz - 2008 - Optimally adaptive integration of univariate lipschitz functions(2).pdf}
}

@inproceedings{barga2007ConsistentStreamingTime,
  title = {Consistent Streaming through Time: {{A}} Vision for Event Stream Processing},
  booktitle = {{{CIDR}}},
  author = {Barga, R. S. and Goldstein, J. and Ali, M. and Hong, M.},
  date = {2007},
  eprint = {cs/0612115},
  eprinttype = {arxiv},
  pages = {363--374},
  abstract = {Event processing will play an increasingly important role in constructing enterprise applications that can immediately react to business critical events. Various technologies have been proposed in recent years, such as event processing, data streams and asynchronous messaging (e.g. pub/sub). We believe these technologies share a common processing model and differ only in target workload, including query language features and consistency requirements. We argue that integrating these technologies is the next step in a natural progression. In this paper, we present an overview and discuss the foundations of CEDR, an event streaming system that embraces a temporal stream model to unify and further enrich query language features, handle imperfections in event delivery, define correctness guarantees, and define operator semantics. We describe specific contributions made so far and outline next steps in developing the CEDR system.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PK8PKIUP/Barga et al. - 2007 - Consistent streaming through time A vision for event stream processing(2).pdf}
}

@inproceedings{barker2001LibraryGenericConcepts,
  title = {A Library of Generic Concepts for Composing Knowledge Bases},
  booktitle = {International {{Conference}} on {{Knowledge Capture}}},
  author = {Barker, K. and Porter, B. and Clark, P.},
  date = {2001},
  pages = {14--21},
  abstract = {Building a knowledge base for a given domain traditionally involves a subject matter expert and a knowledge engineer. One of the goals of our research is to eliminate the knowledge engineer. There are at least two ways to achieve this goal: train domain experts to write axioms (i.e., turn them into knowledge engineers) or create tools that allow users to build knowledge bases without having to write axioms. Our strategy is to create tools that allow users to build knowledge bases through instantiation and assembly of generic knowledge components from a small library. In many ways, creating such a library is like designing an ontology: What are the most general kinds of events and entities? How are these things related hierarchically? What is their meaning and how is it represented? The pressures of making the library usable by domain experts, however, leads to departures from the traditional ontology design goals of coverage, consensus and elegance. In this paper we describe our component library, a hierarchy of reusable, composable, domain-independent knowledge units. The library emphasizes coverage (what is an appropriate set of components for our task), access (how can a domain expert find appropriate components) and semantics (what knowledge and what kind of representation permit useful composition). We have begun building a library on these principles, influenced heavily by linguistic resources. In early evaluations we have put the library into the hands of domain experts (in Biology) having no experience with knowledge bases or knowledge acquisition.},
  isbn = {1-58113-380-4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZZLX39BR/Barker, Porter, Clark - 2001 - A library of generic concepts for composing knowledge bases(2).pdf}
}

@inproceedings{baroni2014DonCountPredict,
  title = {Don’t Count, Predict! {{A}} Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors},
  booktitle = {{{ACL}}},
  author = {Baroni, M. and Dinuand, G. and Kruszewski, G.},
  date = {2014},
  pages = {238--247},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7LXCDQSL/Baroni, Dinuand, Kruszewski - 2014 - Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semanti(2).pdf}
}

@inproceedings{baroni2014DonCountPredicta,
  title = {Don't Count, Predict! {{A}} Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors},
  booktitle = {{{ACL}}},
  author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, Germán},
  date = {2014},
  pages = {238--247},
  publisher = {{Association for Computational Linguistics}},
  location = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-1023},
  url = {http://aclweb.org/anthology/P14-1023},
  urldate = {2020-11-19},
  abstract = {Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts.},
  langid = {english},
  keywords = {unread},
  annotation = {1182 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2IE244X4/Baroni et al. - 2014 - Don't count, predict! A systematic comparison of c.pdf}
}

@inproceedings{barrett2018MeasuringAbstractReasoning,
  title = {Measuring Abstract Reasoning in Neural Networks},
  booktitle = {{{ICML}}},
  author = {Barrett, D. G. T. and Hill, F. and Santoro, A. and Morcos, A. S. and Lillicrap, T.},
  date = {2018},
  eprint = {1807.04225},
  eprinttype = {arxiv},
  issn = {1938-7228},
  url = {http://arxiv.org/abs/1807.04225},
  abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.},
  archiveprefix = {arXiv},
  annotation = {124 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/77VCFI8X/Barrett et al. - 2018 - Measuring abstract reasoning in neural networks(2).pdf}
}

@inproceedings{bart2020,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  booktitle = {{{ACL}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  date = {2020},
  eprint = {1910.13461},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.13461},
  urldate = {2020-09-09},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {506 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3L2TJL9Y/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf}
}

@inproceedings{barzilay2005CollectiveContentSelection,
  title = {Collective Content Selection for Concept-to-Text Generation},
  booktitle = {{{EMNLP}}},
  author = {Barzilay, R. and Lapata, M.},
  date = {2005},
  pages = {331--338},
  doi = {http://dx.doi.org/10.3115/1220575.1220617},
  url = {http://dl.acm.org/citation.cfm?id=1220617},
  abstract = {A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efficient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods.},
  issue = {October},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IEYIUBTP/Barzilay, Lapata - 2005 - Collective content selection for concept-to-text generation(2).pdf}
}

@inproceedings{barzilay2005ModelingLocalCoherence,
  title = {Modeling {{Local Coherence}}: {{An}} Entity Based Approach},
  booktitle = {{{ACL}}},
  author = {Barzilay, R. and Lapata, M.},
  date = {2005},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5VU69JCR/Barzilay, Lapata - 2005 - Modeling Local Coherence An entity based approach(2).pdf}
}

@article{barzilay2008ModelingLocalCoherence,
  title = {Modeling {{Local Coherence}}: {{An Entity-Based Approach}}},
  author = {Barzilay, R. and Lapata, M.},
  date = {2008},
  journaltitle = {Computational Linguistics},
  volume = {34},
  number = {1},
  pages = {1--34},
  issn = {0891-2017},
  doi = {10.1162/coli.2008.34.1.1},
  url = {http://www.mitpressjournals.org/doi/10.1162/coli.2008.34.1.1},
  abstract = {This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.},
  isbn = {1932432515},
  keywords = {unread},
  annotation = {607 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JRPK8MXB/Barzilay, Lapata - 2008 - Modeling Local Coherence An Entity-Based Approach(2).pdf}
}

@inproceedings{beatson2019EfficientOptimizationLoops,
  title = {Efficient {{Optimization}} of {{Loops}} and {{Limits}} with {{Randomized Telescoping Sums}}},
  booktitle = {{{ICML}}},
  author = {Beatson, A. and Adams, R. P.},
  date = {2019},
  eprint = {1905.07006},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.07006},
  abstract = {We consider optimization problems in which the objective requires an inner loop with many steps or is the limit of a sequence of increasingly costly approximations. Meta-learning, training recurrent neural networks, and optimization of the solutions to differential equations are all examples of optimization problems with this character. In such problems, it can be expensive to compute the objective function value and its gradient, but truncating the loop or using less accurate approximations can induce biases that damage the overall solution. We propose randomized telescope (RT) gradient estimators, which represent the objective as the sum of a telescoping series and sample linear combinations of terms to provide cheap unbiased gradient estimates. We identify conditions under which RT estimators achieve optimization convergence rates independent of the length of the loop or the required accuracy of the approximation. We also derive a method for tuning RT estimators online to maximize a lower bound on the expected decrease in loss per unit of computation. We evaluate our adaptive RT estimators on a range of applications including meta-optimization of learning rates, variational inference of ODE parameters, and training an LSTM to model long sequences.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {10 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PQTLULR5/Beatson, Adams - 2019 - Efficient Optimization of Loops and Limits with Randomized Telescoping Sums(2).pdf}
}

@article{beldiceanu1994IntroducingGlobalConstraints,
  title = {Introducing Global Constraints in {{CHIP}}},
  author = {Beldiceanu, N. and Contejean, E.},
  date = {1994},
  journaltitle = {Mathematical and Computer Modelling},
  volume = {20},
  number = {12},
  pages = {97--123},
  issn = {08957177},
  doi = {10.1016/0895-7177(94)90127-9},
  abstract = {The purpose of this paper is to show how the introduction of new primitive constraints (e.g., among, diffn, cycle) over finite domains in the constraint logic programming system CHIP result in finding very rapidly good solutions for a large class of difficult sequencing, scheduling, geometrical placement and vehicle routing problems. The among constraint allows us to specify sequencing constraints in a very concise way. For the first time, the diffn constraint allows us to express and to solve directly multi-dimensional placement problems, where one has to consider nonoverlapping constraints between n-dimensional objects (e.g., rectangles, parallelepipeds). The cycle constraint makes it possible to specify a wide range of graph partitioning problems that could not yet be expressed by using current constraint logic programming languages. One of the main advantages of all these new primitives is to take into account more globally a set of elementary constraints. Finally, we point out that all the previous primitive constraints enhance the power of the CHIP system significantly, allowing us to solve real life problems that were not within reach of constraint technology before. © 1994.},
  annotation = {372 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HG47VGSZ/Beldiceanu, Contejean - 1994 - Introducing global constraints in CHIP(2).pdf}
}

@inproceedings{belkin2018UnderstandDeepLearning,
  title = {To Understand Deep Learning We Need to Understand Kernel Learning},
  booktitle = {{{ICML}}},
  author = {Belkin, M. and Ma, S. and Mandal, S.},
  date = {2018},
  eprint = {1802.01396},
  eprinttype = {arxiv},
  pages = {874--882},
  abstract = {Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, which are typically heavily over-parametrized, tend to fit the training data exactly. Despite this "overfitting", they perform well on test data, a phenomenon not yet fully understood. The first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world and two synthetic datasets, we establish experimentally that kernel machines trained to have zero classification error or near zero regression error (interpolation) perform very well on test data. We proceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exponentially with data size. None of the existing bounds produce non-trivial results for interpolating solutions. We also show experimentally that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels results recently reported for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. Similar performance of overfitted Laplacian and Gaussian classifiers on test, suggests that generalization is tied to the properties of the kernel function rather than the optimization process. Some key phenomena of deep learning are manifested similarly in kernel methods in the modern "overfitted" regime. The combination of the experimental and theoretical results presented in this paper indicates a need for new theoretical ideas for understanding properties of classical kernel methods. We argue that progress on understanding deep learning will be difficult until more tractable "shallow" kernel methods are better understood.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-6796-3},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IFRGWYQP/Belkin, Ma, Mandal - 2018 - To understand deep learning we need to understand kernel learning(2).pdf}
}

@article{belkin2019ReconcilingModernMachinelearning,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias–Variance Trade-Off},
  author = {Belkin, M. and Hsu, D. and Ma, S. and Mandal, S.},
  date = {2019},
  journaltitle = {PNAS},
  eprint = {1812.11118},
  eprinttype = {arxiv},
  pages = {15849--15854},
  issn = {10916490},
  doi = {10.1073/pnas.1903070116},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {282 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EB38C9XQ/Belkin et al. - 2019 - Reconciling modern machine-learning practice and the classical bias–variance trade-off(2).pdf}
}

@misc{beltagy2020LongformerLongDocumentTransformer,
  title = {Longformer: {{The Long-Document Transformer}}},
  shorttitle = {Longformer},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  date = {2020-12-02},
  eprint = {2004.05150},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.05150},
  urldate = {2021-03-10},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  archiveprefix = {arXiv},
  annotation = {169 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5CKB38XC/Beltagy et al. - 2020 - Longformer The Long-Document Transformer.pdf;/home/hiaoxui/.local/share/zotero_files/storage/FERJWD9Q/Peters, Cohan - 2020 - Longformer The Long-Document Transformer(2).pdf;/home/hiaoxui/.local/share/zotero_files/storage/T98PFHB2/2004.html}
}

@unpublished{beltagy2021ParagraphsNLPLong,
  title = {Beyond {{Paragraphs}}: {{NLP}} for {{Long Sequences}}},
  author = {Beltagy, Iz and Cohan, Arman and Hajishirzi, Hanna and Min, Sewon and Peter, Matthew},
  date = {2021},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3LCSVIY7/2021.naacl-tutorials.5.pdf;/home/hiaoxui/.local/share/zotero_files/storage/4J3ITK7C/part1-intro-and-overview-of-tasks.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7EYMV2ES/part4-pretraining-and-finetuning.pdf;/home/hiaoxui/.local/share/zotero_files/storage/AA3PT5YQ/part3-long-sequence-transformers.pdf;/home/hiaoxui/.local/share/zotero_files/storage/K7VN6R8P/part6-future-work-and-conclusion.pdf;/home/hiaoxui/.local/share/zotero_files/storage/TQ3ARM78/part5-use-cases.pdf;/home/hiaoxui/.local/share/zotero_files/storage/WUWA3WDJ/part2-graph-based-methods.pdf}
}

@article{belz2008AutomaticGenerationWeather,
  title = {Automatic Generation of Weather Forecast Texts Using Comprehensive Probabilistic Generation-Space Models},
  author = {Belz, A.},
  date = {2008},
  journaltitle = {Natural Language Engineering},
  volume = {14},
  number = {4},
  pages = {431--455},
  issn = {13513249},
  doi = {10.1017/S1351324907004664},
  url = {http://www.journals.cambridge.org/abstract_S1351324907004664},
  abstract = {Two important recent trends in natural language generation are (i) probabilistic techniques and (ii) comprehensive approaches that move away from traditional strictly modular and sequential models. This paper reports experiments in which pCRU - a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space - was used to semi-automatically create five different versions of a weather forecast generator. The generators were evaluated in terms of output quality, development time and computational efficiency against (i) human forecasters, (ii) a traditional handcrafted pipelined NLG system and (iii) a HALOGEN-style statistical generator. The most striking result is that despite acquiring all decision-making abilities automatically, the best pCRU generators produce outputs of high enough quality to be scored more highly by human judges than forecasts written by experts. © 2007 Cambridge University Press.},
  isbn = {1469-8110},
  annotation = {165 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EKUQZ5GE/Belz - 2008 - Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models(2).pdf}
}

@misc{benchclamp2022,
  title = {{{BenchCLAMP}}: {{A Benchmark}} for {{Evaluating Language Models}} on {{Semantic Parsing}}},
  shorttitle = {{{BenchCLAMP}}},
  author = {Roy, Subhro and Thomson, Sam and Chen, Tongfei and Shin, Richard and Pauls, Adam and Eisner, Jason and Van Durme, Benjamin},
  date = {2022-06-21},
  number = {arXiv:2206.10668},
  eprint = {2206.10668},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.10668},
  urldate = {2022-10-07},
  abstract = {We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, which produces semantic outputs based on the analysis of input text through constrained decoding of a prompted or fine-tuned language model. Developers of pretrained language models currently benchmark on classification, span extraction and free-text generation tasks. Semantic parsing is neglected in language model evaluation because of the complexity of handling task-specific architectures and representations. Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. BenchCLAMP includes context-free grammars for six semantic parsing datasets with varied output meaning representations, as well as a constrained decoding interface to generate outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports both prompt-based learning as well as fine-tuning, and provides an easy-to-use toolkit for language model developers to evaluate on semantic parsing.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SQTXTTN3/Roy et al. - 2022 - BenchCLAMP A Benchmark for Evaluating Language Mo.pdf;/home/hiaoxui/.local/share/zotero_files/storage/286V8BZG/2206.html}
}

@article{bengio2003NeuralProbabilisticLanguage,
  title = {A {{Neural Probabilistic Language Model}}},
  author = {Bengio, Y. and Ducharme, R. and Vincent, P. and Jauvin, C.},
  date = {2003},
  journaltitle = {JMLR},
  issn = {15364046},
  doi = {10.1080/1536383X.2018.1448388},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/STCP399F/Bengio et al. - 2003 - A Neural Probabilistic Language Model(2).pdf}
}

@inproceedings{bengio2007GreedyLayerWiseTraining,
  title = {Greedy {{Layer-Wise Training}} of {{Deep Networks}}},
  booktitle = {{{NeurIPS}}},
  author = {Bengio, Y. and Lamblin, P. and Popovici, D. and Larochelle, H.},
  date = {2007},
  volume = {19},
  number = {1},
  eprint = {19018704},
  eprinttype = {pmid},
  pages = {153},
  issn = {01628828},
  doi = {citeulike-article-id:4640046},
  abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
  isbn = {0-262-19568-2},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SUBIJ8CN/Bengio et al. - 2007 - Greedy Layer-Wise Training of Deep Networks(2).pdf}
}

@inproceedings{bengio2009CurriculumLearning,
  title = {Curriculum Learning},
  booktitle = {{{ICML}}},
  author = {Bengio, Y. and Louradour, J. and Collobert, R. and Weston, J.},
  date = {2009},
  eprint = {5414602},
  eprinttype = {pmid},
  pages = {41--48},
  issn = {0022-5193},
  doi = {10.1145/1553374.1553380},
  url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
  abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illus- trates gradually more concepts, and gradu- ally more complex ones. Here, we formal- ize such training strategies in the context of machine learning, and call them “curricu- lum learning”. In the context of recent re- search studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neu- ral networks), we explore curriculum learn- ing in various set-ups. The experiments show that significant improvements in generaliza- tion can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  archiveprefix = {arXiv},
  isbn = {978-1-60558-516-1},
  annotation = {2294 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NFMBK6FJ/Bengio et al. - 2009 - Curriculum learning(2).pdf}
}

@article{bengio2013RepresentationLearningReview,
  title = {Representation Learning: {{A}} Review and New Perspectives},
  author = {Bengio, Y. and Courville, A. and Vincent, P.},
  date = {2013},
  journaltitle = {PAMI},
  volume = {35},
  number = {8},
  eprint = {23459267},
  eprinttype = {pmid},
  pages = {1798--1828},
  issn = {01628828},
  doi = {10.1109/TPAMI.2013.50},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  archiveprefix = {arXiv},
  isbn = {0162-8828 VO - 35},
  keywords = {unread},
  annotation = {6950 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CQWL88GM/Bengio, Courville, Vincent - 2013 - Representation learning A review and new perspectives(2).pdf}
}

@inproceedings{bengio2014DeepGenerativeStochastic,
  title = {Deep {{Generative Stochastic Networks Trainable}} by {{Backprop}}},
  booktitle = {{{ICML}}},
  author = {Bengio, Y. and Thibodeau-Laufer, É. and Alain, G. and Yosinski, J.},
  date = {2014},
  eprint = {1306.1091},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1306.1091},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.},
  archiveprefix = {arXiv},
  annotation = {314 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/96C9GICH/Bengio et al. - 2014 - Deep Generative Stochastic Networks Trainable by Backprop(2).pdf}
}

@inproceedings{benmalek2019KeepingNotesConditional,
  title = {Keeping {{Notes}}: {{Conditional Natural Language Generation}} with a {{Scratchpad Encoder}}},
  booktitle = {{{ACL}}},
  author = {Benmalek, R. and Khabsa, M. and Desu, S. and Cardie, C. and Banko, M.},
  date = {2019},
  eprint = {1906.05275},
  eprinttype = {arxiv},
  pages = {4157--4167},
  doi = {10.18653/v1/p19-1407},
  abstract = {We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a "scratchpad" memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks --- Machine Translation, Question Generation, and Text Summarization --- and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5TYISUER/Benmalek et al. - 2019 - Keeping Notes Conditional Natural Language Generation with a Scratchpad Encoder(2).pdf}
}

@inproceedings{berant2013SemanticParsingFreebase,
  title = {Semantic {{Parsing}} on {{Freebase}} from {{Question-Answer Pairs}}},
  booktitle = {{{EMNLP}}},
  author = {Berant, J. and Chou, A. and Frostig, R. and Liang, P.},
  date = {2013},
  eprint = {2216100},
  eprinttype = {pmid},
  pages = {1533--1544},
  abstract = {In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai andYates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.},
  isbn = {978-1-937284-97-8},
  issue = {October},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZQPESRCB/Berant et al. - 2013 - Semantic Parsing on Freebase from Question-Answer Pairs(2).pdf}
}

@inproceedings{berant2014SemanticParsingParaphrasing,
  title = {Semantic {{Parsing}} via {{Paraphrasing}}},
  booktitle = {{{ACL}}},
  author = {Berant, J. and Liang, P.},
  date = {2014},
  eprint = {1903399},
  eprinttype = {pmid},
  pages = {1415--1425},
  issn = {00219258},
  doi = {10.3115/v1/P14-1133},
  url = {http://aclweb.org/anthology/P14-1133},
  abstract = {A central challenge in semantic parsing is handling the myriadways in which knowl- edge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this pa- per, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically gener- ate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best para- phrases the input, and output the corre- sponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves state- of-the-art accuracies on two recently re- leased question-answering datasets. 1},
  isbn = {978-1-937284-72-5},
  keywords = {unread},
  annotation = {403 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4NK2VXLG/Berant, Liang - 2014 - Semantic Parsing via Paraphrasing(2).pdf}
}

@article{berant2015ImitationLearningAgendabased,
  title = {Imitation {{Learning}} of {{Agenda-based Semantic Parsers}}},
  author = {Berant, J. and Liang, P.},
  date = {2015},
  journaltitle = {TACL},
  volume = {3},
  pages = {545--558},
  abstract = {Semantic parsers conventionally construct logical forms bottom-up in a fixed order, re- sulting in the generation of many extraneous partial logical forms. In this paper, we com- bine ideas from imitation learning and agenda- based parsing to train a semantic parser that searches partial logical forms in a more strate- gic order. Empirically, our parser reduces the number of constructed partial logical forms by an order of magnitude, and obtains a 6x-9x speedup over fixed-order parsing, while main- taining comparable accuracy.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TSPE82XH/Berant, Liang - 2015 - Imitation Learning of Agenda-based Semantic Parsers(2).pdf}
}

@inproceedings{berg-kirkpatrick2010PainlessUnsupervisedLearning,
  title = {Painless Unsupervised Learning with Features},
  booktitle = {{{NAACL}}},
  author = {Berg-kirkpatrick, T. and Bouchard-Côté, A. and DeNero, J. and Klein, D.},
  date = {2010},
  pages = {582--590},
  url = {http://portal.acm.org/citation.cfm?id=1858082},
  abstract = {We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.},
  isbn = {1-932432-65-5},
  issue = {June},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3QRD3A64/Berg-kirkpatrick et al. - 2010 - Painless unsupervised learning with features(2).pdf}
}

@article{berger1996MaximumEntropyApproach,
  title = {A Maximum Entropy Approach to Natural Language Processing},
  author = {Berger, A. L. and Pietra, S. A. D. and Pietra, V. J. D.},
  date = {1996},
  journaltitle = {Computational Linguistics},
  volume = {22},
  number = {1},
  pages = {39--71},
  issn = {08912017},
  doi = {10.3115/1075812.1075844},
  url = {http://portal.acm.org/citation.cfm?id=234285.234289},
  abstract = {The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.},
  isbn = {1558603573},
  annotation = {154 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/89GSRF9V/Berger, Pietra, Pietra - 1996 - A maximum entropy approach to natural language processing(2).pdf}
}

@inproceedings{bergsma2011LearningBilingualLexicons,
  title = {Learning Bilingual Lexicons Using the Visual Similarity of Labeled Web Images},
  booktitle = {{{IJCAI}}},
  author = {Bergsma, S. and Van Durme, B.},
  date = {2011},
  pages = {1764--1769},
  issn = {10450823},
  doi = {10.5591/978-1-57735-516-8/IJCAI11-296},
  abstract = {Speakers of many different languages use the Internet. A common activity among these users is uploading images and associating these images with words (in their own language) as captions, filenames, or surrounding text. We use these explicit, monolingual, image-to-word connections to successfully learn implicit, bilingual, word-to-word translations. Bilingual pairs of words are proposed as translations if their corresponding images have similar visual features. We generate bilingual lexicons in 15 language pairs, focusing on words that have been automatically identified as physical objects. The use of visual similarity substantially improves performance over standard approaches based on string similarity: for generated lexicons with 1000 translations, including visual information leads to an absolute improvement in accuracy of 8-12\% over string edit distance alone.},
  isbn = {978-1-57735-512-0},
  keywords = {unread},
  annotation = {59 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T5JQSJHF/Bergsma, Van Durme - 2011 - Learning bilingual lexicons using the visual similarity of labeled web images(2).pdf}
}

@inproceedings{bert2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  booktitle = {{{NAACL}}},
  author = {Devlin, J. and Chang, M. and Lee, K. and Toutanova, K.},
  date = {2019},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1810.04805},
  archiveprefix = {arXiv},
  annotation = {9994 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XQV8W66S/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding(2).pdf}
}

@inproceedings{bertero2016LongShortTermMemory,
  title = {A {{Long Short-Term Memory Framework}} for {{Predicting Humor}} in {{Dialogues}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Bertero, D. and Fung, P.},
  date = {2016},
  pages = {130--135},
  abstract = {We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the au- dience would react. We model the setup- punchline relation of conversational humor with a Long Short-Term Memory, with utter- ance encodings obtained from a Convolutional Neural Network. Out neural network frame- work is able to improve the F-score of8\%over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to un- derstand jokes.},
  isbn = {978-1-941643-91-4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CILDBLFM/Bertero, Fung - 2016 - A Long Short-Term Memory Framework for Predicting Humor in Dialogues(2).pdf}
}

@inproceedings{bethard2013ClearTKTimeMLMinimalistApproach,
  title = {{{ClearTK-TimeML}}: {{A}} Minimalist Approach to {{TempEval}} 2013},
  booktitle = {Joint {{Conference}} on {{Lexical}} and {{Computational Semantics}}},
  author = {Bethard, S.},
  date = {2013},
  volume = {2},
  pages = {10--14},
  abstract = {The ClearTK-TimeML submission to Temp-Eval 2013 competed in all English tasks: identi-fying events, identifying times, and identifying temporal relations. The system is a pipeline of machine-learning models, each with a small set of features from a simple morpho-syntactic an-notation pipeline, and where temporal relations are only predicted for a small set of syntac-tic constructions and relation types. ClearTK-TimeML ranked 1 st for temporal relation F1, time extent strict F1 and event tense accuracy.},
  isbn = {978-1-937284-49-7},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6PBXSEW8/Bethard - 2013 - ClearTK-TimeML A minimalist approach to TempEval 2013(2).pdf}
}

@inproceedings{bhattacharjya2018ProximalGraphicalEvent,
  title = {Proximal Graphical Event Models},
  booktitle = {{{NeurIPS}}},
  author = {Bhattacharjya, D. and Subramanian, D. and Gao, T.},
  date = {2018},
  pages = {8136--8145},
  issn = {10495258},
  abstract = {Event datasets include events that occur irregularly over the timeline and are prevalent in numerous domains. We introduce proximal graphical event models (PGEM) as a representation of such datasets. PGEMs belong to a broader family of models that characterize relationships between various types of events, where the rate of occurrence of an event type depends only on whether or not its parents have occurred in the most recent history. The main advantage over the state of the art models is that they are entirely data driven and do not require additional inputs from the user, which can require knowledge of the domain such as choice of basis functions or hyperparameters in graphical event models. We theoretically justify our learning of optimal windows for parental history and the choices of parental sets, and the algorithm are sound and complete in terms of parent structure learning. We present additional efficient heuristics for learning PGEMs from data, demonstrating their effectiveness on synthetic and real datasets.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YHZ4NZWK/Bhattacharjya, Subramanian, Gao - 2018 - Proximal graphical event models(2).pdf}
}

@inproceedings{bisk2019PIQAReasoningPhysical,
  title = {{{PIQA}}: {{Reasoning}} about {{Physical Commonsense}} in {{Natural Language}}},
  shorttitle = {{{PIQA}}},
  booktitle = {{{AAAI}}},
  author = {Bisk, Yonatan and Zellers, Rowan and Bras, Ronan Le and Gao, Jianfeng and Choi, Yejin},
  date = {2019-11-26},
  eprint = {1911.11641},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.11641},
  urldate = {2020-11-20},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {49 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EFRA2GSM/Bisk et al. - 2019 - PIQA Reasoning about Physical Commonsense in Natu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/WQ3DQJTD/1911.html}
}

@article{blei2003LatentDirichletAllocation,
  title = {Latent {{Dirichlet}} Allocation},
  author = {Blei, D. M. and Ng, A. Y. and Jordan, M. I.},
  date = {2003},
  journaltitle = {JMLR},
  volume = {3},
  number = {4-5},
  pages = {993--1022},
  issn = {15324435},
  doi = {10.1016/b978-0-12-411519-4.00006-9},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  annotation = {9994 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IKCVBYEH/Blei, Ng, Jordan - 2003 - Latent Dirichlet allocation(2).pdf}
}

@inproceedings{blondel2018LearningClassifiersFenchelYoung,
  title = {Learning {{Classifiers}} with {{Fenchel-Young Losses}}: {{Generalized Entropies}}, {{Margins}}, and {{Algorithms}}},
  booktitle = {{{ICML}}},
  author = {Blondel, M. and Martins, A. F. T. and Niculae, V.},
  date = {2018},
  eprint = {1805.09717},
  eprinttype = {arxiv},
  pages = {1--21},
  url = {http://arxiv.org/abs/1805.09717},
  abstract = {We study in this paper Fenchel-Young losses, a generic way to construct convex loss functions from a convex regularizer. We provide an in-depth study of their properties in a broad setting and show that they unify many well-known loss functions. When constructed from a generalized entropy, which includes well-known entropies such as Shannon and Tsallis entropies, we show that Fenchel-Young losses induce a predictive probability distribution and develop an efficient algorithm to compute that distribution for separable entropies. We derive conditions for generalized entropies to yield a distribution with sparse support and losses with a separation margin. Finally, we present both primal and dual algorithms to learn predictive models with generic Fenchel-Young losses.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {21 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4JLEJDVA/Blondel, Martins, Niculae - 2018 - Learning Classifiers with Fenchel-Young Losses Generalized Entropies, Margins, and Algorithms(2).pdf}
}

@inproceedings{blum1992Training3nodeNeural,
  title = {Training a 3-Node Neural Network Is {{NP-complete}}},
  booktitle = {{{NeurIPS}}},
  author = {Blum, A. L. and Rivest, R. L.},
  date = {1992},
  volume = {5},
  number = {1},
  pages = {117--127},
  issn = {08936080},
  doi = {10.1016/S0893-6080(05)80010-3},
  abstract = {We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for this network so that it produces output consistent with a given set of training examples. We extend the result to other simple networks. We also present a network for which training is hard but where switching to a more powerful representation makes training easier. These results suggest that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. They also suggest the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with nonlinear functions such as sigmoids. © 1992 Pergamon Press plc.},
  isbn = {0-55869-019-5},
  annotation = {750 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2W2X6IAC/Blum, Rivest - 1992 - Training a 3-node neural network is NP-complete(2).pdf}
}

@inproceedings{blum1998CombiningLabeledUnlabeled,
  title = {Combining Labeled and Unlabeled Data with Co-Training},
  booktitle = {{{COLT}}},
  author = {Blum, A. and Mitchell, T.},
  date = {1998},
  pages = {92--100},
  doi = {10.1145/279943.279962},
  abstract = {The problem of using a large unlabeled sample is considered to boost the performance of a learning algorithm when only a small set of labeled examples is available. In particular, a problem setting is considered to classify web pages, in which the description of each example can be partitioned into two distinct views. A PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data are presented. Also, empirical results on real web-page is giving, indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice.},
  annotation = {5053 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G3CTRJ36/Blum, Mitchell - 1998 - Combining labeled and unlabeled data with co-training(2).pdf}
}

@inproceedings{blunsom2011HierarchicalPitmanYorProcess,
  title = {A Hierarchical {{Pitman-Yor}} Process {{HMM}} for Unsupervised Part of Speech Induction},
  booktitle = {{{ACL-HLT}}},
  author = {Blunsom, P. and Cohn, T.},
  date = {2011},
  pages = {865--874},
  abstract = {In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages. © 2011 Association for Computational Linguistics.},
  isbn = {978-1-932432-87-9},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YE6JBDDS/Blunsom, Cohn - 2011 - A hierarchical Pitman-Yor process HMM for unsupervised part of speech induction(2).pdf}
}

@article{boito2017UnwrittenLanguagesDemand,
  title = {Unwritten {{Languages Demand Attention Too}}! {{Word Discovery}} with {{Encoder-Decoder Models}}},
  author = {Boito, M. Z. and Alexandre, B. and Villavicencio, A. and Besacier, L. and Dev, F.},
  date = {2017},
  journaltitle = {ASRU},
  eprint = {1709.05631v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6D4EKYHW/Boito et al. - 2017 - Unwritten Languages Demand Attention Too! Word Discovery with Encoder-Decoder Models(2).pdf}
}

@article{bojanowski2017EnrichingWordVectors,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, P. and Grave, E. and Joulin, A. and Mikolov, T.},
  date = {2017},
  journaltitle = {TACL},
  volume = {5},
  pages = {135--146},
  doi = {10.1162/tacl_a_00051},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  keywords = {unread},
  annotation = {4523 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XAHEKTHT/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information(2).pdf}
}

@inproceedings{bollacker2008FreebaseCollaborativelyCreated,
  title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
  booktitle = {International {{Conference}} on {{Management}} of {{Data}}},
  author = {Bollacker, K. and Evans, C. and Paritosh, P. and Sturge, T. and Taylor, J.},
  date = {2008},
  eprint = {3105260},
  eprinttype = {pmid},
  pages = {1247--1250},
  issn = {07308078},
  doi = {10.1145/1376616.1376746},
  url = {http://doi.acm.org/10.1145/1376616.1376746},
  abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
  isbn = {978-1-60558-102-6},
  annotation = {3067 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4NUERHKH/Bollacker et al. - 2008 - Freebase a collaboratively created graph database for structuring human knowledge(2).pdf}
}

@inproceedings{bonial2013RenewingRevisingSemLink,
  title = {Renewing and Revising {{SemLink}}},
  booktitle = {The {{GenLex Workshop}} on {{Linked Data}} in {{Linguistics}}},
  author = {Bonial, C. and Stowe, K. and Palmer, M.},
  date = {2013},
  pages = {9--17},
  url = {http://www.aclweb.org/anthology/W13-5503},
  abstract = {This research describes SemLink, a compre- hensive resource for Natural Language Pro- cessing that maps and unifies several high- quality lexical resources: PropBank, VerbNet, FrameNet, and the recently added OntoNotes sense groupings. Each of these resources was created for slightly different purposes, and therefore each carries unique strengths and limitations. SemLink allows users to lever- age the strengths of each resource and provides the groundwork for incorporating these lexi- cal resources effectively into linked data re- sources. SemLink and the resources included therein are discussed with a focus on the value of using lexical resources in a complemen- tary fashion. Recent improvements to Sem- Link, including the addition of a newresource, the OntoNotes sense groupings, are described. Work to address future goals, including further expansion of SemLink, is also discussed. 1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C22FND2Y/Bonial, Stowe, Palmer - 2013 - Renewing and revising SemLink(2).pdf}
}

@inproceedings{bordes2010LabelRankingAmbiguous,
  title = {Label {{Ranking}} under {{Ambiguous Supervision}} for {{Learning Semantic Correspondences}}},
  booktitle = {{{ICML}}},
  author = {Bordes, A. and Usunier, N. and Weston, J. and Kennedy, P.},
  date = {2010},
  pages = {103--110},
  abstract = {This paper studies the problem of learning from ambiguous supervision, focusing on the task of learning semantic correspondences. A learning problem is said to be ambiguously supervised when, for a given training input, a set of output candidates is provided with no prior of which one is correct. We propose to tackle this problem by solving a related unambiguous task with a label ranking ap- proach and show how and why this performs well on the original task, via the method of task-transfer. We apply it to learning to match natural language sentences to a struc- tured representation of their meaning and empirically demonstrate that this competes with the state-of-the-art on two benchmarks.},
  isbn = {978-1-60558-907-7},
  issue = {Icml},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FLKA362T/Bordes et al. - 2010 - Label Ranking under Ambiguous Supervision for Learning Semantic Correspondences(2).pdf}
}

@inproceedings{bordes2014QuestionAnsweringSubgraph,
  title = {Question {{Answering}} with {{Subgraph Embeddings}}},
  booktitle = {{{EMNLP}}},
  author = {Bordes, A. and Chopra, S. and Weston, J.},
  date = {2014},
  eprint = {1406.3676},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1406.3676},
  abstract = {This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a competitive benchmark of the literature.},
  archiveprefix = {arXiv},
  annotation = {486 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K65SLZ2M/Bordes, Chopra, Weston - 2014 - Question Answering with Subgraph Embeddings(2).pdf}
}

@misc{borman2009ExpectationMaximizationAlgorithm,
  title = {The {{Expectation Maximization Algorithm A}} Short Tutorial},
  author = {Borman, S.},
  date = {2009},
  volume = {25},
  number = {x},
  eprint = {19692813},
  eprinttype = {pmid},
  issn = {15360229},
  doi = {10.1097/RLU.0b013e3181b06c41\r00003072-200909000-00002},
  abstract = {This tutorial discusses the ExpectationMaximization (EM) algorithm of Demp- ster, Laird and Rubin 1. The approach taken follows that of an unpublished note by Stuart Russel, but fleshes out some of the gory details. In order to ensure that the presentation is reasonably self-contained, some of the results on which the derivation of the algorithm is based are presented prior to the main results. The EM algorithm has become a popular tool in statistical estimation problems involving incomplete data, or in problems which can be posed in a sim- ilar form, such as mixture estimation 3, 4. The EM algorithm has also been used in various motion estimation frameworks 5 and variants of it have been used in multiframe superresolution restoration methods which combine motion estimation along the lines of 2.},
  isbn = {0387952845},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z4MDDUA4/Borman - 2009 - The Expectation Maximization Algorithm A short tutorial(2).pdf}
}

@inproceedings{bosnjak2017ProgrammingDifferentiableForth,
  title = {Programming with a {{Differentiable Forth Interpreter}}},
  booktitle = {{{ICML}}},
  author = {Bošnjak, M. and Rocktäschel, T. and Naradowsky, J. and Riedel, S.},
  date = {2017},
  eprint = {1605.06640},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1605.06640},
  abstract = {Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {101 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WIBEF9XY/Bošnjak et al. - 2017 - Programming with a Differentiable Forth Interpreter(2).pdf}
}

@inproceedings{bottou2007TradeoffsLargeScale,
  title = {The {{Tradeoffs}} of {{Large Scale Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Bottou, L. and Bousquet, O.},
  date = {2007},
  volume = {58},
  number = {4},
  pages = {1089},
  issn = {03406245},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3BT5IACY/Bottou, Bousquet - 2007 - The Tradeoffs of Large Scale Learning(2).pdf}
}

@inproceedings{bouraoui2020InducingRelationalKnowledge,
  title = {Inducing {{Relational Knowledge}} from {{BERT}}},
  booktitle = {{{AAAI}}},
  author = {Bouraoui, Zied and Camacho-Collados, Jose and Schockaert, Steven},
  date = {2020-04-03},
  volume = {34},
  pages = {7456--7463},
  doi = {10.1609/aaai.v34i05.6242},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/6242},
  urldate = {2020-11-20},
  abstract = {One of the most remarkable properties of word embeddings is the fact that they capture certain types of semantic and syntactic relationships. Recently, pre-trained language models such as BERT have achieved groundbreaking results across a wide range of Natural Language Processing tasks. However, it is unclear to what extent such models capture relational knowledge beyond what is already captured by standard word embeddings. To explore this question, we propose a methodology for distilling relational knowledge from a pre-trained language model. Starting from a few seed instances of a given relation, we first use a large text corpus to find sentences that are likely to express this relation. We then use a subset of these extracted sentences as templates. Finally, we fine-tune a language model to predict whether a given word pair is likely to be an instance of some relation, when given an instantiated template for that relation as input.},
  langid = {english},
  keywords = {unread},
  annotation = {31 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IWYITHS8/Bouraoui et al. - 2020 - Inducing Relational Knowledge from BERT.pdf}
}

@inproceedings{bowman2015LargeAnnotatedCorpus,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  booktitle = {{{EMNLP}}},
  author = {Bowman, S. R. and Angeli, G. and Potts, C. and Manning, C. D.},
  date = {2015},
  eprint = {1508.05326},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1508.05326},
  abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
  archiveprefix = {arXiv},
  annotation = {1699 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BCZFN3J5/Bowman et al. - 2015 - A large annotated corpus for learning natural language inference(2).pdf}
}

@inproceedings{bowman2016GeneratingSentencesContinuous,
  title = {Generating {{Sentences}} from a {{Continuous Space}}},
  booktitle = {{{CoNLL}}},
  author = {Bowman, S. R. and Vilnis, L. and Vinyals, O. and Dai, A. M. and Jozefowicz, R. and Bengio, S.},
  date = {2016},
  eprint = {387149},
  eprinttype = {pmid},
  doi = {10.18653/v1/K16-1002},
  url = {http://arxiv.org/abs/1511.06349},
  abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {1302 citations (Semantic Scholar/DOI) [2021-03-26] 1302 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FAH7TGRJ/Bowman et al. - 2016 - Generating Sentences from a Continuous Space(2).pdf}
}

@inproceedings{bowman2021WhatWillIt,
  title = {What {{Will}} It {{Take}} to {{Fix Benchmarking}} in {{Natural Language Understanding}}?},
  booktitle = {{{NAACL}}},
  author = {Bowman, Samuel R. and Dahl, George},
  date = {2021},
  pages = {4843--4855},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.385},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.385},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZXLSMPJS/Bowman and Dahl - 2021 - What Will it Take to Fix Benchmarking in Natural L.pdf}
}

@article{box1968RecentAdvancesForecasting,
  title = {Some {{Recent Advances}} in {{Forecasting}} and {{Control}}},
  author = {Box, G. E. P. and Jenkins, G. M.},
  date = {1968},
  journaltitle = {Journal of the Royal Statistical Society},
  volume = {17},
  number = {2},
  pages = {91--109},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VXZBTCXQ/Box, Jenkins - 1968 - Some Recent Advances in Forecasting and Control(2).pdf}
}

@inproceedings{boyd1998TRENDSystemGenerating,
  title = {{{TREND}}: {{A System}} for {{Generating Intelligent Descriptions}} of {{Time-Series Data}}},
  booktitle = {{{ICIPS}}},
  author = {Boyd, S.},
  date = {1998},
  pages = {1--5},
  doi = {10.1.1.57.3705},
  abstract = {| A system is described that integrates knowledge-based signal processing and natural lan-guage processing to automatically generate descrip-tions of time-series data. These descriptions are based on short and long-term trends in the data which are detected using wavelet analysis. The basic architec-ture of the system is presented and some experimental results are shown for weather data.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G6XWQQF9/Boyd - 1998 - TREND A System for Generating Intelligent Descriptions of Time-Series Data(2).pdf}
}

@inproceedings{branavan2009ReinforcementLearningMapping,
  title = {Reinforcement Learning for Mapping Instructions to Actions},
  booktitle = {{{ACL-IJCNLP}}},
  author = {Branavan, S. R. K. and Chen, H. and Zettlemoyer, L. S. and Barzilay, R.},
  date = {2009},
  volume = {1},
  eprint = {18493666},
  eprinttype = {pmid},
  pages = {82},
  issn = {1742206X},
  doi = {10.3115/1687878.1687892},
  url = {http://portal.acm.org/citation.cfm?doid=1687878.1687892},
  abstract = {In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.},
  isbn = {978-1-932432-45-9},
  issue = {August},
  keywords = {unread},
  annotation = {236 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K2N8388X/Branavan et al. - 2009 - Reinforcement learning for mapping instructions to actions(2).pdf}
}

@inproceedings{branco2021ShortcuttedCommonsenseData,
  title = {Shortcutted {{Commonsense}}: {{Data Spuriousness}} in {{Deep Learning}} of {{Commonsense Reasoning}}},
  booktitle = {{{EMNLP}}},
  author = {Branco, Ruben and Branco, António and Rodrigues, João António and Silva, João Ricardo},
  date = {2021},
  pages = {18},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z43B2FLN/Branco et al. - Shortcutted Commonsense Data Spuriousness in Deep.pdf}
}

@inproceedings{bratieres2014ScalableGaussianProcess,
  title = {Scalable {{Gaussian}} Process Structured Prediction for Grid Factor Graph Applications},
  booktitle = {{{ICML}}},
  author = {Bratières, S. and Quadrianto, N. and Nowozin, S. and Ghahramani, Z},
  date = {2014},
  volume = {2},
  pages = {1625--1636},
  abstract = {Structured prediction is an important and well- studied problem with many applications across machine learning. GPstruct is a recently proposed structured prediction model that offers appealing properties such as being kernelised, non-parametric, and supporting Bayesian inference (Bratieres et al., 2013). The model places a Gaussian process prior over energy functions which describe relationships between input variables and structured output variables. However, the memory demand of GPstruct is quadratic in the number of latent variables and training runtime scales cubically. This prevents GPstruct from being applied to problems involving grid factor graphs, which are prevalent in computer vision and spatial statistics applications. Here we explore a scalable approach to learning GPstruct models based on ensemble learning, with weak learners (predictors) trained on subsets of the latent variables and bootstrap data, which can easily be distributed. We show experiments with 4M latent variables on image segmentation. Our method outperforms widely-used conditional random field models trained with pseudo-likelihood. Moreover, in image segmentation problems it improves over recent state-of- the-art marginal optimisation methods in terms of predictive performance and uncertainty calibration. Finally, it generalises well on all training set sizes.},
  isbn = {978-1-63439-397-3},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TR7VMWKI/Bratières et al. - 2014 - Scalable Gaussian process structured prediction for grid factor graph applications(2).pdf}
}

@inproceedings{breese1998EmpiricalAnalysisPredictive,
  title = {Empirical {{Analysis}} of {{Predictive Algorithms}} for {{Collaborative Filtering}}},
  booktitle = {{{UAI}}},
  author = {Breese, John S and Heckerman, David and Kadie, Carl},
  date = {1998},
  pages = {10},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/33PNVZ36/Breese et al. - Empirical Analysis of Predictive Algorithms for Co.pdf}
}

@inproceedings{brill2002AnalysisAskMSRQuestionanswering,
  title = {An Analysis of the {{AskMSR}} Question-Answering System},
  booktitle = {{{EMNLP}}},
  author = {Brill, E. and Dumais, S. and Banko, M.},
  date = {2002},
  volume = {10},
  pages = {257--264},
  doi = {10.3115/1118693.1118726},
  url = {http://portal.acm.org/citation.cfm?doid=1118693.1118726},
  abstract = {We describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy. The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers. Because a wrong an-swer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer.},
  issue = {July},
  keywords = {unread},
  annotation = {328 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JKVE7UQV/Brill, Dumais, Banko - 2002 - An analysis of the AskMSR question-answering system(2).pdf}
}

@article{brown1993MathematicsStatisticalMachine,
  title = {The Mathematics of Statistical Machine Translation: {{Parameter}} Estimation},
  author = {Brown, P. F. and Pietra, S. A. D. and Pietra, V. J. D. and Mercer, R. L.},
  date = {1993},
  journaltitle = {Computational Linguistics},
  volume = {19},
  number = {2},
  eprint = {3046723},
  eprinttype = {pmid},
  pages = {263--311},
  issn = {08912017},
  doi = {10.1080/08839514.2011.559906},
  url = {http://www.aclweb.org/anthology/J93-2003},
  abstract = {We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.},
  isbn = {0891-2017},
  annotation = {9 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CT2Y86KZ/Brown et al. - 1993 - The mathematics of statistical machine translation Parameter estimation(2).pdf}
}

@inproceedings{brown2017RichEventOntology,
  title = {The {{Rich Event Ontology}}},
  booktitle = {Events and {{Stories}} in the {{News Workshop}}},
  author = {Brown, S. and Bonial, C. and Obrst, L. and Palmer, M.},
  date = {2017},
  pages = {87--97},
  doi = {10.18653/v1/w17-2712},
  abstract = {In this paper we describe a new lexical semantic resource, The Rich Event Ontol-ogy, which provides an independent conceptual backbone to unify existing semantic role labeling (SRL) schemas and augment them with event-to-event causal and temporal relations. By unifying the FrameNet, VerbNet, Automatic Content Extraction, and Rich Entities, Relations and Events resources, the ontology serves as a shared hub for the disparate annotation schemas and therefore enables the combination of SRL training data into a larger, more diverse corpus. By adding temporal and causal relational information not found in any of the independent resources , the ontology facilitates reasoning on and across documents, revealing relationships between events that come together in temporal and causal chains to build more complex scenarios. We envision the open resource serving as a valuable tool for both moving from the ontology to text to query for event types and scenarios of interest, and for moving from text to the ontology to access interpretations of events using the combined semantic information housed there.},
  annotation = {10 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YJKAEC8E/Brown et al. - 2017 - The Rich Event Ontology(2).pdf}
}

@misc{brown2020LanguageModelsAre,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2020-08-07},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  annotation = {774 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5ZXNZI3D/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@inproceedings{bruna2013SpectralNetworksLocally,
  title = {Spectral {{Networks}} and {{Locally Connected Networks}} on {{Graphs}}},
  booktitle = {{{ICLR}}},
  author = {Bruna, J. and Zaremba, W. and Szlam, A. and LeCun, Y.},
  date = {2013},
  eprint = {1312.6203},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1312.6203},
  abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {1926 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LJR79LH3/Bruna et al. - 2013 - Spectral Networks and Locally Connected Networks on Graphs(2).pdf}
}

@inproceedings{bulat2017SpeakingSeeingUnderstanding,
  title = {Speaking , {{Seeing}} , {{Understanding}} : {{Correlating}} Semantic Models with Conceptual Representation in the Brain},
  booktitle = {{{EMNLP}}},
  author = {Bulat, L. and Clark, S. and Shutova, E.},
  date = {2017},
  number = {2},
  pages = {1081--1091},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3IPQZ5XH/Bulat, Clark, Shutova - 2017 - Speaking , Seeing , Understanding Correlating semantic models with conceptual representation in the br(2).pdf}
}

@inproceedings{buys2017RobustIncrementalNeural,
  title = {Robust {{Incremental Neural Semantic Graph Parsing}}},
  booktitle = {{{ACL}}},
  author = {Buys, J. and Blunsom, P.},
  date = {2017},
  eprint = {1704.07092},
  eprinttype = {arxiv},
  pages = {1215--1226},
  doi = {10.18653/v1/P17-1112},
  url = {http://arxiv.org/abs/1704.07092},
  abstract = {Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focused almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69\% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-75-3},
  keywords = {unread},
  annotation = {56 citations (Semantic Scholar/DOI) [2021-03-26] 56 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A7CHM8SL/Buys, Blunsom - 2017 - Robust Incremental Neural Semantic Graph Parsing(2).pdf}
}

@inproceedings{caglayan2019ProbingNeedVisual,
  title = {Probing the {{Need}} for {{Visual Context}} in {{Multimodal Machine Translation}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Caglayan, O. and Madhyastha, P. and Specia, L. and Barrault, L.},
  date = {2019},
  number = {i},
  eprint = {1903.08678},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1903.08678},
  abstract = {Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {42 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HRGKPIAJ/Caglayan et al. - 2019 - Probing the Need for Visual Context in Multimodal Machine Translation(2).pdf}
}

@inproceedings{cai2013LargescaleSemanticParsing,
  title = {Large-Scale {{Semantic Parsing}} via {{Schema Matching}} and {{Lexicon Extension}}},
  booktitle = {{{ACL}}},
  author = {Cai, Q. and Yates, A.},
  date = {2013},
  pages = {423--433},
  isbn = {978-1-937284-50-3},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VUXTXDWW/Cai, Yates - 2013 - Large-scale Semantic Parsing via Schema Matching and Lexicon Extension(2).pdf}
}

@inproceedings{cai2017CRFAutoencoderUnsupervised,
  title = {{{CRF Autoencoder}} for {{Unsupervised Dependency Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Cai, J. and Jiang, Y. and Tu, K.},
  date = {2017},
  pages = {1638--1643},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CUXZRHR8/Cai, Jiang, Tu - 2017 - CRF Autoencoder for Unsupervised Dependency Parsing(2).pdf}
}

@inproceedings{cai2018FullEndtoEndSemantic,
  title = {A {{Full End-to-End Semantic Role Labeler}}, {{Syntax-agnostic Over Syntax-aware}}?},
  booktitle = {{{COLING}}},
  author = {Cai, Jiaxun and He, Shexia and Li, Zuchao and Zhao, Hai},
  date = {2018},
  pages = {13},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZ5EB76M/Cai et al. - A Full End-to-End Semantic Role Labeler, Syntax-ag.pdf}
}

@inproceedings{cai2019CoreSemanticFirst,
  title = {Core {{Semantic First}}: {{A Top-down Approach}} for {{AMR Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Cai, D. and Lam, W.},
  date = {2019},
  eprint = {1909.04303},
  eprinttype = {arxiv},
  pages = {3790--3800},
  url = {http://arxiv.org/abs/1909.04303},
  abstract = {We introduce a novel scheme for parsing a piece of text into its Abstract Meaning Representation (AMR): Graph Spanning based Parsing (GSP). One novel characteristic of GSP is that it constructs a parse graph incrementally in a top-down fashion. Starting from the root, at each step, a new node and its connections to existing nodes will be jointly predicted. The output graph spans the nodes by the distance to the root, following the intuition of first grasping the main ideas then digging into more details. The \textbackslash textit\{core semantic first\} principle emphasizes capturing the main ideas of a sentence, which is of great interest. We evaluate our model on the latest AMR sembank and achieve the state-of-the-art performance in the sense that no heuristic graph re-categorization is adopted. More importantly, the experiments show that our parser is especially good at obtaining the core semantics.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {15 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J9GECFFG/Cai, Lam - 2019 - Core Semantic First A Top-down Approach for AMR Parsing(2).pdf}
}

@inproceedings{cai2019GraphTransformerGraphtoSequence,
  title = {Graph {{Transformer}} for {{Graph-to-Sequence Learning}}},
  booktitle = {{{AAAI}}},
  author = {Cai, D. and Lam, W.},
  date = {2019},
  eprint = {1911.07470},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.07470},
  abstract = {The dominant graph-to-sequence transduction models employ graph neural networks for graph representation learning, where the structural information is reflected by the receptive field of neurons. Unlike graph neural networks that restrict the information exchange between immediate neighborhood, we propose a new model, known as Graph Transformer, that uses explicit relation encoding and allows direct communication between two distant nodes. It provides a more efficient way for global graph structure modeling. Experiments on the applications of text generation from Abstract Meaning Representation (AMR) and syntax-based neural machine translation show the superiority of our proposed model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU on LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art results by up to 2.2 points. On the syntax-based translation tasks, our model establishes new single-model state-of-the-art BLEU scores, 21.3 for English-to-German and 14.1 for English-to-Czech, improving over the existing best results, including ensembles, by over 1 BLEU.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {31 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DGMN36VD/Cai, Lam - 2019 - Graph Transformer for Graph-to-Sequence Learning(2).pdf}
}

@misc{cai2020AMRParsingGraphSequence,
  title = {{{AMR Parsing}} via {{Graph-Sequence Iterative Inference}}},
  author = {Cai, D. and Lam, W.},
  date = {2020},
  eprint = {2004.05572},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.05572},
  abstract = {We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input \textbackslash textit\{sequence\} to abstract; and (2) where in the output \textbackslash textit\{graph\} to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported \textbackslash textsc\{Smatch\} scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2\textbackslash\% on LDC2017T10 (AMR 2.0) and 75.4\textbackslash\% on LDC2014T12 (AMR 1.0).},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {21 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N665ZDQU/Cai, Lam - 2020 - AMR Parsing via Graph-Sequence Iterative Inference(2).pdf}
}

@article{camacho-collados2018WordSenseEmbeddings,
  title = {From Word to Sense Embeddings: {{A}} Survey on Vector Representations of Meaning},
  author = {Camacho-Collados, J. and Pilehvar, M. T.},
  date = {2018},
  journaltitle = {JAIR},
  volume = {63},
  pages = {743--788},
  issn = {10769757},
  doi = {10.1613/jair.1.11259},
  abstract = {Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.},
  annotation = {142 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZKSBXBH4/Camacho-Collados, Pilehvar - 2018 - From word to sense embeddings A survey on vector representations of meaning(2).pdf}
}

@inproceedings{cao2007LearningRankPairwise,
  title = {Learning to Rank: From Pairwise Approach to Listwise Approach},
  shorttitle = {Learning to Rank},
  booktitle = {{{ICML}}},
  author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
  date = {2007},
  pages = {129--136},
  publisher = {{ACM Press}},
  location = {{Corvalis, Oregon}},
  doi = {10.1145/1273496.1273513},
  url = {http://portal.acm.org/citation.cfm?doid=1273496.1273513},
  urldate = {2021-06-25},
  abstract = {The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative filtering, and many other applications. Several methods for learning to rank have been proposed, which take object pairs as ‘instances’ in learning. We refer to them as the pairwise approach in this paper. Although the pairwise approach offers advantages, it ignores the fact that ranking is a prediction task on list of objects. The paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as ‘instances’ in learning. The paper proposes a new probabilistic method for the approach. Specifically it introduces two probability models, respectively referred to as permutation probability and top k probability, to define a listwise loss function for learning. Neural Network and Gradient Descent are then employed as model and algorithm in the learning method. Experimental results on information retrieval show that the proposed listwise approach performs better than the pairwise approach.},
  eventtitle = {The 24th International Conference},
  isbn = {978-1-59593-793-3},
  langid = {english},
  annotation = {1546 citations (Semantic Scholar/DOI) [2021-06-25]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JET4HXW5/Cao et al. - 2007 - Learning to rank from pairwise approach to listwi.pdf}
}

@inproceedings{cao2022RelationalMultiTaskLearning,
  title = {Relational {{Multi-Task Learning}}: {{Modeling Relations}} between {{Data}} and {{Tasks}}},
  booktitle = {{{ICLR}}},
  author = {Cao, K. and You, J. and Leskovec, J.},
  date = {2022},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PQSWAN5K/relational_multi_task_learning.pdf}
}

@inproceedings{caraballo1999DeterminingSpecificityNouns,
  title = {Determining the Specificity of Nouns from Text},
  booktitle = {{{EMNLP}}},
  author = {Caraballo, S. A. and Charniak, E.},
  date = {1999},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WU8F3ZFI/Caraballo, Charniak - 1999 - Determining the specificity of nouns from text(2).pdf}
}

@inproceedings{cassidy2014AnnotationFrameworkDense,
  title = {An Annotation Framework for Dense Event Ordering},
  booktitle = {{{ACL}}},
  author = {Cassidy, T. and McDowell, B. and Chambers, N. and Bethard, S.},
  date = {2014},
  volume = {2},
  pages = {501--506},
  abstract = {Today's event ordering research is heavily dependent on annotated corpora. Current corpora influence shared evaluations and drive algorithm development. Partly due to this dependence, most research focuses on partial orderings of a document's events. For instance, the TempEval competitions and the TimeBank only annotate small portions of the event graph, focusing on the most salient events or on specific types of event pairs (e.g., only events in the same sentence). Deeper temporal reasoners struggle with this sparsity because the entire temporal picture is not represented. This paper proposes a new annotation process with a mechanism to force annotators to label connected graphs. It generates 10 times more relations per document than the TimeBank, and our TimeBank-Dense corpus is larger than all current corpora. We hope this process and its dense corpus encourages research on new global models with deeper reasoning. © 2014 Association for Computational Linguistics.},
  isbn = {978-1-937284-73-2},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IP4BLMS6/Cassidy et al. - 2014 - An annotation framework for dense event ordering(2).pdf}
}

@misc{castel2021HowOptimalGreedy,
  title = {How {{Optimal}} Is {{Greedy Decoding}} for {{Extractive Question Answering}}?},
  author = {Castel, Or and Ram, Ori and Efrat, Avia and Levy, Omer},
  date = {2021-08-12},
  eprint = {2108.05857},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.05857},
  urldate = {2021-08-14},
  abstract = {Fine-tuned language models use greedy decoding to answer reading comprehension questions with relative success. However, this approach does not ensure that the answer is a span in the given passage, nor does it guarantee that it is the most probable one. Does greedy decoding actually perform worse than an algorithm that does adhere to these properties? To study the performance and optimality of greedy decoding, we present exact-extract, a decoding algorithm that efficiently finds the most probable answer span in the context. We compare the performance of T5 with both decoding algorithms on zero-shot and few-shot extractive question answering. When no training examples are available, exact-extract significantly outperforms greedy decoding. However, greedy decoding quickly converges towards the performance of exact-extract with the introduction of a few training examples, becoming more extractive and increasingly likelier to generate the most probable span as the training set grows. We also show that self-supervised training can bias the model towards extractive behavior, increasing performance in the zero-shot setting without resorting to annotated examples. Overall, our results suggest that pretrained language models are so good at adapting to extractive question answering, that it is often enough to fine-tune on a small training set for the greedy algorithm to emulate the optimal decoding strategy.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-08-14]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WQCZJ8J6/Castel et al. - 2021 - How Optimal is Greedy Decoding for Extractive Ques.pdf;/home/hiaoxui/.local/share/zotero_files/storage/BFJVSN36/2108.html}
}

@inproceedings{chaganty2016HowMuch131,
  title = {How {{Much}} Is 131 {{Million Dollars}}? {{Putting Numbers}} in {{Perspective}} with {{Compositional Descriptions}}},
  booktitle = {{{ACL}}},
  author = {Chaganty, A. and Liang, P.},
  date = {2016},
  eprint = {1609.00070},
  eprinttype = {arxiv},
  pages = {578--587},
  doi = {10.18653/v1/P16-1055},
  url = {http://aclweb.org/anthology/P16-1055},
  abstract = {How much is 131 million US dollars? To help readers put such numbers in con-text, we propose a new task of automati-cally generating short descriptions known as perspectives, e.g. " \$131 million is about the cost to employ everyone in Texas over a lunch period " . First, we collect a dataset of numeric mentions in news arti-cles, where each mention is labeled with a set of rated perspectives. We then pro-pose a system to generate these descrip-tions consisting of two steps: formula con-struction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on fa-miliarity, numeric proximity and seman-tic compatibility. In generation, we con-vert a formula into natural language us-ing a sequence-to-sequence recurrent neu-ral network. Our system obtains a 15.2\% F 1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-2758-5},
  annotation = {13 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4BAFWNHM/Chaganty, Liang - 2016 - How Much is 131 Million Dollars Putting Numbers in Perspective with Compositional Descriptions(2).pdf}
}

@inproceedings{chahuneau2013KnowledgerichMorphologicalPriors,
  title = {Knowledge-Rich Morphological Priors for Bayesian Language Models},
  booktitle = {{{NAACL-HLT}}},
  author = {Chahuneau, V. and Smith, N. A. and Dyer, C.},
  date = {2013},
  pages = {1206--1215},
  abstract = {We present a morphology-Aware nonparametric Bayesian model of language whose prior distribution uses manually constructed finitestate transducers to capture the word formation processes of particular languages. This relaxes the word independence assumption and enables sharing of statistical strength across, for example, stems or inflectional paradigms in different contexts. Our model can be used in virtually any scenario where multinomial distributions over words would be used. We obtain state-of-The-Art results in language modeling, word alignment, and unsupervised morphological disambiguation for a variety of morphologically rich languages.},
  isbn = {978-1-937284-47-3},
  issue = {June},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L6N2TCLV/Chahuneau, Smith, Dyer - 2013 - Knowledge-rich morphological priors for bayesian language models(2).pdf}
}

@inproceedings{chai2022CrossLingualAbilityMultilingual,
  title = {Cross-{{Lingual Ability}} of {{Multilingual Masked Language Models}}: {{A Study}} of {{Language Structure}}},
  shorttitle = {Cross-{{Lingual Ability}} of {{Multilingual Masked Language Models}}},
  booktitle = {{{ACL}}},
  author = {Chai, Yuan and Liang, Yaobo and Duan, Nan},
  date = {2022-03-16},
  eprint = {2203.08430},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.08430},
  urldate = {2022-03-23},
  abstract = {Multilingual pre-trained language models, such as mBERT and XLM-R, have shown impressive cross-lingual ability. Surprisingly, both of them use multilingual masked language model (MLM) without any cross-lingual supervision or aligned data. Despite the encouraging results, we still lack a clear understanding of why cross-lingual ability could emerge from multilingual MLM. In our work, we argue that cross-language ability comes from the commonality between languages. Specifically, we study three language properties: constituent order, composition and word co-occurrence. First, we create an artificial language by modifying property in source language. Then we study the contribution of modified property through the change of cross-language transfer results on target language. We conduct experiments on six languages and two cross-lingual NLP tasks (textual entailment, sentence retrieval). Our main conclusion is that the contribution of constituent order and word co-occurrence is limited, while the composition is more crucial to the success of cross-linguistic transfer.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SHFI2LL2/Chai et al. - 2022 - Cross-Lingual Ability of Multilingual Masked Langu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/9Q98S62N/2203.html}
}

@inproceedings{chalier2020JointReasoningMultiFaceted,
  title = {Joint {{Reasoning}} for {{Multi-Faceted Commonsense Knowledge}}},
  booktitle = {{{AKBC}}},
  author = {Chalier, Y. and Razniewski, S. and Weikum, G.},
  date = {2020},
  eprint = {2001.04170v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BBF4JZH7/Chalier, Razniewski, Weikum - 2020 - Joint Reasoning for Multi-Faceted Commonsense Knowledge(4).pdf;/home/hiaoxui/.local/share/zotero_files/storage/LX9FGCAC/Chalier, Razniewski, Weikum - 2020 - Joint Reasoning for Multi-Faceted Commonsense Knowledge(3).pdf}
}

@article{chambers2014DenseEventOrdering,
  title = {Dense {{Event Ordering}} with a {{Multi-Pass Architecture}}},
  author = {Chambers, N. and Cassidy, T. and McDowell, B. and Bethard, S},
  date = {2014},
  journaltitle = {TACL},
  volume = {2},
  pages = {273--284},
  doi = {10.1162/tacl_a_00182},
  abstract = {The past 10 years of event ordering research has focused on learning partial orderings over document events and time expressions. The most popular corpus, the TimeBank, contains a small subset of the possible ordering graph. Many evaluations follow suit by only testing certain pairs of events (e.g., only main verbs of neighboring sentences). This has led most research to focus on specific learners for partial labelings. This paper attempts to nudge the discussion from identifying some relations to all relations. We present new experiments on strongly connected event graphs that contain ∼10 times more relations per document than the TimeBank. We also describe a shift away from the single learner to a sieve-based architecture that naturally blends multiple learners into a precision-ranked cascade of sieves. Each sieve adds labels to the event graph one at a time, and earlier sieves inform later ones through transitive closure. This paper thus describes innovations in both approach and task. We experiment on the densest event graphs to date and show a 14\% gain over state-of-the-art.},
  keywords = {unread},
  annotation = {105 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TK9TF6PH/Chambers et al. - 2014 - Dense Event Ordering with a Multi-Pass Architecture(2).pdf}
}

@inproceedings{chang2015LearningSearchBetter,
  title = {Learning to Search Better than Your Teacher},
  booktitle = {{{ICML}}},
  author = {Chang, K. W. and Krishnamurthy, A. and Agarwal, A. and Daumé III, H. and Langford, J.},
  date = {2015},
  eprint = {1502.02206},
  eprinttype = {arxiv},
  pages = {2058--2066},
  abstract = {Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-1058-7},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/728HF6SA/Chang et al. - 2015 - Learning to search better than your teacher(2).pdf}
}

@misc{chang2019LanguageModelPretraining,
  title = {Language {{Model Pre-training}} for {{Hierarchical Document Representations}}},
  author = {Chang, Ming-Wei and Toutanova, Kristina and Lee, Kenton and Devlin, Jacob},
  date = {2019-01-25},
  eprint = {1901.09128},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.09128},
  urldate = {2021-06-08},
  abstract = {Hierarchical neural architectures are often used to capture long-distance dependencies and have been applied to many document-level tasks such as summarization, document segmentation, and sentiment analysis. However, effective usage of such a large context can be difficult to learn, especially in the case where there is limited labeled data available. Building on the recent success of language model pretraining methods for learning flat representations of text, we propose algorithms for pre-training hierarchical document representations from unlabeled data. Unlike prior work, which has focused on pre-training contextual token representations or context-independent \{sentence/paragraph\} representations, our hierarchical document representations include fixed-length sentence/paragraph representations which integrate contextual information from the entire documents. Experiments on document segmentation, document-level question answering, and extractive document summarization demonstrate the effectiveness of the proposed pre-training algorithms.},
  archiveprefix = {arXiv},
  annotation = {13 citations (Semantic Scholar/arXiv) [2021-06-08]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NER9SSFI/Chang et al. - 2019 - Language Model Pre-training for Hierarchical Docum.pdf;/home/hiaoxui/.local/share/zotero_files/storage/SJSD9LSH/1901.html}
}

@inproceedings{chen2008LearningSportscastTest,
  title = {Learning to Sportscast: A Test of Grounded Language Acquisition},
  booktitle = {{{ICML}}},
  author = {Chen, D. L. and Mooney, R. J.},
  date = {2008},
  pages = {128--135},
  doi = {http://doi.acm.org/10.1145/1390156.1390173},
  abstract = {We present a novel commentator system that learns language from sportscasts\textbackslash nof simulated soccer games. The system learns to parse and generate\textbackslash ncommentaries without any engineered knowledge about the English language.\textbackslash nTraining is done using only ambiguous supervision in the form of\textbackslash ntextual human commentaries and simulation states of the soccer games.\textbackslash nThe system simultaneously tries to establish correspondences between\textbackslash nthe commentaries and the simulation states as well as build a translation\textbackslash nmodel. We also present a novel algorithm, Iterative Generation Strategy\textbackslash nLearning (IGSL), for deciding which events to comment on. Human evaluations\textbackslash nof the generated commentaries indicate they are of reasonable quality\textbackslash ncompared to human commentaries.},
  isbn = {978-1-60558-205-4},
  issue = {July},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RLPPG6NW/Chen, Mooney - 2008 - Learning to sportscast a test of grounded language acquisition(2).pdf}
}

@article{chen2010TrainingMultilingualSportscaster,
  title = {Training a Multilingual Sportscaster: {{Using}} Perceptual Context to Learn Language},
  author = {Chen, D. L. and Kim, J. and Mooney, R. J.},
  date = {2010},
  journaltitle = {JAIR},
  volume = {37},
  pages = {397--435},
  issn = {10769757},
  doi = {10.1613/jair.2962},
  abstract = {We present a novel framework for learning to interpret and generate language using only per-ceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descrip-tive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.},
  isbn = {1076-9757},
  annotation = {111 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2T7P4KQI/Chen, Kim, Mooney - 2010 - Training a multilingual sportscaster Using perceptual context to learn language(2).pdf}
}

@inproceedings{chen2011LearningInterpretNatural,
  title = {Learning to {{Interpret Natural Language Navigation Instructions}} from {{Observations}}},
  booktitle = {{{AAAI}}},
  author = {Chen, D. L. and Mooney, R. J.},
  date = {2011},
  eprint = {23459267},
  eprinttype = {pmid},
  pages = {859--865},
  issn = {1938-7228},
  doi = {10.1.1.221.8069/???},
  url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/download/3701/3974},
  abstract = {The ability to understand natural-language instructions is crit- ical to building intelligent agents that interact with humans. We present a systemthat learns to transformnatural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environ- ments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation in- structions for these environments is used for training and test- ing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the sys- tem is able to automatically learn to correctly interpret a rea- sonable fraction of the complex instructions in this corpus. 1},
  archiveprefix = {arXiv},
  isbn = {978-1-57735-508-3},
  issue = {August},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MUILCJU9/Chen, Mooney - 2011 - Learning to Interpret Natural Language Navigation Instructions from Observations(2).pdf}
}

@inproceedings{chen2011PanningGoldFinding,
  title = {Panning for Gold: Finding Relevant Semantic Content for Grounded Language Learning},
  booktitle = {{{MLSLP}}},
  author = {Chen, D. L. and Mooney, R. J.},
  date = {2011},
  issue = {June},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WN58ZJDT/Chen, Mooney - 2011 - Panning for gold finding relevant semantic content for grounded language learning(2).pdf}
}

@misc{chen2012LearningLanguageAmbiguous,
  title = {Learning Language from Ambiguous Perceptual Context},
  author = {Chen, D. L.},
  date = {2012},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CVZH5S2T/Chen - 2012 - Learning language from ambiguous perceptual context(2).pdf}
}

@inproceedings{chen2014FastAccurateDependency,
  title = {A {{Fast}} and {{Accurate Dependency Parser}} Using {{Neural Networks}}},
  booktitle = {{{EMNLP}}},
  author = {Chen, D. and Manning, C. D.},
  date = {2014},
  doi = {10.3115/v1/d14-1082},
  abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2\% improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2\% unlabeled attachment score on the English Penn Treebank.},
  annotation = {1460 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TF3GQNQT/Chen, Manning - 2014 - A Fast and Accurate Dependency Parser using Neural Networks(2).pdf}
}

@inproceedings{chen2014UnifiedModelWord,
  title = {A {{Unified Model}} for {{Word Sense Representation}} and {{Disambiguation}}},
  booktitle = {{{EMNLP}}},
  author = {Chen, X. and Liu, Z. and Sun, M.},
  date = {2014},
  doi = {10.3115/v1/d14-1110},
  abstract = {Most word representation methods assume that each word owns a single semantic vec-tor. This is usually problematic because lexical ambiguity is ubiquitous, which is also the problem to be resolved by word sense disambiguation. In this paper, we present a unified model for joint word sense representation and disambiguation, which will assign distinct representation-s for each word sense. 1 The basic idea is that both word sense representation (WS-R) and word sense disambiguation (WS-D) will benefit from each other: (1) high-quality WSR will capture rich informa-tion about words and senses, which should be helpful for WSD, and (2) high-quality WSD will provide reliable disambiguat-ed corpora for learning better sense rep-resentations. Experimental results show that, our model improves the performance of contextual word similarity compared to existing WSR methods, outperforms state-of-the-art supervised methods on domain-specific WSD, and achieves competitive performance on coarse-grained all-words WSD.},
  annotation = {293 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JYMUKEQ9/Chen, Liu, Sun - 2014 - A Unified Model for Word Sense Representation and Disambiguation(2).pdf}
}

@inproceedings{chen2017DiscriminativeInformationRetrieval,
  title = {Discriminative {{Information Retrieval}} for {{Question Answering Sentence Selection}}},
  booktitle = {{{EACL}}},
  author = {Chen, T. and Van Durme, B.},
  date = {2017},
  volume = {2},
  pages = {719--725},
  doi = {10.18653/v1/e17-2114},
  abstract = {We propose a framework for discriminative IR atop linguistic features, trained to improve the recall of answer candidate pas-sage retrieval, the initial step in text-based question answering. We formalize this as an instance of linear feature-based IR, demonstrating a 34\% -43\% improvement in recall for candidate triage for QA.},
  annotation = {15 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VN6RS8YK/Chen, Van Durme - 2017 - Discriminative Information Retrieval for Question Answering Sentence Selection(2).pdf}
}

@inproceedings{chen2017EnhancedLSTMNatural,
  title = {Enhanced {{LSTM}} for {{Natural Language Inference}}},
  booktitle = {{{ACL}}},
  author = {Chen, Q. and Wei, S. and Inkpen, D.},
  date = {2017},
  number = {2017},
  eprint = {1609.06038v3},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FU2C2LBA/Chen, Wei, Inkpen - 2017 - Enhanced LSTM for Natural Language Inference(2).pdf}
}

@inproceedings{chen2017VariationalLossyAutoencoder,
  title = {Variational {{Lossy Autoencoder}}},
  booktitle = {{{ICLR}}},
  author = {Chen, X. and Kingma, D. P. and Salimans, T. and Duan, Y. and Dhariwal, P. and Schulman, J. and Sutskever, I. and Abbeel, P.},
  date = {2017},
  eprint = {1611.02731},
  eprinttype = {arxiv},
  pages = {1--17},
  url = {http://arxiv.org/abs/1611.02731},
  abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution \$p(z)\$ and decoding distribution \$p(x|z)\$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {398 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LMHQ89GP/Chen et al. - 2017 - Variational Lossy Autoencoder(2).pdf}
}

@inproceedings{chen2018LearningKwayDdimensional,
  title = {Learning {{K-way D-dimensional Discrete Codes}} for {{Compact Embedding Representations}}},
  booktitle = {{{ICML}}},
  author = {Chen, T. and Min, M. R. and Sun, Y.},
  date = {2018},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VYF2DMR9/Chen, Min, Sun - 2018 - Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations(2).pdf}
}

@misc{chen2018NeuralOrdinaryDifferential,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, T. Q. and Rubanova, Y. and Bettencourt, J. and Duvenaud, D.},
  date = {2018},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BQWY55JL/Chen et al. - 2018 - Neural Ordinary Differential Equations(2).pdf}
}

@inproceedings{chen2018PreCoLargescaleDataset,
  title = {{{PreCo}}: {{A Large-scale Dataset}} in {{Preschool Vocabulary}} for {{Coreference Resolution}}},
  shorttitle = {{{PreCo}}},
  booktitle = {{{EACL}}},
  author = {Chen, Hong and Fan, Zhenhua and Lu, Hao and Yuille, Alan and Rong, Shu},
  date = {2018},
  pages = {172--181},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1016},
  url = {http://aclweb.org/anthology/D18-1016},
  urldate = {2022-04-01},
  abstract = {We introduce PreCo, a large-scale English dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. To strengthen the training-test overlap, we collect a large corpus of 38K documents and 12.5M words which are mostly from the vocabulary of Englishspeaking preschoolers. Experiments show that with higher training-test overlap, error analysis on PreCo is more efficient than the one on OntoNotes, a popular existing dataset. Furthermore, we annotate singleton mentions making it possible for the first time to quantify the influence that a mention detector makes on coreference resolution performance. The dataset is freely available at https:// preschool-lab.github.io/PreCo/.},
  eventtitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L3XIIBGD/Chen et al. - 2018 - PreCo A Large-scale Dataset in Preschool Vocabula.pdf}
}

@inproceedings{chen2018RecurrentNeuralNetworks,
  title = {Recurrent {{Neural Networks}} as {{Weighted Language Recognizers}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Chen, Y. and Gilroy, S. and Maletti, A. and May, J. and Knight, K.},
  date = {2018},
  pages = {2261--2271},
  doi = {10.18653/v1/N18-1205},
  url = {http://arxiv.org/abs/1711.05408},
  abstract = {We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete and APX-hard. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.},
  annotation = {30 citations (Semantic Scholar/DOI) [2021-03-26] 30 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KC9RD459/Chen et al. - 2018 - Recurrent Neural Networks as Weighted Language Recognizers(2).pdf}
}

@inproceedings{chen2018VariationalSequentialLabelers,
  title = {Variational Sequential Labelers for Semi-Supervised Learning},
  booktitle = {{{EMNLP}}},
  author = {Chen, M. and Tang, Q. and Livescu, K. and Gimpel, K.},
  date = {2018},
  eprint = {1906.09535},
  eprinttype = {arxiv},
  pages = {215--226},
  doi = {10.18653/v1/d18-1020},
  abstract = {We introduce a family of multitask variational methods for semi-supervised sequence labeling. Our model family consists of a latent-variable generative model and a discriminative labeler. The generative models use latent variables to define the conditional probability of a word given its context, drawing inspiration from word prediction objectives commonly used in learning word embeddings. The labeler helps inject discriminative information into the latent space. We explore several latent variable configurations, including ones with hierarchical structure, which enables the model to account for both label-specific and word-specific information. Our models consistently outperform standard sequential baselines on 8 sequence labeling datasets, and improve further with unlabeled data.},
  archiveprefix = {arXiv},
  isbn = {978-1-948087-84-1},
  keywords = {unread},
  annotation = {19 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DXLMEG3N/Chen et al. - 2018 - Variational sequential labelers for semi-supervised learning(2).pdf}
}

@inproceedings{chen2019EnhancingNeuralDataToText,
  title = {Enhancing {{Neural Data-To-Text Generation Models}} with {{External Background Knowledge}}},
  booktitle = {{{EMNLP}}},
  author = {Chen, S. and Wang, J. and Feng, X. and Jiang, F. and Qin, B. and Lin, C.},
  date = {2019},
  pages = {3022--3032},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NXHGVM5Q/Chen et al. - 2019 - Enhancing Neural Data-To-Text Generation Models with External Background Knowledge(2).pdf}
}

@misc{chen2020DifferentiableProductQuantization,
  title = {Differentiable {{Product Quantization}} for {{Learning Compact Embedding Layers}}},
  author = {Chen, T. and Li, L and Sun, Y.},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LE43Y5VI/Chen, Li, Sun - 2020 - Differentiable Product Quantization for Learning Compact Embedding Layers(5).pdf;/home/hiaoxui/.local/share/zotero_files/storage/MTQKQS7D/Chen, Li, Sun - 2020 - Differentiable Product Quantization for Learning Compact Embedding Layers(6).pdf;/home/hiaoxui/.local/share/zotero_files/storage/NIYLQ59Q/Chen, Li, Sun - 2020 - Differentiable Product Quantization for Learning Compact Embedding Layers(4).pdf}
}

@misc{chen2020HierarchicalEntityTyping,
  title = {Hierarchical {{Entity Typing}} via {{Multi-level Learning}} to {{Rank}}},
  author = {Chen, T. and Chen, Y. and Van Durme, B.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CZ9MXXXT/Chen, Chen, Van Durme - 2020 - Hierarchical Entity Typing via Multi-level Learning to Rank(2).pdf}
}

@inproceedings{chen2020JointModelingArguments,
  title = {Joint {{Modeling}} of {{Arguments}} for {{Event Understanding}}},
  booktitle = {Workshop on {{Computational Approaches}} to {{Discourse}}},
  author = {Chen, Yunmo and Chen, Tongfei and Van Durme, Benjamin},
  date = {2020},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CSE565LD/2020.codi-1.10.pdf}
}

@inproceedings{chen2020UncertainNaturalLanguage,
  title = {Uncertain {{Natural Language Inference}}},
  booktitle = {{{ACL}}},
  author = {Chen, T. and Jiang, Z. and Poliak, A. and Sakaguchi, K. and Van Durme, B.},
  date = {2020},
  eprint = {25246403},
  eprinttype = {pmid},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  archiveprefix = {arXiv},
  isbn = {978-85-7811-079-6},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/43VC8JCB/Anonymous - 2020 - Uncertain Natural Language Inference(2).pdf}
}

@misc{chen2021JointMultipleIntent,
  title = {Joint {{Multiple Intent Detection}} and {{Slot Filling}} via {{Self-distillation}}},
  author = {Chen, Lisong and Zhou, Peilin and Zou, Yuexian},
  date = {2021-08-18},
  eprint = {2108.08042},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.08042},
  urldate = {2021-09-05},
  abstract = {Intent detection and slot filling are two main tasks in natural language understanding (NLU) for identifying users' needs from their utterances. These two tasks are highly related and often trained jointly. However, most previous works assume that each utterance only corresponds to one intent, ignoring the fact that a user utterance in many cases could include multiple intents. In this paper, we propose a novel Self-Distillation Joint NLU model (SDJN) for multi-intent NLU. First, we formulate multiple intent detection as a weakly supervised problem and approach with multiple instance learning (MIL). Then, we design an auxiliary loop via self-distillation with three orderly arranged decoders: Initial Slot Decoder, MIL Intent Decoder, and Final Slot Decoder. The output of each decoder will serve as auxiliary information for the next decoder. With the auxiliary knowledge provided by the MIL Intent Decoder, we set Final Slot Decoder as the teacher model that imparts knowledge back to Initial Slot Decoder to complete the loop. The auxiliary loop enables intents and slots to guide mutually in-depth and further boost the overall NLU performance. Experimental results on two public multi-intent datasets indicate that our model achieves strong performance compared to others.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-05]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LEWJL68J/Chen et al. - 2021 - Joint Multiple Intent Detection and Slot Filling v.pdf;/home/hiaoxui/.local/share/zotero_files/storage/4R2A3XCR/2108.html}
}

@inproceedings{chen2021ProbabilisticBoxEmbeddings,
  title = {Probabilistic {{Box Embeddings}} for {{Uncertain Knowledge Graph Reasoning}}},
  booktitle = {{{NAACL}}},
  author = {Chen, Xuelu and Boratko, Michael and Chen, Muhao and Dasgupta, Shib Sankar and Li, Xiang Lorraine and McCallum, Andrew},
  date = {2021},
  pages = {882--893},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.68},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.68},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MGNZAXIN/Chen et al. - 2021 - Probabilistic Box Embeddings for Uncertain Knowled.pdf}
}

@inproceedings{chen2021SkyformerRemodelSelfAttention,
  title = {Skyformer: {{Remodel Self-Attention}} with {{Gaussian Kernel}} and {{Nystr}}\textbackslash "om {{Method}}},
  shorttitle = {Skyformer},
  booktitle = {{{NeurIPS}}},
  author = {Chen, Yifan and Zeng, Qi and Ji, Heng and Yang, Yun},
  date = {2021-10-29},
  eprint = {2111.00035},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.00035},
  urldate = {2021-11-04},
  abstract = {Transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. On the other hand, although kernel machines suffer from the same computation bottleneck in pairwise dot products, several approximation schemes have been successfully incorporated to considerably reduce their computational cost without sacrificing too much accuracy. In this work, we leverage the computation methods for kernel machines to alleviate the high computational cost and introduce Skyformer, which replaces the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nystr\textbackslash "om method to a non-positive semidefinite matrix to accelerate the computation. We further conduct theoretical analysis by showing that the matrix approximation error of our proposed method is small in the spectral norm. Experiments on Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or even better performance than the full self-attention while requiring fewer computation resources.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E87MX8BX/Chen et al. - 2021 - Skyformer Remodel Self-Attention with Gaussian Ke.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7YAGY3VD/2111.html}
}

@misc{chen2021SummScreenDatasetAbstractive,
  title = {{{SummScreen}}: {{A Dataset}} for {{Abstractive Screenplay Summarization}}},
  author = {Chen, Mingda and Chu, Zewei and Wiseman, Sam and Gimpel, Kevin},
  date = {2021},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P6ZWVEFL/Chen et al. - SummScreen A Dataset for Abstractive Screenplay S.pdf}
}

@inproceedings{chen2021ZeroShotCrossLingualTransfer,
  title = {Zero-{{Shot Cross-Lingual Transfer}} of {{Neural Machine Translation}} with {{Multilingual Pretrained Encoders}}},
  booktitle = {{{EMNLP}}},
  author = {Chen, Guanhua and Ma, Shuming and Chen, Yun and Dong, Li and Zhang, Dongdong and Pan, Jia and Wang, Wenping and Wei, Furu},
  date = {2021},
  pages = {12},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5SZ6AWHQ/Chen et al. - Zero-Shot Cross-Lingual Transfer of Neural Machine.pdf}
}

@misc{chen2022IterativeDocumentlevelInformation,
  title = {Iterative {{Document-level Information Extraction}} via {{Imitation Learning}}},
  author = {Chen, Yunmo and Gantt, William and Gu, Weiwei and Chen, Tongfei and White, Aaron Steven and Van Durme, Benjamin},
  date = {2022-10-12},
  number = {arXiv:2210.06600},
  eprint = {2210.06600},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.06600},
  urldate = {2022-10-24},
  abstract = {We present a novel iterative extraction (IterX) model for extracting complex relations, or templates, i.e., N-tuples representing a mapping from named slots to spans of text contained within a document. Documents may support zero or more instances of a template of any particular type, leading to the tasks of identifying the templates in a document, and extracting each template's slot values. Our imitation learning approach relieves the need to use predefined template orders to train an extractor and leads to state-of-the-art results on two established benchmarks -- 4-ary relation extraction on SciREX and template extraction on MUC-4 -- as well as a strong baseline on the new BETTER Granular task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GVDSLS4V/Chen et al. - 2022 - Iterative Document-level Information Extraction vi.pdf;/home/hiaoxui/.local/share/zotero_files/storage/78VGAFC2/2210.html}
}

@inproceedings{chen2022SketchingToolUnderstanding,
  title = {Sketching as a {{Tool}} for {{Understanding}} and {{Accelerating Self-attention}} for {{Long Sequences}}},
  booktitle = {{{NAACL}}},
  author = {Chen, Yifan and Zeng, Qi and Hakkani-Tur, Dilek and Jin, Di and Ji, Heng and Yang, Yun},
  date = {2022},
  eprint = {2112.05359},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.05359},
  urldate = {2022-07-15},
  abstract = {Transformer-based models are not efficient in processing long sequences due to the quadratic space and time complexity of the self-attention modules. To address this limitation, Linformer and Informer are proposed to reduce the quadratic complexity to linear (modulo logarithmic factors) via low-dimensional projection and row selection respectively. These two models are intrinsically connected, and to understand their connection, we introduce a theoretical framework of matrix sketching. Based on the theoretical analysis, we propose Skeinformer to accelerate self-attention and further improve the accuracy of matrix approximation to self-attention with three carefully designed components: column sampling, adaptive row normalization and pilot sampling reutilization. Experiments on the Long Range Arena (LRA) benchmark demonstrate that our methods outperform alternatives with a consistently smaller time/space footprint.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JJJY5PVL/Chen et al. - 2021 - Sketching as a Tool for Understanding and Accelera.pdf}
}

@inproceedings{cheng2013RelationalInferenceWikification,
  title = {Relational {{Inference}} for {{Wikiﬁcation}}},
  booktitle = {{{EMNLP}}},
  author = {Cheng, X. and Roth, D.},
  date = {2013},
  abstract = {Wikification, commonly referred to as Disam- biguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific correspondingWikipedia pages. Previous ap- proaches to D2W focused on the use of lo- cal and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these meth- ods fail (often, embarrassingly), when some level of text understanding is needed to sup- port Wikification. In this paper we introduce a novel approach to Wikification by incorpo- rating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Lin- ear Programming (ILP) formulation of Wik- ification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant im- provements in bothWikification and the TAC Entity Linking task.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4ZTSFAUE/Cheng, Roth - 2013 - Relational Inference for Wikiﬁcation(2).pdf}
}

@inproceedings{cheng2016LongShortTermMemoryNetworks,
  title = {Long {{Short-Term Memory-Networks}} for {{Machine Reading}}},
  booktitle = {{{EMNLP}}},
  author = {Cheng, J. and Dong, L. and Lapata, M.},
  date = {2016},
  eprint = {1601.06733},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1601.06733},
  abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
  archiveprefix = {arXiv},
  annotation = {582 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K4XQEWFQ/Cheng, Dong, Lapata - 2016 - Long Short-Term Memory-Networks for Machine Reading(2).pdf}
}

@inproceedings{cheng2020AttendingEntitiesBetter,
  title = {Attending to {{Entities}} for {{Better Text Understanding}}},
  booktitle = {{{AAAI}}},
  author = {Cheng, P. and Erk, K.},
  date = {2020},
  eprint = {1911.04361},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.04361},
  abstract = {Recent progress in NLP witnessed the development of large-scale pre-trained language models (GPT, BERT, XLNet, etc.) based on Transformer (Vaswani et al. 2017), and in a range of end tasks, such models have achieved state-of-the-art results, approaching human performance. This demonstrates the power of the stacked self-attention architecture when paired with a sufficient number of layers and a large amount of pre-training data. However, on tasks that require complex and long-distance reasoning where surface-level cues are not enough, there is still a large gap between the pre-trained models and human performance. Strubell et al. (2018) recently showed that it is possible to inject knowledge of syntactic structure into a model through supervised self-attention. We conjecture that a similar injection of semantic knowledge, in particular, coreference information, into an existing model would improve performance on such complex problems. On the LAMBADA (Paperno et al. 2016) task, we show that a model trained from scratch with coreference as auxiliary supervision for self-attention outperforms the largest GPT-2 model, setting the new state-of-the-art, while only containing a tiny fraction of parameters compared to GPT-2. We also conduct a thorough analysis of different variants of model architectures and supervision configurations, suggesting future directions on applying similar techniques to other problems.},
  archiveprefix = {arXiv},
  annotation = {6 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NZCJ9CQH/Cheng, Erk - 2020 - Attending to Entities for Better Text Understanding(2).pdf}
}

@inproceedings{chiang2022OvercomingTheoreticalLimitation,
  title = {Overcoming a {{Theoretical Limitation}} of {{Self-Attention}}},
  booktitle = {{{ACL}}},
  author = {Chiang, David and Cholak, Peter},
  date = {2022-02-24},
  eprint = {2202.12172},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.12172},
  urldate = {2022-03-23},
  abstract = {Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions become less and less confident (that is, with cross-entropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahn's lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WLLM7UE4/Chiang and Cholak - 2022 - Overcoming a Theoretical Limitation of Self-Attent.pdf;/home/hiaoxui/.local/share/zotero_files/storage/PSJT2WR7/2202.html}
}

@misc{child2019GeneratingLongSequences,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  date = {2019-04-23},
  eprint = {1904.10509},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.10509},
  urldate = {2021-03-28},
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n \textbackslash sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  archiveprefix = {arXiv},
  annotation = {266 citations (Semantic Scholar/arXiv) [2021-03-27]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RP2BQQMG/Child et al. - 2019 - Generating Long Sequences with Sparse Transformers.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7YF2T6TM/1904.html}
}

@inproceedings{chiticariu2013RulebasedInformationExtraction,
  title = {Rule-Based {{Information Extraction}} Is {{Dead}}! {{Long Live Rule-based Information Extraction Systems}}!},
  booktitle = {{{EMNLP}}},
  author = {Chiticariu, L. and Li, Y. and Reiss, F. R.},
  date = {2013},
  url = {https://www.aclweb.org/anthology/D13-1079},
  abstract = {Abstract The rise of “Big Data” analytics over unstructured text has led to renewed interest in information extraction (IE). We surveyed the landscape of IE technologies and identified a major disconnect between industry and academia: while rule - based IE dominates the ...},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YVBWI8HQ/Chiticariu, Li, Reiss - 2013 - Rule-based Information Extraction is Dead! Long Live Rule-based Information Extraction Systems!(2).pdf}
}

@inproceedings{cho2014LearningPhraseRepresentations,
  title = {Learning Phrase Representations Using {{RNN}} Encoder-Decoder for Statistical Machine Translation},
  booktitle = {{{EMNLP}}},
  author = {Cho, K. and Van Merriënboer, B. and Gulcehre, C. and Bahdanau, D. and Bougares, F. and Schwenk, H. and Bengio, Y.},
  date = {2014},
  eprint = {2079951},
  eprinttype = {pmid},
  pages = {1724--1734},
  issn = {09205691},
  doi = {10.3115/v1/d14-1179},
  url = {http://arxiv.org/abs/1406.1078},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  isbn = {978-1-937284-96-1},
  annotation = {9995 citations (Semantic Scholar/DOI) [2021-03-26] 9995 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P7TG7XF5/Cho et al. - 2014 - Learning phrase representations using RNN encoder-decoder for statistical machine translation(2).pdf}
}

@inproceedings{cho2014PropertiesNeuralMachine,
  title = {On the {{Properties}} of {{Neural Machine Translation}} : {{Encoder}} – {{Decoder Approaches}}},
  booktitle = {Workshop on {{Syntax}}, {{Semantics}} and {{Structure}} in {{Statistical Translation}}},
  author = {Cho, K. and van Merrienboer, B. and Bahdanau, D. and Bengio, Y.},
  options = {useprefix=true},
  date = {2014},
  eprint = {1409.1259},
  eprinttype = {arxiv},
  pages = {103--111},
  doi = {10.3115/v1/W14-4012},
  abstract = {Neural machine translation is a relatively new approach to statistical machine trans-lation based purely on neural networks. The neural machine translation models of-ten consist of an encoder and a decoder. The encoder extracts a fixed-length repre-sentation from a variable-length input sen-tence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the proper-ties of the neural machine translation us-ing two models; RNN Encoder–Decoder and a newly proposed gated recursive con-volutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance de-grades rapidly as the length of the sentence and the number of unknown words in-crease. Furthermore, we find that the pro-posed gated recursive convolutional net-work learns a grammatical structure of a sentence automatically.},
  archiveprefix = {arXiv},
  isbn = {978-1-937284-96-1},
  keywords = {unread},
  annotation = {3118 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KBQVB6RS/Cho et al. - 2014 - On the Properties of Neural Machine Translation Encoder – Decoder Approaches(2).pdf}
}

@inproceedings{choi2018LearningComposeTaskSpecific,
  title = {Learning to {{Compose Task-Specific Tree Structures}}},
  booktitle = {{{AAAI}}},
  author = {Choi, J. and Yoo, K. M. and Lee, S.},
  date = {2018},
  eprint = {1707.02786v4},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ENETHYQ8/Choi, Yoo, Lee - 2018 - Learning to Compose Task-Specific Tree Structures(2).pdf}
}

@inproceedings{choi2018QuACQuestionAnswering,
  title = {{{QuAC}}: {{Question Answering}} in {{Context}}},
  booktitle = {{{EMNLP}}},
  author = {Choi, E. and He, H. and Iyyer, M. and Yatskar, M. and Yih, W. and Choi, Y. and Liang, P. and Zettlemoyer, L. S.},
  date = {2018},
  eprint = {1808.07036},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1808.07036},
  abstract = {We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {248 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2TUETN4K/Choi et al. - 2018 - QuAC Question Answering in Context(2).pdf}
}

@inproceedings{choromanski2021RethinkingAttentionPerformers,
  title = {Rethinking {{Attention}} with {{Performers}}},
  booktitle = {{{ICLR}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  date = {2021-03-09},
  eprint = {2009.14794},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.14794},
  urldate = {2021-12-07},
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G2Y66LYD/Choromanski et al. - 2021 - Rethinking Attention with Performers.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ZUHB8RZZ/2009.html}
}

@inproceedings{christodoulopoulos2010TwoDecadesUnsupervised,
  title = {Two Decades of Unsupervised {{POS}} Induction: {{How}} Far Have We Come?},
  booktitle = {{{EMNLP}}},
  author = {Christodoulopoulos, C. and Goldwater, S. and Steedman, M.},
  date = {2010},
  pages = {575--584},
  abstract = {Part-of-speech (POS) induction is one of the most popular tasks in research on unsuper-vised NLP. Many different methods have been proposed, yet comparisons are difficult to make since there is little consensus on evaluation framework, and many papers evaluate against only one or two competitor systems. Here we evaluate seven different POS induction systems spanning nearly 20 years of work, using a variety of measures. We show that some of the oldest (and simplest) systems stand up surprisingly well against more recent approaches. Since most of these systems were developed and tested using data from the WSJ corpus, we compare their generalization abilities by testing on both WSJ and the multilingual Multext-East corpus. Finally, we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype-driven learner. In most cases, the prototype-driven learner outperforms the unsupervised system used to initialize it, yielding state-of-the-art results on WSJ and improvements on non-English corpora. © 2010 Association for Computational Linguistics.},
  isbn = {1-932432-86-8},
  issue = {October},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E93S7IMU/Christodoulopoulos, Goldwater, Steedman - 2010 - Two decades of unsupervised POS induction How far have we come(2).pdf}
}

@inproceedings{christopoulou2021DistantlySupervisedRelation,
  title = {Distantly {{Supervised Relation Extraction}} with {{Sentence Reconstruction}} and {{Knowledge Base Priors}}},
  booktitle = {{{NAACL}}},
  author = {Christopoulou, Fenia and Miwa, Makoto and Ananiadou, Sophia},
  date = {2021},
  pages = {11--26},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.2},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.2},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TGTVA7JG/Christopoulou et al. - 2021 - Distantly Supervised Relation Extraction with Sent.pdf}
}

@inproceedings{chuang2022DiffCSEDifferencebasedContrastive,
  title = {{{DiffCSE}}: {{Difference-based Contrastive Learning}} for {{Sentence Embeddings}}},
  shorttitle = {{{DiffCSE}}},
  booktitle = {{{NAACL}}},
  author = {Chuang, Yung-Sung and Dangovski, Rumen and Luo, Hongyin and Zhang, Yang and Chang, Shiyu and Soljačić, Marin and Li, Shang-Wen and Yih, Wen-tau and Kim, Yoon and Glass, James},
  date = {2022-04-21},
  eprint = {2204.10298},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.10298},
  urldate = {2022-07-15},
  abstract = {We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning (Dangovski et al., 2021), which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other "harmful" types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic textual similarity tasks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VMC4Q5J6/Chuang et al. - 2022 - DiffCSE Difference-based Contrastive Learning for.pdf;/home/hiaoxui/.local/share/zotero_files/storage/F7VXP9R5/2204.html}
}

@misc{chun2020LearningExplainCausal,
  title = {Learning to {{Explain Causal Rationale}} of {{Stock Price Changes}} in {{Financial Reports}}},
  author = {Chun, Y. E.},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y8HCRWBX/Anonymous - 2020 - Learning to Explain Causal Rationale of Stock Price Changes in Financial Reports(2).pdf}
}

@inproceedings{chung2015RecurrentLatentVariable,
  title = {A {{Recurrent Latent Variable Model}} for {{Sequential Data}}},
  booktitle = {{{NeurIPS}}},
  author = {Chung, J. and Kastner, K. and Dinh, L. and Goel, K. and Courville, A. and Bengio, Y.},
  date = {2015},
  eprint = {1506.02216},
  eprinttype = {arxiv},
  pages = {8},
  issn = {10495258},
  url = {http://arxiv.org/abs/1506.02216},
  abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {647 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UWIVDBHY/Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential Data(2).pdf}
}

@inproceedings{church1988StochasticPartsProgram,
  title = {A {{Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text}}},
  booktitle = {{{ANLC}}},
  author = {Church, K. W.},
  date = {1988},
  pages = {136--143},
  url = {http://www.aclweb.org/anthology/A88-1019},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J6XNV7RG/Church - 1988 - A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text(2).pdf}
}

@article{church1990WordAssociationNorms,
  title = {Word Association Norms, Mutual Information, and Lexicography},
  author = {Church, K. W. and Hanks, P.},
  date = {1990},
  journaltitle = {Computational Linguistics},
  volume = {16},
  number = {1},
  pages = {22--29},
  doi = {10.3115/981623.981633},
  abstract = {The term word association is used in a very particular sense in the\textbackslash npsycholinguistic literature. \{(Generally\} speaking, subjects respond\textbackslash nquicker than normal to the word nurse if it follows a highly associated\textbackslash nword such as doctor. ) We will extend the term to provide the basis\textbackslash nfor a statistical description of a variety of interesting linguistic\textbackslash nphenomena, ranging from semantic relations of the doctor/nurse type\textbackslash n(content word/content word) to lexico-syntactic co-occurrence constraints\textbackslash nbetween verbs and prepositions (content word/function word). This\textbackslash npaper will propose an objective measure based on the information\textbackslash ntheoretic notion of mutual information, for estimating word association\textbackslash nnorms from computer readable corpora. \{(The\} standard method of obtaining\textbackslash nword association norms, testing a few thousand subjects on a few\textbackslash nhundred words, is both costly and unreliable.) The proposed measure,\textbackslash nthe association ratio, estimates word association norms directly\textbackslash nfrom computer readable corpora, making it possible to estimate norms\textbackslash nfor tens of thousands of words.},
  keywords = {unread},
  annotation = {4049 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5LZ2JU2U/Church, Hanks - 1990 - Word association norms, mutual information, and lexicography(2).pdf}
}

@inproceedings{cianflone2018LetItAgain,
  title = {Let’s Do It "Again": {{A First Computational Approach}} to {{Detecting Adverbial Presupposition Triggers}}},
  booktitle = {{{ACL}}},
  author = {Cianflone, A. and Chi, J. and Cheung, K.},
  date = {2018},
  eprint = {1806.04262},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QBHU3MCU/Cianflone, Chi, Cheung - 2018 - Let ’ s do it “ again ” A First Computational Approach to Detecting Adverbial Presupposition Triggers(2).pdf}
}

@inproceedings{clark2018NeuralTextGeneration,
  title = {Neural {{Text Generation}} in {{Stories Using Entity Representations}} as {{Context}}},
  booktitle = {{{NAACL}}},
  author = {Clark, E. and Ji, Y. and Smith, N. A.},
  date = {2018},
  pages = {2250--2260},
  doi = {10.18653/v1/n18-1204},
  abstract = {We introduce an approach to neural text gen-eration that explicitly represents entities men-tioned in the text. Entity representations are vectors that are updated as the text proceeds; they are designed specifically for narrative text like fiction or news stories. Our experiments demonstrate that modeling entities offers a benefit in two automatic evaluations: mention generation (in which a model chooses which entity to mention next and which words to use in the mention) and selection between a correct next sentence and a distractor from later in the same story. We also conduct a human evalu-ation on automatically generated text in story contexts; this study supports our emphasis on entities and suggests directions for further re-search.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FAHC8R67/Clark, Ji, Smith - 2018 - Neural Text Generation in Stories Using Entity Representations as Context(2).pdf}
}

@inproceedings{clark2018SemiSupervisedSequenceModeling,
  title = {Semi-{{Supervised Sequence Modeling}} with {{Cross-View Training}}},
  booktitle = {{{EMNLP}}},
  author = {Clark, Kevin and Luong, Minh-Thang and Manning, Christopher D. and Le, Quoc V.},
  date = {2018-09-21},
  eprint = {1809.08370},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1809.08370},
  urldate = {2021-01-04},
  abstract = {Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {160 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/47EDQN8X/Clark et al. - 2018 - Semi-Supervised Sequence Modeling with Cross-View .pdf;/home/hiaoxui/.local/share/zotero_files/storage/CBZTXD9W/1809.html}
}

@inproceedings{clark2018SimpleEffectiveMultiParagraph,
  title = {Simple and {{Effective Multi-Paragraph Reading Comprehension}}},
  booktitle = {{{ACL}}},
  author = {Clark, Christopher and Gardner, Matt},
  date = {2018},
  eprint = {1710.10723},
  eprinttype = {arxiv},
  abstract = {We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a sharednormalization training objective that encourages the model to produce globally correct output. We combine this method with a stateof-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E64JWYTQ/Clark and Gardner - 2017 - Simple and Effective Multi-Paragraph Reading Compr.pdf}
}

@inproceedings{clark2019DonTakeEasy,
  title = {Don't {{Take}} the {{Easy Way Out}} : {{Ensemble Based Methods}} for {{Avoiding Known Dataset Biases}}},
  booktitle = {{{EMNLP}}},
  author = {Clark, C. and Yatskar, M. and Zettlemoyer, L. S.},
  date = {2019},
  keywords = {review},
  annotation = {EMNLP 2019 review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TGXZX9YK/Clark, Yatskar, Zettlemoyer - 2019 - Don't Take the Easy Way Out Ensemble Based Methods for Avoiding Known Dataset Biases(2).pdf}
}

@inproceedings{clark2019WhatDoesBERT,
  title = {What {{Does BERT Look At}}? {{An Analysis}} of {{BERT}}'s {{Attention}}},
  shorttitle = {What {{Does BERT Look At}}?},
  booktitle = {{{EMNLP}}},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  date = {2019},
  eprint = {1906.04341},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1906.04341},
  urldate = {2021-02-23},
  abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
  archiveprefix = {arXiv},
  annotation = {380 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W268X4FJ/Clark et al. - 2019 - What Does BERT Look At An Analysis of BERT's Atte.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JR4YGD3N/1906.html}
}

@misc{clark2020TransformersSoftReasoners,
  title = {Transformers as {{Soft Reasoners}} over {{Language}}},
  author = {Clark, P. and Tafjord, O. and Richardson, K.},
  date = {2020},
  eprint = {2002.05867},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.05867},
  abstract = {AI has long pursued the goal of having systems reason over *explicitly provided* knowledge, but building suitable representations has proved challenging. Here we explore whether transformers can similarly learn to reason (or emulate reasoning), but using rules expressed in language, thus bypassing a formal representation. We provide the first demonstration that this is possible, and characterize the extent of this capability. To do this, we use a collection of synthetic datasets that test increasing levels of reasoning complexity (number of rules, presence of negation, and depth of chaining). We find transformers appear to learn rule-based reasoning with high (99\%) accuracy on these datasets, and in a way that generalizes to test data requiring substantially deeper chaining than in the training data (95\%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as a limited "soft theorem prover" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering. All datasets and a live demo are available at http://rule-reasoning.apps.allenai.org/},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {25 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D8LQVAAP/Clark, Tafjord, Richardson - 2020 - Transformers as Soft Reasoners over Language(2).pdf}
}

@inproceedings{clarke2010DrivingSemanticParsing,
  title = {Driving {{Semantic Parsing}} from the {{World}}’s {{Response}}},
  booktitle = {{{CoNLL}}},
  author = {Clarke, J. and Goldwasser, D. and Chang, M. and Roth, D.},
  date = {2010},
  pages = {18--27},
  abstract = {Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers},
  isbn = {978-1-932432-83-1},
  issue = {July},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ERPEU79Q/Clarke et al. - 2010 - Driving Semantic Parsing from the World’s Response(2).pdf}
}

@inproceedings{cocos2017WordSenseFiltering,
  title = {Word {{Sense Filtering Improves Embedding-Based Lexical Substitution}}},
  booktitle = {Workshop on {{Sense}}, {{Concept}} and {{Entity Representations}} and Their {{Applications}}},
  author = {Cocos, A. and Apidianaki, M. and Callison-Burch, C.},
  date = {2017},
  volume = {April},
  pages = {110--119},
  issn = {0893-0341, 0893-0341},
  doi = {http://dx.doi.org/10.1097/00002093-199700112-00003},
  abstract = {The role of word sense disambiguation in lexical substitution has been questioned due to the high performance of vector space models which propose good sub-stitutes without explicitly accounting for sense. We show that a filtering mecha-nism based on a sense inventory optimized for substitutability can improve the results of these models. Our sense inventory is constructed using a clustering method which generates paraphrase clusters that are congruent with lexical substitution an-notations in a development set. The re-sults show that lexical substitution can still benefit from senses which can improve the output of vector space paraphrase ranking models.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5744SMH7/Cocos, Apidianaki, Callison-Burch - 2017 - Word Sense Filtering Improves Embedding-Based Lexical Substitution(2).pdf}
}

@inproceedings{cohan2019PretrainedLanguageModels,
  title = {Pretrained {{Language Models}} for {{Sequential Sentence Classification}}},
  booktitle = {{{EMNLP}}},
  author = {Cohan, Arman and Beltagy, Iz and King, Daniel and Dalvi, Bhavana and Weld, Daniel S.},
  date = {2019},
  eprint = {1909.04054},
  eprinttype = {arxiv},
  pages = {3691--3697},
  doi = {10.18653/v1/D19-1383},
  url = {http://arxiv.org/abs/1909.04054},
  urldate = {2022-02-24},
  abstract = {As a step toward better document-level understanding, we explore classification of a sequence of sentences into their corresponding categories, a task that requires understanding sentences in context of the document. Recent successful models for this task have used hierarchical models to contextualize sentence representations, and Conditional Random Fields (CRFs) to incorporate dependencies between subsequent labels. In this work, we show that pretrained language models, BERT (Devlin et al., 2018) in particular, can be used for this task to capture contextual dependencies without the need for hierarchical encoding nor a CRF. Specifically, we construct a joint sentence representation that allows BERT Transformer layers to directly utilize contextual information from all words in all sentences. Our approach achieves state-of-the-art results on four datasets, including a new dataset of structured scientific abstracts.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HSXW6F5L/Cohan et al. - 2019 - Pretrained Language Models for Sequential Sentence.pdf;/home/hiaoxui/.local/share/zotero_files/storage/BIC8DND3/1909.html}
}

@inproceedings{cohen2018SphericalCNNs,
  title = {Spherical {{CNNs}}},
  booktitle = {{{ICLR}}},
  author = {Cohen, T. S. and Geiger, M. and Köhler, J. and Welling, M.},
  date = {2018},
  number = {3},
  eprint = {1801.10130v1},
  eprinttype = {arxiv},
  pages = {1--15},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJ3LLIGC/Cohen et al. - 2018 - Spherical CNNs(2).pdf}
}

@misc{cohen2019DifferentiableRepresentationsMultihop,
  title = {Differentiable {{Representations For Multihop Inference Rules}}},
  author = {Cohen, W. W. and Sun, H. and Hofer, R. A. and Siegler, M.},
  date = {2019},
  volume = {1},
  eprint = {1905.10417},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.10417},
  abstract = {We present efficient differentiable implementations of second-order multi-hop reasoning using a large symbolic knowledge base (KB). We introduce a new operation which can be used to compositionally construct second-order multi-hop templates in a neural model, and evaluate a number of alternative implementations, with different time and memory trade offs. These techniques scale to KBs with millions of entities and tens of millions of triples, and lead to simple models with competitive performance on several learning tasks requiring multi-hop reasoning.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2SRMCBWC/Cohen et al. - 2019 - Differentiable Representations For Multihop Inference Rules(2).pdf}
}

@inproceedings{colbert2020,
  title = {{{ColBERT}}: {{Efficient}} and {{Effective Passage Search}} via {{Contextualized Late Interaction}} over {{BERT}}},
  shorttitle = {{{ColBERT}}},
  booktitle = {{{SIGIR}}},
  author = {Khattab, Omar and Zaharia, Matei},
  date = {2020-07-25},
  pages = {39--48},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  doi = {10.1145/3397271.3401075},
  url = {https://dl.acm.org/doi/10.1145/3397271.3401075},
  urldate = {2022-03-29},
  eventtitle = {{{SIGIR}} '20: {{The}} 43rd {{International ACM SIGIR}} Conference on Research and Development in {{Information Retrieval}}},
  isbn = {978-1-4503-8016-4},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JIBS7A9N/Khattab and Zaharia - 2020 - ColBERT Efficient and Effective Passage Search vi.pdf}
}

@inproceedings{collell2018NeuralNetworkCrossModal,
  title = {Do {{Neural Network Cross-Modal Mappings Really Bridge Modalities}}?},
  booktitle = {{{ACL}}},
  author = {Collell, G. and Moens, M.},
  date = {2018},
  eprint = {1805.07616},
  eprinttype = {arxiv},
  pages = {1--7},
  doi = {arXiv:1805.07616v1},
  url = {http://arxiv.org/abs/1805.07616},
  abstract = {Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space. The predicted vectors are then used to perform e.g., retrieval or labeling. Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors. However, whether this is achieved has not been investigated yet. Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue. In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions. Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors. In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {11 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8GFE3SC6/Collell, Moens - 2018 - Do Neural Network Cross-Modal Mappings Really Bridge Modalities(2).pdf}
}

@thesis{collins1999HeadDrivenStatisticalModels,
  title = {Head-{{Driven Statistical Models}} for {{Natural Language Parsing}}},
  author = {Collins, M.},
  date = {1999},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XE9ENIDX/Collins - 1999 - Head-Driven Statistical Models for Natural Language Parsing(2).pdf}
}

@misc{collins2000MEMMsLogLinearTagging,
  title = {{{MEMMs}} ( {{Log-Linear Tagging Models}} )},
  author = {Collins, M.},
  date = {2000},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3V5BRYBC/Collins - 2000 - MEMMs ( Log-Linear Tagging Models )(2).pdf}
}

@inproceedings{collins2002DiscrimativeTrainingMethods,
  title = {Discrimative {{Training Methods}} for {{Hidden Markov Models}}: {{Theory}} and {{Experiments}} with {{Perceptron Algorithms}}},
  booktitle = {{{EMNLP}}},
  author = {Collins, M.},
  date = {2002},
  pages = {1--8},
  doi = {10.3115/1118693.1118694},
  url = {http://www.aclweb.org/anthology/W02-1001},
  abstract = {New algorithms for training tagging models, as an alternative to MEM and CRF models},
  issue = {July},
  keywords = {unread},
  annotation = {2229 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CN8G83MF/Collins - 2002 - Discrimative Training Methods for Hidden Markov Models Theory and Experiments with Perceptron Algorithms(2).pdf}
}

@inproceedings{collins2005ClauseRestructuringStatistical,
  title = {Clause Restructuring for Statistical Machine Translation},
  booktitle = {{{ACL}}},
  author = {Collins, M. and Koehn, P. and Kučerová, I.},
  date = {2005},
  pages = {531--540},
  doi = {10.3115/1219840.1219906},
  url = {http://portal.acm.org/citation.cfm?doid=1219840.1219906},
  abstract = {We describe a method for incorporating syntactic information in statistical\textbackslash nmachine translation systems. The first step of the method is to parse\textbackslash nthe source language string that is being translated. The second step\textbackslash nis to apply a series of transformations to the parse tree, effectively\textbackslash nreordering the surface string on the source language side of the\textbackslash ntranslation system. The goal of this step is to recover an underlying\textbackslash nword order that is closer to the target language word-order than\textbackslash nthe original string. The reordering approach is applied as a pre-processing\textbackslash nstep in both the training and decoding phases of a phrase-based statistical\textbackslash nMT system. We describe experiments on translation from German to\textbackslash nEnglish, showing an improvement from 25.2\% Bleu score for a baseline\textbackslash nsystem to 26.8\% Bleu score for the system with reordering, a statistically\textbackslash nsignificant improvement.},
  isbn = {1-932432-51-5},
  annotation = {615 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ESCQBQ42/Collins, Koehn, Kučerová - 2005 - Clause restructuring for statistical machine translation(2).pdf}
}

@misc{collins2011ProbabilisticContextFreeGrammars,
  title = {Probabilistic {{Context-Free Grammars}} ({{PCFGs}})},
  author = {Collins, M.},
  date = {2011},
  issn = {0387307680},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3FCKRDGQ/Collins - 2011 - Probabilistic Context-Free Grammars (PCFGs)(2).pdf}
}

@misc{collins2011StatisticalMachineTranslation,
  title = {Statistical {{Machine Translation}}: {{IBM Models}} 1 and 2},
  author = {Collins, M.},
  date = {2011},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HPF48MLJ/Collins - 2011 - Statistical Machine Translation IBM Models 1 and 2(2).pdf}
}

@misc{collins2011TaggingProblemsHidden,
  title = {Tagging {{Problems}}, and {{Hidden Markov Models}}},
  author = {Collins, M.},
  date = {2011},
  volume = {1},
  number = {1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KE6AHTQX/Collins - 2011 - Tagging Problems, and Hidden Markov Models(2).pdf}
}

@misc{collins2013ForwardBackwardAlgorithm,
  title = {The {{Forward-Backward Algorithm}}},
  author = {Collins, M.},
  date = {2013},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LR59RVL7/Collins - 2013 - The Forward-Backward Algorithm(2).pdf}
}

@misc{collins2013InsideOutsideAlgorithm,
  title = {The {{Inside-Outside Algorithm}}},
  author = {Collins, M.},
  date = {2013},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YISTQPET/Collins - 2013 - The Inside-Outside Algorithm(2).pdf}
}

@misc{collins2013LanguageModeling,
  title = {Language {{Modeling}}},
  author = {Collins, M.},
  date = {2013},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FG4PT4QS/Collins - 2013 - Language Modeling(2).pdf}
}

@misc{collins2013LexicalizedProbabilisticContextFree,
  title = {Lexicalized {{Probabilistic Context-Free Grammars Weaknesses}} of {{PCFGs}} as {{Parsing Models}}},
  author = {Collins, M.},
  date = {2013},
  number = {1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QT2QVMJS/Collins - 2013 - Lexicalized Probabilistic Context-Free Grammars Weaknesses of PCFGs as Parsing Models(2).pdf}
}

@misc{collins2013LogLinearModels,
  title = {Log-{{Linear Models}}},
  author = {Collins, M.},
  date = {2013},
  volume = {1},
  number = {1},
  eprint = {1504.02789v1},
  eprinttype = {arxiv},
  doi = {10.1007/978-1-4614-2299-0},
  abstract = {Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers a few seconds beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger. Anticipation requires modeling the driver’s action space, events inside the vehicle such as their head movements, and also the outside environment. Performing this joint modeling makes anticipation a challenging problem. In this work we anticipate driving maneuvers a few seconds before they occur. For this purpose we equip a car with cameras and a computing device to capture the context from both inside and outside of the car. We represent the context with expressive features and propose an Autoregressive Input-Output HMM to model the contextual information. We evaluate our approach on a diverse data set with 1180 miles of natural freeway and city driving and show that we can anticipate maneuvers 3.5 seconds before they occur with over 80\% F1-score. Our computation time during inference is under 3.6 milliseconds.},
  archiveprefix = {arXiv},
  isbn = {9781461422990},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HN4FVIG8/Collins - 2013 - Log-Linear Models(2).pdf}
}

@misc{collins2013LogLinearModelsMEMMs,
  title = {Log-{{Linear Models}}, {{MEMMs}}, and {{CRFs}}},
  author = {Collins, M.},
  date = {2013},
  abstract = {Throughout this note I'll use underline to denote vectors. For example, w ∈ R d will be a vector with components w 1 , w 2 , . . . w d . We use exp(x) for the exponential function, i.e., exp(x) = e x . 2 Log-linear models We have sets X and Y: we will assume that Y is a finite set. Our goal is to build a model that estimates the conditional probability p(y|x) of a label y ∈ Y given an input x ∈ X . For example, x might be a word, and y might be a candidate part-of-speech (noun, verb, preposition etc.) for that word. We have a feature-vector definition φ : X × Y → R d . We also assume a parameter vector w ∈ R d . Given these definitions, log-linear models take the following form: p(y|x; w) =},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JQXGHB8G/Collins - 2013 - Log-Linear Models, MEMMs, and CRFs(2).pdf}
}

@misc{collins2013NaiveBayesModel,
  title = {The {{Naive Bayes Model}}, {{Maximum-Likelihood Estimation}}, and the {{EM Algorithm}}},
  author = {Collins, M.},
  date = {2013},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y2MJSVBW/Collins - 2013 - The Naive Bayes Model, Maximum-Likelihood Estimation, and the EM Algorithm(2).pdf}
}

@misc{collins2013PhraseBasedTranslationModels,
  title = {Phrase-{{Based Translation Models}}},
  author = {Collins, M.},
  date = {2013},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZEMCNUJN/Collins - 2013 - Phrase-Based Translation Models(2).pdf}
}

@misc{cong2020FewShotEventDetection,
  title = {Few-{{Shot Event Detection}} with {{Prototypical Amortized Conditional Random Field}}},
  author = {Cong, Xin and Cui, Shiyao and Yu, Bowen and Liu, Tingwen and Wang, Yubin and Wang, Bin},
  date = {2020-12-03},
  eprint = {2012.02353},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.02353},
  urldate = {2020-12-07},
  abstract = {Event Detection, a fundamental task of Information Extraction, tends to struggle when it needs to recognize novel event types with a few samples, i.e. Few-Shot Event Detection (FSED). Previous identify-then-classify paradigm attempts to solve this problem in the pipeline manner but ignores the trigger discrepancy between event types, thus suffering from the error propagation. In this paper, we present a novel unified joint model which converts the task to a few-shot tagging problem with a double-part tagging scheme. To this end, we first design the Prototypical Amortized Conditional Random Field (PA-CRF) to model the label dependency in the few-shot scenario, which builds prototypical amortization networks to approximate the transition scores between labels based on the label prototypes. Then Gaussian distribution is introduced for the modeling of the transition scores in PA-CRF to alleviate the uncertain estimation resulting from insufficient data. We conduct experiments on the benchmark dataset FewEvent and the experimental results show that the tagging based methods are better than existing pipeline and joint learning methods. In addition, the proposed PA-CRF achieves the best results on the public dataset.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FRDVZBFN/Cong et al. - 2020 - Few-Shot Event Detection with Prototypical Amortiz.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JATKC8PH/2012.html}
}

@inproceedings{conia2021UnifyingCrossLingualSemantic,
  title = {Unifying {{Cross-Lingual Semantic Role Labeling}} with {{Heterogeneous Linguistic Resources}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Conia, Simone and Bacciu, Andrea and Navigli, Roberto},
  date = {2021},
  pages = {14},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MCXL7W3H/Conia et al. - Unifying Cross-Lingual Semantic Role Labeling with.pdf}
}

@article{copestake2005MinimalRecursionSemantics,
  title = {Minimal Recursion Semantics: {{An}} Introduction},
  author = {Copestake, A. and Flickinger, D. and Pollard, C. and Sag, I. A.},
  date = {2005},
  journaltitle = {Research on Language and Computation},
  volume = {3},
  number = {4},
  pages = {281--332},
  issn = {15707075},
  doi = {10.1007/s11168-006-6327-9},
  abstract = {Minimal recursion semantics (MRS) is a framework for computational semantics that is suitable for parsing and generation and that can be implemented in typed feature structure formalisms. We discuss why, in general, a semantic representation with minimal structure is desirable and illustrate how a descriptively adequate representation with a nonrecursive structure may be achieved. MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition. We have integrated MRS with a broad-coverage HPSG grammar.},
  keywords = {unread},
  annotation = {1134 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6TNNQETB/Copestake et al. - 2005 - Minimal recursion semantics An introduction(2).pdf}
}

@inproceedings{cotterell2017ProbabilisticTypologyDeep,
  title = {Probabilistic {{Typology}}: {{Deep Generative Models}} of {{Vowel Inventories}}},
  booktitle = {{{ACL}}},
  author = {Cotterell, R. and Eisner, J. M.},
  date = {2017},
  eprint = {1705.01684},
  eprinttype = {arxiv},
  doi = {10.18653/v1/P17-1109},
  url = {http://arxiv.org/abs/1705.01684},
  abstract = {Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-75-3},
  annotation = {20 citations (Semantic Scholar/DOI) [2021-03-26] 20 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S6DSAFAK/Cotterell, Eisner - 2017 - Probabilistic Typology Deep Generative Models of Vowel Inventories(2).pdf}
}

@inproceedings{cotterell2018AreAllLanguages,
  title = {Are {{All Languages Equally Hard}} to {{Language-Model}}?},
  booktitle = {{{NAACL}}},
  author = {Cotterell, R. and Mielke, S. J. and Eisner, J. M. and Roark, B.},
  date = {2018},
  eprint = {1806.03743v1},
  eprinttype = {arxiv},
  pages = {536--541},
  doi = {10.18653/v1/n18-2085},
  archiveprefix = {arXiv},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N9H2G64H/Cotterell et al. - 2018 - Are All Languages Equally Hard to Language-Model(2).pdf}
}

@article{cour2011LearningPartialLabels,
  title = {Learning from Partial Labels},
  author = {Cour, T. and Sapp, B. and Taskar, B.},
  date = {2011},
  journaltitle = {JMLR},
  volume = {12},
  pages = {1501--1536},
  issn = {15324435},
  doi = {10.1007/978-0-387-31439-6_100092},
  abstract = {We address the problem of partially-labeled multiclass classification, where instead of a single label per instance, the algorithm is given a candidate set of labels, only one of which is correct. Our setting is motivated by a common scenario in many image and video collections, where only partial access to labels is available. The goal is to learn a classifier that can disambiguate the partiallylabeled training instances, and generalize to unseen data. We define an intuitive property of the data distribution that sharply characterizes the ability to learn in this setting and show that effective learning is possible even when all the data is only partially labeled. Exploiting this property of the data, we propose a convex learning formulation based on minimization of a loss function appropriate for the partial label setting. We analyze the conditions under which our loss function is asymptotically consistent, as well as its generalization and transductive performance. We apply our framework to identifying faces culled from web news sources and to naming characters in TV series and movies; in particular, we annotated and experimented on a very large video data set and achieve 6\% error for character naming on 16 episodes of the TV series Lost. © 2011 Timothee Cour, Ben Sapp and Ben Taskar.},
  annotation = {247 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UUIXR77S/Cour, Sapp, Taskar - 2011 - Learning from partial labels(2).pdf}
}

@inproceedings{creutz2002UnsupervisedDiscoveryMorphemes,
  title = {Unsupervised {{Discovery}} of {{Morphemes}}},
  booktitle = {Workshop of the {{ACL Special Interest Group}} in {{Computational Phonology}}},
  author = {Creutz, M. and Lagus, K.},
  date = {2002},
  eprint = {cs/0205057},
  eprinttype = {arxiv},
  pages = {21--30},
  url = {http://arxiv.org/abs/cs/0205057},
  abstract = {We present two methods for unsupervised segmentation of words into morpheme-like units. The model utilized is especially suited for languages with a rich morphology, such as Finnish. The first method is based on the Minimum Description Length (MDL) principle and works online. In the second method, Maximum Likelihood (ML) optimization is used. The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis. Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current state-of-the-art system.},
  archiveprefix = {arXiv},
  issue = {July},
  annotation = {354 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TPBU89JT/Creutz, Lagus - 2002 - Unsupervised Discovery of Morphemes(2).pdf}
}

@inproceedings{croce2019AuditingDeepLearning,
  title = {Auditing {{Deep Learning}} Processes through {{Kernel-based Explanatory Models}}},
  booktitle = {{{EMNLP}}},
  author = {Croce, D. and Rossini, D. and Basili, R.},
  date = {2019},
  pages = {4037--4046},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KAUV2HT5/Croce, Rossini, Basili - 2019 - Auditing Deep Learning processes through Kernel-based Explanatory Models(2).pdf}
}

@inproceedings{csordas2021DevilDetailSimple,
  title = {The {{Devil}} Is in the {{Detail}}: {{Simple Tricks Improve Systematic Generalization}} of {{Transformers}}},
  booktitle = {{{EMNLP}}},
  author = {Csordás, Róbert and Irie, Kazuki and Schmidhuber, Juergen},
  date = {2021},
  pages = {16},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7VKS7374/Csordás et al. - The Devil is in the Detail Simple Tricks Improve .pdf}
}

@inproceedings{csordas2022NeuralDataRouter,
  title = {The {{Neural Data Router}}: {{Adaptive Control Flow}} in {{Transformers Improves Systematic Generalization}}},
  shorttitle = {The {{Neural Data Router}}},
  booktitle = {{{ICLR}}},
  author = {Csordás, Róbert and Irie, Kazuki and Schmidhuber, Jürgen},
  date = {2022-05-05},
  number = {arXiv:2110.07732},
  eprint = {2110.07732},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.07732},
  urldate = {2022-06-03},
  abstract = {Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100\% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4IQ57LSX/Csordás et al. - 2022 - The Neural Data Router Adaptive Control Flow in T.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ER5NPXJD/2110.html}
}

@inproceedings{ct2021,
  title = {Semantic {{Re-Tuning}} with {{Contrastive Tension}}},
  booktitle = {{{ICLR}}},
  author = {Carlsson, Fredrik and Gogoulou, Evangelia and Ylipaa, Erik and Gyllensten, Amaru Cuba and Sahlgren, Magnus},
  date = {2021},
  pages = {21},
  abstract = {Extracting semantically useful natural language sentence representations from pre-trained deep neural networks such as Transformers remains a challenge. We first demonstrate that pre-training objectives impose a significant task bias onto the final layers of models, with a layer-wise survey of the Semantic Textual Similarity (STS) correlations for multiple common Transformer language models. We then propose a new self-supervised method called Contrastive Tension (CT) to counter such biases. CT frames the training objective as a noise-contrastive task between the final layer representations of two independent models, in turn making the final layer representations suitable for feature extraction. Results from multiple common unsupervised and supervised STS tasks indicate that CT outperforms previous State Of The Art (SOTA), and when combining CT with supervised data we improve upon previous SOTA results with large margins.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7DAZY8KE/Carlsson et al. - 2021 - SEMANTIC RE-TUNING WITH CONTRASTIVE TENSION.pdf}
}

@article{culkin2021IterativeParaphrasticAugmentation,
  title = {Iterative {{Paraphrastic Augmentation}} with {{Discriminative Span-based Alignment}}},
  author = {Culkin, R. and Hu, J. E. and Stengel-Eskin, E. and Qin, G. and Van Durme, B.},
  date = {2021},
  journaltitle = {TACL},
  volume = {9},
  pages = {1--16},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6YC5GZFZ/TACL_a_00380-Culkin_Proof1.pdf}
}

@misc{curation2020CurationCorpusBase,
  title = {Curation {{Corpus Base}}},
  author = {Curation},
  date = {2020},
  keywords = {unread}
}

@article{dahab2008TextOntoExAutomaticOntology,
  title = {{{TextOntoEx}}: {{Automatic}} Ontology Construction from Natural {{English}} Text},
  author = {Dahab, M. Y. and Hassan, H. A. and Rafea, A.},
  date = {2008},
  journaltitle = {Expert Systems with Applications},
  volume = {34},
  number = {2},
  pages = {1474--1480},
  issn = {09574174},
  doi = {10.1016/j.eswa.2007.01.043},
  abstract = {Most of existing ontologies construction tools support construction of ontological relations (e.g., taxonomy, equivalence, etc.) but they do not support construction of domain relations, non-taxonomic conceptual relationships (e.g., causes, caused by, treat, treated by, has-member, contain, material-of, operated-by, controls, etc.). Domain relations are found mainly in text sources. TextOntoEx constructs ontology from natural domain text using semantic pattern-based approach. TextOntoEx is a chain between linguistic analysis and ontology engineering. TextOntoEx analyses natural domain text to extract candidate relations and then maps them into meaning representation to facilitate constructing ontology. The paper explains this approach in more details and discusses some experiments on deriving ontology from natural text. © 2007.},
  keywords = {unread},
  annotation = {119 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2HIGTQPH/Dahab, Hassan, Rafea - 2008 - TextOntoEx Automatic ontology construction from natural English text(2).pdf}
}

@inproceedings{dai2015SemisupervisedSequenceLearning,
  title = {Semi-Supervised {{Sequence Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Dai, A. M. and Le, Q. V.},
  date = {2015},
  eprint = {414454},
  eprinttype = {pmid},
  pages = {1--10},
  issn = {10495258},
  url = {http://arxiv.org/abs/1511.01432},
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  archiveprefix = {arXiv},
  annotation = {709 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z93ZB6NP/Dai, Le - 2015 - Semi-supervised Sequence Learning(2).pdf}
}

@inproceedings{dai2017DeformableConvolutionalNetworks,
  title = {Deformable {{Convolutional Networks}}},
  booktitle = {{{ICCV}}},
  author = {Dai, J. and Qi, H. and Xiong, Y. and Li, Y. and Zhang, G. and Hu, H. and Wei, Y.},
  date = {2017},
  eprint = {23459267},
  eprinttype = {pmid},
  issn = {0004-6361},
  doi = {10.1051/0004-6361/201527329},
  url = {http://arxiv.org/abs/1703.06211},
  abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation. The code would be released.},
  archiveprefix = {arXiv},
  isbn = {2-00-401243-9},
  annotation = {144 citations (Semantic Scholar/DOI) [2021-03-26] 1434 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H9HMGMM4/Dai et al. - 2017 - Deformable Convolutional Networks(2).pdf}
}

@inproceedings{dai2019TransformerXLAttentiveLanguage,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  booktitle = {{{ACL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  date = {2019-06-02},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.02860},
  urldate = {2020-11-11},
  archiveprefix = {arXiv},
  annotation = {898 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2WZVCLRF/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/home/hiaoxui/.local/share/zotero_files/storage/RF7BGPER/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/home/hiaoxui/.local/share/zotero_files/storage/76C4WBNU/1901.html;/home/hiaoxui/.local/share/zotero_files/storage/EEC7VLT4/1901.html}
}

@inproceedings{das2011SemisupervisedFramesemanticParsing,
  title = {Semi-Supervised Frame-Semantic Parsing for Unknown Predicates},
  booktitle = {{{ACL-HLT}}},
  author = {Das, D. and Smith, N. A.},
  date = {2011},
  pages = {1435--1444},
  abstract = {We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15\% absolute improvement in frame identification accuracy and over 13\% absolute improvement in full frame-semantic parsing F 1 score on a blind test set, over a state-of-the-art supervised baseline. © 2011 Association for Computational Linguistics.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UZQRRLTH/Das, Smith - 2011 - Semi-supervised frame-semantic parsing for unknown predicates(2).pdf}
}

@article{das2014FrameSemanticParsing,
  title = {Frame-{{Semantic Parsing}}},
  author = {Das, D. and Chen, D. and Martins, A. F. T. and Scneider, N. and Smith, N. A.},
  date = {2014},
  journaltitle = {Computational Linguistics},
  volume = {40},
  number = {1},
  issn = {04194217},
  doi = {10.1162/COLI},
  abstract = {We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in nat- ural language texts. The architecture com- bines various linguistically-motivated clas- sification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually de- fine linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost en- tirely unsupervised and completely language- independent; it relies on few language re- sources and is thus suitable for a large num- ber of languages. Furthermore, unlike much recent work, our approach can identify ex- pressions of various types and syntactic con- structions. We demonstrate a significant im- provement in identification accuracy, com- pared with less sophisticated baselines.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z6DQJTTF/Das et al. - 2014 - Frame-Semantic Parsing(2).pdf}
}

@inproceedings{dasigi2021DatasetInformationSeekingQuestions,
  title = {A {{Dataset}} of {{Information-Seeking Questions}} and {{Answers Anchored}} in {{Research Papers}}},
  booktitle = {{{NAACL}}},
  author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
  date = {2021-05-06},
  eprint = {2105.03011},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.03011},
  urldate = {2021-12-07},
  abstract = {Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/23K6L6EJ/Dasigi et al. - 2021 - A Dataset of Information-Seeking Questions and Ans.pdf;/home/hiaoxui/.local/share/zotero_files/storage/NP3NYYFL/2105.html}
}

@inproceedings{daumeiii2004PhrasebasedHMMApproach,
  title = {A Phrase-Based {{HMM}} Approach to Document/Abstract Alignment.},
  booktitle = {{{EMNLP}}},
  author = {Daumé III, H. and Marcu, D.},
  date = {2004},
  pages = {119--126},
  abstract = {We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical sum-marization systems that can be trained on large cor-pora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outper-forms both the Cut \& Paste alignment model (Jing, 2002) and models developed in the context of ma-chine translation (Brown et al., 1993).},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F3A2KBGU/Daumé III, Marcu - 2004 - A phrase-based HMM approach to documentabstract alignment(2).pdf}
}

@inproceedings{daumeiii2007FrustratinglyEasyDomain,
  title = {Frustratingly {{Easy Domain Adaptation}}},
  booktitle = {{{ACL}}},
  author = {Daumé III, H.},
  date = {2007},
  eprint = {0907.1815},
  eprinttype = {arxiv},
  pages = {256--263},
  url = {http://arxiv.org/abs/0907.1815},
  abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
  archiveprefix = {arXiv},
  issue = {June},
  annotation = {1487 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/787QKURD/Daumé III - 2007 - Frustratingly Easy Domain Adaptation(2).pdf}
}

@article{daumeiii2009SearchbasedStructuredPrediction,
  title = {Search-Based Structured Prediction},
  author = {Daumé III, H. and Langford, J. and Marcu, D.},
  date = {2009},
  journaltitle = {Machine Learning},
  volume = {75},
  number = {3},
  pages = {297--325},
  issn = {08856125},
  doi = {10.1007/s10994-009-5106-x},
  abstract = {We present Searn, an algorithm for integrating search and learning to solve complex structured prediction problems such as those that occur in natural language, speech, computational biology, and vision. Searn is a meta-algorithm that transforms these complex problems into simple classification problems to which any binary classifier may be applied. Unlike current algorithms for structured learning that require decomposition of both the loss function and the feature functions over the predicted structure, Searn is able to learn prediction functions for any loss function and any class of features. Moreover, Searn comes with a strong, natural theoretical guarantee: good performance on the derived classification problems implies good performance on the structured prediction problem. © 2009 Springer Science+Business Media, LLC.},
  annotation = {522 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZQ4W22MQ/Daumé III, Langford, Marcu - 2009 - Search-based structured prediction(2).pdf}
}

@inproceedings{dawborn2014DocrepLightweightEfficient,
  title = {Docrep: {{A}} Lightweight and Efficient Document Representation Framework},
  booktitle = {{{COLING}}},
  author = {Dawborn, Tim and Curran, James R},
  date = {2014},
  pages = {10},
  abstract = {Modelling linguistic phenomena requires highly structured and complex data representations. Document representation frameworks (DRFs) provide an interface to store and retrieve multiple annotation layers over a document. Researchers face a difficult choice: using a heavy-weight DRF or implement a custom DRF. The cost is substantial, either learning a new complex system, or continually adding features to a home-grown system that risks overrunning its original scope.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UW4DI5EH/Dawborn and Curran - docrep A lightweight and efficient document repre.pdf}
}

@misc{day2021MessagePassingNeural,
  title = {Message {{Passing Neural Processes}}},
  author = {Day, B. and Cangea, C. and Jamasb, A. R. and Liò, P.},
  date = {2021},
  langid = {english},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TWSFSYVP/Authors - Message Passing Neural Processes.pdf}
}

@inproceedings{daza2019TranslateLabelEncoderDecoder,
  title = {Translate and {{Label}}! {{An Encoder-Decoder Approach}} for {{Cross-lingual Semantic Role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Daza, A. and Frank, A.},
  date = {2019},
  eprint = {1908.11326},
  eprinttype = {arxiv},
  pages = {603--615},
  url = {http://arxiv.org/abs/1908.11326},
  abstract = {We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our model does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {8 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/22F92S7H/Daza, Frank - 2019 - Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling(2).pdf}
}

@inproceedings{dehghani2019UniversalTransformers,
  title = {Universal {{Transformers}}},
  booktitle = {{{ICLR}}},
  author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Łukasz},
  date = {2019-03-05},
  eprint = {1807.03819},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1807.03819},
  urldate = {2021-04-21},
  abstract = {Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {238 citations (Semantic Scholar/arXiv) [2021-04-21]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J3MYJKSL/Dehghani et al. - 2019 - Universal Transformers.pdf;/home/hiaoxui/.local/share/zotero_files/storage/GPVCNBJ2/1807.html}
}

@inproceedings{dejong2022MentionMemoryIncorporating,
  title = {Mention {{Memory}}: Incorporating Textual Knowledge into {{Transformers}} through Entity Mention Attention},
  booktitle = {{{ICLR}}},
  author = {de Jong, Michiel and Zemlyanskiy, Yury and FitzGerald, Nicholas and Sha, Fei and Cohen, William},
  options = {useprefix=true},
  date = {2022},
  pages = {15},
  abstract = {Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with “mention memory”, a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KXCH4UCT/de Jong et al. - 2022 - MENTION MEMORY INCORPORATING TEXTUAL KNOWLEDGE IN.pdf}
}

@inproceedings{demarcken1996LinguisticStructureComposition,
  title = {Linguistic {{Structure}} as {{Composition}} and {{Perturbation}}},
  booktitle = {{{ACL}}},
  author = {de Marcken, C. G.},
  options = {useprefix=true},
  date = {1996},
  pages = {335--341},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F9XMLJSH/de Marcken - 1996 - Linguistic Structure as Composition and Perturbation(2).pdf}
}

@thesis{demarcken1996UnsupervisedLanguageAcquisition,
  title = {Unsupervised {{Language Acquisition}}},
  author = {de Marcken, C. G.},
  options = {useprefix=true},
  date = {1996},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YP2RVQ7Y/de Marcken - 1996 - Unsupervised Language Acquisition(2).pdf}
}

@article{dempster1977MaximumLikelihoodIncomplete,
  title = {Maximum {{Likelihood}} from {{Incomplete Data}} via the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  date = {1977},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {39},
  number = {1},
  pages = {1--22},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J57J73P9/Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via the EM.pdf}
}

@misc{demszky2018TransformingQuestionAnswering,
  title = {Transforming {{Question Answering Datasets}}},
  author = {Demszky, D. and Guu, K.},
  date = {2018},
  eprint = {1809.02922v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2VMAAT3Y/Demszky, Guu - 2018 - Transforming Question Answering Datasets(2).pdf}
}

@inproceedings{denero2008SamplingAlignmentStructure,
  title = {Sampling Alignment Structure under a {{Bayesian}} Translation Model},
  booktitle = {{{EMNLP}}},
  author = {DeNero, J. and Bouchard-Côté, A. and Klein, D.},
  date = {2008},
  pages = {314--323},
  doi = {10.3115/1613715.1613758},
  url = {http://dl.acm.org/citation.cfm?id=1613758},
  abstract = {We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previ- ous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrase- based translation systems.},
  issue = {October},
  keywords = {unread},
  annotation = {102 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q83FVKQ7/DeNero, Bouchard-Côté, Klein - 2008 - Sampling alignment structure under a Bayesian translation model(2).pdf}
}

@inproceedings{denero2009FastConsensusDecoding,
  title = {Fast Consensus Decoding over Translation Forests},
  booktitle = {{{ACL-IJCNLP}}},
  author = {DeNero, J. and Chiang, D. and Knight, K.},
  date = {2009},
  volume = {3},
  number = {1},
  pages = {1--177},
  issn = {0149645X},
  doi = {10.1109/MWSYM.2003.1211046},
  abstract = {The minimum Bayes risk (MBR) decoding objective\textbackslash nimproves BLEU scores for machine translation\textbackslash noutput relative to the standard Viterbi objective\textbackslash nof maximizing model score. However,\textbackslash nMBR targeting BLEU is prohibitively slow to optimize\textbackslash nover k-best lists for large k. In this paper,\textbackslash nwe introduce and analyze an alternative to\textbackslash nMBR that is equally effective at improving performance,\textbackslash nyet is asymptotically faster—running\textbackslash n80 times faster than MBR in experiments with\textbackslash n1000-best lists. Furthermore, our fast decoding\textbackslash nprocedure can select output sentences based on\textbackslash ndistributions over entire forests of translations, in\textbackslash naddition to k-best lists. We evaluate our procedure\textbackslash non translation forests from two large-scale,\textbackslash nstate-of-the-art hierarchical machine translation\textbackslash nsystems. Our forest-based decoding objective\textbackslash nconsistently outperforms k-best list MBR, giving\textbackslash nimprovements of up to 1.0 BLEU.\textbackslash n1 Introduction\textbackslash nIn statistical},
  isbn = {0-7803-7694-3},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FHVHN9K8/DeNero, Chiang, Knight - 2009 - Fast consensus decoding over translation forests(2).pdf}
}

@inproceedings{deng2017RandomForestBased,
  title = {The {{Random Forest Based Detection}} of {{Shadowsock}}'s {{Traffic}}},
  booktitle = {{{IHMSC}}},
  author = {Deng, Z. and Liu, Z. and Chen, Z. and Guo, Y.},
  date = {2017},
  pages = {75--78},
  doi = {10.1109/IHMSC.2017.132},
  url = {http://ieeexplore.ieee.org/document/8048116/},
  abstract = {—With the development of anonymous communi-cation technology, it has led to the fact that the network monitoring is becoming more and more difficult. If the anonymous traffic can be effectively identified, the abuse of such technology can be prevented. Since the study of machine learning is rapidly developing these years, this paper applies the Random Forest Algorithm ---a semi-supervised learning method ---into the traffic detection of Shadowsocks. We can get over 85\% detection accuracy rate in our experiments after applying Random Forest Algorithm by collecting train set, gathering features, training models and predicting results. With the scale of train set and test set increase, the detection accuracy rate gradually increases until it becomes constant. We will make several adjustments on train set, test set and feature set to reduce the false alarm rate and false rate when detecting.},
  isbn = {978-1-5386-3021-1},
  annotation = {13 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DF3H2HQE/Deng et al. - 2017 - The Random Forest Based Detection of Shadowsock's Traffic(2).pdf}
}

@inproceedings{deng2018LatentAlignmentVariational,
  title = {Latent Alignment and Variational Attention},
  booktitle = {{{NeurIPS}}},
  author = {Deng, Y. and Kim, Y. and Chiu, J. and Guo, D. and Rush, A. M.},
  date = {2018},
  eprint = {1807.03756},
  eprinttype = {arxiv},
  pages = {9712--9724},
  issn = {10495258},
  abstract = {Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training. On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HLZBVFYX/Deng et al. - 2018 - Latent alignment and variational attention(2).pdf}
}

@misc{desmond2022NoCodeLowCodeParadigm,
  title = {A {{No-Code Low-Code Paradigm}} for {{Authoring Business Automations Using Natural Language}}},
  author = {Desmond, Michael and Duesterwald, Evelyn and Isahagian, Vatche and Muthusamy, Vinod},
  date = {2022-07-15},
  number = {arXiv:2207.10648},
  eprint = {2207.10648},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.10648},
  urldate = {2022-08-12},
  abstract = {Most business process automation is still developed using traditional automation technologies such as workflow engines. These systems provide domain specific languages that require both business knowledge and programming skills to effectively use. As such, business users often lack adequate programming skills to fully leverage these code oriented environments. We propose a paradigm for the construction of business automations using natural language. The approach applies a large language model to translate business rules and automations described in natural language, into a domain specific language interpretable by a business rule engine. We compare the performance of various language model configurations, across various target domains, and explore the use of constrained decoding to ensure syntactically correct generation of output.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9BF24FXR/Desmond et al. - 2022 - A No-Code Low-Code Paradigm for Authoring Business.pdf;/home/hiaoxui/.local/share/zotero_files/storage/J53GA88S/2207.html}
}

@inproceedings{dettmers20228bitOptimizersBlockwise,
  title = {8-Bit {{Optimizers}} via {{Block-wise Quantization}}},
  booktitle = {{{ICLR}}},
  author = {Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  date = {2022},
  eprint = {2110.02861},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.02861},
  urldate = {2022-02-24},
  abstract = {Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization compared to plain stochastic gradient descent but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-source our 8-bit optimizers as a drop-in replacement that only requires a two-line code change.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7E6LPCXJ/Dettmers et al. - 2021 - 8-bit Optimizers via Block-wise Quantization.pdf;/home/hiaoxui/.local/share/zotero_files/storage/Q54Z4DAV/2110.html}
}

@inproceedings{deutsch2018DistributionalOrthographicAggregation,
  title = {A {{Distributional}} and {{Orthographic Aggregation Model}} for {{English Derivational Morphology}}},
  booktitle = {{{ACL}}},
  author = {Deutsch, D. and Hewitt, J. and Roth, D.},
  date = {2018},
  pages = {1--10},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TFWPK7SK/Deutsch, Hewitt, Roth - 2018 - A Distributional and Orthographic Aggregation Model for English Derivational Morphology(2).pdf}
}

@inproceedings{devlin2014FastRobustNeural,
  title = {Fast and {{Robust Neural Network Joint Models}} for {{Statistical Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Devlin, J. and Zbib, R. and Huang, Z. and Lamar, T. and Schwartz, R. and Makhoul, J.},
  date = {2014},
  pages = {1370--1380},
  doi = {10.3115/v1/p14-1129},
  abstract = {Recent work has shown success in us-ing neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexi-calized and can be integrated into any MT decoder. We also present several varia-tions of the NNJM which provide signif-icant additive improvements. Although the model is quite simple, it yields strong empirical results. On the NIST OpenMT12 Arabic-English condi-tion, the NNJM features produce a gain of +3.0 BLEU on top of a powerful, feature-rich baseline which already includes a target-only NNLM. The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chi-ang's (2007) original Hiero implementa-tion. Additionally, we describe two novel tech-niques for overcoming the historically high cost of using NNLM-style models in MT decoding. These techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM.},
  keywords = {unread},
  annotation = {492 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7SU3HKM9/Devlin et al. - 2014 - Fast and Robust Neural Network Joint Models for Statistical Machine Translation(2).pdf}
}

@inproceedings{diab2002UnsupervisedMethodWord,
  title = {An Unsupervised Method for Word Sense Tagging Using Parallel Corpora},
  booktitle = {{{ACL}}},
  author = {Diab, M. and Resnik, P.},
  date = {2002},
  doi = {10.3115/1073083.1073126},
  abstract = {With an increasing number of languages making their way to our desktops everyday via the Internet, researchers have come to realize the lack of linguistic knowledge resources for scarcely represented/studied languages. In an attempt to bootstrap some of the required linguistic resources for some of those languages, this paper presents an unsupervised method for automatic multilingual word sense tagging using parallel corpora. The method is evaluated on the English Brown corpus and its translation into three different languages: French, German and Spanish. A preliminary evaluation of the proposed method yielded results of up to 79\% accuracy rate for the English data on 81.8\% of the SemCor manually tagged data.},
  annotation = {274 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/647TD6KP/Diab, Resnik - 2002 - An unsupervised method for word sense tagging using parallel corpora(2).pdf}
}

@inproceedings{ding2019GreedyStrategyWorks,
  title = {Greedy Strategy Works for K-Center Clustering with Outliers and Coreset Construction},
  booktitle = {Leibniz {{International Proceedings}} in {{Informatics}}},
  author = {Ding, H. and Yu, H. and Wang, Z.},
  date = {2019},
  eprint = {1901.08219},
  eprinttype = {arxiv},
  issn = {18688969},
  doi = {10.4230/LIPIcs.ESA.2019.40},
  abstract = {We study the problem of k-center clustering with outliers in arbitrary metrics and Euclidean space. Though a number of methods have been developed in the past decades, it is still quite challenging to design quality guaranteed algorithm with low complexity for this problem. Our idea is inspired by the greedy method, Gonzalez’s algorithm, for solving the problem of ordinary k-center clustering. Based on some novel observations, we show that this greedy strategy actually can handle k-center clustering with outliers efficiently, in terms of clustering quality and time complexity. We further show that the greedy approach yields small coreset for the problem in doubling metrics, so as to reduce the time complexity significantly. Our algorithms are easy to implement in practice. We test our method on both synthetic and real datasets. The experimental results suggest that our algorithms can achieve near optimal solutions and yield lower running times comparing with existing methods.},
  archiveprefix = {arXiv},
  isbn = {978-3-95977-124-5},
  annotation = {7 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/73LH76FF/Ding, Yu, Wang - 2019 - Greedy strategy works for k-center clustering with outliers and coreset construction(2).pdf}
}

@inproceedings{ding2021AttentionRankUnsupervisedKeyphrase,
  title = {{{AttentionRank}}: {{Unsupervised Keyphrase Extraction}} Using {{Self}} and {{Cross Attentions}}},
  booktitle = {{{EMNLP}}},
  author = {Ding, Haoran and Luo, Xiao},
  date = {2021},
  pages = {10},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BJHITJ6Y/Ding and Luo - AttentionRank Unsupervised Keyphrase Extraction u.pdf}
}

@inproceedings{dobrovolskii2021WordLevelCoreferenceResolution,
  title = {Word-{{Level Coreference Resolution}}},
  booktitle = {{{EMNLP}}},
  author = {Dobrovolskii, Vladimir},
  date = {2021-09-09},
  eprint = {2109.04127},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.04127},
  urldate = {2021-09-12},
  abstract = {Recent coreference resolution models rely heavily on span representations to find coreference links between word spans. As the number of spans is \$O(n\^2)\$ in the length of text and the number of potential links is \$O(n\^4)\$, various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider coreference links between individual words rather than word spans and then reconstruct the word spans. This reduces the complexity of the coreference model to \$O(n\^2)\$ and allows it to consider all potential mentions without pruning any of them out. We also demonstrate that, with these changes, SpanBERT for coreference resolution will be significantly outperformed by RoBERTa. While being highly efficient, our model performs competitively with recent coreference resolution systems on the OntoNotes benchmark.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-12]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H5E6DI49/Dobrovolskii - 2021 - Word-Level Coreference Resolution.pdf;/home/hiaoxui/.local/share/zotero_files/storage/PBP5EUMQ/2109.html}
}

@inproceedings{dodge2019RNNArchitectureLearning,
  title = {{{RNN Architecture Learning}} with {{Sparse Regularization}}},
  booktitle = {{{EMNLP}}},
  author = {Dodge, J. and Schwartz, R. and Peng, H. and Smith, N. A.},
  date = {2019},
  eprint = {1909.03011},
  eprinttype = {arxiv},
  abstract = {Neural models for NLP typically use large numbers of parameters to reach state-of-the-art performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learning sparse, parameter-efficient NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018), a family of models that is closely connected to weighted finite-state automata (WFSAs). We take advantage of rational RNNs' natural grouping of the weights, so the group lasso penalty directly removes WFSA states, substantially reducing the number of parameters in the model. Our experiments on a number of sentiment analysis datasets, using both GloVe and BERT embeddings, show that our approach learns neural structures which have fewer parameters without sacrificing performance relative to parameter-rich baselines. Our method also highlights the interpretable properties of rational RNNs. We show that sparsifying such models makes them easier to visualize, and we present models that rely exclusively on as few as three WFSAs after pruning more than 90\% of the weights. We publicly release our code.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BKZNN62W/Dodge et al. - 2019 - RNN Architecture Learning with Sparse Regularization(2).pdf}
}

@inproceedings{dodge2019ShowYourWork,
  title = {Show {{Your Work}}: {{Improved Reporting}} of {{Experimental Results}}},
  booktitle = {{{EMNLP}}},
  author = {Dodge, J. and Gururangan, S. and Card, D. and Schwartz, R. and Smith, N. A.},
  date = {2019},
  eprint = {1909.03004},
  eprinttype = {arxiv},
  pages = {2185--2194},
  url = {http://arxiv.org/abs/1909.03004},
  abstract = {Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {78 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F7U5AB4H/Dodge et al. - 2019 - Show Your Work Improved Reporting of Experimental Results(2).pdf}
}

@misc{doersch2016TutorialVariationalAutoencoders,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, C.},
  date = {2016},
  eprint = {1606.05908v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QSD6WTR8/Doersch - 2016 - Tutorial on Variational Autoencoders(2).pdf}
}

@inproceedings{dohan2022LanguageModelCascades,
  title = {Language {{Model Cascades}}},
  booktitle = {{{ICML}}},
  author = {Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A. and Sohl-dickstein, Jascha and Murphy, Kevin and Sutton, Charles},
  date = {2022-07-28},
  eprint = {2207.10342},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.10342},
  urldate = {2022-08-12},
  abstract = {Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UZ2H5MFH/Dohan et al. - 2022 - Language Model Cascades.pdf;/home/hiaoxui/.local/share/zotero_files/storage/LI6Q86QN/2207.html}
}

@book{donaldson1978ChildrenMinds,
  title = {Children's {{Minds}}},
  author = {Donaldson, M. C.},
  date = {1978},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EVW5NEDX/Donaldson - 1978 - Children's Minds.pdf}
}

@inproceedings{dong2015QuestionAnsweringFreebase,
  title = {Question {{Answering}} over {{Freebase}} with {{Multi-Column Convolutional Neural Networks}}},
  booktitle = {{{ACL}}},
  author = {Dong, L. and Wei, F. and Zhou, M. and Xu, K.},
  date = {2015},
  eprint = {1683577},
  eprinttype = {pmid},
  pages = {260--269},
  abstract = {Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct exten- sive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.},
  isbn = {978-1-941643-72-3},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FS8IQWKX/Dong et al. - 2015 - Question Answering over Freebase with Multi-Column Convolutional Neural Networks(2).pdf}
}

@inproceedings{dong2016LanguageLogicalForm,
  title = {Language to {{Logical Form}} with {{Neural Attention}}},
  booktitle = {{{ACL}}},
  author = {Dong, L. and Lapata, M.},
  date = {2016},
  eprint = {1601.01280},
  eprinttype = {arxiv},
  pages = {33--43},
  doi = {10.18653/v1/P16-1004},
  url = {http://arxiv.org/abs/1601.01280},
  abstract = {Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-2758-5},
  annotation = {431 citations (Semantic Scholar/DOI) [2021-03-26] 431 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WQ6TDSTI/Dong, Lapata - 2016 - Language to Logical Form with Neural Attention(2).pdf}
}

@inproceedings{dong2018CoarsetoFineDecodingNeural,
  title = {Coarse-to-{{Fine Decoding}} for {{Neural Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Dong, L. and Lapata, M.},
  date = {2018},
  eprint = {1805.04793},
  eprinttype = {arxiv},
  pages = {1--12},
  url = {http://arxiv.org/abs/1805.04793},
  abstract = {Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {188 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2QA59852/Dong, Lapata - 2018 - Coarse-to-Fine Decoding for Neural Semantic Parsing(2).pdf}
}

@inproceedings{dong2019NeuralLogicMachines,
  title = {Neural Logic Machines},
  booktitle = {{{ICLR}}},
  author = {Dong, H. and Mao, J. and Lin, T. and Wang, C. and Li, L. and Zhou, D.},
  date = {2019},
  eprint = {1904.11694},
  eprinttype = {arxiv},
  abstract = {We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks-as function approximators, and logic programming-as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W3MCLUUG/Dong et al. - 2019 - Neural logic machines(2).pdf}
}

@inproceedings{dong2021MapREEffectiveSemantic,
  title = {{{MapRE}}: {{An Effective Semantic Mapping Approach}} for {{Low-resource Relation Extraction}}},
  shorttitle = {{{MapRE}}},
  booktitle = {{{EMNLP}}},
  author = {Dong, Manqing and Pan, Chunguang and Luo, Zhipeng},
  date = {2021-09-09},
  eprint = {2109.04108},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.04108},
  urldate = {2021-09-12},
  abstract = {Neural relation extraction models have shown promising results in recent years; however, the model performance drops dramatically given only a few training samples. Recent works try leveraging the advance in few-shot learning to solve the low resource problem, where they train label-agnostic models to directly compare the semantic similarities among context sentences in the embedding space. However, the label-aware information, i.e., the relation label that contains the semantic knowledge of the relation itself, is often neglected for prediction. In this work, we propose a framework considering both label-agnostic and label-aware semantic mapping information for low resource relation extraction. We show that incorporating the above two types of mapping information in both pretraining and fine-tuning can significantly improve the model performance on low-resource relation extraction tasks.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-12]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UUXSZZCU/Dong et al. - 2021 - MapRE An Effective Semantic Mapping Approach for .pdf;/home/hiaoxui/.local/share/zotero_files/storage/7VEAG3N9/2109.html}
}

@inproceedings{dou2018Data2TextStudioAutomated,
  title = {{{Data2Text Studio}} : {{Automated Text Generation}} from {{Structured Data}}},
  booktitle = {{{EMNLP}}},
  author = {Dou, L. and Qin, G. and Wang, J. and Yao, J. and Lin, C.},
  date = {2018},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3GLSZ3JC/Dou et al. - 2018 - Data2Text Studio Automated Text Generation from Structured Data(2).pdf}
}

@inproceedings{dou2019InvestigatingMetaLearningAlgorithms,
  title = {Investigating {{Meta-Learning Algorithms}} for {{Low-Resource Natural Language Understanding Tasks}}},
  booktitle = {{{EMNLP}}},
  author = {Dou, Z. and Yu, K. and Anastasopoulos, A.},
  date = {2019},
  eprint = {1908.10423},
  eprinttype = {arxiv},
  pages = {1192--1197},
  abstract = {Learning general representations of text is a fundamental problem for many natural language understanding (NLU) tasks. Previously, researchers have proposed to use language model pre-training and multi-task learning to learn robust representations. However, these methods can achieve sub-optimal performance in low-resource scenarios. Inspired by the recent success of optimization-based meta-learning algorithms, in this paper, we explore the model-agnostic meta-learning algorithm (MAML) and its variants for low-resource NLU tasks. We validate our methods on the GLUE benchmark and show that our proposed models can outperform several strong baselines. We further empirically demonstrate that the learned representations can be adapted to new tasks efficiently and effectively.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TTAP78IN/Dou, Yu, Anastasopoulos - 2019 - Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks(2).pdf}
}

@incollection{doucet2009TutorialParticleFiltering,
  title = {A Tutorial on Particle Filtering and Smoothing: Fifteen Years Later},
  booktitle = {Handbook of {{Nonlinear Filtering}}},
  author = {Doucet, A. and Johansen, A. M.},
  date = {2009},
  issn = {01677152},
  doi = {10.1.1.157.772},
  abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented},
  isbn = {978-0-19-953290-2},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QESU3G6B/Doucet, Johansen - 2009 - A tutorial on particle filtering and smoothing fifteen years later(2).pdf}
}

@article{downey2010AnalysisProbabilisticModel,
  title = {Analysis of a Probabilistic Model of Redundancy in Unsupervised Information Extraction},
  author = {Downey, Doug and Etzioni, Oren and Soderland, Stephen},
  date = {2010-07},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {174},
  number = {11},
  pages = {726--748},
  issn = {00043702},
  doi = {10.1016/j.artint.2010.04.024},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370210000688},
  urldate = {2020-09-27},
  abstract = {Unsupervised Information Extraction (UIE) is the task of extracting knowledge from text without the use of hand-labeled training examples. Because UIE systems do not require human intervention, they can recursively discover new relations, attributes, and instances in a scalable manner. When applied to massive corpora such as the Web, UIE systems present an approach to a primary challenge in artificial intelligence: the automatic accumulation of massive bodies of knowledge.},
  langid = {english},
  keywords = {unread},
  annotation = {28 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7BVYKLDU/Downey et al. - 2010 - Analysis of a probabilistic model of redundancy in.pdf}
}

@inproceedings{dozat2017DeepBiaffineAttention,
  title = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {{{ICLR}}},
  author = {Dozat, T. and Manning, C. D.},
  date = {2017},
  eprint = {1611.01734},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5KTSH7M3/Dozat, Manning - 2017 - Deep biaffine attention for neural dependency parsing(2).pdf}
}

@inproceedings{dozat2017SimplerMoreAccurate,
  title = {Simpler but {{More Accurate Semantic Dependency Parsing}}},
  booktitle = {{{ACL}}},
  author = {Dozat, T. and Manning, C. D},
  date = {2017},
  eprint = {1807.01396v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M2QYAIM3/Dozat, Manning - 2017 - Simpler but More Accurate Semantic Dependency Parsing(2).pdf}
}

@inproceedings{dredze2008ConfidenceweightedLinearClassification,
  title = {Confidence-Weighted Linear Classification},
  booktitle = {{{ICML}}},
  author = {Dredze, M. and Crammer, K. and Pereira, F.},
  date = {2008},
  pages = {264--271},
  doi = {10.1145/1390156.1390190},
  abstract = {We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and eovarianee of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training. Copyright 2008 by the author(s)/owner(s).},
  annotation = {391 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L5V8HCHA/Dredze, Crammer, Pereira - 2008 - Confidence-weighted linear classification(2).pdf}
}

@inproceedings{dror2020DeepDominanceHow,
  title = {Deep Dominance - {{How}} to Properly Compare Deep Neural Models},
  booktitle = {{{ACL}}},
  author = {Dror, R. and Shlomov, S. and Reichart, R.},
  date = {2020},
  pages = {2773--2785},
  abstract = {Comparing between Deep Neural Network (DNN) models based on their performance on unseen data is crucial for the progress of the NLP field. However, these models have a large number of hyper-parameters and, being non-convex, their convergence point depends on the random values chosen at initialization and during training. Proper DNN comparison hence requires a comparison between their empirical score distributions on unseen data, rather than between single evaluation scores as is standard for more simple, convex models. In this paper, we propose to adapt to this problem a recently proposed test for the Almost Stochastic Dominance relation between two distributions. We define the criteria for a high quality comparison method between DNNs, and show, both theoretically and through analysis of extensive experimental results with leading DNN models for sequence tagging tasks, that the proposed test meets all criteria while previously proposed methods fail to do so. We hope the test we propose here will set a new working practice in the NLP community.},
  isbn = {978-1-950737-48-2},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C7QF9KCX/Dror, Shlomov, Reichart - 2020 - Deep dominance - How to properly compare deep neural models(2).pdf}
}

@inproceedings{drozdov2019UnsupervisedLatentTree,
  title = {Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders},
  booktitle = {{{NAACL-HLT}}},
  author = {Drozdov, A. and Verga, P. and Yadav, M. and Iyyer, M. and McCallum, A.},
  date = {2019},
  eprint = {1904.02142},
  eprinttype = {arxiv},
  pages = {1129--1141},
  abstract = {We introduce deep inside-outside recursive autoencoders (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence and uses inside-outside dynamic programming to consider all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI.},
  archiveprefix = {arXiv},
  isbn = {978-1-950737-13-0},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2JIFG7S4/4645-Article Text-7684-1-10-20190707.pdf;/home/hiaoxui/.local/share/zotero_files/storage/73PMWJUG/Drozdov et al. - 2019 - Unsupervised latent tree induction with deep inside-outside recursive autoencoders(2).pdf}
}

@inproceedings{dsouza2013ClassifyingTemporalRelations,
  title = {Classifying Temporal Relations with Rich Linguistic Knowledge},
  booktitle = {{{NAACL-HLT}}},
  author = {D'Souza, J. and Ng, V.},
  date = {2013},
  pages = {918--927},
  abstract = {We examine the task of temporal relation clas- sification. Unlike existing approaches to this task, we (1) classify an event-event or event- time pair as one of the 14 temporal relations defined in the TimeBank corpus, rather than as one of the six relations collapsed from the original 14; (2) employ sophisticated linguis- tic knowledge derived from a variety of se- mantic and discourse relations, rather than fo- cusing on morpho-syntactic knowledge; and (3) leverage a novel combination of rule-based and learning-based approaches, rather than re- lying solely on one or the other. Experiments with the TimeBank corpus demonstrate that our knowledge-rich, hybrid approach yields a 15–16\% relative reduction in error over a state-of-the-art learning-based baseline sys- tem.},
  isbn = {978-1-937284-47-3},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q5CIUBG2/D'Souza, Ng - 2013 - Classifying temporal relations with rich linguistic knowledge(2).pdf}
}

@inproceedings{du2016RecurrentMarkedTemporal,
  title = {Recurrent {{Marked Temporal Point Processes}} : {{Embedding Event History}} to {{Vector}}},
  booktitle = {{{KDD}}},
  author = {Du, N. and Tech, G. and Gomez-rodriguez, M. and Tech, G.},
  date = {2016},
  pages = {1555--1564},
  doi = {10.1145/2939672.2939875},
  url = {http://dl.acm.org/citation.cfm?doid=2939672.2939875},
  abstract = {Large volumes of event data are becoming increasingly available in a wide variety of applications, such as healthcare analytics, smart cities and social network analysis. The precise time interval or the exact distance between two events carries a great deal of information about the dynamics of the underlying systems. These characteristics make such data fundamentally different from independently and identically distributed data and time-series data where time and space are treated as indexes rather than random variables. Marked temporal point processes are the mathematical framework for modeling event data with covariates. However, typical point process models often make strong assumptions about the generative processes of the event data, which may or may not reflect the reality, and the specifically fixed parametric assumptions also have restricted the expressive power of the respective processes. Can we obtain a more expressive model of marked temporal point processes? How can we learn such a model from massive data? In this paper, we propose the Recurrent Marked Temporal Point Process (RMTPP) to simultaneously model the event timings and the markers. The key idea of our approach is to view the intensity function of a temporal point process as a nonlinear function of the history, and use a recurrent neural network to automatically learn a representation of influences from the event history. We develop an efficient stochastic gradient algorithm for learning the model parameters which can readily scale up to millions of events. Using both synthetic and real world datasets, we show that, in the case where the true models have parametric specifications, RMTPP can learn the dynamics of such models without the need to know the actual parametric forms; and in the case where the true models are unknown, RMTPP can also learn the dynamics and achieve better predictive performance than other parametric alternatives based on particular prior assumptions.},
  isbn = {978-1-4503-4232-2},
  annotation = {294 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RPL8DNSK/Du et al. - 2016 - Recurrent Marked Temporal Point Processes Embedding Event History to Vector(2).pdf}
}

@inproceedings{du2019GradientDescentProvably,
  title = {Gradient {{Descent Provably Optimizes Over-parameterized Neural Networks}}},
  booktitle = {{{ICLR}}},
  author = {Du, S. S. and Zhai, X. and Poczos, B. and Singh, A.},
  date = {2019},
  eprint = {1810.02054},
  eprinttype = {arxiv},
  pages = {1--15},
  url = {http://arxiv.org/abs/1810.02054},
  abstract = {One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an \$m\$ hidden node shallow neural network with ReLU activation and \$n\$ training data, we show as long as \$m\$ is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function. Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {499 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D22FB9NU/Du et al. - 2019 - Gradient Descent Provably Optimizes Over-parameterized Neural Networks(2).pdf}
}

@inproceedings{du2020DocumentLevelEventRole,
  title = {Document-{{Level Event Role Filler Extraction}} Using {{Multi-Granularity Contextualized Encoding}}},
  booktitle = {{{ACL}}},
  author = {Du, Xinya and Cardie, Claire},
  date = {2020},
  pages = {8010--8020},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.714},
  url = {https://www.aclweb.org/anthology/2020.acl-main.714},
  urldate = {2021-06-24},
  abstract = {Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-toend neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {unread},
  annotation = {9 citations (Semantic Scholar/DOI) [2021-06-24]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CNMFKYGD/Du and Cardie - 2020 - Document-Level Event Role Filler Extraction using .pdf}
}

@inproceedings{du2021GRITGenerativeRolefiller,
  title = {{{GRIT}}: {{Generative Role-filler Transformers}} for {{Document-level Event Entity Extraction}}},
  booktitle = {{{EACL}}},
  author = {Du, Xinya and Rush, Alexander and Cardie, Claire},
  date = {2021},
  pages = {13},
  abstract = {We revisit the classic problem of documentlevel role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoderdecoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NRT8A6F7/Du et al. - GRIT Generative Role-filler Transformers for Docu.pdf}
}

@inproceedings{du2021TemplateFillingGenerative,
  title = {Template {{Filling}} with {{Generative Transformers}}},
  booktitle = {{{NAACL}}},
  author = {Du, Xinya and Rush, Alexander and Cardie, Claire},
  date = {2021},
  pages = {909--914},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.70},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.70},
  urldate = {2021-06-24},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IZIESIIU/Du et al. - 2021 - Template Filling with Generative Transformers.pdf}
}

@inproceedings{dua2019DROPReadingComprehension,
  title = {{{DROP}}: {{A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs}}},
  shorttitle = {{{DROP}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  date = {2019-04-16},
  eprint = {1903.00161},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1903.00161},
  urldate = {2020-10-28},
  abstract = {Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7\% F1 on our generalized accuracy metric, while expert human performance is 96.0\%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0\% F1.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {171 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5YGSIZU8/Dua et al. - 2019 - DROP A Reading Comprehension Benchmark Requiring .pdf;/home/hiaoxui/.local/share/zotero_files/storage/KI5VIG8P/1903.html}
}

@article{duchi2011AdaptiveSubgradientMethods,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, J. and Hazan, E. and Singer, Y.},
  date = {2011},
  journaltitle = {JMLR},
  volume = {12},
  eprint = {2868127},
  eprinttype = {pmid},
  pages = {2121--2159},
  issn = {15324435},
  doi = {10.1109/CDC.2012.6426698},
  url = {http://jmlr.org/papers/v12/duchi11a.html},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  archiveprefix = {arXiv},
  isbn = {9780982252925},
  keywords = {unread},
  annotation = {48 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B5NVWDUF/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization(2).pdf}
}

@inproceedings{dufter2019AnalyticalMethodsInterpretable,
  title = {Analytical {{Methods}} for {{Interpretable Ultradense Word Embeddings}}},
  booktitle = {{{EMNLP}}},
  author = {Dufter, P. and Schütze, H.},
  date = {2019},
  eprint = {1904.08654},
  eprinttype = {arxiv},
  pages = {1185--1191},
  url = {http://arxiv.org/abs/1904.08654},
  abstract = {Word embeddings are useful for a wide variety of tasks, but they lack interpretability. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the embeddings without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. While DensRay is very closely related to the Densifier, it can be computed in closed form, is hyperparameter-free and thus more robust than the Densifier. We evaluate the methods on lexicon induction and set-based word analogy and conclude that analytical methods such as DensRay and SVMs are preferable. For word analogy we propose a new method to solve the task which outperforms the previous state of the art by large margins.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JH97JXJX/Dufter, Schütze - 2019 - Analytical Methods for Interpretable Ultradense Word Embeddings(2).pdf}
}

@article{durrett2014JointModelEntity,
  title = {A {{Joint Model}} for {{Entity Analysis}}: {{Coreference}}, {{Typing}}, and {{Linking}}},
  shorttitle = {A {{Joint Model}} for {{Entity Analysis}}},
  author = {Durrett, Greg and Klein, Dan},
  date = {2014-12},
  journaltitle = {TACL},
  shortjournal = {TACL},
  volume = {2},
  pages = {477--490},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00197},
  url = {https://direct.mit.edu/tacl/article/43330},
  urldate = {2021-03-28},
  abstract = {We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.},
  langid = {english},
  annotation = {202 citations (Semantic Scholar/DOI) [2021-03-28]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4GLE38EW/Durrett and Klein - 2014 - A Joint Model for Entity Analysis Coreference, Ty.pdf}
}

@inproceedings{duvenaud2013StructureDiscoveryNonparametric,
  title = {Structure Discovery in Nonparametric Regression through Compositional Kernel Search},
  booktitle = {{{ICML}}},
  author = {Duvenaud, D. and Lloyd, J. R. and Grosse, R. and Tenenbaum, J. B. and Ghahramani, Z.},
  date = {2013},
  volume = {28},
  pages = {2203--2211},
  abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.},
  issue = {PART 3},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KBDFJZF9/Duvenaud et al. - 2013 - Structure discovery in nonparametric regression through compositional kernel search(2).pdf}
}

@inproceedings{dyer2015TransitionBasedDependencyParsing,
  title = {Transition-{{Based Dependency Parsing}} with {{Stack Long Short-Term Memory}}},
  booktitle = {{{ACL}}},
  author = {Dyer, C. and Ballesteros, M. and Ling, W. and Matthews, A. and Smith, N. A.},
  date = {2015},
  eprint = {1505.08075},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1505.08075},
  abstract = {We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {664 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HJ73DBHJ/Dyer et al. - 2015 - Transition-Based Dependency Parsing with Stack Long Short-Term Memory(2).pdf}
}

@inproceedings{dyer2016RecurrentNeuralNetwork,
  title = {Recurrent Neural Network Grammars},
  booktitle = {{{NAACL}}},
  author = {Dyer, C. and Kuncoro, A. and Ballesteros, M. and Smith, N. A.},
  date = {2016},
  eprint = {1602.07776},
  eprinttype = {arxiv},
  pages = {199--209},
  doi = {10.18653/v1/n16-1024},
  archiveprefix = {arXiv},
  isbn = {978-1-941643-91-4},
  keywords = {unread},
  annotation = {344 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6GFG9ERF/Dyer et al. - 2016 - Recurrent neural network grammars.pdf}
}

@inproceedings{dziedzic2019BandlimitedTrainingInference,
  title = {Band-Limited {{Training}} and {{Inference}} for {{Convolutional Neural Networks}}},
  booktitle = {{{ICML}}},
  author = {Dziedzic, A. and Paparrizos, J. and Krishnan, S. and Elmore, A. and Franklin, M.},
  date = {2019},
  pages = {1745--1754},
  url = {http://proceedings.mlr.press/v97/dziedzic19a.html},
  abstract = {The convolutional layers are core building blocks of neural network architectures. In general, a convolutional filter applies to the entire frequency spectrum of the input data. We explore artificially constraining the frequency spectra of these filters and data, called band-limiting, during training. The frequency domain constraints apply to both the feed-forward and back-propagation steps. Experimentally, we observe that Convolutional Neural Networks (CNNs) are resilient to this compression scheme and results suggest that CNNs learn to leverage lower-frequency components. In particular, we found: (1) band-limited training can effectively control the resource usage (GPU and memory); (2) models trained with band-limited layers retain high prediction accuracy; and (3) requires no modification to existing training algorithms or neural network architectures to use unlike other compression schemes.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/25ENWSRN/Dziedzic et al. - 2019 - Band-limited Training and Inference for Convolutional Neural Networks(2).pdf}
}

@inproceedings{ebner2019BagofWordsTransferNonContextual,
  title = {Bag-of-{{Words Transfer}}: {{Non-Contextual Techniques}} for {{Multi-Task Learning}}},
  booktitle = {{{EMNLP}}},
  author = {Ebner, S. and Wang, F. and Van Durme, B.},
  date = {2019},
  pages = {40--46},
  doi = {10.18653/v1/d19-6105},
  abstract = {Many architectures for multi-task learning (MTL) have been proposed to take advantage of transfer among tasks, often involving complex models and training procedures. In this paper, we ask if the sentence-level representations learned in previous approaches provide …},
  keywords = {unread},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2WXDMJUB/Ebner, Wang, Van Durme - 2019 - Bag-of-Words Transfer Non-Contextual Techniques for Multi-Task Learning(2).pdf}
}

@inproceedings{ebner2020MultiSentenceArgumentLinking,
  title = {Multi-{{Sentence Argument Linking}}},
  booktitle = {{{ACL}}},
  author = {Ebner, Seth and Xia, Patrick and Culkin, Ryan and Rawlins, Kyle and Van Durme, Benjamin},
  date = {2020},
  eprint = {1911.03766},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.03766},
  abstract = {We introduce a dataset with annotated Roles Across Multiple Sentences (RAMS), consisting of over 9,000 annotated events. This enables the development of a novel span-based labeling framework that operates at the document level, which connects related ideas in sentence-level semantic role labeling and coreference resolution. We achieve 68.1 F1 on RAMS when given argument span boundaries and 73.2 F1 when also given gold event types. We additionally illustrate the applicability of the approach to the slot filling task in the Gun Violence Database.},
  archiveprefix = {arXiv},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZNGBR2K/2020.acl-main.718.pdf}
}

@article{edmonds2002NearSynonymyLexicalChoice,
  title = {Near-{{Synonymy}} and {{Lexical Choice}}},
  author = {Edmonds, P. and Hirst, G.},
  date = {2002},
  journaltitle = {Computational Linguistics},
  volume = {28},
  number = {2},
  pages = {105--144},
  issn = {0891-2017},
  doi = {10.1162/089120102760173625},
  url = {http://www.mitpressjournals.org/doi/10.1162/089120102760173625},
  abstract = {We develop a new computational model for representing the fine-grained meanings of near-synonyms and the differences between them. We also develop a lexical-choice process that can decide which of several near-synonyms is most appropriate in a particular situation. This research has direct applications in machine translation and text generation.We first identify the problems of representing near-synonyms in a computational lexicon and show that no previous model adequately accounts for near-synonymy. We then propose a preliminary theory to account for near-synonymy, relying crucially on the notion of granularity of representation, in which the meaning of a word arises out of a context-dependent combination of a context-independent core meaning and a set of explicit differences to its near-synonyms. That is, near-synonyms cluster together.We then develop a clustered model of lexical knowledge, derived from the conventional ontological model. The model cuts off the ontology at a coarse grain, thus avoiding an awkward proliferation of language-dependent concepts in the ontology, yet maintaining the advantages of efficient computation and reasoning. The model groups near-synonyms into subconceptual clusters that are linked to the ontology. A cluster differentiates near-synonyms in terms of fine-grained aspects of denotation, implication, expressed attitude, and style. The model is general enough to account for other types of variation, for instance, in collocational behavior.An efficient, robust, and flexible fine-grained lexical-choice process is a consequence of a clustered model of lexical knowledge. To make it work, we formalize criteria for lexical choice as preferences to express certain concepts with varying indirectness, to express attitudes, and to establish certain styles. The lexical-choice process itself works on two tiers: between clusters and between near-synonyns of clusters. We describe our prototype implementation of the system, called I-Saurus.},
  isbn = {0891-2017},
  annotation = {226 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/843XAR76/Edmonds, Hirst - 2002 - Near-Synonymy and Lexical Choice(2).pdf}
}

@inproceedings{edunov2017ClassicalStructuredPrediction,
  title = {Classical {{Structured Prediction Losses}} for {{Sequence}} to {{Sequence Learning}}},
  booktitle = {{{NAACL}}},
  author = {Edunov, S. and Ott, M. and Auli, M. and Grangier, D. and Ranzato, M.},
  date = {2017},
  eprint = {1711.04956},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.04956},
  abstract = {There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT'14 German-English translation as well as Gigaword abstractive summarization. On the larger WMT'14 English-French translation task, sequence-level training achieves 41.5 BLEU which is on par with the state of the art.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {107 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FPNDBYAP/Edunov et al. - 2017 - Classical Structured Prediction Losses for Sequence to Sequence Learning(2).pdf}
}

@inproceedings{eisenstein2008BayesianUnsupervisedTopic,
  title = {Bayesian Unsupervised Topic Segmentation},
  booktitle = {{{EMNLP}}},
  author = {Eisenstein, J. and Barzilay, R.},
  date = {2008},
  pages = {334},
  doi = {10.3115/1613715.1613760},
  url = {http://portal.acm.org/citation.cfm?doid=1613715.1613760},
  abstract = {This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven by lexical cohesion: the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the Bayesian framework. 1},
  issue = {October},
  keywords = {unread},
  annotation = {227 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AAGIZF3Q/Eisenstein, Barzilay - 2008 - Bayesian unsupervised topic segmentation(2).pdf}
}

@inproceedings{eisner2004DynaDeclarativeLanguage,
  title = {Dyna: {{A Declarative Language}} for {{Implementing Dynamic Programs}}},
  booktitle = {{{ACL}}},
  author = {Eisner, J. M. and Goldlust, E. and Smith, N. A.},
  date = {2004},
  pages = {218--221},
  abstract = {We present the first version of a new declarative programming language. Dyna has many uses but was designed especially for rapid development of new statistical NLP systems. A Dyna program is a small set of equations, resembling Prolog inference rules, that specify the abstract structure of a dynamic programming algorithm. It compiles into efficient, portable, C++ classes that can be easily invoked from a larger application. By default, these classes run a generalization of agenda-based parsing, prioritizing the partial parses by some figure of merit. The classes can also perform an exact backward (outside) pass in the service of parameter training. The compiler already knows several implementation tricks, algorithmic transforms, and numerical optimization techniques. It will acquire more over time: we intend for it to generalize and encapsulate best practices, and serve as a testbed for new practices. Dyna is now being used for parsing, machine translation, morphological analysis, grammar induction, and finite-state modeling.},
  isbn = {0-00-140110-6},
  issue = {July},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B3CB9RQZ/Eisner, Goldlust, Smith - 2004 - Dyna A Declarative Language for Implementing Dynamic Programs(2).pdf}
}

@inproceedings{eisner2008DynaNonProbabilisticProgramming,
  title = {Dyna: {{A Non-Probabilistic Programming Language}} for {{Probabilistic AI}}},
  booktitle = {{{NeurIPS}}},
  author = {Eisner, J. M.},
  date = {2008},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6TK62GEM/Eisner - 2008 - Dyna A Non-Probabilistic Programming Language for Probabilistic AI(2).pdf}
}

@incollection{eisner2010DynaExtendingDatalog,
  title = {Dyna: {{Extending Datalog}} for Modern {{AI}}},
  booktitle = {Datalog {{Reloaded}}},
  author = {Eisner, J. M. and Filardo, N. W.},
  date = {2010},
  abstract = {Abstract. Modern statistical AI systems are quite large and complex; this interferes with research, development, and education. We point out that most of the computation involves database-like queries and updates on complex views of the data. Specifically, recursive ...},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5LLZWB4S/Eisner, Filardo - 2010 - Dyna Extending Datalog for modern AI(2).pdf}
}

@inproceedings{eisner2016InsideOutsideForwardBackwardAlgorithms,
  title = {Inside-{{Outside}} and {{Forward-Backward Algorithms Are Just Backprop}}},
  booktitle = {{{EMNLP}}},
  author = {Eisner, J. M.},
  date = {2016},
  pages = {1--17},
  abstract = {A probabilistic or weighted grammar implies a posterior probability distribution over possi-ble parses of a given input sentence. One often needs to extract information from this distri-bution, by computing the expected counts (in the unknown parse) of various grammar rules, constituents, transitions, or states. This re-quires an algorithm such as inside-outside or forward-backward that is tailored to the gram-mar formalism. Conveniently, each such al-gorithm can be obtained by automatically dif-ferentiating an " inside " algorithm that merely computes the log-probability of the evidence (the sentence). This mechanical procedure produces correct and efficient code. As for any other instance of back-propagation, it can be carried out manually or by software. This pedagogical paper carefully spells out the con-struction and relates it to traditional and non-traditional views of these algorithms.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WNQ6XIXL/Eisner - 2016 - Inside-Outside and Forward-Backward Algorithms Are Just Backprop(2).pdf}
}

@inproceedings{ejerhed1988FindingClausesUnrestricted,
  title = {Finding {{Clauses In Unrestricted Text By Finitary And Stochastic Methods}}},
  booktitle = {{{ANLC}}},
  author = {Ejerhed, E. I.},
  date = {1988},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {219--227},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {The Working Group I contribution to the IPCC's Fifth Assessment Report (AR5) considers new evidence of climate change based on many independent scientific analyses from observations of the climate system, paleoclimate archives, theoretical studies of climate processes and simulations using climate models. It builds upon the Working Group I contribution to the IPCC’s Fourth Assessment Report (AR4), and incorporates subsequent new findings of research. As a component of the fifth assessment cycle, the IPCC Special Report on Managing the Risks of Extreme Events to Advance Climate Change Adaptation (SREX) is an important basis for information on changing weather and climate extremes. This Summary for Policymakers (SPM) follows the structure of the Working Group I report. The narrative is supported by a series of overarching highlighted conclusions which, taken together, provide a concise summary. Main sections are introduced with a brief paragraph in italics which outlines the methodological basis of the assessment. The degree of certainty in key findings in this assessment is based on the author teams’ evaluations of underlying scientific understanding and is expressed as a qualitative level of confidence (from very low to very high) and, when possible, probabilistically with a quantified likelihood (from exceptionally unlikely to virtually certain). Confidence in the validity of a finding is based on the type, amount, quality, and consistency of evidence (e.g., data, mechanistic understanding, theory, models, expert judgment) and the degree of agreement1. Probabilistic estimates of quantified measures of uncertainty in a finding are based on statistical analysis of observations or model results, or both, and expert judgment2. Where appropriate, findings are also formulated as statements of fact without using uncertainty qualifiers. (See Chapter 1 and Box TS.1 for more details about the specific language the IPCC uses to communicate uncertainty) The basis for substantive paragraphs in this Summary for Policymakers can be found in the chapter sections of the underlying report and in the Technical Summary. These references are given},
  archiveprefix = {arXiv},
  isbn = {978-85-7811-079-6},
  keywords = {unread},
  annotation = {1013 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NE3VNLL8/Ejerhed - 1988 - Finding Clauses In Unrestricted Text By Finitary And Stochastic Methods(2).pdf}
}

@inproceedings{elsahar2018TRExLargeScale,
  title = {T-{{REx}}: {{A Large Scale Alignment}} of {{Natural Language}} with {{Knowledge Base Triples}}},
  booktitle = {{{LREC}}},
  author = {Elsahar, Hady and Vougiouklis, Pavlos and Remaci, Arslen and Gravier, Christophe and Hare, Jonathon and Simperl, Elena and Laforest, Frederique},
  date = {2018},
  pages = {5},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XMPBV7H6/Elsahar et al. - T-REx A Large Scale Alignment of Natural Language.pdf}
}

@inproceedings{estival2004OntologyBasedNaturalLanguage,
  title = {Towards {{Ontology-Based Natural Language Processing}}},
  booktitle = {Workshop on {{NLP}} and {{XML}}},
  author = {Estival, D. and Nowak, C. and Zschorn, A.},
  date = {2004},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z8DXCBDB/Estival, Nowak, Zschorn - 2004 - Towards Ontology-Based Natural Language Processing(2).pdf}
}

@misc{etesami2016LearningNetworkMultivariate,
  title = {Learning {{Network}} of {{Multivariate Hawkes Processes}}: {{A Time Series Approach}}},
  author = {Etesami, J. and Kiyavash, N. and Zhang, K. and Singhal, K.},
  date = {2016},
  eprint = {1603.04319},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1603.04319},
  abstract = {Learning the influence structure of multiple time series data is of great interest to many disciplines. This paper studies the problem of recovering the causal structure in network of multivariate linear Hawkes processes. In such processes, the occurrence of an event in one process affects the probability of occurrence of new events in some other processes. Thus, a natural notion of causality exists between such processes captured by the support of the excitation matrix. We show that the resulting causal influence network is equivalent to the Directed Information graph (DIG) of the processes, which encodes the causal factorization of the joint distribution of the processes. Furthermore, we present an algorithm for learning the support of excitation matrix (or equivalently the DIG). The performance of the algorithm is evaluated on synthesized multivariate Hawkes networks as well as a stock market and MemeTracker real-world dataset.},
  archiveprefix = {arXiv},
  isbn = {9781510827806},
  keywords = {unread},
  annotation = {36 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U4JKF6JH/Etesami et al. - 2016 - Learning Network of Multivariate Hawkes Processes A Time Series Approach(2).pdf}
}

@inproceedings{ethayarajh2019HowContextualAre,
  title = {How {{Contextual}} Are {{Contextualized Word Representations}}? {{Comparing}} the {{Geometry}} of {{BERT}}, {{ELMo}}, and {{GPT-2 Embeddings}}},
  booktitle = {{{EMNLP}}},
  author = {Ethayarajh, K.},
  date = {2019},
  eprint = {1909.00512},
  eprinttype = {arxiv},
  pages = {55--65},
  url = {http://arxiv.org/abs/1909.00512},
  abstract = {Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5\% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.},
  archiveprefix = {arXiv},
  annotation = {84 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PURY538B/Ethayarajh - 2019 - How Contextual are Contextualized Word Representations Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings(2).pdf}
}

@inproceedings{ethayarajh2021AttentionFlowsAre,
  title = {Attention {{Flows}} Are {{Shapley Value Explanations}}},
  booktitle = {{{ACL}}},
  author = {Ethayarajh, Kawin and Jurafsky, Dan},
  date = {2021-05-30},
  eprint = {2105.14652},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.14652},
  urldate = {2021-06-21},
  abstract = {Shapley Values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance of features, embeddings, and even neurons. In NLP, however, leave-one-out and attention-based explanations still predominate. Can we draw a connection between these different methods? We formally prove that -- save for the degenerate case -- attention weights and leave-one-out values cannot be Shapley Values. \$\textbackslash textit\{Attention flow\}\$ is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph. Perhaps surprisingly, we prove that attention flows are indeed Shapley Values, at least at the layerwise level. Given the many desirable theoretical qualities of Shapley Values -- which has driven their adoption among the ML community -- we argue that NLP practitioners should, when possible, adopt attention flow explanations alongside more traditional ones.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-06-21]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E7676YQF/Ethayarajh and Jurafsky - 2021 - Attention Flows are Shapley Value Explanations.pdf;/home/hiaoxui/.local/share/zotero_files/storage/8URUEQ8T/2105.html}
}

@inproceedings{eyuboglu2022DominoDiscoveringSystematic,
  title = {Domino: {{Discovering Systematic Errors}} with {{Cross-Modal Embeddings}}},
  booktitle = {{{ICLR}}},
  author = {Eyuboglu, S. and Varma, M. and Saab, K. K. and Delbrouck, J. and Lee-Messer, C. and Dunnmon, J. and Zou, M. and Re, C.},
  date = {2022},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H7PCEBZ4/domino_discovering_systematic_.pdf}
}

@inproceedings{facenet2015,
  title = {{{FaceNet}}: {{A Unified Embedding}} for {{Face Recognition}} and {{Clustering}}},
  shorttitle = {{{FaceNet}}},
  booktitle = {{{CVPR}}},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  date = {2015-06},
  eprint = {1503.03832},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {815--823},
  doi = {10.1109/CVPR.2015.7298682},
  url = {http://arxiv.org/abs/1503.03832},
  urldate = {2022-10-03},
  abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QJ35B3PP/Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition .pdf;/home/hiaoxui/.local/share/zotero_files/storage/Q52SGHFL/1503.html}
}

@misc{faez2020DeepGraphGenerators,
  title = {Deep {{Graph Generators}}: {{A Survey}}},
  shorttitle = {Deep {{Graph Generators}}},
  author = {Faez, Faezeh and Ommi, Yassaman and Baghshah, Mahdieh Soleymani and Rabiee, Hamid R.},
  date = {2020-12-31},
  eprint = {2012.15544},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.15544},
  urldate = {2021-01-16},
  abstract = {Deep generative models have achieved great success in areas such as image, speech, and natural language processing in the past few years. Thanks to the advances in graph-based deep learning, and in particular graph representation learning, deep graph generation methods have recently emerged with new applications ranging from discovering novel molecular structures to modeling social networks. This paper conducts a comprehensive survey on deep learning-based graph generation approaches and classifies them into five broad categories, namely, autoregressive, autoencoder-based, RL-based, adversarial, and flow-based graph generators, providing the readers a detailed description of the methods in each class. We also present publicly available source codes, commonly used datasets, and the most widely utilized evaluation metrics. Finally, we highlight the existing challenges and discuss future research directions.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/875PXZ7C/Faez et al. - 2020 - Deep Graph Generators A Survey.pdf;/home/hiaoxui/.local/share/zotero_files/storage/L965ZVL3/2012.html}
}

@inproceedings{falke2017BringingStructureSummaries,
  title = {Bringing {{Structure}} into {{Summaries}}: {{Crowdsourcing}} a {{Benchmark Corpus}} of {{Concept Maps}}},
  booktitle = {{{EMNLP}}},
  author = {Falke, T. and Gurevych, I.},
  date = {2017},
  eprint = {1704.04452},
  eprinttype = {arxiv},
  pages = {2951--2961},
  url = {http://arxiv.org/abs/1704.04452},
  abstract = {Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {20 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6XNFARUG/Falke, Gurevych - 2017 - Bringing Structure into Summaries Crowdsourcing a Benchmark Corpus of Concept Maps(2).pdf}
}

@inproceedings{fan2018HierarchicalNeuralStory,
  title = {Hierarchical {{Neural Story Generation}}},
  booktitle = {{{ACL}}},
  author = {Fan, A. and Lewis, M. and Dauphin, Y.},
  date = {2018},
  eprint = {1805.04833},
  eprinttype = {arxiv},
  pages = {1--10},
  url = {http://arxiv.org/abs/1805.04833},
  abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {309 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ITY66JGK/Fan, Lewis, Dauphin - 2018 - Hierarchical Neural Story Generation(2).pdf}
}

@misc{fang2021TransformerbasedConditionalVariational,
  title = {Transformer-Based {{Conditional Variational Autoencoder}} for {{Controllable Story Generation}}},
  author = {Fang, Le and Zeng, Tao and Liu, Chaochun and Bo, Liefeng and Dong, Wen and Chen, Changyou},
  date = {2021-07-08},
  eprint = {2101.00828},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.00828},
  urldate = {2022-04-28},
  abstract = {We investigate large-scale latent variable models (LVMs) for neural story generation—an under-explored application for open-domain long text—with objectives in two threads: generation effectiveness and controllability. LVMs, especially the variational autoencoder (VAE), have achieved both effective and controllable generation through exploiting flexible distributional latent representations. Recently, Transformers and its variants have achieved remarkable effectiveness without explicit latent representation learning, thus lack satisfying controllability in generation. In this paper, we advocate to revive latent variable modeling, essentially the power of representation learning, in the era of Transformers to enhance controllability without hurting state-of-the-art generation effectiveness. Specifically, we integrate latent representation vectors with a Transformer-based pre-trained architecture to build conditional variational autoencoder (CVAE). Model components such as encoder, decoder and the variational posterior are all built on top of pre-trained language models—GPT2 specifically in this paper. Experiments demonstrate state-of-the-art conditional generation ability of our model, as well as its excellent representation learning capability and controllability.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R2C7QRY3/Fang et al. - 2021 - Transformer-based Conditional Variational Autoenco.pdf}
}

@article{fawzi2022DiscoveringFasterMatrix,
  title = {Discovering Faster Matrix Multiplication Algorithms with Reinforcement Learning},
  author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
  date = {2022-10-06},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {610},
  number = {7930},
  pages = {47--53},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-022-05172-4},
  url = {https://www.nature.com/articles/s41586-022-05172-4},
  urldate = {2022-10-06},
  abstract = {Abstract                            Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems—from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero               1               for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4\,×\,4 matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago               2               . We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor’s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7KADJ4N4/certifiably_robust_transformer.pdf;/home/hiaoxui/.local/share/zotero_files/storage/DV52B695/Fawzi et al. - 2022 - Discovering faster matrix multiplication algorithm.pdf}
}

@misc{fedus2021SwitchTransformersScaling,
  title = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle = {Switch {{Transformers}}},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  date = {2021-01-11},
  eprint = {2101.03961},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.03961},
  urldate = {2021-01-16},
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  archiveprefix = {arXiv},
  annotation = {17 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I46AM4TA/Fedus et al. - 2021 - Switch Transformers Scaling to Trillion Parameter.pdf;/home/hiaoxui/.local/share/zotero_files/storage/3HVGBBCU/2101.html}
}

@article{feldman2004EmbodiedMeaningNeural,
  title = {Embodied Meaning in a Neural Theory of Language},
  author = {Feldman, J. and Narayanan, S.},
  date = {2004},
  journaltitle = {Brain and Language},
  volume = {89},
  number = {2},
  eprint = {15068922},
  eprinttype = {pmid},
  pages = {385--392},
  issn = {0093934X},
  doi = {10.1016/S0093-934X(03)00355-9},
  abstract = {In this paper, we outline an explicitly neural theory of language (NTL) that attempts to explain how many brain functions (including emotion and social cognition) work together to understand and learn language. The focus will be on the required representations and computations, although there will be some discussion of results on specific brain structures. In this approach, one does not expect to find brain areas specialized only for language or to find language processing confined to only a few areas.},
  isbn = {0093-934X, 0093-934X},
  keywords = {unread},
  annotation = {326 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MJM5VP4J/Feldman, Narayanan - 2004 - Embodied meaning in a neural theory of language(2).pdf}
}

@inproceedings{feldman2019CommonsenseKnowledgeMining,
  title = {Commonsense {{Knowledge Mining}} from {{Pretrained Models}}},
  booktitle = {{{EMNLP}}},
  author = {Feldman, J. and Davison, J. and Rush, A. M.},
  date = {2019},
  eprint = {1909.00505},
  eprinttype = {arxiv},
  pages = {1173--1178},
  url = {http://arxiv.org/abs/1909.00505},
  abstract = {Inferring commonsense knowledge is a key challenge in natural language processing, but due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple's validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though this method performs worse on a test set than models explicitly trained on a corresponding training set, it outperforms these methods when mining commonsense knowledge from new sources, suggesting that unsupervised techniques may generalize better than current supervised approaches.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {54 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AEUJV3PD/Feldman, Davison, Rush - 2019 - Commonsense Knowledge Mining from Pretrained Models(2).pdf}
}

@inproceedings{feng2018PathologiesNeuralModels,
  title = {Pathologies of {{Neural Models Make Interpretations Difficult}}},
  booktitle = {{{EMNLP}}},
  author = {Feng, S. and Wallace, E. and Grissom, A. and Iyyer, M. and Rodriguez, P. and Boyd-Graber, J.},
  date = {2018},
  eprint = {1804.07781},
  eprinttype = {arxiv},
  pages = {3719--3728},
  issn = {19342608},
  doi = {http://dx.doi.org/10.1016/0255-2701(91)80005-A},
  url = {http://arxiv.org/abs/1804.07781},
  abstract = {One way to interpret neural model predictions is to highlight the most important input features---for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word's importance is determined by either input perturbation---measuring the decrease in model confidence when that word is removed---or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.},
  archiveprefix = {arXiv},
  isbn = {0255-2701},
  annotation = {108 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z4AFGKUC/Feng et al. - 2018 - Pathologies of Neural Models Make Interpretations Difficult(2).pdf}
}

@inproceedings{feng2020ExploringEndtoEndDifferentiable,
  title = {Exploring {{End-to-End Differentiable Natural Logic Modeling}}},
  booktitle = {{{COLING}}},
  author = {Feng, Yufei and Zheng, Zi’ou and Liu, Quan and Greenspan, Michael and Zhu, Xiaodan},
  date = {2020},
  pages = {1172--1185},
  publisher = {{International Committee on Computational Linguistics}},
  location = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.101},
  url = {https://www.aclweb.org/anthology/2020.coling-main.101},
  urldate = {2021-02-12},
  abstract = {We explore end-to-end trained differentiable models that integrate natural logic with neural networks, aiming to keep the backbone of natural language reasoning based on the natural logic formalism while introducing subsymbolic vector representations and neural components. The proposed model adapts module networks to model natural logic operations, which is enhanced with a memory component to model contextual information. Experiments show that the proposed framework can effectively model monotonicity-based reasoning, compared to the baseline neural network models without built-in inductive bias for monotonicity-based reasoning. Our proposed model shows to be robust when transferred from upward to downward inference. We perform further analyses on the performance of the proposed model on aggregation, showing the effectiveness of the proposed subcomponents on helping achieve better intermediate aggregation performance.},
  eventtitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CXFI78CN/Feng et al. - 2020 - Exploring End-to-End Differentiable Natural Logic .pdf}
}

@misc{feng2021DivideRuleRecurrent,
  title = {Divide and {{Rule}}: {{Recurrent Partitioned Network}} for {{Dynamic Processes}}},
  author = {Feng, Qianyu and Zhang, Bang and Yang, Yi},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ELMFC8BM/divide_and_rule_recurrent_partitioned_network_for_dynamic_processes.pdf}
}

@inproceedings{feng2022LearnRememberTransformer,
  title = {Learn {{To Remember}}: {{Transformer}} with {{Recurrent Memory}} for {{Document-Level Machine Translation}}},
  shorttitle = {Learn {{To Remember}}},
  booktitle = {{{NAACL}}},
  author = {Feng, Yukun and Li, Feng and Song, Ziang and Zheng, Boyuan and Koehn, Philipp},
  date = {2022-05-03},
  eprint = {2205.01546},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.01546},
  urldate = {2022-07-14},
  abstract = {The Transformer architecture has led to significant gains in machine translation. However, most studies focus on only sentence-level translation without considering the context dependency within documents, leading to the inadequacy of document-level coherence. Some recent research tried to mitigate this issue by introducing an additional context encoder or translating with multiple sentences or even the entire document. Such methods may lose the information on the target side or have an increasing computational complexity as documents get longer. To address such problems, we introduce a recurrent memory unit to the vanilla Transformer, which supports the information exchange between the sentence and previous context. The memory unit is recurrently updated by acquiring information from sentences, and passing the aggregated knowledge back to subsequent sentence states. We follow a two-stage training strategy, in which the model is first trained at the sentence level and then finetuned for document-level translation. We conduct experiments on three popular datasets for document-level machine translation and our model has an average improvement of 0.91 s-BLEU over the sentence-level baseline. We also achieve state-of-the-art results on TED and News, outperforming the previous work by 0.36 s-BLEU and 1.49 d-BLEU on average.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QFZSVCC7/Feng et al. - 2022 - Learn To Remember Transformer with Recurrent Memo.pdf;/home/hiaoxui/.local/share/zotero_files/storage/XFVJQ7ZE/2205.html}
}

@inproceedings{ferreira2016MoreVariationText,
  title = {Towards More Variation in Text Generation: {{Developing}} and Evaluating Variation Models for Choice of Referential Form},
  booktitle = {{{ACL}}},
  author = {Ferreira, T. C. and Krahmer, E. and Wubben, S.},
  date = {2016},
  pages = {568--577},
  url = {http://www.aclweb.org/anthology/P16-1054},
  abstract = {In this study, we introduce a nondeterministic method for referring expression generation. We describe two models that account for individual variation in the choice of referential form in automatically generated text: a Naive Bayes model and a Recurrent Neural Network. Both are evaluated using the VaREG corpus. Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those produced by a random baseline model.},
  isbn = {978-1-5108-2758-5},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VHNHUW7D/Ferreira, Krahmer, Wubben - 2016 - Towards more variation in text generation Developing and evaluating variation models for choice of(2).pdf}
}

@inproceedings{ferreira2019NeuralDatatotextGeneration,
  title = {Neural Data-to-Text Generation: {{A}} Comparison between Pipeline and End-to-End Architectures},
  booktitle = {{{EMNLP}}},
  author = {Ferreira, T. C. and van der Lee, C. and van Miltenburg, E. and Krahmer, E.},
  options = {useprefix=true},
  date = {2019},
  eprint = {1908.09022},
  eprinttype = {arxiv},
  pages = {552--562},
  url = {http://arxiv.org/abs/1908.09022},
  abstract = {Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. In contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in-between. This study introduces a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Both architectures were implemented making use of state-of-the art deep learning methods as the encoder-decoder Gated-Recurrent Units (GRU) and Transformer. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {45 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W68ITM6P/Ferreira et al. - 2019 - Neural data-to-text generation A comparison between pipeline and end-to-end architectures(2).pdf}
}

@inproceedings{fevry2020EmpiricalEvaluationPretraining,
  title = {Empirical {{Evaluation}} of {{Pretraining Strategies}} for {{Supervised Entity Linking}}},
  booktitle = {{{AKBC}}},
  author = {Févry, Thibault and FitzGerald, Nicholas and Soares, Livio Baldini and Kwiatkowski, Tom},
  date = {2020-05-28},
  eprint = {2005.14253},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.14253},
  urldate = {2020-09-29},
  abstract = {In this work, we present an entity linking model which combines a Transformer architecture with large scale pretraining from Wikipedia links. Our model achieves the state-of-the-art on two commonly used entity linking datasets: 96.7\% on CoNLL and 94.9\% on TAC-KBP. We present detailed analyses to understand what design choices are important for entity linking, including choices of negative entity candidates, Transformer architecture, and input perturbations. Lastly, we present promising results on more challenging settings such as end-to-end entity linking and entity linking without in-domain training data.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CW2AECH9/Févry et al. - 2020 - Empirical Evaluation of Pretraining Strategies for.pdf;/home/hiaoxui/.local/share/zotero_files/storage/287LSD43/2005.html}
}

@article{fillmore1976FrameSemanticsNature,
  title = {Frame {{Semantics}} and the {{Nature}} of {{Language}}},
  author = {Fillmore, C. J.},
  date = {1976},
  journaltitle = {Annals of the New York Academy of Sciences},
  volume = {280},
  number = {1},
  pages = {20--32},
  issn = {17496632},
  doi = {10.1111/j.1749-6632.1976.tb25467.x},
  abstract = {When the question of the origin of language is considered from an evolutionary perspective, it loses much of its clarity and simplicity. Should we be looking for the first step in the chain of events that led to what we now see as human language? the first step away from what? Or should we be trying to determine the last step},
  keywords = {unread},
  annotation = {862 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/894QX9D4/Fillmore - 1976 - Frame Semantics and the Nature of Language(2).pdf}
}

@incollection{fillmore1982FrameSemantics,
  title = {Frame {{Semantics}}},
  booktitle = {Linguistics in the {{Morning Calm}}},
  author = {Fillmore, C. J.},
  date = {1982},
  publisher = {{Hanshin Publishing Company}},
  location = {{Seoul, Korea}},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YSZSD9AD/Fillmore - 1982 - Frame Semantics(2).pdf}
}

@inproceedings{fincke2021LanguageModelPriming,
  title = {Language {{Model Priming}} for {{Cross-Lingual Event Extraction}}},
  booktitle = {{{AAAI}}},
  author = {Fincke, Steven and Agarwal, Shantanu and Miller, Scott and Boschee, Elizabeth},
  date = {2021-09-25},
  eprint = {2109.12383},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.12383},
  urldate = {2021-11-15},
  abstract = {We present a novel, language-agnostic approach to "priming" language models for the task of event extraction, providing particularly effective performance in low-resource and zero-shot cross-lingual settings. With priming, we augment the input to the transformer stack's language model differently depending on the question(s) being asked of the model at runtime. For instance, if the model is being asked to identify arguments for the trigger "protested", we will provide that trigger as part of the input to the language model, allowing it to produce different representations for candidate arguments than when it is asked about arguments for the trigger "arrest" elsewhere in the same sentence. We show that by enabling the language model to better compensate for the deficits of sparse and noisy training data, our approach improves both trigger and argument detection and classification significantly over the state of the art in a zero-shot cross-lingual setting.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7F2N7RCS/Fincke et al. - 2021 - Language Model Priming for Cross-Lingual Event Ext.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QW9IYLPQ/2109.html}
}

@inproceedings{finkel2009NestedNamedEntity,
  title = {Nested Named Entity Recognition},
  booktitle = {{{EMNLP}}},
  author = {Finkel, Jenny Rose and Manning, Christopher D.},
  date = {2009},
  volume = {1},
  pages = {141},
  publisher = {{Association for Computational Linguistics}},
  location = {{Singapore}},
  doi = {10.3115/1699510.1699529},
  isbn = {978-1-932432-59-6},
  langid = {english},
  annotation = {200 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BQRRYKK8/Finkel and Manning - 2009 - Nested named entity recognition.pdf}
}

@inproceedings{finn2017ModelAgnosticMetaLearningFast,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  booktitle = {{{ICML}}},
  author = {Finn, C. and Abbeel, P. and Levine, L.},
  date = {2017},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B63U3KIP/1703.03400.pdf}
}

@inproceedings{fitzgerald2013LearningDistributionsLogical,
  title = {Learning {{Distributions}} over {{Logical Forms}} for {{Referring Expression Generation}}},
  booktitle = {{{EMNLP}}},
  author = {FitzGerald, N. and Artzi, Y. and Zettlemoyer, L. S.},
  date = {2013},
  pages = {1914--1925},
  abstract = {We present a new approach to referring expression generation, casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world. Despite an extremely large space of possible expressions, we demonstrate effective learning of a globally normalized log-linear distribution. This learning is enabled by a new, multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms. We train and evaluate the approach on a new corpus of references to sets of visual objects. Experiments show the approach is able to learn accurate models, which generate over 87\% of the expressions people used. Additionally, on the previously studied special case of single object reference, we show a 35\% relative error reduction over previous state of the art.},
  isbn = {978-1-937284-97-8},
  issue = {October},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y6TX2H7T/FitzGerald, Artzi, Zettlemoyer - 2013 - Learning Distributions over Logical Forms for Referring Expression Generation(2).pdf}
}

@inproceedings{fitzgerald2015SemanticRoleLabeling,
  title = {Semantic Role Labeling with Neural Network Factors},
  booktitle = {{{EMNLP}}},
  author = {Fitzgerald, N. and Täckström, O. and Ganchev, K. and Das, D.},
  date = {2015},
  pages = {960--970},
  doi = {10.18653/v1/d15-1112},
  abstract = {We present a new method for semantic role labeling in which arguments and semantic roles are jointly embedded in a shared vector space for a given predicate. These embeddings belong to a neural network, whose output represents the potential functions of a graphical model designed for the SRL task. We consider both local and structured learning methods and obtain strong results on standard PropBank and FrameNet corpora with a straightforward product-of-experts model. We further show how the model can learn jointly from PropBank and FrameNet annotations to obtain additional improvements on the smaller FrameNet dataset.},
  isbn = {978-1-941643-32-7},
  keywords = {unread},
  annotation = {115 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7ZVR6SNI/Fitzgerald et al. - 2015 - Semantic role labeling with neural network factors(2).pdf}
}

@inproceedings{fitzgerald2018LargeScaleQASRLParsing,
  title = {Large-{{Scale QA-SRL Parsing}}},
  booktitle = {{{ACL}}},
  author = {FitzGerald, N. and Michael, J. and He, L. and Zettlemoyer, L. S.},
  date = {2018},
  eprint = {1805.05377},
  eprinttype = {arxiv},
  pages = {1--10},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/75PSZ2ZQ/FitzGerald et al. - 2018 - Large-Scale QA-SRL Parsing(2).pdf}
}

@inproceedings{fleischman2002EmotionalVariationNatural,
  title = {Towards Emotional Variation in Natural Language Generation},
  booktitle = {{{INLG}}},
  author = {Fleischman, M. and Hovy, E.},
  date = {2002},
  pages = {1--8},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJIQY7WY/Fleischman, Hovy - 2002 - Towards emotional variation in natural language generation(2).pdf}
}

@inproceedings{formal2022DistillationHardNegative,
  title = {From {{Distillation}} to {{Hard Negative Sampling}}: {{Making Sparse Neural IR Models More Effective}}},
  shorttitle = {From {{Distillation}} to {{Hard Negative Sampling}}},
  booktitle = {{{SIGIR}}},
  author = {Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, Stéphane},
  date = {2022-05-12},
  eprint = {2205.04733},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.04733},
  urldate = {2022-10-03},
  abstract = {Neural retrievers based on dense representations combined with Approximate Nearest Neighbors search have recently received a lot of attention, owing their success to distillation and/or better sampling of examples for training -- while still relying on the same backbone architecture. In the meantime, sparse representation learning fueled by traditional inverted indexing techniques has seen a growing interest, inheriting from desirable IR priors such as explicit lexical matching. While some architectural variants have been proposed, a lesser effort has been put in the training of such models. In this work, we build on SPLADE -- a sparse expansion-based retriever -- and show to which extent it is able to benefit from the same training improvements as dense models, by studying the effect of distillation, hard-negative mining as well as the Pre-trained Language Model initialization. We furthermore study the link between effectiveness and efficiency, on in-domain and zero-shot settings, leading to state-of-the-art results in both scenarios for sufficiently expressive models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4SZ4HA2J/Formal et al. - 2022 - From Distillation to Hard Negative Sampling Makin.pdf;/home/hiaoxui/.local/share/zotero_files/storage/TFYC62MA/2205.html}
}

@inproceedings{framenet1998,
  title = {The {{Berkeley FrameNet Project}}},
  booktitle = {{{ACL-COLING}}},
  author = {Baker, C. F. and Fillmore, C. J. and Lowe, J. B.},
  date = {1998},
  doi = {10.3115/980845.980860},
  abstract = {FrameNet is a three-year NSF-supported project in corpus-based computational lexicog-raphy, now in its second year (NSF IRI-9618838, "Tools for Lexicon Building"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generaliza-tions, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words de-scribed, and (b) the valence representation (se-mantic and syntactic) of several thousand words and phrases, each accompanied by (c) a repre-sentative collection of annotated corpus attes-tations, which jointly exemplify the observed linkings between "frame elements" and their syntactic realizations (e.g. grammatical func-tion, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computa-tional tools that have been adapted or created in-house for this work.},
  annotation = {2671 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NZ9VIL9X/Baker, Fillmore, Lowe - 1998 - The Berkeley FrameNet Project(2).pdf}
}

@misc{frankle2018LotteryTicketHypothesis,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Small}}, {{Trainable Neural Networks}}},
  author = {Frankle, J. and Carbin, M.},
  date = {2018},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  doi = {arXiv:1803.03635v1},
  url = {http://arxiv.org/abs/1803.03635},
  abstract = {Neural network compression techniques are able to reduce the parameter counts of trained networks by over 90\%--decreasing storage requirements and improving inference performance--without compromising accuracy. However, contemporary experience is that it is difficult to train small architectures from scratch, which would similarly improve training performance. We articulate a new conjecture to explain why it is easier to train large networks: the "lottery ticket hypothesis." It states that large networks that train successfully contain subnetworks that--when trained in isolation--converge in a comparable number of iterations to comparable accuracy. These subnetworks, which we term "winning tickets," have won the initialization lottery: their connections have initial weights that make training particularly effective. We find that a standard technique for pruning unnecessary network weights naturally uncovers a subnetwork which, at the start of training, comprised a winning ticket. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis. We consistently find winning tickets that are less than 20\% of the size of several fully-connected, convolutional, and residual architectures for MNIST and CIFAR10. Furthermore, winning tickets at moderate levels of pruning (20-50\% of the original network size) converge up to 6.7x faster than the original network and exhibit higher test accuracy.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {671 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JM44HNSG/Frankle, Carbin - 2018 - The Lottery Ticket Hypothesis Finding Small, Trainable Neural Networks(2).pdf}
}

@inproceedings{frankle2019LotteryTicketHypothesis,
  title = {The Lottery Ticket Hypothesis: {{Finding}} Sparse, Trainable Neural Networks},
  booktitle = {{{ICLR}}},
  author = {Frankle, J. and Carbin, M.},
  date = {2019},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F5HD3BJ9/Frankle, Carbin - 2019 - The lottery ticket hypothesis Finding sparse, trainable neural networks(2).pdf}
}

@misc{frydenlund2021LanguageModellingLearning,
  title = {Language {{Modelling}} via {{Learning}} to {{Rank}}},
  author = {Frydenlund, Arvid and Singh, Gagandeep and Rudzicz, Frank},
  date = {2021-10-13},
  eprint = {2110.06961},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.06961},
  urldate = {2021-10-29},
  abstract = {We consider language modelling (LM) as a multi-label structured prediction task by re-framing training from solely predicting a single ground-truth word to ranking a set of words which could continue a given context. To avoid annotating top-\$k\$ ranks, we generate them using pre-trained LMs: GPT-2, BERT, and Born-Again models. This leads to a rank-based form of knowledge distillation (KD). We also develop a method using \$N\$-grams to create a non-probabilistic teacher which generates the ranks without the need of a pre-trained LM. We confirm the hypotheses that we can treat LMing as a ranking task and that we can do so without the use of a pre-trained LM. We show that rank-based KD generally improves perplexity (PPL), often with statistical significance, when compared to Kullback-Leibler-based KD. Surprisingly, given the simplicity of the method, \$N\$-grams act as competitive teachers and achieve similar performance as using either BERT or a Born-Again model teachers. GPT-2 always acts as the best teacher, though, and using it and a Transformer-XL student on Wiki-02, rank-based KD reduces a cross-entropy baseline from 65.27 to 55.94 and against a KL-based KD of 56.70.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZJPQJBCY/Frydenlund et al. - 2021 - Language Modelling via Learning to Rank.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ULPULBEW/2110.html}
}

@inproceedings{fu2021NestedNamedEntity,
  title = {Nested {{Named Entity Recognition}} with {{Partially-Observed TreeCRFs}}},
  booktitle = {{{AAAI}}},
  author = {Fu, Yao and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Huang, Fei},
  date = {2021},
  eprint = {2012.08478},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.08478},
  urldate = {2020-12-17},
  abstract = {Named entity recognition (NER) is a well-studied task in natural language processing. However, the widely-used sequence labeling framework is difficult to detect entities with nested structures. In this work, we view nested NER as constituency parsing with partially-observed trees and model it with partially-observed TreeCRFs. Specifically, we view all labeled entity spans as observed nodes in a constituency tree, and other spans as latent nodes. With the TreeCRF we achieve a uniform way to jointly model the observed and the latent nodes. To compute the probability of partial trees with partial marginalization, we propose a variant of the Inside algorithm, the \textbackslash textsc\{Masked Inside\} algorithm, that supports different inference operations for different nodes (evaluation for the observed, marginalization for the latent, and rejection for nodes incompatible with the observed) with efficient parallelized implementation, thus significantly speeding up training and inference. Experiments show that our approach achieves the state-of-the-art (SOTA) F1 scores on the ACE2004, ACE2005 dataset, and shows comparable performance to SOTA models on the GENIA dataset. Our approach is implemented at: \textbackslash url\{https://github.com/FranxYao/Partially-Observed-TreeCRFs\}.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N392BR4N/Fu et al. - 2020 - Nested Named Entity Recognition with Partially-Obs.pdf;/home/hiaoxui/.local/share/zotero_files/storage/AUJ2E4D2/2012.html}
}

@inproceedings{fukui2016MultimodalCompactBilinear,
  title = {Multimodal {{Compact Bilinear Pooling}} for {{Visual Question Answering}} and {{Visual Grounding}}},
  booktitle = {{{EMNLP}}},
  author = {Fukui, A. and Park, D. H. and Yang, D. and Rohrbach, A. and Darrell, T. and Rohrbach, M.},
  date = {2016},
  eprint = {1606.01847},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.01847},
  abstract = {Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.},
  archiveprefix = {arXiv},
  annotation = {840 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8LK2D5YK/Fukui et al. - 2016 - Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding(2).pdf}
}

@inproceedings{gaddy2018WhatGoingNeural,
  title = {What’s {{Going On}} in {{Neural Constituency Parsers}}? {{An Analysis}}},
  shorttitle = {What’s {{Going On}} in {{Neural Constituency Parsers}}?},
  booktitle = {{{NAACL}}},
  author = {Gaddy, David and Stern, Mitchell and Klein, Dan},
  date = {2018},
  pages = {999--1010},
  publisher = {{Association for Computational Linguistics}},
  location = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1091},
  url = {http://aclweb.org/anthology/N18-1091},
  urldate = {2020-08-07},
  abstract = {A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and featurerich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.},
  langid = {english},
  annotation = {40 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CKB5WXGN/Gaddy et al. - 2018 - What’s Going On in Neural Constituency Parsers An.pdf}
}

@inproceedings{gaddy2018WhatGoingNeurala,
  title = {What's {{Going On}} in {{Neural Constituency Parsers}}? {{An Analysis}}},
  shorttitle = {What's {{Going On}} in {{Neural Constituency Parsers}}?},
  booktitle = {{{NAACL-HLT}}},
  author = {Gaddy, David and Stern, Mitchell and Klein, Dan},
  date = {2018-04-20},
  eprint = {1804.07853},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.07853},
  urldate = {2021-04-15},
  abstract = {A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.},
  archiveprefix = {arXiv},
  annotation = {40 citations (Semantic Scholar/arXiv) [2021-04-14]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NLUPR54D/Gaddy et al. - 2018 - What's Going On in Neural Constituency Parsers An.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JCXK22E8/1804.html}
}

@inproceedings{gal2016DropoutBayesianApproximation,
  title = {Dropout as a {{Bayesian Approximation}} : {{Representing Model Uncertainty}} in {{Deep Learning}}},
  booktitle = {{{ICML}}},
  author = {Gal, Y. and Ghahramani, Z.},
  date = {2016},
  volume = {48},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QKP9CHQH/Gal, Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning(2).pdf}
}

@article{ganchev2010PosteriorRegularizationStructured,
  title = {Posterior {{Regularization}} for {{Structured Latent Variable Models}}},
  author = {Ganchev, K. and Graça, J. V. and Gillenwater, J. and Taskar, B.},
  date = {2010},
  journaltitle = {JMLR},
  volume = {11},
  issn = {15324435},
  url = {http://dl.acm.org/citation.cfm?id=1756006.1859918},
  abstract = {We present Posterior Regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efficiently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior Regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efficiency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efficient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment.},
  isbn = {1532-4435},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HW777KE9/Ganchev et al. - 2010 - Posterior Regularization for Structured Latent Variable Models(2).pdf}
}

@inproceedings{gangal2020BERTeringRAMSWhat,
  title = {{{BERTering RAMS}}: {{What}} and {{How Much}} Does {{BERT Already Know About Event Arguments}}? -- {{A Study}} on the {{RAMS Dataset}}},
  shorttitle = {{{BERTering RAMS}}},
  booktitle = {{{EMNLP}}},
  author = {Gangal, Varun and Hovy, Eduard},
  date = {2020-10-09},
  eprint = {2010.04098},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.04098},
  urldate = {2021-02-17},
  abstract = {Using the attention map based probing frame-work from (Clark et al., 2019), we observe that, on the RAMS dataset (Ebner et al., 2020), BERT's attention heads have modest but well above-chance ability to spot event arguments sans any training or domain finetuning, vary-ing from a low of 17.77\% for Place to a high of 51.61\% for Artifact. Next, we find that linear combinations of these heads, estimated with approx 11\% of available total event argument detection supervision, can push performance well-higher for some roles - highest two being Victim (68.29\% Accuracy) and Artifact(58.82\% Accuracy). Furthermore, we investigate how well our methods do for cross-sentence event arguments. We propose a procedure to isolate "best heads" for cross-sentence argument detection separately of those for intra-sentence arguments. The heads thus estimated have superior cross-sentence performance compared to their jointly estimated equivalents, albeit only under the unrealistic assumption that we already know the argument is present in an-other sentence. Lastly, we seek to isolate to what extent our numbers stem from lexical frequency based associations between gold arguments and roles. We propose NONCE, a scheme to create adversarial test examples by replacing gold arguments with randomly generated "nonce" words. We find that learnt linear combinations are robust to NONCE, though individual best heads can be more sensitive.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N5YU8ZE5/Gangal and Hovy - 2020 - BERTering RAMS What and How Much does BERT Alread.pdf;/home/hiaoxui/.local/share/zotero_files/storage/54SBLGAP/2010.html}
}

@inproceedings{gao2015CompactBilinearPooling,
  title = {Compact {{Bilinear Pooling}}},
  booktitle = {{{CVPR}}},
  author = {Gao, Y. and Beijbom, O. and Zhang, N. and Darrell, T.},
  date = {2015},
  number = {2},
  eprint = {1511.06062},
  eprinttype = {arxiv},
  pages = {317--326},
  issn = {10636919},
  doi = {10.1109/CVPR.2016.41},
  url = {http://arxiv.org/abs/1511.06062},
  abstract = {Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.},
  archiveprefix = {arXiv},
  isbn = {978-1-4673-8851-1},
  annotation = {462 citations (Semantic Scholar/DOI) [2021-03-26] 462 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FMULMNXN/Gao et al. - 2015 - Compact Bilinear Pooling(2).pdf}
}

@misc{gao2020Pile800GBDataset,
  title = {The {{Pile}}: {{An 800GB Dataset}} of {{Diverse Text}} for {{Language Modeling}}},
  shorttitle = {The {{Pile}}},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  date = {2020-12-31},
  eprint = {2101.00027},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.00027},
  urldate = {2021-04-05},
  abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \textbackslash textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-04-04]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XK2AC4T7/Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf;/home/hiaoxui/.local/share/zotero_files/storage/V8XPSLT7/2101.html}
}

@inproceedings{gao2021MakingPretrainedLanguage,
  title = {Making {{Pre-trained Language Models Better Few-shot Learners}}},
  booktitle = {{{ACL}}},
  author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  date = {2021-06-02},
  eprint = {2012.15723},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.15723},
  urldate = {2022-04-06},
  abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PNTBAHI5/Gao et al. - 2021 - Making Pre-trained Language Models Better Few-shot.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ASTGSU72/2012.html}
}

@inproceedings{gardner2018AllenNLPDeepSemantic,
  title = {{{AllenNLP}}: {{A Deep Semantic Natural Language Processing Platform}}},
  shorttitle = {{{AllenNLP}}},
  booktitle = {{{ACL}}},
  author = {Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke},
  date = {2018-05-31},
  eprint = {1803.07640},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.07640},
  urldate = {2022-02-14},
  abstract = {This paper describes AllenNLP, a platform for research on deep learning methods in natural language understanding. AllenNLP is designed to support researchers who want to build novel language understanding models quickly and easily. It is built on top of PyTorch, allowing for dynamic computation graphs, and provides (1) a flexible data API that handles intelligent batching and padding, (2) high-level abstractions for common operations in working with text, and (3) a modular and extensible experiment framework that makes doing good science easy. It also includes reference implementations of high quality approaches for both core semantic problems (e.g. semantic role labeling (Palmer et al., 2005)) and language understanding applications (e.g. machine comprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source effort maintained by engineers and researchers at the Allen Institute for Artificial Intelligence.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W78HBKQZ/Gardner et al. - 2018 - AllenNLP A Deep Semantic Natural Language Process.pdf;/home/hiaoxui/.local/share/zotero_files/storage/V9G65KVU/1803.html}
}

@inproceedings{gardner2018NeuralSemanticParsing,
  title = {Neural {{Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Gardner, M. and Dasigi, P. and Iyer, S. and Suhr, A. and Zettlemoyer, L. S.},
  date = {2018},
  eprint = {6142362},
  eprinttype = {pmid},
  pages = {17--18},
  doi = {10.1523/JNEUROSCI.5952-09.2010.Orbitofrontal},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2X723RDP/Gardner et al. - 2018 - Neural Semantic Parsing(2).pdf}
}

@misc{garg2022WhatCanTransformers,
  title = {What {{Can Transformers Learn In-Context}}? {{A Case Study}} of {{Simple Function Classes}}},
  shorttitle = {What {{Can Transformers Learn In-Context}}?},
  author = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy and Valiant, Gregory},
  date = {2022-08-01},
  number = {arXiv:2208.01066},
  eprint = {2208.01066},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.01066},
  urldate = {2022-08-12},
  abstract = {In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FNRPQHD5/Garg et al. - 2022 - What Can Transformers Learn In-Context A Case Stu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/526D6DV8/2208.html}
}

@inproceedings{garnelo2018NeuralProcesses,
  title = {Neural {{Processes}}},
  booktitle = {{{ICML}}},
  author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  date = {2018-07-04},
  eprint = {1807.01622},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1807.01622},
  urldate = {2021-03-01},
  abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  archiveprefix = {arXiv},
  annotation = {91 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YLJNPRJP/Garnelo et al. - 2018 - Neural Processes.pdf;/home/hiaoxui/.local/share/zotero_files/storage/525YPWCZ/1807.html}
}

@inproceedings{gasthaus2019ProbabilisticForecastingSpline,
  title = {Probabilistic {{Forecasting}} with {{Spline Quantile Function RNNs}}},
  booktitle = {{{AISTATS}}},
  author = {Gasthaus, J. and Benidis, K. and Wang, Y. and Rangapuram, S. S. and Salinas, D. and Flunkert, V. and Januschowski, T.},
  date = {2019},
  volume = {89},
  pages = {1901--1910},
  url = {http://proceedings.mlr.press/v89/gasthaus19a.html},
  abstract = {In this paper, we propose a flexible method for probabilistic modeling with conditional quantile functions using monotonic regression splines. The shape of the spline is parameterized by a neural network whose parameters are learned by minimizing the continuous ranked probability score. Within this framework, we propose a method for probabilistic time series forecasting, which combines the modeling capacity of recurrent neural networks with the flexibility of a spline-based representation of the output distribution. Unlike methods based on parametric probability density functions and maximum likelihood estimation, the proposed method can flexibly adapt to different output distributions without manual intervention. We empirically demonstrate the effectiveness of the approach on synthetic and real-world data sets.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S6BYQHN7/Gasthaus et al. - 2019 - Probabilistic Forecasting with Spline Quantile Function RNNs(2).pdf}
}

@misc{gatt2017SurveyStateArt,
  title = {Survey of the {{State}} of the {{Art}} in {{Natural Language Generation}}: {{Core}} Tasks, Applications and Evaluation},
  author = {Gatt, A. and Krahmer, E.},
  date = {2017},
  number = {c},
  eprint = {1703.09902},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.09902},
  abstract = {This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artificial intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of Natural Language Processing, with an emphasis on different evaluation methods and the relationships between them.},
  archiveprefix = {arXiv},
  annotation = {331 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BEYJT6NF/Gatt, Krahmer - 2017 - Survey of the State of the Art in Natural Language Generation Core tasks, applications and evaluation(2).pdf}
}

@inproceedings{ge2005StatisticalSemanticParser,
  title = {A {{Statistical Semantic Parser}} That {{Integrates Syntax}} and {{Semantics}}},
  booktitle = {{{CoNLL}}},
  author = {Ge, R. and Mooney, R.},
  date = {2005},
  pages = {9--16},
  doi = {10.3115/1706543.1706546},
  abstract = {We introduce a learning semantic parser, SCISSOR, thatmaps natural-language sentences to a detailed, formal, meaning-representation language. It first uses an integrated statistical parser to produce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We present experimental results demonstrating that SCISSOR produces more accurate semantic representations than several previous approaches.},
  issue = {June},
  keywords = {unread},
  annotation = {189 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V7EMI57P/Ge, Mooney - 2005 - A Statistical Semantic Parser that Integrates Syntax and Semantics(2).pdf}
}

@inproceedings{ge2006DiscriminativeRerankingSemantic,
  title = {Discriminative Reranking for Semantic Parsing},
  booktitle = {{{ACL}}},
  author = {Ge, R. and Mooney, R. J.},
  date = {2006},
  pages = {263},
  doi = {10.3115/1273073.1273107},
  url = {http://portal.acm.org/citation.cfm?id=1273107},
  abstract = {Semantic parsing is the task of mapping natural language sentences to complete formal meaning representations. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. In this paper, we investigate discriminative reranking upon a baseline semantic parser, SCISSOR, where the composition of meaning representations is guided by syntax. We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics. We report experimental results on two real applications, an interpreter for coaching instructions in robotic soccer and a natural-language database interface. The results show that reranking can improve the performance on the coaching interpreter, but not on the database interface.},
  issue = {July},
  keywords = {unread},
  annotation = {54 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B6PQ8QEG/Ge, Mooney - 2006 - Discriminative reranking for semantic parsing(2).pdf}
}

@inproceedings{ghoshal2021LearningBetterStructured,
  title = {Learning {{Better Structured Representation Using Low-Rank Adaptive Label Smoothing}}},
  booktitle = {{{ICLR}}},
  author = {Ghoshal, Asish and Chen, Xilun and Gupta, Sonal and Zettlemoyer, Luke and Mehdad, Yashar},
  date = {2021},
  pages = {15},
  abstract = {Training with soft targets instead of hard targets has been shown to improve performance and calibration of deep neural networks. Label smoothing is a popular way of computing soft targets, where one-hot encoding of a class is smoothed with a uniform distribution. Owing to its simplicity, label smoothing has found widespread use for training deep neural networks on a wide variety of tasks, ranging from image and text classification to machine translation and semantic parsing.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MLMGY3ZG/Ghoshal et al. - 2021 - LEARNING BETTER STRUCTURED REPRESENTATIONS USING L.pdf}
}

@article{gildea2000AutomaticLabelingSemantic,
  title = {Automatic Labeling of Semantic Roles},
  author = {Gildea, D. and Jurafsky, D.},
  date = {2000},
  journaltitle = {Computational Linguistics},
  number = {1972},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {512--520},
  issn = {0891-2017},
  doi = {10.3115/1075218.1075283},
  url = {http://portal.acm.org/citation.cfm?doid=1075218.1075283},
  abstract = {We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data. 1},
  archiveprefix = {arXiv},
  isbn = {0891-2017},
  keywords = {unread},
  annotation = {931 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/93TKTRIA/Gildea, Jurafsky - 2000 - Automatic labeling of semantic roles(2).pdf}
}

@article{giles1992LearningExtractingFinite,
  title = {Learning and {{Extracting Finite State Automata}} with {{Second-Order Recurrent Neural Networks}}},
  author = {Giles, C. L. and Miller, C. B. and Chen, D. and Chen, H. H. and Sun, G. Z. and Lee, Y. C.},
  date = {1992},
  journaltitle = {Neural Computation},
  volume = {4},
  number = {3},
  pages = {393--405},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.3.393},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.3.393#.Vr3oBfnhAuU},
  abstract = {We show that a recurrent, second-order neural network using a real-time, forward training algorithm readily learns to infer small regular grammars from positive and negative string training samples. We present simulations that show the effect of initial conditions, training set size and order, and neural network architecture. All simulations were performed with random initial weight strengths and usually converge after approximately a hundred epochs of training. We discuss a quantization algorithm for dynamically extracting finite state automata during and after training. For a well-trained neural net, the extracted automata constitute an equivalence class of state machines that are reducible to the minimal machine of the inferred grammar. We then show through simulations that many of the neural net state machines are dynamically stable, that is, they correctly classify many long unseen strings. In addition, some of these extracted automata actually outperform the trained neural network for classification...},
  keywords = {unread},
  annotation = {472 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JT6YWNRL/Giles et al. - 1992 - Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks(2).pdf}
}

@inproceedings{giorgi2021DeCLUTRDeepContrastive,
  title = {{{DeCLUTR}}: {{Deep Contrastive Learning}} for {{Unsupervised Textual Representations}}},
  shorttitle = {{{DeCLUTR}}},
  booktitle = {{{ACL}}},
  author = {Giorgi, John and Nitski, Osvald and Wang, Bo and Bader, Gary},
  date = {2021-05-27},
  eprint = {2006.03659},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2006.03659},
  urldate = {2022-10-03},
  abstract = {Sentence embeddings are an important component of many natural language processing (NLP) systems. Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval. Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. Inspired by recent advances in deep metric learning (DML), we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data. When used to extend the pretraining of transformer-based language models, our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders. Importantly, our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data. Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CYNHESNR/Giorgi et al. - 2021 - DeCLUTR Deep Contrastive Learning for Unsupervise.pdf;/home/hiaoxui/.local/share/zotero_files/storage/FK25AX2J/2006.html}
}

@inproceedings{girshick2014RichFeatureHierarchies,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  booktitle = {{{CVPR}}},
  author = {Girshick, R. and Donahue, J. and Darrell, T. and Malik, J.},
  date = {2014},
  eprint = {26656583},
  eprinttype = {pmid},
  pages = {2--9},
  issn = {10636919},
  doi = {10.1109/CVPR.2014.81},
  archiveprefix = {arXiv},
  isbn = {978-1-4799-5118-5},
  annotation = {9996 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6PIBPW5Y/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation(2).pdf}
}

@inproceedings{girshick2015FastRCNN,
  title = {Fast {{R-CNN}}},
  booktitle = {{{ICCV}}},
  author = {Girshick, R.},
  date = {2015},
  volume = {2015 Inter},
  eprint = {23739795},
  eprinttype = {pmid},
  pages = {1440--1448},
  issn = {15505499},
  doi = {10.1109/ICCV.2015.169},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  archiveprefix = {arXiv},
  isbn = {978-1-4673-8391-2},
  annotation = {9993 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NW8SARPG/Girshick - 2015 - Fast R-CNN(2).pdf}
}

@misc{gkatzia2016ContentSelectionDatatoText,
  title = {Content {{Selection}} in {{Data-to-Text Systems}}: {{A Survey}}},
  author = {Gkatzia, D.},
  date = {2016},
  eprint = {1610.08375v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UVT99BDT/Gkatzia - 2016 - Content Selection in Data-to-Text Systems A Survey(2).pdf}
}

@inproceedings{glove2014,
  title = {{{GloVe}}: {{Global Vector}} for {{Word Representation}}},
  booktitle = {{{EMNLP}}},
  author = {Pennington, J. and Socher, R. and Manning, C. D.},
  date = {2014},
  eprint = {1710995},
  eprinttype = {pmid},
  issn = {00047554},
  doi = {10.3115/v1/D14-1162},
  url = {http://nlp.},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  archiveprefix = {arXiv},
  isbn = {978-1-937284-96-1},
  annotation = {9993 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CJE4K685/Pennington, Socher, Manning - 2014 - GloVe Global Vector for Word Representation(2).pdf}
}

@misc{goldberg2019AssessingBERTSyntactic,
  title = {Assessing {{BERT}}'s {{Syntactic Abilities}}},
  author = {Goldberg, Yoav},
  date = {2019-01-16},
  eprint = {1901.05287},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.05287},
  urldate = {2020-11-19},
  archiveprefix = {arXiv},
  annotation = {190 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ILFTMEJI/Goldberg - 2019 - Assessing BERT's Syntactic Abilities.pdf;/home/hiaoxui/.local/share/zotero_files/storage/TTWDDREB/1901.html}
}

@inproceedings{gong2018HeavyKeeperAccurateAlgorithm,
  title = {{{HeavyKeeper}}: {{An Accurate Algorithm}} for {{Finding Top-k Elephant Flows}}},
  booktitle = {{{USENIX Annual Technical Conference}}},
  author = {Gong, J. and Yang, T. and Zhang, H. and Hao, L. and Uhlig, S. and Chen, S. and Uden, L. and Li, X.},
  date = {2018},
  issn = {1063-6692},
  doi = {10.1109/tnet.2019.2933868},
  abstract = {Finding top-k elephant flows is a critical task in network traffic measurement, with many applications in congestion control, anomaly detection and traffic engineering. As the line rates keep increasing in today's networks, designing accurate and fast algorithms for online identification of elephant flows becomes more and more challenging. The prior algorithms are seriously limited in achieving accuracy under the constraints of heavy traffic and small on-chip memory in use. We observe that the basic strategies adopted by these algorithms either require significant space overhead to measure the sizes of all flows or incur significant inaccuracy when deciding which flows to keep track of. In this paper, we adopt a new strategy, called count-with-exponential-decay, to achieve space-accuracy balance by actively removing small flows through decaying, while minimizing the impact on large flows, so as to achieve high precision in finding top-k elephant flows. Moreover, the proposed algorithm called HeavyKeeper incurs small, constant processing overhead per packet and thus supports high line rates. Experimental results show that HeavyKeeper algorithm achieves 99.99\% precision with a small memory size, and reduces the error by around 3 orders of magnitude on average compared to the state-of-the-art.},
  isbn = {978-1-939133-02-1},
  annotation = {21 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4GWWS9RJ/Gong et al. - 2018 - HeavyKeeper An Accurate Algorithm for Finding Top-k Elephant Flows(2).pdf}
}

@inproceedings{goodfellow2014GenerativeAdversarialNets,
  title = {Generative {{Adversarial Nets}}},
  booktitle = {{{NeurIPS}}},
  author = {Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
  date = {2014},
  eprint = {1000183096},
  eprinttype = {pmid},
  pages = {2672--2680},
  issn = {10495258},
  doi = {10.1017/CBO9781139058452},
  abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
  archiveprefix = {arXiv},
  isbn = {1406.2661},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M6UCMDKC/Goodfellow et al. - 2014 - Generative Adversarial Nets(2).pdf}
}

@inproceedings{goodman2016UCLSheffieldSemEval2016,
  title = {{{UCL}}+{{Sheffield}} at {{SemEval-2016}} Task 8: {{Imitation}} Learning for Amr Parsing with an α-Bound},
  booktitle = {{{SemEval}}},
  author = {Goodman, J. and Vlachos, A. and Naradowsky, J.},
  date = {2016},
  pages = {1167--1172},
  doi = {10.18653/v1/s16-1180},
  abstract = {We develop a novel transition-based parsing algorithm for the abstract meaning representation parsing task using exact imitation learning, in which the parser learns a statistical model by imitating the actions of an expert on the training data. We then use the imitation learning algorithm DAgger to improve the performance, and apply an α-bound as a simple noise reduction technique. Our performance on the test set was 60\% in F-score, and the performance gains on the development set due to DAgger was up to 1.1 points of F-score. The α-bound improved performance by up to 1.8 points.},
  annotation = {17 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HITSGN75/Goodman, Vlachos, Naradowsky - 2016 - UCLSheffield at SemEval-2016 task 8 Imitation learning for amr parsing with an α-bound(2).pdf}
}

@inproceedings{gormley2014LowresourceSemanticRole,
  title = {Low-Resource Semantic Role Labeling},
  booktitle = {{{ACL}}},
  author = {Gormley, M. R. and Mitchell, M. and Van Durme, B. and Dredze, Mark},
  date = {2014},
  pages = {1177--1187},
  abstract = {We explore the extent to which highresource manual annotations such as treebanks are necessary for the task of semantic role labeling (SRL). We examine how performance changes without syntactic supervision, comparing both joint and pipelined methods to induce latent syntax. This work highlights a new application of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax. Our best models obtain competitive results in the high-resource setting and state-ofthe- art results in the low resource setting, reaching 72.48\% F1 averaged across languages. We release our code for this work along with a larger toolkit for specifying arbitrary graphical structure. © 2014 Association for Computational Linguistics.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZGJZ7X2A/Gormley et al. - 2014 - Low-resource semantic role labeling(2).pdf}
}

@article{gorniak2007SituatedLanguageUnderstanding,
  title = {Situated Language Understanding as Filtering Perceived Affordances.},
  author = {Gorniak, P. and Roy, D.},
  date = {2007},
  journaltitle = {Cognitive Science},
  volume = {31},
  number = {2},
  eprint = {21635295},
  eprinttype = {pmid},
  pages = {197--231},
  issn = {0364-0213},
  doi = {10.1080/15326900701221199},
  abstract = {We introduce a computational theory of situated language understanding in which the meaning of words and utterances depends on the physical environment and the goals and plans of communication partners. According to the theory, concepts that ground linguistic meaning are neither internal nor external to language users, but instead span the objective-subjective boundary. To model the possible interactions between subject and object, the theory relies on the notion of perceived affordances: structured units of interaction that can be used for prediction at multiple levels of abstraction. Language understanding is treated as a process of filtering perceived affordances. The theory accounts for many aspects of the situated nature of human language use and provides a unified solution to a number of demands on any theory of language understanding including conceptual combination, prototypicality effects, and the generative nature of lexical items. To support the theory, we describe an implemented system that understands verbal commands situated in a virtual gaming environment. The implementation uses probabilistic hierarchical plan recognition to generate perceived affordances. The system has been evaluated on its ability to correctly interpret free-form spontaneous verbal commands recorded from unrehearsed game play between human players. The system is able to "step into the shoes" of human players and correctly respond to a broad range of verbal commands in which linguistic meaning depends on social and physical context. We quantitatively compare the system's predictions in response to direct player commands with the actions taken by human players and show generalization to unseen data across a range of situations and verbal constructions.},
  isbn = {0364-0213 1551-6709},
  keywords = {unread},
  annotation = {62 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5RE95XFD/Gorniak, Roy - 2007 - Situated language understanding as filtering perceived affordances(2).pdf}
}

@inproceedings{goulding2016EventSeriesPrediction,
  title = {Event Series Prediction via Non-Homogeneous Poisson Process Modelling},
  booktitle = {{{ICDM}}},
  author = {Goulding, J. and Preston, S. and Smith, G.},
  date = {2016},
  publisher = {{IEEE}},
  issn = {15504786},
  doi = {10.1109/ICDM.2016.150},
  abstract = {© 2016 IEEE. Data streams whose events occur at random arrival times rather than at the regular, tick-Tock intervals of traditional time series are increasingly prevalent. Event series are continuous, irregular and often highly sparse, differing greatly in nature to the regularly sampled time series traditionally the concern of hard sciences. As mass sets of such data have become more common, so interest in predicting future events in them has grown. Yet repurposing of traditional forecasting approaches has proven ineffective, in part due to issues such as sparsity, but often due to inapplicable underpinning assumptions such as stationarity and ergodicity. In this paper we derive a principled new approach to forecasting event series that avoids such assumptions, based upon: 1. The processing of event series datasets in order to produce a first parameterized mixture model of non-homogeneous Poisson processes, and 2. Application of a technique called parallel forecasting that uses these processes' rate functions to directly generate accurate temporal predictions for new query realizations. This approach uses forerunners of a stochastic process to shed light on the distribution of future events, not for themselves, but for realizations that subsequently follow in their footsteps.},
  isbn = {978-1-5090-5472-5},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HT47K5FF/Goulding, Preston, Smith - 2016 - Event series prediction via non-homogeneous poisson process modelling(2).pdf}
}

@misc{govindarajan2019DecomposingGeneralizationModels,
  title = {Decomposing {{Generalization}}: {{Models}} of {{Generic}}, {{Habitual}}, and {{Episodic Statements}}},
  author = {Govindarajan, V. and Van Durme, B. and White, A. S.},
  date = {2019},
  eprint = {1901.11429},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.11429},
  abstract = {We present a novel semantic framework for modeling linguistic expressions of generalization---generic, habitual, and episodic statements---as combinations of simple, real-valued referential properties of predicates and their arguments. We use this framework to construct a dataset covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to probe the efficacy of type-level and token-level information---including hand-engineered features and static (GloVe) and contextual (ELMo) word embeddings---for predicting expressions of generalization. Data and code are available at decomp.io.},
  archiveprefix = {arXiv},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JEEFAPZC/Govindarajan, Van Durme, White - 2019 - Decomposing Generalization Models of Generic, Habitual, and Episodic Statements(2).pdf}
}

@inproceedings{goyal2011ApproximateScalableBounded,
  title = {Approximate Scalable Bounded Space Sketch for Large Data {{NLP}}},
  booktitle = {{{EMNLP}}},
  author = {Goyal, A. and Daumé III, H.},
  date = {2011},
  pages = {250--261},
  abstract = {We exploit sketch techniques, especially the Count-Min sketch, a memory, and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself. These methods use hashing to deal with massive amounts of streaming text. We apply Count-Min sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory). © 2011 Association for Computational Linguistics.},
  isbn = {1-937284-11-5},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IREKBSAD/Goyal, Daumé III - 2011 - Approximate scalable bounded space sketch for large data NLP(2).pdf}
}

@inproceedings{goyal2011LossyConservativeUpdate,
  title = {Lossy Conservative Update ({{LCU}}) Sketch: {{Succinct}} Approximate Count Storage},
  booktitle = {{{AAAI}}},
  author = {Goyal, A. and Daumé III, H.},
  date = {2011},
  pages = {878--883},
  abstract = {In this paper, we propose a variant of the conservative-update Count-Min sketch to further reduce the over-estimation error incurred. Inspired by ideas from lossy counting, we divide a stream of items into multiple windows, and decrement certain counts in the sketch at window boundaries. We refer to this approach as a lossy conservative update (LCU). The reduction in over-estimation error of counts comes at the cost of introducing under-estimation error in counts. However, in our intrinsic evaluations, we show that the reduction in over-estimation is much greater than the under-estimation error introduced by our method LCU. We apply our LCU framework to scale distributional similarity computations to web-scale corpora. We show that this technique is more efficient in terms of memory, and time, and more robust than conservative update with Count-Min (CU) sketch on this task. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.},
  isbn = {978-1-57735-508-3},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9D2PP5D3/Goyal, Daumé III - 2011 - Lossy conservative update (LCU) sketch Succinct approximate count storage(2).pdf}
}

@misc{goyal2021NeuralProductionSystems,
  title = {Neural {{Production Systems}}},
  author = {Goyal, Anirudh and Didolkar, Aniket and Ke, Nan Rosemary and Blundell, Charles and Beaudoin, Philippe and Heess, Nicolas and Mozer, Michael and Bengio, Yoshua},
  date = {2021-03-02},
  eprint = {2103.01937},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.01937},
  urldate = {2021-06-21},
  abstract = {Visual environments are structured, consisting of distinct objects or entities. These entities have properties -- both visible and latent -- that determine the manner in which they interact with one another. To partition images into entities, deep-learning researchers have proposed structural inductive biases such as slot-based architectures. To model interactions among entities, equivariant graph neural nets (GNNs) are used, but these are not particularly well suited to the task for two reasons. First, GNNs do not predispose interactions to be sparse, as relationships among independent entities are likely to be. Second, GNNs do not factorize knowledge about interactions in an entity-conditional manner. As an alternative, we take inspiration from cognitive science and resurrect a classic approach, production systems, which consist of a set of rule templates that are applied by binding placeholder variables in the rules to specific entities. Rules are scored on their match to entities, and the best fitting rules are applied to update entity properties. In a series of experiments, we demonstrate that this architecture achieves a flexible, dynamic flow of control and serves to factorize entity-specific and rule-based information. This disentangling of knowledge achieves robust future-state prediction in rich visual environments, outperforming state-of-the-art methods using GNNs, and allows for the extrapolation from simple (few object) environments to more complex environments.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-06-21]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z5ERKW7N/Goyal et al. - 2021 - Neural Production Systems.pdf;/home/hiaoxui/.local/share/zotero_files/storage/SSCN44NL/2103.html}
}

@inproceedings{graca2008ExpectationMaximizationPosterior,
  title = {Expectation {{Maximization}} and {{Posterior Constraints}}},
  booktitle = {{{NeurIPS}}},
  author = {Graça, J. V. and Ganchev, K. and Taskar, B.},
  date = {2008},
  abstract = {The expectation maximization (EM) algorithm is a widely used\textbackslash nmaximum likeli- hood estimation procedure for statistical models\textbackslash nwhen the values of some of the variables in the model are not\textbackslash nobserved. Very often, however, our aim is primar- ily to find a\textbackslash nmodel that assigns values to the latent variables that have\textbackslash nintended meaning for our data and maximizing expected likelihood\textbackslash nonly sometimes ac- complishes this. Unfortunately, it is\textbackslash ntypically difficult to add even simple a-priori information about\textbackslash nlatent variables in graphical models without making the models\textbackslash noverly complex or intractable. In this paper, we present an\textbackslash nefficient, principled way to inject rich constraints on the\textbackslash nposteriors of latent variables into the EM algorithm. Our method\textbackslash ncan be used to learn tractable graphical models that sat- isfy\textbackslash nadditional, otherwise intractable constraints. Focusing on\textbackslash nclustering and the alignment problem for statistical machine\textbackslash ntranslation, we show that simple, in- tuitive posterior\textbackslash nconstraints can greatly improve the performance over standard\textbackslash nbaselines and be competitive with more complex, intractable\textbackslash nmodels.},
  isbn = {1-60560-352-X},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/59B5RJCT/Graça, Ganchev, Taskar - 2008 - Expectation Maximization and Posterior Constraints(2).pdf}
}

@article{graca2010LearningTractableWord,
  title = {Learning {{Tractable Word Alignment Models}} with {{Complex Constraints}}},
  author = {Graça, J. V. and Ganchev, K. and Taskar, B.},
  date = {2010},
  journaltitle = {Computational Linguistics},
  volume = {36},
  number = {3},
  pages = {481--504},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00007},
  abstract = {Word-level alignment of bilingual text is a critical resource for a growing variety of tasks. Proba- bilistic models for word alignment present a fundamental trade-off between richness of captured constraints and correlations versus efficiency and tractability of inference. In this article, we use the Posterior Regularization framework (Gra¸ ca, Ganchev, and Taskar 2007) to incorporate complex constraints into probabilistic models during learning without changing the efficiency of the underlying model. We focus on the simple and tractable hidden Markov model, and present an efficient learning algorithm for incorporating approximate bijectivity and symmetry constraints. Models estimated with these constraints produce a significant boost in performance as measured by both precision and recall of manually annotated alignments for six language pairs. We also report experiments on two different tasks where word alignments are required: phrase-based machine translation and syntax transfer, and show promising improvements over standard methods},
  isbn = {0891-2017},
  annotation = {32 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V8WBDPEH/Graça, Ganchev, Taskar - 2010 - Learning Tractable Word Alignment Models with Complex Constraints(2).pdf}
}

@inproceedings{grachev2017NeuralNetworksCompression,
  title = {Neural {{Networks Compression}} for {{Language Modeling}}},
  booktitle = {International {{Conference}} on {{Pattern Recognition}} and {{Artificial Intelligence}}},
  author = {Grachev, A. M. and Ignatov, D. I. and Savchenko, A. V.},
  date = {2017},
  eprint = {1708.05963v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JR6SV26J/Grachev, Ignatov, Savchenko - 2017 - Neural Networks Compression for Language Modeling(2).pdf}
}

@inproceedings{grave2015ConvexFeaturerichDiscriminative,
  title = {A Convex and Feature-Rich Discriminative Approach to Dependency Grammar Induction},
  booktitle = {{{ACL-IJCNLP}}},
  author = {Grave, É. and Elhadad, N.},
  date = {2015},
  pages = {1375--1384},
  abstract = {In this paper, we introduce a new method for the problem of unsupervised depen-dency parsing. Most current approaches are based on generative models. Learning the parameters of such models relies on solving a non-convex optimization prob-lem, thus making them sensitive to initial-ization. We propose a new convex formu-lation to the task of dependency grammar induction. Our approach is discriminative, allowing the use of different kinds of fea-tures. We describe an efficient optimiza-tion algorithm to learn the parameters of our model, based on the Frank-Wolfe algo-rithm. Our method can easily be general-ized to other unsupervised learning prob-lems. We evaluate our approach on ten languages belonging to four different fam-ilies, showing that our method is competi-tive with other state-of-the-art methods.},
  isbn = {978-1-941643-72-3},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VIHHQUPE/Grave, Elhadad - 2015 - A convex and feature-rich discriminative approach to dependency grammar induction(2).pdf}
}

@article{graves2016HybridComputingUsing,
  title = {Hybrid Computing Using a Neural Network with Dynamic External Memory},
  author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  date = {2016-10},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {538},
  number = {7626},
  pages = {471--476},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature20101},
  url = {http://www.nature.com/articles/nature20101},
  urldate = {2021-09-07},
  langid = {english},
  annotation = {1067 citations (Semantic Scholar/DOI) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VRRIDX2L/2016-graves.pdf}
}

@misc{graves2017AutomatedCurriculumLearning,
  title = {Automated {{Curriculum Learning}} for {{Neural Networks}}},
  author = {Graves, A. and Bellemare, M. G. and Menick, J. and Munos, R. and Kavukcuoglu, K.},
  date = {2017},
  eprint = {1704.03003},
  eprinttype = {arxiv},
  issn = {1938-7228},
  url = {http://arxiv.org/abs/1704.03003},
  abstract = {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.},
  archiveprefix = {arXiv},
  isbn = {9781510855144},
  annotation = {229 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UZEFR3B3/Graves et al. - 2017 - Automated Curriculum Learning for Neural Networks(2).pdf}
}

@article{green1995ReversibleJumpMarkov,
  title = {Reversible {{Jump Markov Chain Monte Carlo Computation}} and {{Bayesian Model Determination}}},
  author = {Green, P. J.},
  date = {1995},
  journaltitle = {Biometrika},
  volume = {82},
  number = {4},
  pages = {711--732},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SAEHPA6D/Green - 1995 - Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination(2).pdf}
}

@inproceedings{grenager2005UnsupervisedLearningField,
  title = {Unsupervised {{Learning}} of {{Field Segmentation Models}} for {{Information Extraction}}},
  booktitle = {{{ACL}}},
  author = {Grenager, T. and Klein, D. and Manning, C. D.},
  date = {2005},
  pages = {371--378},
  doi = {10.3115/1219840.1219886},
  url = {http://portal.acm.org/citation.cfm?id=1219886&amp;dl=},
  abstract = {The applicability of many current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.},
  isbn = {1-932432-51-5},
  issue = {June},
  keywords = {unread},
  annotation = {88 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H9UE7SLR/Grenager, Klein, Manning - 2005 - Unsupervised Learning of Field Segmentation Models for Information Extraction(2).pdf}
}

@inproceedings{grivas2022LowRankSoftmaxCan,
  title = {Low-{{Rank Softmax Can Have Unargmaxable Classes}} in {{Theory}} but {{Rarely}} in {{Practice}}},
  booktitle = {{{ACL}}},
  author = {Grivas, Andreas and Bogoychev, Nikolay and Lopez, Adam},
  date = {2022-03-21},
  eprint = {2203.06462},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.06462},
  urldate = {2022-03-23},
  abstract = {Classifiers in natural language processing (NLP) often have a large number of output classes. For example, neural language models (LMs) and machine translation (MT) models both predict tokens from a vocabulary of thousands. The Softmax output layer of these models typically receives as input a dense feature representation, which has much lower dimensionality than the output. In theory, the result is some words may be impossible to be predicted via argmax, irrespective of input features, and empirically, there is evidence this happens in small language models. In this paper we ask whether it can happen in practical large language models and translation models. To do so, we develop algorithms to detect such \textbackslash emph\{unargmaxable\} tokens in public models. We find that 13 out of 150 models do indeed have such tokens; however, they are very infrequent and unlikely to impact model quality. We release our code so that others can inspect their models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6CSUT6I9/Grivas et al. - 2022 - Low-Rank Softmax Can Have Unargmaxable Classes in .pdf;/home/hiaoxui/.local/share/zotero_files/storage/IUH2L49A/2203.html}
}

@article{grosz1986AttentionIntentionsStructure,
  title = {Attention, Intentions, and the Structure of Discourse},
  author = {Grosz, B. J. and Sidner, C. L.},
  date = {1986},
  journaltitle = {Computational Linguistics},
  volume = {12},
  number = {3},
  pages = {175--204},
  issn = {08912017},
  doi = {10.14348/molcells.2014.0104},
  url = {http://dl.acm.org/citation.cfm?id=12458},
  abstract = {In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse. In this theory, discourse structure is composed of three separate but interre- lated components: the structure of the sequence of utterances (called the linguistic structure), a struc- ture of purposes (called the intentional structure), and the state of focus of attention (called the attentional state). The linguistic structure consists of segments of the discourse into which the utter- ances naturally aggregate. The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds. The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse. The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions. The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses. Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored. This theory provides a framework for describing the processing of utterances in a discourse. Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and track- ing the discourse through the operation of the mechanisms associated with attentional state. This processing description specifies in these recognition tasks the role of information from the discourse and from the participants' knowledge of the domain.},
  isbn = {0891-2017},
  keywords = {unread},
  annotation = {92 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T9JY45CH/Grosz, Sidner - 1986 - Attention, intentions, and the structure of discourse(2).pdf}
}

@inproceedings{gu2018NonautoregressiveNeuralMachine,
  title = {Non-Autoregressive Neural Machine Translation},
  booktitle = {{{ICLR}}},
  author = {Gu, J. and Bradbury, J. and Xiong, C. and Li, V. O. K. and Socher, R.},
  date = {2018},
  eprint = {1808.08583},
  eprinttype = {arxiv},
  doi = {10.18653/v1/d18-1044},
  abstract = {Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {28 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G6ECYTZM/Gu et al. - 2018 - Non-autoregressive neural machine translation(2).pdf}
}

@inproceedings{gu2019LevenshteinTransformer,
  title = {Levenshtein {{Transformer}}},
  booktitle = {{{NeurIPS}}},
  author = {Gu, J. and Wang, C. and Zhao, J},
  date = {2019},
  eprint = {1905.11006},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.11006},
  abstract = {Modern neural sequence generation models are built to either generate tokens step-by-step from scratch or (iteratively) modify a sequence of tokens bounded by a fixed length. In this work, we develop Levenshtein Transformer, a new partially autoregressive model devised for more flexible and amenable sequence generation. Unlike previous approaches, the atomic operations of our model are insertion and deletion. The combination of them facilitates not only generation but also sequence refinement allowing dynamic length changes. We also propose a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable performance but much-improved efficiency on both generation (e.g. machine translation, text summarization) and refinement tasks (e.g. automatic post-editing). We further confirm the flexibility of our model by showing a Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {79 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KPXPBLUY/Gu, Wang, Zhao - 2019 - Levenshtein Transformer(2).pdf}
}

@inproceedings{gu2020HiPPORecurrentMemory,
  title = {{{HiPPO}}: {{Recurrent Memory}} with {{Optimal Polynomial Projections}}},
  shorttitle = {{{HiPPO}}},
  booktitle = {{{NeurIPS}}},
  author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and Re, Christopher},
  date = {2020-10-22},
  eprint = {2008.07669},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2008.07669},
  urldate = {2022-02-28},
  abstract = {A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3\%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40\% accuracy.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KN2ZI457/Gu et al. - 2020 - HiPPO Recurrent Memory with Optimal Polynomial Pr.pdf;/home/hiaoxui/.local/share/zotero_files/storage/MEACC22G/2008.html}
}

@article{gu2020InsertionbasedDecodingAutomatically,
  title = {Insertion-Based {{Decoding}} with Automatically {{Inferred Generation Order}}},
  author = {Gu, J. and Liu, Q. and Cho, K.},
  date = {2020},
  journaltitle = {TACL},
  eprint = {1902.01370},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1902.01370},
  abstract = {Conventional neural autoregressive decoding commonly assumes a fixed left-to-right generation order, which may be sub-optimal. In this work, we propose a novel decoding algorithm -- InDIGO -- which supports flexible sequence generation in arbitrary orders through insertion operations. We extend Transformer, a state-of-the-art sequence generation model, to efficiently implement the proposed approach, enabling it to be trained with either a pre-defined generation order or adaptive orders obtained from beam-search. Experiments on four real-world tasks, including word order recovery, machine translation, image caption and code generation, demonstrate that our algorithm can generate sequences following arbitrary orders, while achieving competitive or even better performance compared to the conventional left-to-right generation. The generated sequences show that InDIGO adopts adaptive generation orders based on input information.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {60 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S6IS7H3U/Gu, Liu, Cho - 2020 - Insertion-based Decoding with automatically Inferred Generation Order(2).pdf}
}

@inproceedings{gu2021CombiningRecurrentConvolutional,
  title = {Combining {{Recurrent}}, {{Convolutional}}, and {{Continuous-time Models}} with {{Linear State-Space Layers}}},
  booktitle = {{{NeurIPS}}},
  author = {Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and Ré, Christopher},
  date = {2021},
  pages = {14},
  abstract = {Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence u → y by simply simulating a linear continuous-time state-space representation x˙ = Ax + Bu, y = Cx + Du. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices A that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use handcrafted features on 100x shorter sequences.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XCVKM54L/Gu et al. - Combining Recurrent, Convolutional, and Continuous.pdf}
}

@inproceedings{gu2022EfficientlyModelingLong,
  title = {Efficiently {{Modeling Long Sequences}} with {{Structured State Spaces}}},
  booktitle = {{{ICLR}}},
  author = {Gu, Albert and Goel, Karan and Ré, Christopher},
  date = {2022},
  eprint = {2111.00396},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.00396},
  urldate = {2022-02-24},
  abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \textbackslash ( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \textbackslash ), and showed that for appropriate choices of the state matrix \textbackslash ( A \textbackslash ), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space (S4) sequence model based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \textbackslash ( A \textbackslash ) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\textbackslash\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60\textbackslash times\$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YYCDTMH3/Gu et al. - 2021 - Efficiently Modeling Long Sequences with Structure.pdf;/home/hiaoxui/.local/share/zotero_files/storage/VH8Q52FV/2111.html}
}

@inproceedings{gu2022EmpiricalStudyFinding,
  title = {An {{Empirical Study}} on {{Finding Spans}}},
  booktitle = {{{EMNLP}}},
  author = {Gu, Weiwei and Zheng, Boyuan and Chen, Yunmo and Chen, Tongfei and Van Durme, Benjamin},
  date = {2022-10-13},
  eprint = {2210.06824},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.06824},
  urldate = {2022-10-24},
  abstract = {We present an empirical study on methods for span finding, the selection of consecutive tokens in text for some downstream tasks. We focus on approaches that can be employed in training end-to-end information extraction systems, and find there is no definitive solution without considering task properties, and provide our observations to help with future design choices: 1) a tagging approach often yields higher precision while span enumeration and boundary prediction provide higher recall; 2) span type information can benefit a boundary prediction approach; 3) additional contextualization does not help span finding in most cases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2HLRXNBI/Gu et al. - 2022 - An Empirical Study on Finding Spans.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ASZ3CPHP/2210.html}
}

@inproceedings{gui2019LexiconBasedGraphNeural,
  title = {A {{Lexicon-Based Graph Neural Network}} for {{Chinese NER}}},
  booktitle = {{{EMNLP}}},
  author = {Gui, T. and Zou, Y. and Zhang, Q.},
  date = {2019},
  pages = {1040--1050},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UUPHLP85/Gui, Zou, Zhang - 2019 - A Lexicon-Based Graph Neural Network for Chinese NER(2).pdf}
}

@inproceedings{guo2018DialogtoActionConversationalQuestion,
  title = {Dialog-to-{{Action}}: {{Conversational Question Answering Over}} a {{Large-Scale Knowledge Base}}},
  booktitle = {{{NeurIPS}}},
  author = {Guo, Daya and Tang, Duyu and Duan, Nan and Zhou, Ming and Yin, Jian},
  date = {2018},
  pages = {10},
  abstract = {We present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. To handle enormous ellipsis phenomena in conversation, we introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances. Dialog memory management is embodied in a generative model, in which a logical form is interpreted in a top-down manner following a small and flexible grammar. We learn the model from denotations without explicit annotation of logical forms, and evaluate it on a large-scale dataset consisting of 200K dialogs over 12.8M entities. Results verify the benefits of modeling dialog memory, and show that our semantic parsing-based approach outperforms a memory network based encoder-decoder model by a huge margin.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2NBHMT7Y/Guo et al. - Dialog-to-Action Conversational Question Answerin.pdf;/home/hiaoxui/.local/share/zotero_files/storage/DZD86BWW/Guo et al. - Dialog-to-Action Conversational Question Answerin.pdf}
}

@inproceedings{guo2021LICHEEImprovingLanguage,
  title = {{{LICHEE}}: {{Improving Language Model Pre-training}} with {{Multi-grained Tokenization}}},
  shorttitle = {{{LICHEE}}},
  booktitle = {{{ACL}}},
  author = {Guo, Weidong and Zhao, Mingjun and Zhang, Lusheng and Niu, Di and Luo, Jinwen and Liu, Zhenhua and Li, Zhenyang and Tang, Jianbo},
  date = {2021},
  eprint = {2108.00801},
  eprinttype = {arxiv},
  pages = {1383--1392},
  doi = {10.18653/v1/2021.findings-acl.119},
  url = {http://arxiv.org/abs/2108.00801},
  urldate = {2021-09-07},
  abstract = {Language model pre-training based on large corpora has achieved tremendous success in terms of constructing enriched contextual representations and has led to significant performance gains on a diverse range of Natural Language Understanding (NLU) tasks. Despite the success, most current pre-trained language models, such as BERT, are trained based on single-grained tokenization, usually with fine-grained characters or sub-words, making it hard for them to learn the precise meaning of coarse-grained words and phrases. In this paper, we propose a simple yet effective pre-training method named LICHEE to efficiently incorporate multi-grained information of input text. Our method can be applied to various pre-trained language models and improve their representation capability. Extensive experiments conducted on CLUE and SuperGLUE demonstrate that our method achieves comprehensive improvements on a wide variety of NLU tasks in both Chinese and English with little extra inference cost incurred, and that our best ensemble model achieves the state-of-the-art performance on CLUE benchmark competition.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-07] 0 citations (Semantic Scholar/DOI) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YYNZYBKY/Guo et al. - 2021 - LICHEE Improving Language Model Pre-training with.pdf;/home/hiaoxui/.local/share/zotero_files/storage/PRSMJ495/2108.html}
}

@inproceedings{gupta2020NeuralModuleNetworks,
  title = {Neural {{Module Networks}} for {{Reasoning}} over {{Text}}},
  booktitle = {{{ICLR}}},
  author = {Gupta, Nitish and Lin, Kevin and Roth, Dan and Singh, Sameer and Gardner, Matt},
  date = {2020-02-15},
  eprint = {1912.04971},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1912.04971},
  urldate = {2021-03-02},
  abstract = {Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains. However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning. Our proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules.},
  archiveprefix = {arXiv},
  annotation = {25 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8XQPZ5I7/Gupta et al. - 2020 - Neural Module Networks for Reasoning over Text.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KUXIYYEH/1912.html}
}

@inproceedings{gururangan2018AnnotationArtifactsNatural,
  title = {Annotation {{Artifacts}} in {{Natural Language Inference Data}}},
  booktitle = {{{NAACL}}},
  author = {Gururangan, S. and Swayamdipta, S. and Levy, O. and Schwartz, R. and Bowman, S. R. and Smith, N. A.},
  date = {2018},
  eprint = {1803.02324},
  eprinttype = {arxiv},
  issn = {1702.00887},
  doi = {10.18653/v1/N18-2017},
  url = {http://arxiv.org/abs/1803.02324},
  abstract = {Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67\% of SNLI (Bowman et. al, 2015) and 53\% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-2758-5},
  annotation = {379 citations (Semantic Scholar/DOI) [2021-03-26] 379 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EMD4CGKX/Gururangan et al. - 2018 - Annotation Artifacts in Natural Language Inference Data(2).pdf}
}

@article{gutmann2012NoiseContrastiveEstimationUnnormalized,
  title = {Noise-{{Contrastive Estimation}} of {{Unnormalized Statistical Models}}, with {{Applications}} to {{Natural Image Statistics}}},
  author = {Gutmann, M. U. and Hyvärine, A.},
  date = {2012},
  journaltitle = {JMLR},
  volume = {13},
  pages = {307--361},
  issn = {1532-4435},
  abstract = {We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, themodel is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective func- tion for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially gener- ated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to be- have like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimationmethods for unnormalizedmodels. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.},
  isbn = {1532-4435},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6TSLL5SE/Gutmann, Hyvärine - 2012 - Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statis(2).pdf}
}

@inproceedings{guu2017LanguageProgramsBridging,
  title = {From {{Language}} to {{Programs}}: {{Bridging Reinforcement Learning}} and {{Maximum Marginal Likelihood}}},
  booktitle = {{{ACL}}},
  author = {Guu, K. and Pasupat, P. and Liu, E. Z. and Liang, P.},
  date = {2017},
  eprint = {1704.07926},
  eprinttype = {arxiv},
  pages = {1051--1062},
  doi = {10.18653/v1/P17-1097},
  url = {http://arxiv.org/abs/1704.07926},
  abstract = {Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-75-3},
  keywords = {unread},
  annotation = {122 citations (Semantic Scholar/DOI) [2021-03-26] 122 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5VYI6PF3/Guu et al. - 2017 - From Language to Programs Bridging Reinforcement Learning and Maximum Marginal Likelihood(2).pdf}
}

@inproceedings{hahn2016KnowledgeAcceleratorBig,
  title = {The {{Knowledge Accelerator}}: {{Big Picture Thinking}} in {{Small Pieces}}},
  booktitle = {Human {{Factors}} in {{Computing Systems}}},
  author = {Hahn, N. and Chang, J. and Kim, J. E. and Kittur, A.},
  date = {2016},
  pages = {2258--2270},
  doi = {10.1145/2858036.2858364},
  abstract = {Crowdsourcing offers a powerful new paradigm for online work. However, real world tasks are often interdependent, requiring a big picture view of the difference pieces involved. Existing crowdsourcing approaches that support such tasks -- ranging from Wikipedia to flash teams -- are bottlenecked by relying on a small number of individuals to maintain the big picture. In this paper, we explore the idea that a computational system can scaffold an emerging interdependent, big picture view entirely through the small contributions of individuals, each of whom sees only a part of the whole. To investigate the viability, strengths, and weaknesses of this approach we instantiate the idea in a prototype system for accomplishing distributed information synthesis and evaluate its output across a variety of topics. We also contribute a set of design patterns that may be informative for other systems aimed at supporting big picture thinking in small pieces.},
  isbn = {978-1-4503-3362-7},
  keywords = {unread},
  annotation = {40 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZEXGTKXQ/Hahn et al. - 2016 - The Knowledge Accelerator Big Picture Thinking in Small Pieces(2).pdf}
}

@inproceedings{hajishirzi2011ReasoningRoboCupSoccer,
  title = {Reasoning about {{RoboCup Soccer Narratives}}},
  booktitle = {{{UAI}}},
  author = {Hajishirzi, H. and Hockenmaier, J. and Mueller, E. T. and Amir, E.},
  date = {2011},
  eprint = {1202.3728},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1202.3728},
  abstract = {This paper presents an approach for learning to translate simple narratives, i.e., texts (sequences of sentences) describing dynamic systems, into coherent sequences of events without the need for labeled training data. Our approach incorporates domain knowledge in the form of preconditions and effects of events, and we show that it outperforms state-of-the-art supervised learning systems on the task of reconstructing RoboCup soccer games from their commentaries.},
  archiveprefix = {arXiv},
  isbn = {978-0-9749039-7-2},
  annotation = {21 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V6EFWPVE/Hajishirzi et al. - 2011 - Reasoning about RoboCup Soccer Narratives(2).pdf}
}

@inproceedings{hale2018FindingSyntaxHuman,
  title = {Finding {{Syntax}} in {{Human Encephalography}} with {{Beam Search}}},
  booktitle = {{{ACL}}},
  author = {Hale, J. and Dyer, C. and Kuncoro, A. and Brennan, J. R.},
  date = {2018},
  number = {2014},
  eprint = {1806.04127},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ABMQ7EPJ/Hale et al. - 2018 - Finding Syntax in Human Encephalography with Beam Search(2).pdf}
}

@article{hallett2007ComposingQuestionsConceptual,
  title = {Composing {{Questions}} through {{Conceptual Authoring}}},
  author = {Hallett, C. and Scott, D. and Power, R.},
  date = {2007},
  journaltitle = {Computational Linguistics},
  volume = {33},
  number = {1},
  pages = {105--133},
  issn = {0891-2017},
  doi = {10.1162/coli.2007.33.1.105},
  url = {http://www.mitpressjournals.org/doi/10.1162/coli.2007.33.1.105},
  abstract = {This article describes a method for composing fluent and complex natural language questions, while avoiding the standard pitfalls of free text queries. The method, based on Conceptual Authoring, is targeted at question-answering systems where reliability and transparency are critical, and where users cannot be expected to undergo extensive training in question composition. This scenario is found in most corporate domains, especially in applications that are risk-averse. We present a proof-of-concept system we have developed: a question-answering interface to a large repository of medical histories in the area of cancer. We show that the method allows users to successfully and reliably compose complex queries with minimal training.},
  isbn = {0891-2017},
  annotation = {91 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DXVF75WV/Hallett, Scott, Power - 2007 - Composing Questions through Conceptual Authoring(2).pdf}
}

@inproceedings{hamilton2017InductiveRepresentationLearning,
  title = {Inductive {{Representation Learning}} on {{Large Graphs}}},
  booktitle = {{{NeurIPS}}},
  author = {Hamilton, W. L. and Ying, R. and Leskovec, J.},
  date = {2017},
  eprint = {1710.09471},
  eprinttype = {arxiv},
  pages = {1--11},
  url = {http://arxiv.org/abs/1710.09471},
  abstract = {Graphs (networks) are ubiquitous and allow us to model entities (nodes) and the dependencies (edges) between them. Learning a useful feature representation from graph data lies at the heart and success of many machine learning tasks such as classification, anomaly detection, link prediction, among many others. Many existing techniques use random walks as a basis for learning features or estimating the parameters of a graph model for a downstream prediction task. Examples include recent node embedding methods such as DeepWalk, node2vec, as well as graph-based deep learning algorithms. However, the simple random walk used by these methods is fundamentally tied to the identity of the node. This has three main disadvantages. First, these approaches are inherently transductive and do not generalize to unseen nodes and other graphs. Second, they are not space-efficient as a feature vector is learned for each node which is impractical for large graphs. Third, most of these approaches lack support for attributed graphs. To make these methods more generally applicable, we propose a framework for inductive network representation learning based on the notion of attributed random walk that is not tied to node identity and is instead based on learning a function \$\textbackslash Phi : \textbackslash mathrm\{\textbackslash rm \textbackslash bf x\} \textbackslash rightarrow w\$ that maps a node attribute vector \$\textbackslash mathrm\{\textbackslash rm \textbackslash bf x\}\$ to a type \$w\$. This framework serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many other previous methods that leverage traditional random walks.},
  archiveprefix = {arXiv},
  issue = {Nips},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BTR5N8JL/Hamilton, Ying, Leskovec - 2017 - Inductive Representation Learning on Large Graphs(2).pdf}
}

@inproceedings{hammerl2022CombiningStaticContextualised,
  title = {Combining {{Static}} and {{Contextualised Multilingual Embeddings}}},
  booktitle = {{{ACL}}},
  author = {Hämmerl, Katharina and Libovický, Jindřich and Fraser, Alexander},
  date = {2022-03-17},
  eprint = {2203.09326},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.09326},
  urldate = {2022-03-23},
  abstract = {Static and contextual multilingual embeddings have complementary strengths. Static embeddings, while less expressive than contextual language models, can be more straightforwardly aligned across multiple languages. We combine the strengths of static and contextual models to improve multilingual representations. We extract static embeddings for 40 languages from XLM-R, validate those embeddings with cross-lingual word retrieval, and then align them using VecMap. This results in high-quality, highly multilingual static embeddings. Then we apply a novel continued pre-training approach to XLM-R, leveraging the high quality alignment of our static embeddings to better align the representation space of XLM-R. We show positive results for multiple complex semantic tasks. We release the static embeddings and the continued pre-training code. Unlike most previous work, our continued pre-training approach does not require parallel text.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DNYHEI2U/Hämmerl et al. - 2022 - Combining Static and Contextualised Multilingual E.pdf;/home/hiaoxui/.local/share/zotero_files/storage/MT9AT2M3/2203.html}
}

@inproceedings{han2019EpisodicMemoryReader,
  title = {Episodic {{Memory Reader}}: {{Learning What}} to {{Remember}} for {{Question Answering}} from {{Streaming Data}}},
  shorttitle = {Episodic {{Memory Reader}}},
  booktitle = {{{ACL}}},
  author = {Han, Moonsu and Kang, Minki and Jung, Hyunwoo and Hwang, Sung Ju},
  date = {2019},
  pages = {4407--4417},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1434},
  url = {https://www.aclweb.org/anthology/P19-1434},
  urldate = {2021-04-21},
  eventtitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-04-21]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9KR6HUCZ/Han et al. - 2019 - Episodic Memory Reader Learning What to Remember .pdf}
}

@misc{han2021ExploringTaskDifficulty,
  title = {Exploring {{Task Difficulty}} for {{Few-Shot Relation Extraction}}},
  author = {Han, Jiale and Cheng, Bo and Lu, Wei},
  date = {2021-09-12},
  eprint = {2109.05473},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.05473},
  urldate = {2021-09-15},
  abstract = {Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a task, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing models still perform suboptimally when handling hard FSRE tasks, where the relations are fine-grained and similar to each other. We argue this is largely because existing models do not distinguish hard tasks from easy ones in the learning process. In this paper, we introduce a novel approach based on contrastive learning that learns better representations by exploiting relation label information. We further design a method that allows the model to adaptively learn how to focus on hard tasks. Experiments on two standard datasets demonstrate the effectiveness of our method.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-15]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RAS4AQUU/Han et al. - 2021 - Exploring Task Difficulty for Few-Shot Relation Ex.pdf;/home/hiaoxui/.local/share/zotero_files/storage/72932FHC/2109.html}
}

@inproceedings{hancock2018TrainingClassifiersNatural,
  title = {Training Classifiers with Natural Language Explanations},
  booktitle = {{{ACL}}},
  author = {Hancock, B. and Bringmann, M. and Varma, P. and Liang, P. and Wang, S. and Ré, C.},
  date = {2018},
  volume = {1},
  eprint = {1805.03818v4},
  eprinttype = {arxiv},
  pages = {1884--1895},
  abstract = {Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100× faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.},
  archiveprefix = {arXiv},
  isbn = {978-1-948087-32-2},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/55HL335D/Hancock et al. - 2018 - Training classifiers with natural language explanations(2).pdf}
}

@inproceedings{hanin2018WhichNeuralNet,
  title = {Which {{Neural Net Architectures Give Rise}} to {{Exploding}} and {{Vanishing Gradients}}?},
  booktitle = {{{NeurIPS}}},
  author = {Hanin, Boris},
  date = {2018},
  pages = {10},
  abstract = {We give a rigorous analysis of the statistical behavior of gradients in a randomly initialized fully connected network N with ReLU activations. Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in a simple architecture-dependent constant , given by the sum of the reciprocals of the hidden layer widths. When is large, the gradients computed by N at initialization vary wildly. Our approach complements the mean field theory analysis of random networks. From this point of view, we rigorously compute finite width corrections to the statistics of gradients at the edge of chaos.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7HM9V6WQ/Hanin - Which Neural Net Architectures Give Rise to Explod.pdf}
}

@inproceedings{hartmann2017OutofdomainFrameNetSemantic,
  title = {Out-of-Domain {{FrameNet}} Semantic Role Labeling},
  booktitle = {{{EACL}}},
  author = {Hartmann, S. and Kuznetsov, I. and Martin, T. and Gurevych, I.},
  date = {2017},
  doi = {10.18653/v1/e17-1045},
  abstract = {Domain dependence of NLP systems is one of the major obstacles to their application in large-scale text analysis, also restricting the applicability of FrameNet semantic role labeling (SRL) systems. Yet, current FrameNet SRL systems are still only evaluated on a single in-domain test set. For the first time, we study the domain dependence of FrameNet SRL on a wide range of benchmark sets. We create a novel test set for FrameNet SRL based on user-generated web text and find that the major bottleneck for out-of-domain FrameNet SRL is the frame identification step. To address this problem, we develop a simple, yet efficient system based on distributed word representations. Our system closely approaches the state-of-the-art in-domain while outperforming the best available frame identification system out-of-domain. We publish our system and test data for research purposes.},
  isbn = {978-1-5108-3860-4},
  annotation = {30 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G2RECFFX/Hartmann et al. - 2017 - Out-of-domain FrameNet semantic role labeling(2).pdf}
}

@inproceedings{hashimoto2018FairnessDemographicsRepeated,
  title = {Fairness {{Without Demographics}} in {{Repeated Loss Minimization}}},
  booktitle = {{{ICML}}},
  author = {Hashimoto, T. B. and Srivastava, M. and Namkoong, H. and Liang, P.},
  date = {2018},
  eprint = {1806.08010},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1806.08010},
  abstract = {Machine learning models (e.g., speech recognizers) are usually trained to minimize average loss, which results in representation disparity---minority groups (e.g., non-native speakers) contribute less to the training objective and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even make initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {112 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M6QQPJBY/Hashimoto et al. - 2018 - Fairness Without Demographics in Repeated Loss Minimization(2).pdf}
}

@inproceedings{haviv2021BERTeseLearningSpeak,
  title = {{{BERTese}}: {{Learning}} to {{Speak}} to {{BERT}}},
  shorttitle = {{{BERTese}}},
  booktitle = {{{EACL}}},
  author = {Haviv, Adi and Berant, Jonathan and Globerson, Amir},
  date = {2021-03-11},
  eprint = {2103.05327},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.05327},
  urldate = {2021-04-21},
  abstract = {Large pre-trained language models have been shown to encode large amounts of world and commonsense knowledge in their parameters, leading to substantial interest in methods for extracting that knowledge. In past work, knowledge was extracted by taking manually-authored queries and gathering paraphrases for them using a separate pipeline. In this work, we propose a method for automatically rewriting queries into "BERTese", a paraphrase query that is directly optimized towards better knowledge extraction. To encourage meaningful rewrites, we add auxiliary loss functions that encourage the query to correspond to actual language tokens. We empirically show our approach outperforms competing baselines, obviating the need for complex pipelines. Moreover, BERTese provides some insight into the type of language that helps language models perform knowledge extraction.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-04-21]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JVM3SY3G/Haviv et al. - 2021 - BERTese Learning to Speak to BERT.pdf;/home/hiaoxui/.local/share/zotero_files/storage/X2VGRHSE/2103.html}
}

@inproceedings{havrylov2017EmergenceLanguageMultiagent,
  title = {Emergence of {{Language}} with {{Multi-agent Games}}: {{Learning}} to {{Communicate}} with {{Sequences}} of {{Symbols}}},
  booktitle = {{{NeurIPS}}},
  author = {Havrylov, S. and Titov, I.},
  date = {2017},
  eprint = {1705.11192},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.11192},
  abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {129 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GFRCA2E7/Havrylov, Titov - 2017 - Emergence of Language with Multi-agent Games Learning to Communicate with Sequences of Symbols(2).pdf}
}

@inproceedings{hayashi2019NoncommutativeBilinearModel,
  title = {A {{Non-commutative Bilinear Model}} for {{Answering Path Queries}} in {{Knowledge Graphs}}},
  booktitle = {{{EMNLP}}},
  author = {Hayashi, K. and Shimbo, M.},
  date = {2019},
  eprint = {1909.01567},
  eprinttype = {arxiv},
  pages = {2422--2430},
  doi = {10.18653/v1/d19-1246},
  abstract = {Bilinear diagonal models for knowledge graph embedding (KGE), such as DistMult and ComplEx, balance expressiveness and computational efficiency by representing relations as diagonal matrices. Although they perform well in predicting atomic relations, composite relations (relation paths) cannot be modeled naturally by the product of relation matrices, as the product of diagonal matrices is commutative and hence invariant with the order of relations. In this paper, we propose a new bilinear KGE model, called BlockHolE, based on block circulant matrices. In BlockHolE, relation matrices can be non-commutative, allowing composite relations to be modeled by matrix product. The model is parameterized in a way that covers a spectrum ranging from diagonal to full relation matrices. A fast computation technique is developed on the basis of the duality of the Fourier transform of circulant matrices.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BIZBXF96/Hayashi, Shimbo - 2019 - A Non-commutative Bilinear Model for Answering Path Queries in Knowledge Graphs(2).pdf}
}

@inproceedings{he2013DynamicFeatureSelection,
  title = {Dynamic Feature Selection for Dependency Parsing},
  booktitle = {{{EMNLP}}},
  author = {He, H. and Daumé III, H. and Eisner, J. M.},
  date = {2013},
  pages = {1455--1464},
  abstract = {Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30\% of the feature templates.},
  isbn = {978-1-937284-97-8},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I3HM72PT/He, Daumé III, Eisner - 2013 - Dynamic feature selection for dependency parsing(2).pdf}
}

@inproceedings{he2014LearningSearchBranchandbound,
  title = {Learning to Search in Branch-and-Bound Algorithms},
  booktitle = {{{NeurIPS}}},
  author = {He, H. and Daumé III, H. and Eisner, J. M.},
  date = {2014},
  pages = {3293--3301},
  issn = {10495258},
  abstract = {Branch-and-bound is a widely used method in combinatorial optimization, including mixed integer programming, structured prediction and MAP inference. While most work has been focused on developing problem-specific techniques, little is known about how to systematically design the node searching strategy on a branch-and-bound tree. We address the key challenge of learning an adaptive node searching order for any class of problem solvable by branch-and-bound. Our strategies are learned by imitation learning. We apply our algorithm to linear programming based branch-and-bound for solving mixed integer programs (MIP). We compare our method with one of the fastest open-source solvers, SCIP; and a very efficient commercial solver, Gurobi. We demonstrate that our approach achieves better solutions faster on four MIP libraries.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LMQFIUZN/He, Daumé III, Eisner - 2014 - Learning to search in branch-and-bound algorithms(2).pdf}
}

@inproceedings{he2015QuestionanswerDrivenSemantic,
  title = {Question-Answer Driven Semantic Role Labeling: {{Using}} Natural Language to Annotate Natural Language},
  booktitle = {{{EMNLP}}},
  author = {He, L. and Lewis, M. and Zettlemoyer, L. S.},
  date = {2015},
  pages = {643--653},
  doi = {10.18653/v1/d15-1076},
  abstract = {This paper introduces the task of questionanswer driven semantic role labeling (QA-SRL), where question-answer pairs are used to represent predicate-argument structure. For example, the verb "introduce" in the previous sentence would be labeled with the questions "What is introduced?", and "What introduces something?", each paired with the phrase from the sentence that gives the correct answer. Posing the problem this way allows the questions themselves to define the set of possible roles, without the need for predefined frame or thematic role ontologies. It also allows for scalable data collection by annotators with very little training and no linguistic expertise. We gather data in two domains, newswire text and Wikipedia articles, and introduce simple classifierbased models for predicting which questions to ask and what their answers should be. Our results show that non-expert annotators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task.},
  isbn = {978-1-941643-32-7},
  keywords = {unread},
  annotation = {113 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4JCG8S65/He, Lewis, Zettlemoyer - 2015 - Question-answer driven semantic role labeling Using natural language to annotate natural language(2).pdf}
}

@article{he2015SpatialPyramidPooling,
  title = {Spatial {{Pyramid Pooling}} in {{Deep Convolutional Networks}} for {{Visual Recognition}}},
  author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
  date = {2015},
  journaltitle = {PAMI},
  volume = {37},
  number = {9},
  eprint = {26353135},
  eprinttype = {pmid},
  pages = {1904--1916},
  issn = {01628828},
  doi = {10.1109/TPAMI.2015.2389824},
  abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224x224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170x faster than the recent leading method R-CNN (and 24-64x faster overall), while achieving better or comparable accuracy on Pascal VOC 2007.},
  archiveprefix = {arXiv},
  isbn = {9783319105772},
  keywords = {unread},
  annotation = {787 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6HLA27KL/He et al. - 2015 - Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(2).pdf}
}

@inproceedings{he2017DeepResidualLearning,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {{{CVPR}}},
  author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
  date = {2017},
  eprint = {23554596},
  eprinttype = {pmid},
  pages = {770--778},
  issn = {1664-1078},
  doi = {10.1109/CVPR.2016.90},
  url = {http://arxiv.org/abs/1703.10722},
  abstract = {We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 23.36.},
  archiveprefix = {arXiv},
  isbn = {978-1-4673-8851-1},
  annotation = {9995 citations (Semantic Scholar/DOI) [2021-03-26] 75 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W3F89Y6H/He et al. - 2017 - Deep Residual Learning for Image Recognition(2).pdf}
}

@inproceedings{he2017DeepSemanticRole,
  title = {Deep {{Semantic Role Labeling}}: {{What Works}} and {{What}}'s {{Next}}},
  booktitle = {{{ACL}}},
  author = {He, L. and Lee, K. and Lewis, M. and Zettlemoyer, L. S.},
  date = {2017},
  issn = {1881803X},
  abstract = {In this paper, we have proposed a non intrusive fatigue detection systembased on eye tracking of drivers. Initially, the face is located through skincolor information to ensure the presence of driver in video frame. This isfollowed by the detection of pupils on the basis of radii, inter pupil distanceand angle between the pupils. The pupils are constantly being tracked throughKalrnan filter to predict their future position in successive frames. Ondetection of low vigilance, monitored through eye state, the driver is informedabout his/her poor state of driving through an alarm. The system has been testedusing real data, with different sequences recorded in day and night drivingconditions, and with users belonging to different race and gender. ICIC International ?? 2010.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4G5WMRS2/He et al. - 2017 - Deep Semantic Role Labeling What Works and What's Next(2).pdf}
}

@inproceedings{he2017DeepSemanticRolea,
  title = {Deep {{Semantic Role Labeling}}: {{What Works}} and {{What}}’s {{Next}}},
  booktitle = {{{ACL}}},
  author = {He, L. and Lee, K. and Lewis, M. and Zettlemoyer, L. S.},
  date = {2017},
  pages = {473--483},
  doi = {10.18653/v1/P17-1044},
  url = {http://aclweb.org/anthology/P17-1044},
  abstract = {We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10\% relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.},
  isbn = {978-1-945626-75-3},
  annotation = {301 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3HZKZK3M/He et al. - 2017 - Deep Semantic Role Labeling What Works and What’s Next(2).pdf}
}

@inproceedings{he2017LearningSymmetricCollaborative,
  title = {Learning {{Symmetric Collaborative Dialogue Agents}} with {{Dynamic Knowledge Graph Embeddings}}},
  booktitle = {{{ACL}}},
  author = {He, H. and Balakrishnan, A. and Eric, M. and Liang, P.},
  date = {2017},
  eprint = {1704.07130},
  eprinttype = {arxiv},
  doi = {10.18653/v1/P17-1162},
  url = {http://arxiv.org/abs/1704.07130},
  abstract = {We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-75-3},
  keywords = {unread},
  annotation = {88 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9K8ANVVW/He et al. - 2017 - Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings(2).pdf}
}

@inproceedings{he2018JointlyPredictingPredicates,
  title = {Jointly {{Predicting Predicates}} and {{Arguments}} in {{Neural Semantic Role Labeling}}},
  booktitle = {{{ACL}}},
  author = {He, L. and Lee, K. and Levy, O. and Zettlemoyer, L. S.},
  date = {2018},
  eprint = {1805.04787},
  eprinttype = {arxiv},
  pages = {1--6},
  abstract = {Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the relations between them. The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LCRBNMXE/He et al. - 2018 - Jointly Predicting Predicates and Arguments in Neu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QXF692KJ/He et al. - 2018 - Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling(2).pdf}
}

@inproceedings{he2018SyntaxSemanticRole,
  title = {Syntax for {{Semantic Role Labeling}}, {{To Be}}, {{Or Not To Be}}},
  booktitle = {{{ACL}}},
  author = {He, Shexia and Li, Zuchao and Zhao, Hai and Bai, Hongxiao},
  date = {2018},
  pages = {11},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CPN8R35I/He et al. - Syntax for Semantic Role Labeling, To Be, Or Not T.pdf}
}

@inproceedings{he2019PunGenerationSurprise,
  title = {Pun {{Generation}} with {{Surprise}}},
  booktitle = {{{NAACL}}},
  author = {He, H. and Peng, N. and Liang, P.},
  date = {2019},
  eprint = {1904.06828v1},
  eprinttype = {arxiv},
  pages = {1734--1744},
  doi = {10.18653/v1/n19-1172},
  abstract = {We tackle the problem of generating a pun sentence given a pair of homophones (e.g., "died" and "dyed"). Supervised text generation is inappropriate due to the lack of a large corpus of puns, and even if such a corpus existed, mimicry is at odds with generating novel content. In this paper, we propose an unsupervised approach to pun generation using a corpus of unhumorous text and what we call the local-global surprisal principle: we posit that in a pun sentence, there is a strong association between the pun word (e.g., "dyed") and the distant context, as well as a strong association between the alternative word (e.g., "died") and the immediate context. This contrast creates surprise and thus humor. We instantiate this principle for pun generation in two ways: (i) as a measure based on the ratio of probabilities under a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Human evaluation shows that our retrieve-and-edit approach generates puns successfully 31\% of the time, tripling the success rate of a neural generation baseline.},
  archiveprefix = {arXiv},
  annotation = {20 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BPL3LA94/He, Peng, Liang - 2019 - Pun Generation with Surprise(2).pdf}
}

@inproceedings{he2020MomentumContrastUnsupervised,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  booktitle = {{{CVPR}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date = {2020-06},
  pages = {9726--9735},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00975},
  url = {https://ieeexplore.ieee.org/document/9157636/},
  urldate = {2022-04-06},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HAKY4F5G/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf}
}

@misc{he2021DeBERTaV3ImprovingDeBERTa,
  title = {{{DeBERTaV3}}: {{Improving DeBERTa}} Using {{ELECTRA-Style Pre-Training}} with {{Gradient-Disentangled Embedding Sharing}}},
  shorttitle = {{{DeBERTaV3}}},
  author = {He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  date = {2021-11-18},
  eprint = {2111.09543},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.09543},
  urldate = {2021-11-23},
  abstract = {This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37\% average score, which is 1.37\% over DeBERTa and 1.91\% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mDeBERTa and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTa Base achieves a 79.8\% zero-shot cross-lingual accuracy on XNLI and a 3.6\% improvement over XLM-R Base, creating a new SOTA on this benchmark. We have made our pre-trained models and inference code publicly available at https://github.com/microsoft/DeBERTa.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J373SQVG/He et al. - 2021 - DeBERTaV3 Improving DeBERTa using ELECTRA-Style P.pdf;/home/hiaoxui/.local/share/zotero_files/storage/I88BYYAB/2111.html}
}

@inproceedings{he2021RealFormerTransformerLikes,
  title = {{{RealFormer}}: {{Transformer Likes Residual Attention}}},
  shorttitle = {{{RealFormer}}},
  booktitle = {{{ACL}}},
  author = {He, Ruining and Ravula, Anirudh and Kanagal, Bhargav and Ainslie, Joshua},
  date = {2021-09-10},
  eprint = {2012.11747},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.11747},
  urldate = {2022-03-21},
  abstract = {Transformer is the backbone of modern NLP models. In this paper, we propose RealFormer, a simple and generic technique to create Residual Attention Layer Transformer networks that significantly outperform the canonical Transformer and its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked Language Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA, Natural Questions, and OpenKP. We also observe empirically that RealFormer stabilizes training and leads to models with sparser attention. Source code and pre-trained checkpoints for RealFormer can be found at https://github.com/google-research/google-research/tree/master/realformer.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QGM8JVFA/He et al. - 2021 - RealFormer Transformer Likes Residual Attention.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QIK5EWCS/2012.html}
}

@inproceedings{heafield2013ScalableModifiedKneserNey,
  title = {Scalable {{Modified Kneser-Ney Language Model Estimation}}},
  booktitle = {{{ACL}}},
  author = {Heafield, K. and Pouzyrevsky, I. and Clark, J. H. and Koehn, P.},
  date = {2013},
  pages = {690--696},
  abstract = {We present an efficient algorithm to es-timate large modified Kneser-Ney mod-els including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7\% of the RAM and 14.0\% of the wall time taken by SRILM. The code is open source as part of KenLM.},
  isbn = {978-1-937284-51-0},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B7JMPXEV/Heafield et al. - 2013 - Scalable Modified Kneser-Ney Language Model Estimation(2).pdf}
}

@inproceedings{hearst1992AutomaticAcquisitionHyponyms,
  title = {Automatic {{Acquisition}} of {{Hyponyms}} from {{Large Text Corpora}}},
  booktitle = {{{COLING}}},
  author = {Hearst, M. A.},
  date = {1992},
  abstract = {We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoid- ance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily rec- ognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discov- ering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the struc- ture of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ML9UMBIE/Hearst - 1992 - Automatic Acquisition of Hyponyms from Large Text Corpora(2).pdf}
}

@inproceedings{heinzerling2017TrustVerifyBetter,
  title = {Trust, but {{Verify}}! {{Better Entity Linking}} through {{Automatic Verification}}},
  booktitle = {{{EACL}}},
  author = {Heinzerling, B. and Strube, M. and Lin, C.},
  date = {2017},
  volume = {1},
  pages = {828--838},
  abstract = {We introduce automatic verification as a post-processing step for entity linking (EL). The proposed method trusts EL sys-tem results collectively, by assuming en-tity mentions are mostly linked correctly, in order to create a semantic profile of the given text using geospatial and temporal information, as well as fine-grained entity types. This profile is then used to auto-matically verify each linked mention indi-vidually, i.e., to predict whether it has been linked correctly or not. Verification allows leveraging a rich set of global and pairwise features that would be prohibitively expen-sive for EL systems employing global in-ference. Evaluation shows consistent im-provements across datasets and systems. In particular, when applied to state-of-the-art systems, our method yields an abso-lute improvement in linking performance of up to 1.7 F 1 on AIDA/CoNLL'03 and up to 2.4 F 1 on the English TAC KBP 2015 TEDL dataset.},
  isbn = {978-1-5108-3860-4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H927UJLC/Heinzerling, Strube, Lin - 2017 - Trust, but Verify! Better Entity Linking through Automatic Verification(2).pdf}
}

@inproceedings{henaff2015DeepConvolutionalNetworks,
  title = {Deep {{Convolutional Networks}} on {{Graph-Structured Data}}},
  booktitle = {{{NeurIPS}}},
  author = {Henaff, M. and Bruna, J. and LeCun, Y.},
  date = {2015},
  eprint = {1506.05163},
  eprinttype = {arxiv},
  pages = {1--10},
  url = {http://arxiv.org/abs/1506.05163},
  abstract = {Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {894 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TYDFRWAU/Henaff, Bruna, LeCun - 2015 - Deep Convolutional Networks on Graph-Structured Data(2).pdf}
}

@inproceedings{hermann2014SemanticFrameIdentification,
  title = {Semantic Frame Identification with Distributed Word Representations},
  booktitle = {{{ACL}}},
  author = {Hermann, K. M. and Das, D. and Weston, J. and Ganchev, K.},
  date = {2014},
  pages = {1448--1458},
  doi = {10.3115/v1/p14-1136},
  abstract = {We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-ofthe- art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. © 2014 Association for Computational Linguistics.},
  isbn = {978-1-937284-72-5},
  annotation = {104 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4CLJUFD4/Hermann et al. - 2014 - Semantic frame identification with distributed word representations(2).pdf}
}

@inproceedings{hernandez2022NaturalLanguageDescriptions,
  title = {Natural {{Language Descriptions}} of {{Deep Visual Features}}},
  booktitle = {{{ICLR}}},
  author = {Hernandez, Evan and Schwettmann, Sarah and Bau, David and Bagashvili, Teona and Torralba, Antonio and Andreas, Jacob},
  date = {2022},
  pages = {21},
  abstract = {Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual-information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to protected categories like race and gender in models trained on datasets intended to obscure these features. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/34BW4RL4/Hernandez et al. - 2022 - NATURAL LANGUAGE DESCRIPTIONS OF DEEP VISUAL FEATU.pdf}
}

@inproceedings{hershcovich2017TransitionBasedDirectedAcyclic,
  title = {A {{Transition-Based Directed Acyclic Graph Parser}} for {{UCCA}}},
  booktitle = {{{ACL}}},
  author = {Hershcovich, D. and Abend, O. and Rappoport, A.},
  date = {2017},
  eprint = {1704.00552},
  eprinttype = {arxiv},
  pages = {1127--1138},
  doi = {10.18653/v1/P17-1104},
  url = {http://arxiv.org/abs/1704.00552},
  abstract = {We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-75-3},
  keywords = {unread},
  annotation = {72 citations (Semantic Scholar/DOI) [2021-03-26] 72 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E7ZNZYFV/Hershcovich, Abend, Rappoport - 2017 - A Transition-Based Directed Acyclic Graph Parser for UCCA(2).pdf}
}

@inproceedings{herzig2019DonParaphraseDetect,
  title = {Don’t Paraphrase, Detect! {{Rapid}} and {{Effective Data Collection}} for {{Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Herzig, J. and Berant, J.},
  date = {2019},
  number = {2015},
  eprint = {1908.09940},
  eprinttype = {arxiv},
  pages = {3808--3818},
  doi = {10.18653/v1/d19-1394},
  abstract = {A major hurdle on the road to conversational interfaces is the difficulty in collecting data that maps language utterances to logical forms. One prominent approach for data collection has been to automatically generate pseudo-language paired with logical forms, and paraphrase the pseudo-language to natural language through crowdsourcing (Wang et al., 2015). However, this data collection procedure often leads to low performance on real data, due to a mismatch between the true distribution of examples and the distribution induced by the data collection procedure. In this paper, we thoroughly analyze two sources of mismatch in this process: the mismatch in logical form distribution and the mismatch in language distribution between the true and induced distributions. We quantify the effects of these mismatches, and propose a new data collection approach that mitigates them. Assuming access to unlabeled utterances from the true distribution, we combine crowdsourcing with a paraphrase model to detect correct logical forms for the unlabeled utterances. On two datasets, our method leads to 70.6 accuracy on average on the true distribution, compared to 51.3 in paraphrasing-based data collection.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {11 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J5MTT44J/Herzig, Berant - 2019 - Don’t paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing(2).pdf}
}

@inproceedings{herzig2019SpanbasedSemanticParsing,
  title = {Span-Based {{Semantic Parsing}} for {{Compositional Generalization}}},
  booktitle = {{{EMNLP}}},
  author = {Herzig, Jonathan and Berant, Jonathan},
  date = {2019},
  eprint = {2009.06040},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.06040},
  urldate = {2020-09-27},
  abstract = {Despite the success of sequence-to-sequence (seq2seq) models in semantic parsing, recent work has shown that they fail in compositional generalization, i.e., the ability to generalize to new structures built of components observed during training. In this work, we posit that a span-based parser should lead to better compositional generalization. we propose SpanBasedSP, a parser that predicts a span tree over an input utterance, explicitly encoding how partial programs compose over spans in the input. SpanBasedSP extends Pasupat et al. (2019) to be comparable to seq2seq models by (i) training from programs, without access to gold trees, treating trees as latent variables, (ii) parsing a class of non-projective trees through an extension to standard CKY. On GeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong seq2seq baselines on random splits, but dramatically improves performance compared to baselines on splits that require compositional generalization: from \$69.8 \textbackslash rightarrow 95.3\$ average accuracy.},
  archiveprefix = {arXiv},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6TATT6EB/Herzig and Berant - 2020 - Span-based Semantic Parsing for Compositional Gene.pdf;/home/hiaoxui/.local/share/zotero_files/storage/GENHN3NL/2009.html}
}

@inproceedings{hewitt2018LearningTranslationsImages,
  title = {Learning Translations via Images with a Massively Multilingual Image Dataset},
  booktitle = {{{ACL}}},
  author = {Hewitt, J. and Ippolito, D. and Callahan, B. and Kriz, R. and Wijaya, D. and Callison-Burch, C.},
  date = {2018},
  pages = {2566--2576},
  doi = {10.18653/v1/p18-1239},
  abstract = {We conduct the most comprehensive study to date into translating words via images. To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents. Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings. In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages. We run experiments on a dozen high resource languages and 20 low resources languages, demonstrating the effect of word concreteness and part-of-speech on translation quality. To improve image-based translation, we introduce a novel method of predicting word concreteness from images, which improves on a previous state-of-the-art unsupervised technique. This allows us to predict when image-based translation may be effective, enabling consistent improvements to a state-of-the-art text-based word translation system. Our code and the Massively Multilingual Image Dataset (MMID) are available at http://multilingual-images.org/.},
  isbn = {978-1-948087-32-2},
  keywords = {unread},
  annotation = {17 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZYKV2727/Hewitt et al. - 2018 - Learning translations via images with a massively multilingual image dataset(2).pdf}
}

@inproceedings{hewitt2019DesigningInterpretingProbes,
  title = {Designing and {{Interpreting Probes}} with {{Control Tasks}}},
  booktitle = {{{EMNLP}}},
  author = {Hewitt, J. and Liang, P.},
  date = {2019},
  eprint = {1909.03368},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.03368},
  abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe's capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {91 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9J9IR8DI/Hewitt, Liang - 2019 - Designing and Interpreting Probes with Control Tasks(2).pdf;/home/hiaoxui/.local/share/zotero_files/storage/GFETH4QL/Hewitt and Liang - 2019 - Designing and Interpreting Probes with Control Tas.pdf}
}

@inproceedings{hinton1983OptimalPerceptualInference,
  title = {Optimal {{Perceptual Inference}}},
  booktitle = {{{CVPR}}},
  author = {Hinton, G. E. and Sejnowski, T. J.},
  date = {1983},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/799X5B2A/Hinton, Sejnowski - 1983 - Optimal Perceptual Inference(2).pdf}
}

@inproceedings{hinton1994AutoencodersMinimumDescription,
  title = {Autoencoders, {{Minimum Description Length}} and {{Helmholtz}} Free {{Energy}}},
  booktitle = {{{NeurIPS}}},
  author = {Hinton, G. E. and Zemel, R. S.},
  date = {1994},
  eprint = {20148535},
  eprinttype = {pmid},
  issn = {15205207},
  doi = {10.1021/jp906511z},
  abstract = {An autoencoder network uses a aset of recognition weights to convert the input veecotre into a code vectore. It then uses set of generative weights to convert the code vector inot an approximate reconstruction of the input vector. We derive and objective function for training autoencoderss based on the minimum descrption length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconsrtuction error. WE show that this information is minimzed by choosing code vectors stochastiacally according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approixmation gives an upper bound on the description length. Even hen this bound is poor, it can be used a Lyapuov function for learning both the generative and recognition weights. We demonstrate that this approach can be used to learn factorial codes.},
  isbn = {1049-5258},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M3FZ6Q5S/Hinton, Zemel - 1994 - Autoencoders, Minimum Description Length and Helmholtz free Energy(2).pdf}
}

@article{hinton2006FastLearningAlgorithm,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, G. E. and Osindero, S. and Teh, Y.},
  date = {2006},
  journaltitle = {Neural Computation},
  volume = {18},
  eprint = {16764513},
  eprinttype = {pmid},
  pages = {1527--1554},
  issn = {0899-7667},
  doi = {10.1162/neco.2006.18.7.1527},
  abstract = {We show how to use complementary priors to eliminate the explaining- away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa- tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive ver- sion of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribu- tion of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning al- gorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  archiveprefix = {arXiv},
  isbn = {0899-7667},
  annotation = {9995 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TPGCRIE5/Hinton, Osindero, Teh - 2006 - A Fast Learning Algorithm for Deep Belief Nets(2).pdf}
}

@article{hinton2006ReducingDimensionalityData,
  title = {Reducing the Dimensionality of Data with Neural Networks},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  date = {2006},
  journaltitle = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  issn = {00368075},
  doi = {10.1126/science.1127647},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  annotation = {9990 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KMPTDSUW/Hinton, Salakhutdinov - 2006 - Reducing the dimensionality of data with neural networks(2).pdf}
}

@inproceedings{hinton2015DistillingKnowledgeNeural,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  booktitle = {{{NeurIPS}}},
  author = {Hinton, G. E. and Vinyals, O. and Dean, J.},
  date = {2015},
  eprint = {18249735},
  eprinttype = {pmid},
  pages = {1--9},
  issn = {0022-2488},
  doi = {10.1063/1.4931082},
  url = {http://arxiv.org/abs/1503.02531},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  isbn = {3-531-20785-7},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-03-26] 5306 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8QYUXCVG/Hinton, Vinyals, Dean - 2015 - Distilling the Knowledge in a Neural Network(2).pdf}
}

@misc{hinton2021HowRepresentPartwhole,
  title = {How to Represent Part-Whole Hierarchies in a Neural Network},
  author = {Hinton, Geoffrey},
  date = {2021-02-24},
  eprint = {2102.12627},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.12627},
  urldate = {2021-02-27},
  abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language},
  archiveprefix = {arXiv},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LT5RPTPN/Hinton - 2021 - How to represent part-whole hierarchies in a neura.pdf;/home/hiaoxui/.local/share/zotero_files/storage/967TCEN7/2102.html}
}

@article{hochreiter1997LongShortTermMemory,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, S. and Schmidhuber, J.},
  date = {1997},
  journaltitle = {Neural Computation},
  volume = {1780},
  pages = {1735--1780},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WP4VGP83/Hochreiter, Schmidhuber - 1997 - Long Short-Term Memory(2).pdf}
}

@inproceedings{holtzman2020CuriousCaseNeural,
  title = {The {{Curious Case}} of {{Neural Text Degeneration}}},
  booktitle = {{{ICLR}}},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  date = {2020-02-14},
  eprint = {1904.09751},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.09751},
  urldate = {2021-07-19},
  abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {489 citations (Semantic Scholar/arXiv) [2021-07-19]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AHN8SSLX/Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf;/home/hiaoxui/.local/share/zotero_files/storage/L957YY64/1904.html}
}

@inproceedings{holzenberger2020DatasetStatutoryReasoning,
  title = {A {{Dataset}} for {{Statutory Reasoning}} in {{Tax Law Entailment}} and {{Question Answering}}},
  booktitle = {{{KDD}}},
  author = {Holzenberger, Nils and Blair-Stanek, Andrew and Van Durme, Benjamin},
  date = {2020-08-12},
  eprint = {2005.05257},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.05257},
  urldate = {2021-08-06},
  abstract = {Legislation can be viewed as a body of prescriptive rules expressed in natural language. The application of legislation to facts of a case we refer to as statutory reasoning, where those facts are also expressed in natural language. Computational statutory reasoning is distinct from most existing work in machine reading, in that much of the information needed for deciding a case is declared exactly once (a law), while the information needed in much of machine reading tends to be learned through distributional language statistics. To investigate the performance of natural language understanding approaches on statutory reasoning, we introduce a dataset, together with a legal-domain text corpus. Straightforward application of machine reading models exhibits low out-of-the-box performance on our questions, whether or not they have been fine-tuned to the legal domain. We contrast this with a hand-constructed Prolog-based system, designed to fully solve the task. These experiments support a discussion of the challenges facing statutory reasoning moving forward, which we argue is an interesting real-world task that can motivate the development of models able to utilize prescriptive rules specified in natural language.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {10 citations (Semantic Scholar/arXiv) [2021-08-06]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IRNIWQS2/Holzenberger et al. - 2020 - A Dataset for Statutory Reasoning in Tax Law Entai.pdf;/home/hiaoxui/.local/share/zotero_files/storage/TDRTWZDH/2005.html}
}

@inproceedings{hong2016BuildingCrossdocumentEventEvent,
  title = {Building a {{Cross-document Event-Event Relation Corpus}}},
  booktitle = {Linguistic {{Annotation Workshop}}},
  author = {Hong, Y. and Zhang, T. and O'Gorman, T. and Horowit-Hendler, S. and Ji, H. and Palmer, M.},
  date = {2016},
  pages = {1--6},
  doi = {10.18653/v1/w16-1701},
  abstract = {We propose a new task of extracting event-event relations across documents. We present our efforts at designing an anno-tation schema and building a corpus for this task. Our schema includes five main types of relations: Inheritance, Expan-sion, Contingency, Comparison and Tem-porality, along with 21 subtypes. We also lay out the main challenges based on de-tailed inter-annotator disagreement and er-ror analysis. We hope these resources can serve as a benchmark to encourage re-search on this new problem.},
  keywords = {unread},
  annotation = {14 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J9CIBF3S/Hong et al. - 2016 - Building a Cross-document Event-Event Relation Corpus(2).pdf}
}

@inproceedings{hope2018AcceleratingInnovationAnalogy,
  title = {Accelerating Innovation through Analogy Mining},
  booktitle = {{{KDD}}},
  author = {Hope, T. and Chan, J. and Kittur, A. and Shahaf, D.},
  date = {2018},
  volume = {2018-July},
  eprint = {1706.05585v1},
  eprinttype = {arxiv},
  pages = {5274--5278},
  issn = {10450823},
  doi = {10.1145/3097983.3098038},
  abstract = {The availability of large idea repositories (e.g., patents) could significantly accelerate innovation and discovery by providing people inspiration from solutions to analogous problems. However, finding useful analogies in these large, messy, real-world repositories remains a persistent challenge for both humans and computers. Previous approaches include costly hand-created databases that do not scale, or machine-learning similarity metrics that struggle to account for structural similarity, which is central to analogy. In this paper we explore the viability and value of learning simple structural representations. Our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions. We demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional methods. In an ideation experiment, analogies retrieved by our models significantly increased people's likelihood of generating creative ideas.},
  archiveprefix = {arXiv},
  isbn = {978-0-9992411-2-7},
  keywords = {unread},
  annotation = {17 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EI4DQKL2/Hope et al. - 2018 - Accelerating innovation through analogy mining(2).pdf}
}

@inproceedings{hosking2021FactorisingMeaningForm,
  title = {Factorising {{Meaning}} and {{Form}} for {{Intent-Preserving Paraphrasing}}},
  booktitle = {{{ACL}}},
  author = {Hosking, Tom and Lapata, Mirella},
  date = {2021-05-31},
  eprint = {2105.15053},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.15053},
  urldate = {2021-06-21},
  abstract = {We propose a method for generating paraphrases of English questions that retain the original intent but use a different surface form. Our model combines a careful choice of training objective with a principled information bottleneck, to induce a latent encoding space that disentangles meaning and form. We train an encoder-decoder model to reconstruct a question from a paraphrase with the same meaning and an exemplar with the same surface form, leading to separated encoding spaces. We use a Vector-Quantized Variational Autoencoder to represent the surface form as a set of discrete latent variables, allowing us to use a classifier to select a different surface form at test time. Crucially, our method does not require access to an external source of target exemplars. Extensive experiments and a human evaluation show that we are able to generate paraphrases with a better tradeoff between semantic preservation and syntactic novelty compared to previous methods.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-06-21]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YWRTMHB9/Hosking and Lapata - 2021 - Factorising Meaning and Form for Intent-Preserving.pdf;/home/hiaoxui/.local/share/zotero_files/storage/GDCEZEAA/2105.html}
}

@inproceedings{hovy2006Ontonotes90Solution,
  title = {Ontonotes: {{The}} 90\% Solution},
  booktitle = {{{NAACL}}},
  author = {Hovy, E. and Marcus, M. and Palmer, M. and Ramshaw, L. and Weischedel, R.},
  date = {2006},
  pages = {4},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4SSGUZXL/Palmer et al. - Proceedings of the....pdf}
}

@misc{hrinchuk2016TensorizedEmbeddingLayers,
  title = {Tensorized {{Embedding Layers}}},
  author = {Hrinchuk, O. and Khrulkov, V. and Mirvakhabova, L. and Orlova, E. and Oseledets, I.},
  date = {2016},
  eprint = {1901.10787v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BVNRLLSM/Hrinchuk et al. - 2016 - Tensorized Embedding Layers(2).pdf}
}

@inproceedings{hsu2022ContrastiveRepresentationLearning,
  title = {Contrastive {{Representation Learning}} for {{Cross-Document Coreference Resolution}} of {{Events}} and {{Entities}}},
  booktitle = {{{NAACL}}},
  author = {Hsu, Benjamin and Horwood, Graham},
  date = {2022-05-23},
  eprint = {2205.11438},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.11438},
  urldate = {2022-07-15},
  abstract = {Identifying related entities and events within and across documents is fundamental to natural language understanding. We present an approach to entity and event coreference resolution utilizing contrastive representation learning. Earlier state-of-the-art methods have formulated this problem as a binary classification problem and leveraged large transformers in a cross-encoder architecture to achieve their results. For large collections of documents and corresponding set of \$n\$ mentions, the necessity of performing \$n\^\{2\}\$ transformer computations in these earlier approaches can be computationally intensive. We show that it is possible to reduce this burden by applying contrastive learning techniques that only require \$n\$ transformer computations at inference time. Our method achieves state-of-the-art results on a number of key metrics on the ECB+ corpus and is competitive on others.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QHA3GV8K/Hsu and Horwood - 2022 - Contrastive Representation Learning for Cross-Docu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/NXBLDPRJ/2205.html}
}

@inproceedings{hu2016DeepNeuralNetworks,
  title = {Deep {{Neural Networks}} with {{Massive Learned Knowledge}}},
  booktitle = {{{EMNLP}}},
  author = {Hu, Z. and Yang, Z. and Salakhutdinov, R. and Xing, E. P.},
  date = {2016},
  issn = {00192120},
  abstract = {Regulating deep neural networks (DNNs) with human structured knowledge has shown to be of great benefit for improved accuracy and in-terpretability. We develop a general framework that enables learning knowledge and its confidence jointly with the DNNs, so that the vast amount of fuzzy knowledge can be incorporated and automatically optimized with little manual efforts. We apply the framework to sentence sentiment analysis, augmenting a DNN with massive linguistic constraints on discourse and polarity structures. Our model substantially enhances the performance using less training data, and shows improved inter-pretability. The principled framework can also be applied to posterior regularization for regulating other statistical models.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NIAUJ3SD/Hu et al. - 2016 - Deep Neural Networks with Massive Learned Knowledge(2).pdf}
}

@inproceedings{hu2016HarnessingDeepNeural,
  title = {Harnessing {{Deep Neural Networks}} with {{Logic Rules}}},
  booktitle = {{{ACL}}},
  author = {Hu, Z. and Ma, X. and Liu, Z. and Hovy, E. and Xing, E. P.},
  date = {2016},
  eprint = {18925972},
  eprinttype = {pmid},
  pages = {2410--2420},
  issn = {1541-1672},
  doi = {10.18653/v1/P16-1228},
  url = {http://arxiv.org/abs/1603.06318},
  abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-2758-5},
  annotation = {350 citations (Semantic Scholar/DOI) [2021-03-26] 350 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X9Z7BSPH/Hu et al. - 2016 - Harnessing Deep Neural Networks with Logic Rules(2).pdf}
}

@inproceedings{hu2017ControlledGenerationText,
  title = {Toward {{Controlled Generation}} of {{Text}}},
  booktitle = {{{ICML}}},
  author = {Hu, Z. and Yang, Z. and Liang, X. and Salakhutdinov, R. and Xing, E. P.},
  date = {2017},
  eprint = {1703.00955},
  eprinttype = {arxiv},
  doi = {arXiv:1},
  url = {http://arxiv.org/abs/1703.00955},
  abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {534 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L73ACW9M/Hu et al. - 2017 - Toward Controlled Generation of Text(2).pdf}
}

@inproceedings{hu2018ReadVerifyMachine,
  title = {Read + {{Verify}}: {{Machine Reading Comprehension}} with {{Unanswerable Questions}}},
  booktitle = {{{AAAI}}},
  author = {Hu, M. and Wei, F. and Peng, Y. and Huang, Z. and Yang, N. and Li, D.},
  date = {2018},
  number = {1},
  eprint = {18618904},
  eprinttype = {pmid},
  issn = {0006-3592},
  doi = {10.1002/bit.260440802},
  url = {http://arxiv.org/abs/1810.06638},
  abstract = {Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. A key subtask is to reliably predict whether the question is unanswerable. In this paper, we propose a unified model, called U-Net, with three important components: answer pointer, no-answer pointer, and answer verifier. We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. The universal node encodes the fused information from both the question and passage, and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U-Net. Different from the state-of-art pipeline models, U-Net can be learned in an end-to-end fashion. The experimental results on the SQuAD 2.0 dataset show that U-Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0.},
  archiveprefix = {arXiv},
  annotation = {57 citations (Semantic Scholar/DOI) [2021-03-26] 31 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HALJPUFT/Hu et al. - 2018 - Read Verify Machine Reading Comprehension with Unanswerable Questions(2).pdf}
}

@inproceedings{hu2018SqueezeandExcitationNetworks,
  title = {Squeeze-and-{{Excitation Networks}}},
  booktitle = {{{CVPR}}},
  author = {Hu, J. and Shen, L. and Sun, G.},
  date = {2018},
  eprint = {1709.01507},
  eprinttype = {arxiv},
  pages = {7132--7141},
  issn = {10636919},
  doi = {10.1109/CVPR.2018.00745},
  url = {http://arxiv.org/abs/1709.01507},
  abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the 'Squeeze-and-Excitation' (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251\%, achieving a \textasciitilde 25\% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.},
  archiveprefix = {arXiv},
  isbn = {978-1-5386-6420-9},
  annotation = {1536 citations (Semantic Scholar/DOI) [2021-03-26] 1536 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7P465Z56/Hu, Shen, Sun - 2018 - Squeeze-and-Excitation Networks(2).pdf}
}

@inproceedings{hu2019ActiveLearningPartial,
  title = {Active Learning with Partial Feedback},
  booktitle = {{{ICLR}}},
  author = {Hu, P. and Lipton, Z. C. and Anandkumar, A. and Ramanan, D.},
  date = {2019},
  eprint = {1802.07427},
  eprinttype = {arxiv},
  abstract = {While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback). To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available. To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask. At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class. Each answer eliminates some classes, leaving the learner with a partial label. The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled. Active learning with partial labels requires (i) a sampling strategy to choose (example, class) pairs, and (ii) learning from partial labels between rounds. Experiments on Tiny ImageNet demonstrate that our most effective method improves 26\% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30\% of the annotation budget that would be required (naively) to annotate the dataset. Moreover, ALPF-learners fully annotate TinyImageNet at 42\% lower cost. Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GLSE9V8W/Hu et al. - 2019 - Active learning with partial feedback(2).pdf}
}

@inproceedings{hu2019LargeScaleDiverseParaphrastic,
  title = {Large-{{Scale}}, {{Diverse}}, {{Paraphrastic Bitexts}} via {{Sampling}} and {{Clustering}}},
  booktitle = {{{CoNLL}}},
  author = {Hu, J. Edward and Singh, Abhinav and Holzenberger, Nils and Post, Matt and Van Durme, Benjamin},
  date = {2019},
  pages = {44--54},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/K19-1005},
  url = {https://www.aclweb.org/anthology/K19-1005},
  urldate = {2022-05-11},
  abstract = {Producing diverse paraphrases of a sentence is a challenging task. Natural paraphrase corpora are scarce and limited, while existing large-scale resources are automatically generated via back-translation and rely on beam search, which tends to lack diversity. We describe PARABANK 2, a new resource that contains multiple diverse sentential paraphrases, produced from a bilingual corpus using negative constraints, inference sampling, and clustering. We show that PARABANK 2 significantly surpasses prior work in both lexical and syntactic diversity while being meaningpreserving, as measured by human judgments and standardized metrics. Further, we illustrate how such paraphrastic resources may be used to refine contextualized encoders, leading to improvements in downstream tasks.},
  eventtitle = {Proceedings of the 23rd {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/878ZMCEQ/Hu et al. - 2019 - Large-Scale, Diverse, Paraphrastic Bitexts via Sam.pdf}
}

@inproceedings{hu2019ParaBankMonolingualBitext,
  title = {{{ParaBank}}: {{Monolingual Bitext Generation}} and {{Sentential Paraphrasing}} via {{Lexically-constrained Neural Machine Translation}}},
  booktitle = {{{AAAI}}},
  author = {Hu, J. E. and Rudinger, R. and Post, M. and Van Durme, B.},
  date = {2019},
  eprint = {1901.03644},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.03644},
  abstract = {We present ParaBank, a large-scale English paraphrase dataset that surpasses prior work in both quantity and quality. Following the approach of ParaNMT, we train a Czech-English neural machine translation (NMT) system to generate novel paraphrases of English reference sentences. By adding lexical constraints to the NMT decoding procedure, however, we are able to produce multiple high-quality sentential paraphrases per source sentence, yielding an English paraphrase resource with more than 4 billion generated tokens and exhibiting greater lexical diversity. Using human judgments, we also demonstrate that ParaBank's paraphrases improve over ParaNMT on both semantic similarity and fluency. Finally, we use ParaBank to train a monolingual NMT model with the same support for lexically-constrained decoding for sentence rewriting tasks.},
  archiveprefix = {arXiv},
  annotation = {23 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RVCYSJ8I/Hu et al. - 2019 - ParaBank Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-constrained Neural Machine Transl(2).pdf}
}

@inproceedings{hu2021R2D2RecursiveTransformer,
  title = {{{R2D2}}: {{Recursive Transformer}} Based on {{Differentiable Tree}} for {{Interpretable Hierarchical Language Modeling}}},
  shorttitle = {{{R2D2}}},
  booktitle = {{{ACL}}},
  author = {Hu, Xiang and Mi, Haitao and Wen, Zujie and Wang, Yafang and Su, Yi and Zheng, Jing and de Melo, Gerard},
  options = {useprefix=true},
  date = {2021-07-02},
  eprint = {2107.00967},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.00967},
  urldate = {2021-09-07},
  abstract = {Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. This paper proposes a recursive Transformer model based on differentiable CKY style binary trees to emulate the composition process. We extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruned tree induction algorithm to enable encoding in just a linear number of composition steps. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XUZKB7H9/Hu et al. - 2021 - R2D2 Recursive Transformer based on Differentiabl.pdf;/home/hiaoxui/.local/share/zotero_files/storage/NHU7YVPS/2107.html}
}

@misc{huang1989SemiContinuousHiddenMarkov,
  title = {Semi-{{Continuous Hidden Markov Models}}},
  author = {Huang, X.},
  date = {1989},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HM35FBR3/Huang - 1989 - Semi-Continuous Hidden Markov Models(2).pdf}
}

@inproceedings{huang2012StructuredPerceptronInexact,
  title = {Structured {{Perceptron}} with {{Inexact Search}}},
  booktitle = {{{NAACL}}},
  author = {Huang, Liang and Fayong, Suphan and Guo, Yang},
  date = {2012},
  pages = {10},
  abstract = {Most existing theory of structured prediction assumes exact inference, which is often intractable in many practical problems. This leads to the routine use of approximate inference such as beam search but there is not much theory behind it. Based on the structured perceptron, we propose a general framework of “violation-fixing” perceptrons for inexact search with a theoretical guarantee for convergence under new separability conditions. This framework subsumes and justifies the popular heuristic “early-update” for perceptron with beam search (Collins and Roark, 2004). We also propose several new update methods within this framework, among which the “max-violation” method dramatically reduces training time (by 3 fold as compared to earlyupdate) on state-of-the-art part-of-speech tagging and incremental parsing systems.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F3Z2FBPW/Huang et al. - Structured Perceptron with Inexact Search.pdf}
}

@misc{huang2015BidirectionalLSTMCRFModels,
  title = {Bidirectional {{LSTM-CRF Models}} for {{Sequence Tagging}}},
  author = {Huang, Z. and Xu, W. and Yu, K.},
  date = {2015},
  eprint = {25246403},
  eprinttype = {pmid},
  issn = {1098-6596},
  doi = {10.18653/v1/P16-1101},
  url = {http://arxiv.org/abs/1508.01991},
  abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
  archiveprefix = {arXiv},
  isbn = {9781510827585},
  annotation = {1727 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/97EQ5U9Z/Huang, Xu, Yu - 2015 - Bidirectional LSTM-CRF Models for Sequence Tagging(2).pdf}
}

@inproceedings{huang2017LearningFineGrainedExpressions,
  title = {Learning {{Fine-Grained Expressions}} to {{Solve Math Word Problems}}},
  booktitle = {{{EMNLP}}},
  author = {Huang, D. and Shi, S. and Yin, J. and Lin, C.},
  date = {2017},
  pages = {816--825},
  url = {http://www.aclweb.org/anthology/D17-1084},
  abstract = {This paper presents a novel template-based method to solve math word prob-lems. This method learns the mappings between math concept phrases in math word problems and their math expressions from training data. For each equation tem-plate, we automatically construct a rich template sketch by aggregating informa-tion from various problems with the same template. Our approach is implemented in a two-stage system. It first retrieves a few relevant equation system templates and aligns numbers in math word problems to those templates for candidate equation generation. It then does a fine-grained in-ference to obtain the final answer. Ex-periment results show that our method achieves an accuracy of 28.4\% on the lin-ear Dolphin18K benchmark, which is 10\% (54\% relative) higher than previous state-of-the-art systems while achieving an ac-curacy increase of 12\% (59\% relative) on the TS6 benchmark subset.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SFQCIDNB/Huang et al. - 2017 - Learning Fine-Grained Expressions to Solve Math Word Problems(2).pdf}
}

@inproceedings{huang2018ZeroShotTransferLearning,
  title = {Zero-{{Shot Transfer Learning}} for {{Event Extraction}}},
  booktitle = {{{ACL}}},
  author = {Huang, Lifu and Ji, Heng and Cho, Kyunghyun and Voss, Clare R.},
  date = {2018},
  eprint = {1707.01066},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.01066},
  urldate = {2021-06-28},
  abstract = {Most previous event extraction studies have relied heavily on features derived from annotated event mentions, thus cannot be applied to new event types without annotation effort. In this work, we take a fresh look at event extraction and model it as a grounding problem. We design a transferable neural architecture, mapping event mentions and types jointly into a shared semantic space using structural and compositional neural networks, where the type of each event mention can be determined by the closest of all candidate types . By leveraging (1) available manual annotations for a small set of existing event types and (2) existing event ontologies, our framework applies to new event types without requiring additional annotation. Experiments on both existing event types (e.g., ACE, ERE) and new event types (e.g., FrameNet) demonstrate the effectiveness of our approach. Without any manual annotations for 23 new event types, our zero-shot framework achieved performance comparable to a state-of-theart supervised model which is trained from the annotations of 500 event mentions.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {62 citations (Semantic Scholar/arXiv) [2021-06-28]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CLMUAD8G/Huang et al. - 2017 - Zero-Shot Transfer Learning for Event Extraction.pdf}
}

@inproceedings{huang2019COSMOSQAMachine,
  title = {{{COSMOS QA}}: {{Machine Reading Comprehension}} with {{Contextual Commonsense Reasoning}}},
  booktitle = {{{EMNLP}}},
  author = {Huang, L. and Bras, R. L. and Bhagavatula, C. and Choi, Y.},
  date = {2019},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZFV7DZDF/Huang et al. - 2019 - COSMOS QA Machine Reading Comprehension with Contextual Commonsense Reasoning(2).pdf}
}

@inproceedings{huang2021DisentanglingSemanticsSyntax,
  title = {Disentangling {{Semantics}} and {{Syntax}} in {{Sentence Embeddings}} with {{Pre-trained Language Models}}},
  booktitle = {{{NAACL}}},
  author = {Huang, James Y. and Huang, Kuan-Hao and Chang, Kai-Wei},
  date = {2021},
  pages = {1372--1379},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.108},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.108},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YPRAHLGD/Huang et al. - 2021 - Disentangling Semantics and Syntax in Sentence Emb.pdf}
}

@inproceedings{huang2021EfficientAttentionsLong,
  title = {Efficient {{Attentions}} for {{Long Document Summarization}}},
  booktitle = {{{NAACL}}},
  author = {Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
  date = {2021},
  pages = {1419--1436},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.112},
  url = {https://aclanthology.org/2021.naacl-main.112},
  urldate = {2022-02-08},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PEEC5NZ2/Huang et al. - 2021 - Efficient Attentions for Long Document Summarizati.pdf}
}

@misc{hudson2022MuLDMultitaskLong,
  title = {{{MuLD}}: {{The Multitask Long Document Benchmark}}},
  shorttitle = {{{MuLD}}},
  author = {Hudson, G. Thomas and Moubayed, Noura Al},
  date = {2022-02-15},
  number = {arXiv:2202.07362},
  eprint = {2202.07362},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.07362},
  urldate = {2022-07-24},
  abstract = {The impressive progress in NLP techniques has been driven by the development of multi-task benchmarks such as GLUE and SuperGLUE. While these benchmarks focus on tasks for one or two input sentences, there has been exciting work in designing efficient techniques for processing much longer inputs. In this paper, we present MuLD: a new long document benchmark consisting of only documents over 10,000 tokens. By modifying existing NLP tasks, we create a diverse benchmark which requires models to successfully model long-term dependencies in the text. We evaluate how existing models perform, and find that our benchmark is much more challenging than their `short document' equivalents. Furthermore, by evaluating both regular and efficient transformers, we show that models with increased context length are better able to solve the tasks presented, suggesting that future improvements in these models are vital for solving similar long document problems. We release the data and code for baselines to encourage further research on efficient NLP models.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NC94ZHCL/Hudson and Moubayed - 2022 - MuLD The Multitask Long Document Benchmark.pdf;/home/hiaoxui/.local/share/zotero_files/storage/IPYHAEQK/2202.html}
}

@inproceedings{humeau2020PolyencodersTransformerArchitectures,
  title = {Poly-Encoders: {{Transformer Architectures}} and {{Pre-training Strategies}} for {{Fast}} and {{Accurate Multi-sentence Scoring}}},
  shorttitle = {Poly-Encoders},
  booktitle = {{{ICLR}}},
  author = {Humeau, Samuel and Shuster, Kurt and Lachaux, Marie-Anne and Weston, Jason},
  date = {2020-03-25},
  eprint = {1905.01969},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.01969},
  urldate = {2021-03-26},
  abstract = {The use of deep pre-trained bidirectional transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on three existing tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.},
  archiveprefix = {arXiv},
  annotation = {40 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4CJHQSDX/Humeau et al. - 2020 - Poly-encoders Transformer Architectures and Pre-t.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JAXE5AHI/1905.html}
}

@article{hupkes2020CompositionalityDecomposedHow,
  title = {Compositionality {{Decomposed}}: {{How}} Do {{Neural Networks Generalise}}?},
  shorttitle = {Compositionality {{Decomposed}}},
  author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  date = {2020-04-12},
  journaltitle = {JAIR},
  shortjournal = {jair},
  volume = {67},
  pages = {757--795},
  issn = {1076-9757},
  doi = {10.1613/jair.1.11674},
  url = {https://jair.org/index.php/jair/article/view/11674},
  urldate = {2022-04-01},
  abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models’ composition operations are local or global (iv) if models’ predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6BQJN8DN/Hupkes et al. - 2020 - Compositionality Decomposed How do Neural Network.pdf}
}

@misc{inala2022FaultAwareNeuralCode,
  title = {Fault-{{Aware Neural Code Rankers}}},
  author = {Inala, Jeevana Priya and Wang, Chenglong and Yang, Mei and Codas, Andres and Encarnación, Mark and Lahiri, Shuvendu K. and Musuvathi, Madanlal and Gao, Jianfeng},
  date = {2022-06-04},
  number = {arXiv:2206.03865},
  eprint = {2206.03865},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.03865},
  urldate = {2022-08-12},
  abstract = {Large language models (LLMs) have demonstrated an impressive ability to generate code for various programming tasks. In many instances, LLMs can generate a correct program for a task when given numerous trials. Consequently, a recent trend is to do large scale sampling of programs using a model and then filtering/ranking the programs based on the program execution on a small number of known unit tests to select one candidate solution. However, these approaches assume that the unit tests are given and assume the ability to safely execute the generated programs (which can do arbitrary dangerous operations such as file manipulations). Both of the above assumptions are impractical in real-world software development. In this paper, we propose fault-aware neural code rankers that can predict the correctness of a sampled program without executing it. The fault-aware rankers are trained to predict different kinds of execution information such as predicting the exact compile/runtime error type (e.g., an IndexError or a TypeError). We show that our fault-aware rankers can significantly increase the pass@1 accuracy of various code generation models (including Codex, GPT-Neo, GPT-J) on APPS, HumanEval and MBPP datasets.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6P4BKAKA/Inala et al. - 2022 - Fault-Aware Neural Code Rankers.pdf;/home/hiaoxui/.local/share/zotero_files/storage/VX3RAQUK/2206.html}
}

@misc{inan2019ImprovingSemanticParsing,
  title = {Improving {{Semantic Parsing}} with {{Neural Generator-Reranker Architecture}}},
  author = {Inan, H. A. and Tomar, G. S. and Pan, H.},
  date = {2019},
  eprint = {1909.12764},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.12764},
  abstract = {Semantic parsing is the problem of deriving machine interpretable meaning representations from natural language utterances. Neural models with encoder-decoder architectures have recently achieved substantial improvements over traditional methods. Although neural semantic parsers appear to have relatively high recall using large beam sizes, there is room for improvement with respect to one-best precision. In this work, we propose a generator-reranker architecture for semantic parsing. The generator produces a list of potential candidates and the reranker, which consists of a pre-processing step for the candidates followed by a novel critic network, reranks these candidates based on the similarity between each candidate and the input sentence. We show the advantages of this approach along with how it improves the parsing performance through extensive analysis. We experiment our model on three semantic parsing datasets (GEO, ATIS, and OVERNIGHT). The overall architecture achieves the state-of-the-art results in all three datasets.},
  archiveprefix = {arXiv},
  keywords = {review},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26] ICLR 2020 review (secondary for Hongyuan)},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/376EZ5LA/Inan, Tomar, Pan - 2019 - Improving Semantic Parsing with Neural Generator-Reranker Architecture(2).pdf}
}

@article{ioffe2015BatchNormalizationAccelerating,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  author = {Ioffe, S. and Szegedy, C.},
  date = {2015},
  journaltitle = {JMLR},
  eprint = {15003161},
  eprinttype = {pmid},
  issn = {0717-6163},
  doi = {10.1007/s13398-014-0173-7.2},
  url = {http://arxiv.org/abs/1502.03167},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  isbn = {9780874216561},
  keywords = {unread},
  annotation = {43 citations (Semantic Scholar/DOI) [2021-03-26] 9993 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IGACLL98/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift(2).pdf}
}

@inproceedings{irsoy2014OpinionMiningDeep,
  title = {Opinion {{Mining}} with {{Deep Recurrent Neural Networks}}},
  booktitle = {{{EMNLP}}},
  author = {Irsoy, O. and Cardie, C.},
  date = {2014},
  doi = {10.3115/v1/d14-1080},
  abstract = {Recurrent neural networks (RNNs) are con-nectionist models of sequential data that are naturally applicable to the analysis of natural language. Recently, " depth in space " — as an orthogonal notion to " depth in time " — in RNNs has been investigated by stacking mul-tiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architec-ture. In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task. Experimental results show that deep, narrow RNNs outperform traditional shallow, wide RNNs with the same number of parame-ters. Furthermore, our approach outperforms previous CRF-based baselines, including the state-of-the-art semi-Markov CRF model, and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF, as well as without the standard layer-by-layer pre-training typically required of RNN architectures.},
  annotation = {301 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XWPDJIJ5/Irsoy, Cardie - 2014 - Opinion Mining with Deep Recurrent Neural Networks(2).pdf}
}

@inproceedings{iyer2019LearningProgrammaticIdioms,
  title = {Learning {{Programmatic Idioms}} for {{Scalable Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Iyer, S. and Cheung, A. and Zettlemoyer, L. S.},
  date = {2019},
  eprint = {1904.09086},
  eprinttype = {arxiv},
  pages = {5425--5434},
  doi = {10.18653/v1/d19-1545},
  abstract = {Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art (SOTA) semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and train semantic parsers to apply these idioms during decoding. Applying idiom-based decoding on a recent context-dependent semantic parsing task improves the SOTA by 2.2\textbackslash\% BLEU score while reducing training time by more than 50\textbackslash\%. This improved speed enables us to scale up the model by training on an extended training set that is 5\$\textbackslash times\$ larger, to further move up the SOTA by an additional 2.3\textbackslash\% BLEU and 0.9\textbackslash\% exact match. Finally, idioms also significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {12 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YBWF358G/Iyer, Cheung, Zettlemoyer - 2019 - Learning Programmatic Idioms for Scalable Semantic Parsing(2).pdf}
}

@inproceedings{iyyer2014NeuralNetworkFactoid,
  title = {A {{Neural Network}} for {{Factoid Question Answering}} over {{Paragraphs}}},
  booktitle = {{{EMNLP}}},
  author = {Iyyer, M. and Boyd-Graber, J. and Claudino, L. and Socher, R. and Daumé III, H.},
  date = {2014},
  doi = {10.3115/v1/d14-1070},
  abstract = {Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations. These methods are ineffective when question text contains very few individual words (e.g., named entities) that are indicative of the answer. We introduce a recursive neural network (rnn) model that can reason over such input by modeling textual compositionality. We apply our model, qanta, to a dataset of questions from a trivia competition called quiz bowl. Unlike previous rnn models, qanta learns word and phrase-level representations that combine across sentences to reason about entities. The model outperforms multiple baselines and, when combined with information retrieval methods, rivals the best human players.},
  annotation = {280 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U4KHU9SF/Iyyer et al. - 2014 - A Neural Network for Factoid Question Answering over Paragraphs(2).pdf}
}

@inproceedings{izacard2021LeveragingPassageRetrieval,
  title = {Leveraging {{Passage Retrieval}} with {{Generative Models}} for {{Open Domain Question Answering}}},
  booktitle = {{{EACL}}},
  author = {Izacard, Gautier and Grave, Edouard},
  date = {2021-02-03},
  number = {arXiv:2007.01282},
  eprint = {2007.01282},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2007.01282},
  urldate = {2022-05-20},
  abstract = {Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XLI5ZWR9/Izacard and Grave - 2021 - Leveraging Passage Retrieval with Generative Model.pdf;/home/hiaoxui/.local/share/zotero_files/storage/9F586FAU/2007.html}
}

@misc{izacard2022ContrastivePretrainingZeroShot,
  title = {Contrastive {{Pre-training}} for {{Zero-Shot Information Retrieval}}},
  author = {Izacard, G. and Caron, M. and Hosseini, L. and Riedel, S. and Bojanowski, P. and Joulin, A. and Grave, E.},
  date = {2022},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NBTWG7WC/contrastive_pre_training_for_z.pdf}
}

@misc{izacard2022FewshotLearningRetrieval,
  title = {Few-Shot {{Learning}} with {{Retrieval Augmented Language Models}}},
  author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  date = {2022-08-08},
  number = {arXiv:2208.03299},
  eprint = {2208.03299},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.03299},
  urldate = {2022-08-12},
  abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42\% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3\% despite having 50x fewer parameters.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7ZV7JXHY/Izacard et al. - 2022 - Few-shot Learning with Retrieval Augmented Languag.pdf;/home/hiaoxui/.local/share/zotero_files/storage/LKVBQ37K/2208.html}
}

@inproceedings{jain2019ScalableRuleLearning,
  title = {Scalable {{Rule Learning}} in {{Probabilistic Knowledge Bases}}},
  booktitle = {{{AKBC}}},
  author = {Jain, A. and Friedman, T. and Kuzelka, O. and Van den Broeck, G. and Raedt, L. D.},
  date = {2019},
  pages = {1--17},
  abstract = {Knowledge Bases (KBs) are becoming increasingly large, sparse and probabilistic. These KBs are typically used to perform query inferences and rule mining. But their efficacy is only as high as their completeness. Efficiently utilizing incomplete KBs remains a major challenge as the current KB completion techniques either do not take into account the inherent uncertainty associated with each KB tuple or do not scale to large KBs. Probabilistic rule learning not only considers the probability of every KB tuple but also tackles the problem of KB completion in an explainable way. For any given probabilistic KB, it learns probabilistic first-order rules from its relations to identify interesting patterns. But, the current probabilistic rule learning techniques perform grounding to do probabilistic inference for evaluation of candidate rules. It does not scale well to large KBs as the time complexity of inference using grounding is exponential over the size of the KB. In this paper, we present SafeLearner-a scalable solution to probabilistic KB completion that performs probabilistic rule learning using lifted probabilistic inference-as faster approach instead of grounding. We compared SafeLearner to the state-of-the-art probabilistic rule learner ProbFOIL + and to its deterministic contemporary AMIE+ on standard probabilistic KBs of NELL (Never-Ending Language Learner) and Yago. Our results demonstrate that SafeLearner scales as good as AMIE+ when learning simple rules and is also significantly faster than ProbFOIL + .},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VE4446PQ/Jain et al. - 2019 - Scalable Rule Learning in Probabilistic Knowledge Bases(2).pdf}
}

@inproceedings{jain2020SciREXChallengeDataset,
  title = {{{SciREX}}: {{A Challenge Dataset}} for {{Document-Level Information Extraction}}},
  shorttitle = {{{SciREX}}},
  booktitle = {{{ACL}}},
  author = {Jain, Sarthak and van Zuylen, Madeleine and Hajishirzi, Hannaneh and Beltagy, Iz},
  options = {useprefix=true},
  date = {2020-05-01},
  eprint = {2005.00512},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.00512},
  urldate = {2021-06-08},
  abstract = {Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level \$N\$-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX},
  archiveprefix = {arXiv},
  annotation = {17 citations (Semantic Scholar/arXiv) [2021-06-08]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MGP3KIMK/Jain et al. - 2020 - SciREX A Challenge Dataset for Document-Level Inf.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QGZD45T7/2005.html}
}

@inproceedings{jang2017CategoricalReparameterizationGumbelSoftmax,
  title = {Categorical {{Reparameterization}} with {{Gumbel-Softmax}}},
  booktitle = {{{ICLR}}},
  author = {Jang, E. and Gu, S. and Poole, B.},
  date = {2017},
  eprint = {1611.01144},
  eprinttype = {arxiv},
  pages = {1--13},
  issn = {1611.01144},
  url = {http://arxiv.org/abs/1611.01144},
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  archiveprefix = {arXiv},
  annotation = {1681 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D8X9XAFU/Jang, Gu, Poole - 2017 - Categorical Reparameterization with Gumbel-Softmax(2).pdf}
}

@inproceedings{jean2015UsingVeryLarge,
  title = {On {{Using Very Large Target Vocabulary}} for {{Neural Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Jean, S. and Cho, K. and Memisevic, R. and Bengio, Y.},
  date = {2015},
  volume = {000},
  eprint = {1412.2007},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1412.2007},
  abstract = {Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English-{$>$}German translation and almost as high performance as state-of-the-art English-{$>$}French translation system.},
  archiveprefix = {arXiv},
  annotation = {767 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NWZKX48T/Jean et al. - 2015 - On Using Very Large Target Vocabulary for Neural Machine Translation(2).pdf}
}

@inproceedings{jersak2001CombiningComplexEvent,
  title = {Combining Complex Event Models and Timing Constraints},
  booktitle = {High-{{Level Design Validation}} and {{Test Workshop}}},
  author = {Jersak, M. and Richter, K. and Ernst, R.},
  date = {2001},
  pages = {89--94},
  issn = {15526674},
  doi = {10.1109/HLDVT.2001.972813},
  abstract = {Sophisticated models of event streams including jitter and bursts as well as the possibility to specify a variety of system-level timing constraints are prerequisites for modem analysis and synthesis techniques in the area of embedded real-time systems. Currently, there is no commonly used specification that models events and timing constraints in a sufficiently general way. In this paper, we first identify a duality between event models and timing constraints and as a result present a specification that can be used for both. Our specification covers most current analysis and synthesis techniques and is easily extensible. We then show how the duality between event models and timing constraints can be applied at different points in a design flow. A real-time video transmission is used as an example.},
  isbn = {0-7695-1411-1},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WKEYZ2FY/Jersak, Richter, Ernst - 2001 - Combining complex event models and timing constraints(2).pdf}
}

@inproceedings{jia2016DataRecombinationNeural,
  title = {Data {{Recombination}} for {{Neural Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Jia, R. and Liang, P.},
  date = {2016},
  eprint = {22251136},
  eprinttype = {pmid},
  pages = {12--22},
  issn = {04194217},
  doi = {10.18653/v1/P16-1002},
  url = {http://arxiv.org/abs/1606.03622},
  abstract = {Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-2758-5},
  annotation = {281 citations (Semantic Scholar/DOI) [2021-03-26] 281 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AR3FDQAW/Jia, Liang - 2016 - Data Recombination for Neural Semantic Parsing(2).pdf}
}

@inproceedings{jia2017AdversarialExamplesEvaluating,
  title = {Adversarial {{Examples}} for {{Evaluating Reading Comprehension Systems}}},
  booktitle = {{{EMNLP}}},
  author = {Jia, R. and Liang, P.},
  date = {2017},
  eprint = {1707.07328},
  eprinttype = {arxiv},
  pages = {2021--2031},
  url = {http://arxiv.org/abs/1707.07328},
  abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of \$75\textbackslash\%\$ F1 score to \$36\textbackslash\%\$; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to \$7\textbackslash\%\$. We hope our insights will motivate the development of new models that understand language more precisely.},
  archiveprefix = {arXiv},
  annotation = {711 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TSEX85J8/Jia, Liang - 2017 - Adversarial Examples for Evaluating Reading Comprehension Systems(2).pdf}
}

@inproceedings{jia2019CertifiedRobustnessAdversarial,
  title = {Certified {{Robustness}} to {{Adversarial Word Substitutions}}},
  booktitle = {{{EMNLP}}},
  author = {Jia, R. and Raghunathan, A. and Göksel, K. and Liang, P.},
  date = {2019},
  eprint = {1909.00986},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.00986},
  abstract = {State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain \$75\textbackslash\%\$ adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI. In comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only \$8\textbackslash\%\$ and \$35\textbackslash\%\$, respectively.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {61 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5I9MJE7I/Jia et al. - 2019 - Certified Robustness to Adversarial Word Substitutions(2).pdf}
}

@inproceedings{jia2019DocumentLevelNaryRelation,
  title = {Document-{{Level N-ary Relation Extraction}} with {{Multiscale Representation Learning}}},
  booktitle = {{{NAACL}}},
  author = {Jia, R. and Wong, C. and Poon, H.},
  date = {2019},
  eprint = {1904.02347},
  eprinttype = {arxiv},
  pages = {3693--3704},
  doi = {10.18653/v1/n19-1370},
  abstract = {Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, \$n\$-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level \$n\$-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system's purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous \$n\$-ary relation extraction methods.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {30 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2K3RP6TV/Jia, Wong, Poon - 2019 - Document-Level N-ary Relation Extraction with Multiscale Representation Learning(2).pdf}
}

@inproceedings{jiang2014SelfPacedLearningDiversity,
  title = {Self-{{Paced Learning}} with {{Diversity}}},
  booktitle = {{{NeurIPS}}},
  author = {Jiang, L. and Meng, D. and Yu, S. and Lan, Z. and Shan, S. and Hauptmann, A.},
  date = {2014},
  pages = {2078--2086},
  issn = {10495258},
  url = {http://papers.nips.cc/paper/5568-self-paced-learning-with-diversity},
  abstract = {Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which for-malizes the preference for both easy and diverse samples into a general regularizer. This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method signifi-cantly outperforms the conventional SPL on three real-world datasets. Specifical-ly, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RS4LJ7QY/Jiang et al. - 2014 - Self-Paced Learning with Diversity(2).pdf}
}

@inproceedings{jiang2016UnsupervisedNeuralDependency,
  title = {Unsupervised {{Neural Dependency Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Jiang, Y. and Han, W. and Tu, K.},
  date = {2016},
  number = {61503248},
  pages = {763--771},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T34BH9YU/Jiang, Han, Tu - 2016 - Unsupervised Neural Dependency Parsing(2).pdf}
}

@inproceedings{jiang2019MultiInputMultiOutputSequence,
  title = {Multi-{{Input Multi-Output Sequence Labeling}} for {{Joint Extraction}} of {{Fact}} and {{Condition Tuples}} from {{Scientific Text}}},
  booktitle = {{{EMNLP}}},
  author = {Jiang, T. and Zhao, T. and Qin, B. and Liu, T. and Chawla, N. and Jiang, M.},
  date = {2019},
  pages = {302--312},
  doi = {10.18653/v1/d19-1029},
  abstract = {Condition is essential in scientific statement. Without the conditions (e.g., equipment, environment) that were precisely specified, facts (e.g., observations) in the statements may no longer be valid. Existing ScienceIE methods, which aim at extracting factual tuples from scientific text, do not consider the conditions. In this work, we propose a new sequence labeling framework (as well as a new tag schema) to jointly extract the fact and condition tuples from statement sentences. The framework has (1) a multi-output module to generate one or multiple tuples and (2) a multi-input module to feed in multiple types of signals as sequences. It improves F1 score relatively by 4.2\% on BioNLP2013 and by 6.2\% on a new bio-text dataset for tuple extraction.},
  keywords = {unread},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SYCTJJMJ/Jiang et al. - 2019 - Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and Condition Tuples from Scientific Te(2).pdf}
}

@inproceedings{jiang2019YouKnowThat,
  title = {Do {{You Know That Florence Is Packed}} with {{Visitors}}? {{Evaluating State-of-the-art Models}} of {{Speaker Commitment}}},
  booktitle = {{{ACL}}},
  author = {Jiang, N. and de Marneffe, M.},
  options = {useprefix=true},
  date = {2019},
  pages = {4208--4213},
  doi = {10.18653/v1/p19-1412},
  abstract = {When a speaker, Mary, asks Do you know that Florence is packed with visitors?, we take her to believe that Florence is packed with visitors, but not if she asks Do you think that Florence is packed with visitors? Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here we explore the hypothesis that linguistic deficits drive the error patterns of speaker commitment models by analyzing the linguistic correlates of model errors on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The Com-mitmentBank is annotated with speaker commitment towards the content of the complement (Florence is packed with visitors in our example) of clause-embedding verbs (know, think) under four entailment-canceling environments. We found that a linguistically-informed model outperforms a LSTM-based one, suggesting that linguistic knowledge is needed to capture such challenging naturalis-tic data. A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.},
  keywords = {unread},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WI7HUJDY/Jiang, de Marneffe - 2019 - Do You Know That Florence Is Packed with Visitors Evaluating State-of-the-art Models of Speaker Commitmen(2).pdf}
}

@article{jiang2020HowCanWe,
  title = {How {{Can We Know What Language Models Know}}?},
  author = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
  date = {2020-05-03},
  journaltitle = {TACL},
  eprint = {1911.12543},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.12543},
  urldate = {2020-10-21},
  abstract = {Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as "Obama is a \_ by profession". These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as "Obama worked as a \_" may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1\% to 39.6\%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {48 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/URXQNDFQ/Jiang et al. - 2020 - How Can We Know What Language Models Know.pdf;/home/hiaoxui/.local/share/zotero_files/storage/XHG8CD2P/1911.html}
}

@inproceedings{jiang2020SMARTRobustEfficient,
  title = {{{SMART}}: {{Robust}} and {{Efficient Fine-Tuning}} for {{Pre-trained Natural Language Models}} through {{Principled Regularized Optimization}}},
  shorttitle = {{{SMART}}},
  booktitle = {{{ACL}}},
  author = {Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Zhao, Tuo},
  date = {2020},
  eprint = {1911.03437},
  eprinttype = {arxiv},
  pages = {2177--2190},
  doi = {10.18653/v1/2020.acl-main.197},
  url = {http://arxiv.org/abs/1911.03437},
  urldate = {2021-08-14},
  abstract = {Transfer learning has fundamentally changed the landscape of natural language processing (NLP) research. Many existing state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely large capacity of pre-trained models, aggressive fine-tuning often causes the adapted model to overfit the data of downstream tasks and forget the knowledge of the pre-trained model. To address the above issue in a more principled manner, we propose a new computational framework for robust and efficient fine-tuning for pre-trained language models. Specifically, our proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the capacity of the model; 2. Bregman proximal point optimization, which is a class of trust-region methods and can prevent knowledge forgetting. Our experiments demonstrate that our proposed method achieves the state-of-the-art performance on multiple NLP benchmarks.},
  archiveprefix = {arXiv},
  annotation = {72 citations (Semantic Scholar/arXiv) [2021-08-14] 72 citations (Semantic Scholar/DOI) [2021-08-14]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HXPNL7II/Jiang et al. - 2020 - SMART Robust and Efficient Fine-Tuning for Pre-tr.pdf;/home/hiaoxui/.local/share/zotero_files/storage/BPACHH3Y/1911.html}
}

@misc{jiang2021BoningKnifeJointEntity,
  title = {{{BoningKnife}}: {{Joint Entity Mention Detection}} and {{Typing}} for {{Nested NER}} via Prior {{Boundary Knowledge}}},
  shorttitle = {{{BoningKnife}}},
  author = {Jiang, Huiqiang and Wang, Guoxin and Chen, Weile and Zhang, Chengxi and Karlsson, Börje F.},
  date = {2021-07-20},
  eprint = {2107.09429},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.09429},
  urldate = {2021-07-23},
  abstract = {While named entity recognition (NER) is a key task in natural language processing, most approaches only target flat entities, ignoring nested structures which are common in many scenarios. Most existing nested NER methods traverse all sub-sequences which is both expensive and inefficient, and also don't well consider boundary knowledge which is significant for nested entities. In this paper, we propose a joint entity mention detection and typing model via prior boundary knowledge (BoningKnife) to better handle nested NER extraction and recognition tasks. BoningKnife consists of two modules, MentionTagger and TypeClassifier. MentionTagger better leverages boundary knowledge beyond just entity start/end to improve the handling of nesting levels and longer spans, while generating high quality mention candidates. TypeClassifier utilizes a two-level attention mechanism to decouple different nested level representations and better distinguish entity types. We jointly train both modules sharing a common representation and a new dual-info attention layer, which leads to improved representation focus on entity-related information. Experiments over different datasets show that our approach outperforms previous state of the art methods and achieves 86.41, 85.46, and 94.2 F1 scores on ACE2004, ACE2005, and NNE, respectively.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XWQUEJPE/Jiang et al. - 2021 - BoningKnife Joint Entity Mention Detection and Ty.pdf;/home/hiaoxui/.local/share/zotero_files/storage/FQSTXGXP/2107.html}
}

@inproceedings{jiang2021ExploitingDefinitionsFrame,
  title = {Exploiting {{Deﬁnitions}} for {{Frame Identiﬁcation}}},
  booktitle = {{{EACL}}},
  author = {Jiang, Tianyu and Riloff, Ellen},
  date = {2021},
  pages = {6},
  abstract = {Frame identification is one of the key challenges for frame-semantic parsing. The goal of this task is to determine which frame best captures the meaning of a target word or phrase in a sentence. We present a new model for frame identification that uses a pre-trained transformer model to generate representations for frames and lexical units (senses) using their formal definitions in FrameNet. Our frame identification model assesses the suitability of a frame for a target word in a sentence based on the semantic coherence of their meanings. We evaluate our model on three data sets and show that it consistently achieves better performance than previous systems.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZXRNV9JU/Jiang and Riloff - Exploiting Deﬁnitions for Frame Identiﬁcation.pdf}
}

@inproceedings{jiang2021IncorporatingSyntaxSemantics,
  title = {Incorporating {{Syntax}} and {{Semantics}} in {{Coreference Resolution}} with {{Heterogeneous Graph Attention Network}}},
  booktitle = {{{NAACL}}},
  author = {Jiang, Fan and Cohn, Trevor},
  date = {2021},
  pages = {1584--1591},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.125},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.125},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IKQJCUS7/Jiang and Cohn - 2021 - Incorporating Syntax and Semantics in Coreference .pdf}
}

@inproceedings{jin2019FineGrainedEntityTyping,
  title = {Fine-{{Grained Entity Typing}} via {{Hierarchical Multi Graph Convolutional Networks}}},
  booktitle = {{{EMNLP}}},
  author = {Jin, H. and Hou, L.},
  date = {2019},
  pages = {4969--4978},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5EETI4R8/Jin, Hou - 2019 - Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks(2).pdf}
}

@inproceedings{jin2022LifelongPretrainingContinually,
  title = {Lifelong {{Pretraining}}: {{Continually Adapting Language Models}} to {{Emerging Corpora}}},
  shorttitle = {Lifelong {{Pretraining}}},
  booktitle = {{{NAACL}}},
  author = {Jin, Xisen and Zhang, Dejiao and Zhu, Henghui and Xiao, Wei and Li, Shang-Wen and Wei, Xiaokai and Arnold, Andrew and Ren, Xiang},
  date = {2022-05-12},
  eprint = {2110.08534},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.08534},
  urldate = {2022-07-15},
  abstract = {Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data distributions that deviate from what the PTLM was initially trained on. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and a chronologically-ordered tweet stream, we incrementally pretrain a PTLM with different continual learning algorithms, and keep track of the downstream task performance (after fine-tuning). We evaluate PTLM's ability to adapt to new corpora while retaining learned knowledge in earlier corpora. Our experiments show distillation-based approaches to be most effective in retaining downstream performance in earlier domains. The algorithms also improve knowledge transfer, allowing models to achieve better downstream performance over the latest data, and improve temporal generalization when distribution gaps exist between training and evaluation because of time. We believe our problem formulation, methods, and analysis will inspire future studies towards continual pretraining of language models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YZQ4DMJ2/Jin et al. - 2022 - Lifelong Pretraining Continually Adapting Languag.pdf;/home/hiaoxui/.local/share/zotero_files/storage/AIREN73E/2110.html}
}

@misc{jing2017NeuralStyleTransfer,
  title = {Neural {{Style Transfer}}: {{A Review}}},
  author = {Jing, Y. and Yang, Y. and Feng, Z. and Ye, J. and Song, M.},
  date = {2017},
  eprint = {1705.04058},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.04058},
  abstract = {The recent work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNN) in creating artistic fantastic imagery by separating and recombing the image content and style. This process of using CNN to migrate the semantic content of one image to different styles is referred to as Neural Style Transfer. Since then, Neural Style Transfer has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention from computer vision researchers and several methods are proposed to either improve or extend the original neural algorithm proposed by Gatys et al. However, there is no comprehensive survey presenting and summarizing recent Neural Style Transfer literature. This review aims to provide an overview of the current progress towards Neural Style Transfer, as well as discussing its various applications and open problems for future research.},
  archiveprefix = {arXiv},
  annotation = {208 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BB3BHMII/Jing et al. - 2017 - Neural Style Transfer A Review(2).pdf}
}

@inproceedings{jing2017NeuralSurvivalRecommender,
  title = {Neural Survival Recommender},
  booktitle = {{{WSDM}}},
  author = {Jing, H. and Smola, A. J.},
  date = {2017},
  pages = {515--524},
  doi = {10.1145/3018661.3018719},
  abstract = {The ability to predict future user activity is invaluable when it comes to content recommendation and personalization. For instance, knowing when users will return to an online music service and what they will listen to increases user satisfaction and therefore user retention. We present a model based on Long-Short Term Memory to estimate when a user will return to a site and what their future listening behavior will be. In doing so, we aim to solve the problem of Just-In-Time recommendation, that is, to recommend the right items at the right time. We use tools from survival analysis for return time prediction and exponential families for future activity analysis. We show that the resulting multitask problem can be solved accurately, when applied to two real-world datasets.},
  isbn = {978-1-4503-4675-7},
  annotation = {89 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DJJHACR7/Jing, Smola - 2017 - Neural survival recommender(2).pdf}
}

@inproceedings{jing2021MultiplexGraphNeural,
  title = {Multiplex {{Graph Neural Network}} for {{Extractive Text Summarization}}},
  booktitle = {{{EMNLP}}},
  author = {Jing, Baoyu and You, Zeyu and Yang, Tao and Fan, Wei and Tong, Hanghang},
  date = {2021-08-29},
  eprint = {2108.12870},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.12870},
  urldate = {2021-09-07},
  abstract = {Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity \& natural connection), nor model intra-sentential relationships (e.g, semantic \& syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate the effectiveness and superiority of our method.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6GKUTPBL/Jing et al. - 2021 - Multiplex Graph Neural Network for Extractive Text.pdf;/home/hiaoxui/.local/share/zotero_files/storage/9YNLXZJY/2108.html}
}

@inproceedings{johansson2007ExtendedConstituenttodependencyConversion,
  title = {Extended Constituent-to-Dependency Conversion for {{English}}},
  booktitle = {{{NODALIDA}}},
  author = {Johansson, R. and Nugues, P.},
  date = {2007},
  pages = {105--112},
  abstract = {We describe a new method to convert En- glish constituent trees using the Penn Tree- bank annotation style into dependency trees. The new format was inspired by annota- tion practices used in other dependency tree- banks with the intention to produce a better interface to further semantic processing than existing methods. In particular, we used a richer set of edge labels and introduced links to handle long-distance phenomena such as wh-movement and topicalization. The resulting trees generally have a more complex dependency structure. For exam- ple, 6\%of the trees contain at least one non- projective link, which is difficult for many parsing algorithms. As can be expected, the more complex structure and the enriched set of edge labels make the trees more difficult to predict, and we observed a decrease in parsing accuracy when applying two depen- dency parsers to the new corpus. However, the richer information contained in the new trees resulted in a 23\% error reduction in a baseline FrameNet semantic role labeler that relied on dependency arc labels only.},
  isbn = {978-9985-4-0514-7},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UP9CYSQQ/Johansson, Nugues - 2007 - Extended constituent-to-dependency conversion for English(2).pdf}
}

@inproceedings{johansson2007LTHSemanticStructure,
  title = {{{LTH}}: {{Semantic}} Structure Extraction Using Nonprojective Dependency Trees},
  booktitle = {{{SemEval}}},
  author = {Johansson, R. and Nugues, P.},
  date = {2007},
  pages = {227--230},
  abstract = {We describe our contribution to the SemEval task on Frame-Semantic Structure Extraction. Unlike most previous systems described in literature, ours is based on dependency syntax. We also describe a fully automatic method to add words to the FrameNet lexical database, which gives an improvement in the recall of frame detection.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YX495HN6/Johansson, Nugues - 2007 - LTH Semantic structure extraction using nonprojective dependency trees(2).pdf}
}

@inproceedings{johnson2016ComposingGraphicalModels,
  title = {Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference},
  booktitle = {{{NeurIPS}}},
  author = {Johnson, M. J. and Duvenaud, D. and Wiltschko, A. B. and Datta, S. R. and Adams, R. P.},
  date = {2016},
  eprint = {1603.06277v5},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3FCF6ZIQ/Johnson et al. - 2016 - Composing graphical models with neural networks for structured representations and fast inference(2).pdf}
}

@article{johnson2021BillionScaleSimilaritySearch,
  title = {Billion-{{Scale Similarity Search}} with {{GPUs}}},
  author = {Johnson, Jeff and Douze, Matthijs and Jegou, Herve},
  date = {2021-07-01},
  journaltitle = {IEEE Transactions on Big Data},
  shortjournal = {IEEE Trans. Big Data},
  volume = {7},
  number = {3},
  pages = {535--547},
  issn = {2332-7790, 2372-2096},
  doi = {10.1109/TBDATA.2019.2921572},
  url = {https://ieeexplore.ieee.org/document/8733051/},
  urldate = {2022-03-30},
  abstract = {Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a novel design for k-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 × faster than prior GPU state of the art. It enables the construction of a high accuracy k-NN graph on 95 million images from the YFCC100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RKV66T6L/Johnson et al. - 2021 - Billion-Scale Similarity Search with GPUs.pdf}
}

@inproceedings{joshi2017TriviaQALargeScale,
  title = {{{TriviaQA}}: {{A Large Scale Distantly Supervised Challenge Dataset}} for {{Reading Comprehension}}},
  shorttitle = {{{TriviaQA}}},
  booktitle = {{{ACL}}},
  author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
  date = {2017-05-13},
  eprint = {1705.03551},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.03551},
  urldate = {2022-01-18},
  abstract = {We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6PBSAE73/Joshi et al. - 2017 - TriviaQA A Large Scale Distantly Supervised Chall.pdf;/home/hiaoxui/.local/share/zotero_files/storage/RKNDCJAV/1705.html}
}

@inproceedings{joshi2018ExtendingParserDistanta,
  title = {Extending a {{Parser}} to {{Distant Domains Using}} a {{Few Dozen Partially Annotated Examples}}},
  booktitle = {{{ACL}}},
  author = {Joshi, Vidur and Peters, Matthew and Hopkins, Mark},
  date = {2018},
  pages = {1190--1199},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1110},
  url = {http://aclweb.org/anthology/P18-1110},
  urldate = {2021-04-17},
  eventtitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  keywords = {unread},
  annotation = {32 citations (Semantic Scholar/DOI) [2021-04-17]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DYTV5TYT/Joshi et al. - 2018 - Extending a Parser to Distant Domains Using a Few .pdf;/home/hiaoxui/.local/share/zotero_files/storage/WVFTW65B/Joshi et al. - 2018 - Extending a Parser to Distant Domains Using a Few .pdf;/home/hiaoxui/.local/share/zotero_files/storage/89MQGYSK/1805.html}
}

@inproceedings{joshi2019BERTCoreferenceResolution,
  title = {{{BERT}} for {{Coreference Resolution}}: {{Baselines}} and {{Analysis}}},
  shorttitle = {{{BERT}} for {{Coreference Resolution}}},
  booktitle = {{{EMNLP}}},
  author = {Joshi, Mandar and Levy, Omer and Weld, Daniel S. and Zettlemoyer, Luke},
  date = {2019-12-22},
  eprint = {1908.09091},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1908.09091},
  urldate = {2021-03-10},
  abstract = {We apply BERT to coreference resolution, achieving strong improvements on the OntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available.},
  archiveprefix = {arXiv},
  annotation = {81 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6WHJAGHW/Joshi et al. - 2019 - BERT for Coreference Resolution Baselines and Ana.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JYK5YUWE/1908.html}
}

@article{joshi2020SpanBERTImprovingPretraining,
  title = {{{SpanBERT}}: {{Improving Pre-training}} by {{Representing}} and {{Predicting Spans}}},
  shorttitle = {{{SpanBERT}}},
  author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
  date = {2020-01-17},
  journaltitle = {TACL},
  volume = {8},
  eprint = {1907.10529},
  eprinttype = {arxiv},
  pages = {64--77},
  url = {http://arxiv.org/abs/1907.10529},
  urldate = {2020-08-04},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {356 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3R73GZQD/Joshi et al. - 2020 - SpanBERT Improving Pre-training by Representing a.pdf}
}

@inproceedings{kahardipraja2021IncrementalTransformersEmpirical,
  title = {Towards {{Incremental Transformers}}: {{An Empirical Analysis}} of {{Transformer Models}} for {{Incremental NLU}}},
  booktitle = {{{EMNLP}}},
  author = {Kahardipraja, Patrick and Madureira, Brielen and Schlangen, David},
  date = {2021},
  pages = {12},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JQTK9APD/Kahardipraja et al. - Towards Incremental Transformers An Empirical Ana.pdf}
}

@inproceedings{kalchbrenner2013RecurrentContinuousTranslation,
  title = {Recurrent {{Continuous Translation Models}}},
  booktitle = {{{EMNLP}}},
  author = {Kalchbrenner, N. and Blunsom, P.},
  date = {2013},
  url = {https://www.aclweb.org/anthology/D13-1176},
  abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is {$>$} 43\% lower than that of state-of-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and mean- ing of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2GIB29NS/Kalchbrenner, Blunsom - 2013 - Recurrent Continuous Translation Models(2).pdf}
}

@inproceedings{kalchbrenner2014ConvolutionalNeuralNetwork,
  title = {A {{Convolutional Neural Network}} for {{Modelling Sentences}}},
  booktitle = {{{ACL}}},
  author = {Kalchbrenner, N. and Grefenstette, E. and Blunsom, P.},
  date = {2014},
  eprint = {1404.2188},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1404.2188},
  abstract = {The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25\% error reduction in the last task with respect to the strongest baseline.},
  archiveprefix = {arXiv},
  annotation = {2561 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CU5TLIPS/Kalchbrenner, Grefenstette, Blunsom - 2014 - A Convolutional Neural Network for Modelling Sentences(2).pdf}
}

@article{kamper2017EmbeddedSegmentalKMeans,
  title = {An {{Embedded Segmental K-Means Model}} for {{Unsupervised Segmentation}} and {{Clustering}} of {{Speech}}},
  author = {Kamper, H. and Livescu, K. and Goldwater, S.},
  date = {2017},
  journaltitle = {Computational Linguistics},
  eprint = {1703.08135v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J3P37GME/Kamper, Livescu, Goldwater - 2017 - An Embedded Segmental K-Means Model for Unsupervised Segmentation and Clustering of Speech(2).pdf}
}

@inproceedings{karamanolakis2021SelfTrainingWeakSupervision,
  title = {Self-{{Training}} with {{Weak Supervision}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Karamanolakis, Giannis and Mukherjee, Subhabrata and Zheng, Guoqing and Awadallah, Ahmed Hassan},
  date = {2021-04-12},
  eprint = {2104.05514},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.05514},
  urldate = {2021-05-27},
  abstract = {State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domain-specific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind. In this work, we develop a weak supervision framework (ASTRA) that leverages all the available data for a given task. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-05-27]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FASHYBV4/Karamanolakis et al. - 2021 - Self-Training with Weak Supervision.pdf;/home/hiaoxui/.local/share/zotero_files/storage/5PDPH745/2104.html}
}

@inproceedings{karpathy2016VisualizingUnderstandingRecurrent,
  title = {Visualizing and {{Understanding Recurrent Networks}}},
  booktitle = {{{ICLR}}},
  author = {Karpathy, A. and Johnson, J. and Fei-Fei, L.},
  date = {2016},
  eprint = {1506.02078v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S54VR6AS/Karpathy, Johnson, Fei-Fei - 2016 - Visualizing and Understanding Recurrent Networks(2).pdf}
}

@inproceedings{karpukhin2020DensePassageRetrieval,
  title = {Dense {{Passage Retrieval}} for {{Open-Domain Question Answering}}},
  booktitle = {{{EMNLP}}},
  author = {Karpukhin, Vladimir and Oğuz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  date = {2020-09-30},
  eprint = {2004.04906},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.04906},
  urldate = {2022-03-29},
  abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6GHSTDKK/Karpukhin et al. - 2020 - Dense Passage Retrieval for Open-Domain Question A.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JHXNIB6C/2004.html}
}

@article{karras2012MaximizingParallelismConstruction,
  title = {Maximizing {{Parallelism}} in the {{Construction}} of {{BVHs}} , {{Octrees}} , and k-d {{Trees}}},
  author = {Karras, T.},
  date = {2012},
  journaltitle = {High Performance Graphics},
  eprint = {cs/9903011},
  eprinttype = {arxiv},
  pages = {33--37},
  issn = {00043702},
  doi = {10.2312/EGGH/HPG12/033-037},
  abstract = {A number of methods for constructing bounding volume hierarchies and point-based octrees on the GPU are based on the idea of ordering primitives along a space-filling curve. A major shortcoming with these methods is that they construct levels of the tree sequentially, which limits the amount of parallelism that they can achieve. We present a novel approach that improves scalability by constructing the entire tree in parallel. Our main contribution is an in-place algorithm for constructing binary radix trees, which we use as a building block for other types of trees.},
  archiveprefix = {arXiv},
  isbn = {978-3-905674-41-5},
  annotation = {148 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/84XU3XQ7/Karras - 2012 - Maximizing Parallelism in the Construction of BVHs , Octrees , and k-d Trees(2).pdf}
}

@misc{kasai2021FinetuningPretrainedTransformers,
  title = {Finetuning {{Pretrained Transformers}} into {{RNNs}}},
  author = {Kasai, Jungo and Peng, Hao and Zhang, Yizhe and Yogatama, Dani and Ilharco, Gabriel and Pappas, Nikolaos and Mao, Yi and Chen, Weizhu and Smith, Noah A.},
  date = {2021-03-24},
  eprint = {2103.13076},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.13076},
  urldate = {2021-04-17},
  abstract = {Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. This comes with a significant computational overhead, as the attention mechanism scales with a quadratic complexity in sequence length. Efficient transformer variants have received increasing interest from recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train or yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving the efficiency while retaining the accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process needs lower training cost than training these recurrent variants from scratch. As many recent models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-04-17]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5D9FYY7U/Kasai et al. - 2021 - Finetuning Pretrained Transformers into RNNs.pdf;/home/hiaoxui/.local/share/zotero_files/storage/BA9ZW537/2103.html}
}

@article{kass1995BayesFactors,
  title = {Bayes {{Factors}}},
  author = {Kass, R. E. and Raftery, A. E.},
  date = {1995},
  journaltitle = {Journal of the American Statistical Association},
  volume = {90},
  number = {430},
  pages = {773--795},
  issn = {1537274X},
  doi = {10.1080/01621459.1995.10476572},
  abstract = {Abstract In a 1935 paper and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: •From Jeffreys' Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory.•Bayes factors offer a way of evaluating evidence in favor of a null hypothesis.•Bayes factors provide a way of incorporating external information into the evalu...},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HZ3E98W4/Kass, Raftery - 1995 - Bayes Factors(2).pdf}
}

@inproceedings{kate2005LearningTransformNatural,
  title = {Learning to Transform Natural to Formal Languages},
  booktitle = {{{AAAI}}},
  author = {Kate, R. J. and Wong, Y. W. and Mooney, R. J.},
  date = {2005},
  volume = {20},
  number = {3},
  pages = {1062--1068},
  url = {http://www.aaai.org/Library/AAAI/2005/aaai05-168.php},
  abstract = {This paper presents a method for inducing transformation rules that map natural-language sentences into a formal query or command language. The approach assumes a formal gram- mar for the target representation language and learns trans- formation rules that exploit the non-terminal symbols in this grammar. The learned transformation rules incrementally map a natural-language sentence or its syntactic parse tree into a parse-tree for the target formal language. Experimental results are presented for two corpora, one which maps En- glish instructions into an existing formal coaching language for simulated RoboCup soccer agents, and another which maps EnglishU.S.-geography questions into a database query language. We show that our method performs overall better and faster than previous approaches in both domains.},
  isbn = {1-57735-236-X},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ITUMQP79/Kate, Wong, Mooney - 2005 - Learning to transform natural to formal languages(2).pdf}
}

@inproceedings{kate2006UsingStringkernelsLearning,
  title = {Using String-Kernels for Learning Semantic Parsers},
  booktitle = {{{ACL}}},
  author = {Kate, R. J. and Mooney, R. J.},
  date = {2006},
  pages = {913--920},
  doi = {10.3115/1220175.1220290},
  url = {http://dl.acm.org/citation.cfm?id=1220290},
  isbn = {1-932432-65-5},
  issue = {July},
  annotation = {255 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8H3Z5IHY/Kate, Mooney - 2006 - Using string-kernels for learning semantic parsers(2).pdf}
}

@inproceedings{kate2007LearningLanguageSemantics,
  title = {Learning Language Semantics from Ambiguous Supervision},
  booktitle = {{{AAAI}}},
  author = {Kate, R. J. and Mooney, R. J.},
  date = {2007},
  pages = {895--900},
  abstract = {This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.},
  isbn = {978-1-57735-323-2},
  issue = {July},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XWLEMXQI/Kate, Mooney - 2007 - Learning language semantics from ambiguous supervision(2).pdf}
}

@inproceedings{katharopoulos2020TransformersAreRNNs,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  booktitle = {{{ICML}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
  date = {2020-08-31},
  eprint = {2006.16236},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.16236},
  urldate = {2021-06-28},
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \$\textbackslash mathcal\{O\}\textbackslash left(N\^2\textbackslash right)\$ to \$\textbackslash mathcal\{O\}\textbackslash left(N\textbackslash right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  archiveprefix = {arXiv},
  annotation = {94 citations (Semantic Scholar/arXiv) [2021-06-28]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7WJ2J2GJ/Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf;/home/hiaoxui/.local/share/zotero_files/storage/XDM7KD2C/2006.html}
}

@inproceedings{kaushik2018HowMuchReading,
  title = {How {{Much Reading Does Reading Comprehension Require}} ?},
  booktitle = {{{EMNLP}}},
  author = {Kaushik, D. and Lipton, Z. C.},
  date = {2018},
  eprint = {1808.04926v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VGT3ENQH/Kaushik, Lipton - 2018 - How Much Reading Does Reading Comprehension Require(2).pdf}
}

@inproceedings{kazemzadeh2014ReferItGameReferringObjects,
  title = {{{ReferItGame}}: {{Referring}} to {{Objects}} in {{Photographs}} of {{Natural Scenes}}},
  booktitle = {{{EMNLP}}},
  author = {Kazemzadeh, S. and Ordonez, V. and Matten, M. and Berg, T.},
  date = {2014},
  doi = {10.3115/v1/d14-1086},
  abstract = {In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.},
  annotation = {406 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CDNXAK9M/Kazemzadeh et al. - 2014 - ReferItGame Referring to Objects in Photographs of Natural Scenes(2).pdf}
}

@article{kennedy1999EventStructureScale,
  title = {From Event Structure to Scale Structure: {{Degree}} Modification in Deverbal Adjectives},
  author = {Kennedy, C. and McNally, L.},
  date = {1999},
  journaltitle = {SALT},
  number = {2},
  pages = {163--180},
  url = {http://elanguage.net/journals/salt/article/download/9.163/1681},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NBVMM46X/Kennedy, McNally - 1999 - From event structure to scale structure Degree modification in deverbal adjectives(2).pdf}
}

@article{kennedy2005ScaleStructureDegree,
  title = {Scale {{Structure}}, {{Degree Modification}} and the {{Semantics}} of {{Gradable Predicates}}},
  author = {Kennedy, C. and McNally, L.},
  date = {2005},
  journaltitle = {Language},
  volume = {81},
  number = {2},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {345--381},
  issn = {1535-0665},
  doi = {10.1353/lan.2005.0071},
  abstract = {In this article we develop a semantic typology of gradable predicates, with special emphasis on deverbal adjectives. We argue for the linguistic relevance of this typology by demonstrating that the distribution and interpretation of degree modifiers is sensitive to its twomajor classificatory parameters: (1) whether a gradable predicate is associated with what we call an open or closed scale, and (2) whether the standard of comparison for the applicability of the predicate is absolute or relative to a context. We further showthat the classification of an important subclass of adjectives within the typology is largely predictable. Specifically, the scale structure of a deverbal gradable adjective correlates either with the algebraic part structure of the event denoted by its source verb or with the part structure of the entities to which the adjective applies. These correla- tions underscore the fact that gradability is characteristic not only of adjectives but also of verbs and nouns, and that scalar properties are shared by categorially distinct but derivationally related expressions.},
  archiveprefix = {arXiv},
  isbn = {00978507},
  annotation = {768 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H2GM5GWU/Kennedy, McNally - 2005 - Scale Structure, Degree Modification and the Semantics of Gradable Predicates(2).pdf}
}

@inproceedings{kerenidis2019QmeansQuantumAlgorithm,
  title = {Q-Means: {{A}} Quantum Algorithm for Unsupervised Machine Learning},
  booktitle = {{{NeurIPS}}},
  author = {Kerenidis, I. and Landman, J. and Luongo, A. and Prakash, A.},
  date = {2019},
  eprint = {1812.03584},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.03584},
  abstract = {Quantum machine learning is one of the most promising applications of a full-scale quantum computer. Over the past few years, many quantum machine learning algorithms have been proposed that can potentially offer considerable speedups over the corresponding classical algorithms. In this paper, we introduce q-means, a new quantum algorithm for clustering which is a canonical problem in unsupervised machine learning. The \$q\$-means algorithm has convergence and precision guarantees similar to \$k\$-means, and it outputs with high probability a good approximation of the \$k\$ cluster centroids like the classical algorithm. Given a dataset of \$N\$ \$d\$-dimensional vectors \$v\_i\$ (seen as a matrix \$V \textbackslash in \textbackslash mathbb\{R\}\^\{N \textbackslash times d\})\$ stored in QRAM, the running time of q-means is \$\textbackslash widetilde\{O\}\textbackslash left( k d \textbackslash frac\{\textbackslash eta\}\{\textbackslash delta\^2\}\textbackslash kappa(V)(\textbackslash mu(V) + k \textbackslash frac\{\textbackslash eta\}\{\textbackslash delta\}) + k\^2 \textbackslash frac\{\textbackslash eta\^\{1.5\}\}\{\textbackslash delta\^2\} \textbackslash kappa(V)\textbackslash mu(V) \textbackslash right)\$ per iteration, where \$\textbackslash kappa(V)\$ is the condition number, \$\textbackslash mu(V)\$ is a parameter that appears in quantum linear algebra procedures and \$\textbackslash eta = \textbackslash max\_\{i\} ||v\_\{i\}||\^\{2\}\$. For a natural notion of well-clusterable datasets, the running time becomes \$\textbackslash widetilde\{O\}\textbackslash left( k\^2 d \textbackslash frac\{\textbackslash eta\^\{2.5\}\}\{\textbackslash delta\^3\} + k\^\{2.5\} \textbackslash frac\{\textbackslash eta\^2\}\{\textbackslash delta\^3\} \textbackslash right)\$ per iteration, which is linear in the number of features \$d\$, and polynomial in the rank \$k\$, the maximum square norm \$\textbackslash eta\$ and the error parameter \$\textbackslash delta\$. Both running times are only polylogarithmic in the number of datapoints \$N\$. Our algorithm provides substantial savings compared to the classical \$k\$-means algorithm that runs in time \$O(kdN)\$ per iteration, particularly for the case of large datasets.},
  archiveprefix = {arXiv},
  annotation = {44 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/77FABXJB/Kerenidis et al. - 2019 - q-means A quantum algorithm for unsupervised machine learning(2).pdf}
}

@inproceedings{kerenidis2020QuantumExpectationMaximizationGaussian,
  title = {Quantum {{Expectation-Maximization}} for {{Gaussian Mixture Models}}},
  booktitle = {{{ICML}}},
  author = {Kerenidis, I. and Luongo, A. and Prakash, A.},
  date = {2020},
  eprint = {1908.06657v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BVL4JFFV/Kerenidis, Luongo, Prakash - 2019 - Quantum Expectation-Maximization for Gaussian Mixture Models(4).pdf;/home/hiaoxui/.local/share/zotero_files/storage/KZNYC95U/Kerenidis, Luongo, Prakash - 2019 - Quantum Expectation-Maximization for Gaussian Mixture Models(6).pdf;/home/hiaoxui/.local/share/zotero_files/storage/U2NJWRW8/Kerenidis, Luongo, Prakash - 2019 - Quantum Expectation-Maximization for Gaussian Mixture Models(5).pdf}
}

@misc{khalitov2022ChordMixerScalableNeural,
  title = {{{ChordMixer}}: {{A Scalable Neural Attention Model}} for {{Sequences}} with {{Different Lengths}}},
  shorttitle = {{{ChordMixer}}},
  author = {Khalitov, Ruslan and Yu, Tong and Cheng, Lei and Yang, Zhirong},
  date = {2022-06-12},
  number = {arXiv:2206.05852},
  eprint = {2206.05852},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.05852},
  urldate = {2022-10-12},
  abstract = {Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models.},
  archiveprefix = {arXiv},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KHQVCBYE/2023 - ChordMixer A Scalable Neural Attention Model for .pdf;/home/hiaoxui/.local/share/zotero_files/storage/PIMW8ZIX/Khalitov et al. - 2022 - ChordMixer A Scalable Neural Attention Model for .pdf;/home/hiaoxui/.local/share/zotero_files/storage/C7T2UPWV/2206.html}
}

@inproceedings{khandelwal2020GeneralizationMemorizationNearest,
  title = {Generalization through {{Memorization}}: {{Nearest Neighbor Language Models}}},
  booktitle = {{{ICLR}}},
  author = {Khandelwal, U. and Levy, O. and Jurafsky, D. and Zettlemoyer, L. S. and Lewis, M.},
  date = {2020},
  eprint = {1911.00172},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.00172},
  abstract = {We introduce \$k\$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a \$k\$-nearest neighbors (\$k\$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our \$k\$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {50 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/33HBQNV3/Khandelwal et al. - 2020 - Generalization through Memorization Nearest Neighbor Language Models(2).pdf}
}

@inproceedings{khani2016UnanimousPrediction100,
  title = {Unanimous {{Prediction}} for 100\% {{Precision}} with {{Application}} to {{Learning Semantic Mappings}}},
  booktitle = {{{ACL}}},
  author = {Khani, F. and Rinard, M. and Liang, P.},
  date = {2016},
  eprint = {1606.06368},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.06368},
  abstract = {Can we train a system that, on any new input, either says "don't know" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100\% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-2758-5},
  annotation = {9 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9FT7UTLB/Khani, Rinard, Liang - 2016 - Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings(2).pdf}
}

@misc{khashabi2020ParsiNLUSuiteLanguage,
  title = {{{ParsiNLU}}: {{A Suite}} of {{Language Understanding Challenges}} for {{Persian}}},
  shorttitle = {{{ParsiNLU}}},
  author = {Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and Gheini, Mozhdeh and Kabiri, Arman and Mahabadi, Rabeeh Karimi and Memarrast, Omid and Mosallanezhad, Ahmadreza and Noury, Erfan and Raji, Shahab and Rasooli, Mohammad Sadegh and Sadeghi, Sepideh and Azer, Erfan Sadeqi and Samghabadi, Niloofar Safi and Shafaei, Mahsa and Sheybani, Saber and Tazarv, Ali and Yaghoobzadeh, Yadollah},
  date = {2020-12-11},
  eprint = {2012.06154},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.06154},
  urldate = {2021-06-21},
  abstract = {Despite the progress made in recent years in addressing natural language understanding (NLU) challenges, the majority of this progress remains to be concentrated on resource-rich languages like English. This work focuses on Persian language, one of the widely spoken languages in the world, and yet there are few NLU datasets available for this rich language. The availability of high-quality evaluation datasets is a necessity for reliable assessment of the progress on different NLU tasks and domains. We introduce ParsiNLU, the first benchmark in Persian language that includes a range of high-level tasks -- Reading Comprehension, Textual Entailment, etc. These datasets are collected in a multitude of ways, often involving manual annotations by native speakers. This results in over 14.5\$k\$ new instances across 6 distinct NLU tasks. Besides, we present the first results on state-of-the-art monolingual and multi-lingual pre-trained language-models on this benchmark and compare them with human performance, which provides valuable insights into our ability to tackle natural language understanding challenges in Persian. We hope ParsiNLU fosters further research and advances in Persian language understanding.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-06-21]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y53Y9F3M/Khashabi et al. - 2020 - ParsiNLU A Suite of Language Understanding Challe.pdf;/home/hiaoxui/.local/share/zotero_files/storage/L8XZ7A4D/2012.html}
}

@inproceedings{kiddon2016GloballyCoherentText,
  title = {Globally Coherent Text Generation with Neural Checklist Models},
  booktitle = {{{EMNLP}}},
  author = {Kiddon, C. and Zettlemoyer, L. and Choi, Y.},
  date = {2016},
  pages = {329--339},
  doi = {10.18653/v1/d16-1032},
  abstract = {Recurrent neural networks can generate locally coherent text but often have difficulties representing what has already been generated and what still needs to be said - especially when constructing long texts. We present the neural checklist model, a recurrent neural network that models global coherence by storing and updating an agenda of text strings which should be mentioned somewhere in the output. The model generates output by dynamically adjusting the interpolation among a language model and a pair of attention models that encourage references to agenda items. Evaluations on cooking recipes and dialogue system responses demonstrate high coherence with greatly improved semantic coverage of the agenda.},
  annotation = {137 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PUFXL7ZP/Kiddon, Zettlemoyer, Choi - 2016 - Globally coherent text generation with neural checklist models(2).pdf}
}

@article{kilgarriff1997DonBelieveWord,
  title = {I Don't Believe in Word Senses},
  author = {Kilgarriff, A.},
  date = {1997},
  journaltitle = {Computers and the Humanities},
  volume = {31},
  number = {2},
  eprint = {cmp-lg/9712006},
  eprinttype = {arxiv},
  pages = {91--113},
  issn = {1574020X},
  doi = {10.1023/A:1000583911091},
  abstract = {Word sense disambiguation assumes word senses. Within the lexicography and linguistics literature, they are known to be very slippery entities. The paper looks at problems with existing accounts of 'word sense' and describes the various kinds of ways in which a word's meaning can deviate from its core meaning. An analysis is presented in which word senses are abstractions from clusters of corpus citations, in accordance with current lexicographic practice. The corpus citations, not the word senses, are the basic objects in the ontology. The corpus citations will be clustered into senses according to the purposes of whoever or whatever does the clustering. In the absence of such purposes, word senses do not exist. Word sense disambiguation also needs a set of word senses to disambiguate between. In most recent work, the set has been taken from a general-purpose lexical resource, with the assumption that the lexical resource describes the word senses of English/French/..., between which NLP applications will need to disambiguate. The implication of the paper is, by contrast, that word senses exist only relative to a task. © 1997 Kluwer Academic Publishers.},
  archiveprefix = {arXiv},
  isbn = {9783110895698},
  keywords = {unread},
  annotation = {429 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8JISK6SI/Kilgarriff - 1997 - I don't believe in word senses(2).pdf}
}

@inproceedings{kim2010GenerativeAlignmentSemantic,
  title = {Generative Alignment and Semantic Parsing for Learning from Ambiguous Supervision},
  booktitle = {{{COLING}}},
  author = {Kim, J. and Mooney, R. J.},
  date = {2010},
  volume = {2},
  pages = {543--551},
  url = {http://dl.acm.org/citation.cfm?id=1944628%5Cnpapers3://publication/uuid/EEC4CDC8-A12E-4837-8EF4-57020ABF3026},
  abstract = {We present a probabilistic generative model for learning semantic parsers from ambiguous supervision. Our approach learns from natural language sentences paired with world states consisting of multiple potential logical meaning representations. It disambiguates the meaning of each sentence while simultaneously learning a semantic parser that maps sentences into logical form. Compared to a previous generative model for semantic alignment, it also supports full semantic parsing. Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DT6BIU82/Kim, Mooney - 2010 - Generative alignment and semantic parsing for learning from ambiguous supervision(2).pdf}
}

@inproceedings{kim2012UnsupervisedPCFGInduction,
  title = {Unsupervised {{PCFG}} Induction for Grounded Language Learning with Highly Ambiguous Supervision},
  booktitle = {{{EMNLP}}},
  author = {Kim, J. and Mooney, R. J.},
  date = {2012},
  pages = {433--444},
  abstract = {“Grounded” language learning employs train- ing data in the form of sentences paired with relevant but ambiguous perceptual contexts. B¨ orschinger et al. (2011) introduced an ap- proach to grounded language learning based on unsupervised PCFG induction. Their ap- proach works well when each sentence po- tentially refers to one of a small set of pos- sible meanings, such as in the sportscasting task. However, it does not scale to prob- lems with a large set of potential meanings for each sentence, such as the navigation in- struction following task studied by Chen and Mooney (2011). This paper presents an en- hancement of the PCFG approach that scales to such problems with highly-ambiguous su- pervision. Experimental results on the naviga- tion task demonstrates the effectiveness of our approach.},
  isbn = {978-1-937284-43-5},
  issue = {July},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QNK9SELX/Kim, Mooney - 2012 - Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision(2).pdf}
}

@inproceedings{kim2014ConvolutionalNeuralNetworks,
  title = {Convolutional {{Neural Networks}} for {{Sentence Classification}}},
  booktitle = {{{EMNLP}}},
  author = {Kim, Y.},
  date = {2014},
  eprint = {1408.5882},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1408.5882},
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  archiveprefix = {arXiv},
  annotation = {7755 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N6L5TRJZ/Kim - 2014 - Convolutional Neural Networks for Sentence Classification(2).pdf}
}

@inproceedings{kim2016FrustratinglyEasyNeural,
  title = {Frustratingly {{Easy Neural Domain Adaptation}}},
  booktitle = {{{COLING}}},
  author = {Kim, Y. and Stratos, K. and Sarikaya, R.},
  date = {2016},
  eprint = {0907.1815},
  eprinttype = {arxiv},
  pages = {387--396},
  url = {http://arxiv.org/abs/0907.1815},
  abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
  archiveprefix = {arXiv},
  annotation = {1487 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6CZAIJ96/Kim, Stratos, Sarikaya - 2016 - Frustratingly Easy Neural Domain Adaptation(2).pdf}
}

@inproceedings{kim2017StructuredAttentionNetworks,
  title = {Structured {{Attention Networks}}},
  booktitle = {{{ICLR}}},
  author = {Kim, Y. and Denton, C. and Hoang, L. and Rush, A. M.},
  date = {2017},
  eprint = {2830291},
  eprinttype = {pmid},
  pages = {1--21},
  issn = {0271-678X},
  doi = {10.1007/978-1-4615-5533-9_4},
  url = {http://arxiv.org/abs/1702.00887},
  abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
  archiveprefix = {arXiv},
  isbn = {978-1-4613-7529-6},
  keywords = {unread},
  annotation = {22 citations (Semantic Scholar/DOI) [2021-03-26] 237 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9M8WYQU4/Kim et al. - 2017 - Structured Attention Networks(2).pdf}
}

@inproceedings{kim2019ProbingWhatDifferent,
  title = {Probing {{What Different NLP Tasks Teach Machines}} about {{Function Word Comprehension}}},
  booktitle = {Joint {{Conference}} on {{Lexical}} and {{Computational Semantics}}},
  author = {Kim, N. and Patel, R. and Poliak, A. and Wang, A. and Xia, P. and McCoy, R. T. and Tenney, I. and Ross, A. and Linzen, T. and Van Durme, B. and Bowman, S. R. and Pavlick, E.},
  date = {2019},
  eprint = {1904.11544},
  eprinttype = {arxiv},
  pages = {235--249},
  url = {http://arxiv.org/abs/1904.11544},
  abstract = {We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, CCG supertagging and natural language inference (NLI)) on the learned representations. Our results show that pretraining on CCG---our most syntactic objective---performs the best on average across our probing tasks, suggesting that syntactic knowledge helps function word comprehension. Language modeling also shows strong performance, supporting its widespread use for pretraining state-of-the-art NLP models. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that NLI helps the comprehension of negation.},
  archiveprefix = {arXiv},
  annotation = {34 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JUEYI5QG/Kim et al. - 2019 - Probing What Different NLP Tasks Teach Machines about Function Word Comprehension(2).pdf}
}

@misc{kim2021LearnedTokenPruning,
  title = {Learned {{Token Pruning}} for {{Transformers}}},
  author = {Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Hassoun, Joseph and Keutzer, Kurt},
  date = {2021-07-02},
  eprint = {2107.00910},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.00910},
  urldate = {2021-09-07},
  abstract = {A major challenge in deploying transformer models is their prohibitive inference cost, which quadratically scales with the input sequence length. This makes it especially difficult to use transformers for processing long sequences. To address this, we present a novel Learned Token Pruning (LTP) method that reduces redundant tokens as the data passes through the different layers of the transformer. In particular, LTP prunes tokens with an attention score below a threshold value, which is learned during training. Importantly, our threshold based method avoids algorithmically expensive operations such as top-k token selection which are used in prior token pruning methods, and also leads to structured pruning. We extensively test the performance of our approach on multiple GLUE tasks and show that our learned threshold based method consistently outperforms the prior state-of-the-art top-k token based method by up to \textasciitilde 2\% higher accuracy with the same amount of FLOPs. Furthermore, our preliminary results show up to 1.4x and 1.9x throughput improvement on Tesla T4 GPU and Intel Haswell CPU, respectively, with less than 1\% of accuracy drop (and up to 2.1x FLOPs reduction). Our code has been developed in PyTorch and has been open-sourced.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9A4DXWUF/Kim et al. - 2021 - Learned Token Pruning for Transformers.pdf;/home/hiaoxui/.local/share/zotero_files/storage/5YNT93CU/2107.html}
}

@inproceedings{kim2021SelfGuidedContrastiveLearning,
  title = {Self-{{Guided Contrastive Learning}} for {{BERT Sentence Representations}}},
  booktitle = {{{ACL}}},
  author = {Kim, Taeuk and Yoo, Kang Min and Lee, Sang-goo},
  date = {2021-06-03},
  eprint = {2106.07345},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.07345},
  urldate = {2022-10-03},
  abstract = {Although BERT and its variants have reshaped the NLP landscape, it still remains unclear how best to derive sentence embeddings from such pre-trained Transformers. In this work, we propose a contrastive learning method that utilizes self-guidance for improving the quality of BERT sentence representations. Our method fine-tunes BERT in a self-supervised fashion, does not rely on data augmentation, and enables the usual [CLS] token embeddings to function as sentence vectors. Moreover, we redesign the contrastive learning objective (NT-Xent) and apply it to sentence representation learning. We demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks. We also show it is efficient at inference and robust to domain shifts.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HBTJVDSZ/Kim et al. - 2021 - Self-Guided Contrastive Learning for BERT Sentence.pdf;/home/hiaoxui/.local/share/zotero_files/storage/YS3BS3QH/s41586-022-05172-4.pdf;/home/hiaoxui/.local/share/zotero_files/storage/D542FWI2/2106.html}
}

@misc{kim2021SelfsupervisedTexttoSQLLearning,
  title = {Self-Supervised {{Text-to-SQL Learning With Header Alignment Training}}},
  author = {Kim, Donggyu and Lee, Seanie},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6FDZMAK5/143_file_Paper.pdf}
}

@inproceedings{kimmig2012ShortIntroductionProbabilistic,
  title = {A {{Short Introduction}} to {{Probabilistic Soft Logic}}},
  booktitle = {{{NeurIPS}}},
  author = {Kimmig, A. and Bach, S. H. and Broecheler, M. and Huang, B. and Getoor, L.},
  date = {2012},
  number = {1},
  pages = {1--4},
  abstract = {Probabilistic soft logic (PSL) is a framework for collective, probabilistic reasoning in relational domains. PSL uses first order logic rules as a template language for graphical models over random variables with soft truth values from the interval [0, 1]. Inference in this setting is a continuous optimization task, which can be solved efficiently. This paper provides an overview of the PSL language and its techniques for inference and weight learning. An implementation of PSL is available at http://psl.umiacs.umd.edu/.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XISIQI68/Kimmig et al. - 2012 - A Short Introduction to Probabilistic Soft Logic(2).pdf}
}

@inproceedings{kingma2014AutoEncodingVariationalBayes,
  title = {Auto-{{Encoding Variational Bayes}}},
  booktitle = {{{ICLR}}},
  author = {Kingma, D. P. and Welling, M.},
  date = {2014},
  eprint = {1312.6114v10},
  eprinttype = {arxiv},
  pages = {1--14},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/593X8AXM/Kingma, Welling - 2014 - Auto-Encoding Variational Bayes(2).pdf}
}

@inproceedings{kingma2014SemisupervisedLearningDeep,
  title = {Semi-Supervised {{Learning}} with {{Deep Generative Models}}},
  booktitle = {{{NeurIPS}}},
  author = {Kingma, D. P. and Rezende, D. J. and Mohamed, S. and Welling, M.},
  date = {2014},
  eprint = {1406.5298v1},
  eprinttype = {arxiv},
  pages = {1--9},
  issn = {10495258},
  url = {http://arxiv.org/abs/1406.5298},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  archiveprefix = {arXiv},
  annotation = {1624 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZNB3MGKV/Kingma et al. - 2014 - Semi-supervised Learning with Deep Generative Models(2).pdf}
}

@inproceedings{kingma2015AdamMethodStochastic,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  booktitle = {{{ICLR}}},
  author = {Kingma, D. P. and Ba, J. L.},
  date = {2015},
  eprint = {172668},
  eprinttype = {pmid},
  pages = {1--15},
  issn = {09252312},
  doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
  url = {http://arxiv.org/abs/1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  isbn = {978-1-4503-0072-8},
  keywords = {unread},
  annotation = {9988 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P4HVI7R7/Kingma, Ba - 2015 - Adam A Method for Stochastic Optimization(2).pdf}
}

@inproceedings{kingma2015VariationalDropoutLocal,
  title = {Variational {{Dropout}} and the {{Local Reparameterization Trick}}},
  booktitle = {{{NeurIPS}}},
  author = {Kingma, D. P. and Salimans, T. and Welling, M.},
  date = {2015},
  eprint = {15062530},
  eprinttype = {pmid},
  pages = {1--14},
  issn = {10495258},
  doi = {10.1016/S0733-8619(03)00096-3},
  url = {http://arxiv.org/abs/1506.02557},
  abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
  archiveprefix = {arXiv},
  isbn = {1506.02557},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/DOI) [2021-03-26] 90 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MV5RJ2CP/Kingma, Salimans, Welling - 2015 - Variational Dropout and the Local Reparameterization Trick(2).pdf}
}

@inproceedings{kingsbury2002TreeBankPropBank,
  title = {From {{TreeBank}} to {{PropBank}}},
  booktitle = {{{LREC}}},
  author = {Kingsbury, Paul and Palmer, Martha},
  date = {2002},
  pages = {5},
  abstract = {This paper describes our approach to the development of a Proposition Bank, which involves the addition of semantic information to the Penn English Treebank. Our primary goal is the labeling of syntactic nodes with specific argument labels that preserve the similarity of roles such as the window in John broke the window and the window broke. After motivating the need for explicit predicate argument structure labels, we briefly discuss the theoretical considerations of predicate argument structure and the need to maintain consistency across syntactic alternations. The issues of consistency of argument structure across both polysemous and synonymous verbs are also discussed and we present our actual guidelines for these types of phenomena, along with numerous examples of tagged sentences and verb frames. Metaframes are introduced as a technique for handling similar frames among near− synonymous verbs. We conclude with a summary of the current status of annotation process.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/34LDLG8A/Kingsbury and Palmer - From TreeBank to PropBank.pdf}
}

@inproceedings{kipf2016VariationalGraphAutoEncoders,
  title = {Variational {{Graph Auto-Encoders}}},
  booktitle = {{{NeurIPS}}},
  author = {Kipf, T. N. and Welling, M.},
  date = {2016},
  number = {2},
  eprint = {1611.07308},
  eprinttype = {arxiv},
  pages = {1--3},
  url = {http://arxiv.org/abs/1611.07308},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  archiveprefix = {arXiv},
  annotation = {726 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZVHUTKTR/Kipf, Welling - 2016 - Variational Graph Auto-Encoders(2).pdf}
}

@inproceedings{kipf2017SemiSupervisedClassificationGraph,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  booktitle = {{{ICLR}}},
  author = {Kipf, T. N. and Welling, M.},
  date = {2017},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  pages = {1--14},
  url = {http://arxiv.org/abs/1609.02907},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {6206 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XBXRS4XF/Kipf, Welling - 2017 - Semi-Supervised Classification with Graph Convolutional Networks(2).pdf}
}

@article{kirkpatrick2017OvercomingCatastrophicForgetting,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  date = {2017-01-25},
  journaltitle = {PNAS},
  eprint = {1612.00796},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1612.00796},
  urldate = {2020-10-21},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  annotation = {1560 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HWG9E2BE/Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural netwo.pdf}
}

@inproceedings{kirstain2021CoreferenceResolutionSpan,
  title = {Coreference {{Resolution}} without {{Span Representations}}},
  booktitle = {{{ACL}}},
  author = {Kirstain, Yuval and Ram, Ori and Levy, Omer},
  date = {2021-05-31},
  eprint = {2101.00434},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.00434},
  urldate = {2022-01-28},
  abstract = {The introduction of pretrained language models has reduced many complex task-specific NLP models to simple lightweight layers. An exception to this trend is coreference resolution, where a sophisticated task-specific model is appended to a pretrained transformer encoder. While highly effective, the model has a very large memory footprint -- primarily due to dynamically-constructed span and span-pair representations -- which hinders the processing of complete documents and the ability to train on multiple instances in a single batch. We introduce a lightweight end-to-end coreference model that removes the dependency on span representations, handcrafted features, and heuristics. Our model performs competitively with the current standard model, while being simpler and more efficient.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2Q77LR84/Kirstain et al. - 2021 - Coreference Resolution without Span Representation.pdf;/home/hiaoxui/.local/share/zotero_files/storage/9SUP4QCI/2101.html}
}

@inproceedings{kitaev2020ReformerEfficientTransformer,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  booktitle = {{{ICLR}}},
  author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  date = {2020-02-18},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2001.04451},
  urldate = {2021-02-09},
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L\^2\$) to O(\$L\textbackslash log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  archiveprefix = {arXiv},
  annotation = {224 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M9SVK8D3/Kitaev et al. - 2020 - Reformer The Efficient Transformer.pdf;/home/hiaoxui/.local/share/zotero_files/storage/U5DE45QR/2001.html}
}

@article{klauer2010HierarchicalMultinomialProcessing,
  title = {Hierarchical Multinomial Processing Tree Models: {{A}} Latent-Trait Approach},
  author = {Klauer, K. C.},
  date = {2010},
  journaltitle = {Psychometrika},
  volume = {75},
  number = {1},
  pages = {70--98},
  issn = {00333123},
  doi = {10.1007/s11336-009-9141-0},
  abstract = {Multinomial processing tree models are widely used in many areas of psychology. A hierarchical extension of the model class is proposed, using a multivariate normal distribution of person-level parameters with the mean and covariance matrix to be estimated from the data. The hierarchical model allows one to take variability between persons into account and to assess parameter correlations. The model is estimated using Bayesian methods with weakly informative hyperprior distribution and a Gibbs sampler based on two steps of data augmentation. Estimation, model checks, and hypotheses tests are discussed. The new method is illustrated using a real data set, and its performance is evaluated in a simulation study. © 2009 The Psychometric Society.},
  annotation = {165 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NJVVAWTF/Klauer - 2010 - Hierarchical multinomial processing tree models A latent-trait approach(2).pdf}
}

@inproceedings{klein2002CorpusBasedInductionSyntactic,
  title = {Corpus-{{Based Induction}} of {{Syntactic Structure}} : {{Models}} of {{Dependency}} and {{Constituency}}},
  booktitle = {{{ACL}}},
  author = {Klein, D. and Manning, C. D.},
  date = {2002},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C7EUSBJ9/Klein, Manning - 2002 - Corpus-Based Induction of Syntactic Structure Models of Dependency and Constituency(2).pdf}
}

@article{knuth1972MathematicalAnalysisAlgorithms,
  title = {Mathematical {{Analysis}} of {{Algorithms}}},
  author = {Knuth, D. E.},
  date = {1972},
  journaltitle = {Information Processing},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K8TKBHEB/Knuth - 1972 - Mathematical Analysis of Algorithms(2).pdf}
}

@article{koch2018BeginnerGuideAnalysis,
  title = {A {{Beginner}}'s {{Guide}} to {{Analysis}} of {{RNA Sequencing Data}}},
  author = {Koch, Clarissa M. and Chiu, Stephen F. and Akbarpour, Mahzad and Bharat, Ankit and Ridge, Karen M. and Bartom, Elizabeth T. and Winter, Deborah R.},
  date = {2018-08},
  journaltitle = {American Journal of Respiratory Cell and Molecular Biology},
  shortjournal = {Am J Respir Cell Mol Biol},
  volume = {59},
  number = {2},
  pages = {145--157},
  issn = {1044-1549, 1535-4989},
  doi = {10.1165/rcmb.2017-0430TR},
  url = {https://www.atsjournals.org/doi/10.1165/rcmb.2017-0430TR},
  urldate = {2021-01-04},
  langid = {english},
  keywords = {unread},
  annotation = {14 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S5PS8RG6/Koch et al. - 2018 - A Beginner’s Guide to Analysis of RNA Sequencing D.pdf}
}

@inproceedings{koehn2003StatisticalPhrasebasedTranslation,
  title = {Statistical Phrase-Based Translation},
  booktitle = {{{NAACL}}},
  author = {Koehn, P. and Och, F. J. and Marcu, D.},
  date = {2003},
  pages = {48--54},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D5CMF656/Koehn, Och, Marcu - 2003 - Statistical phrase-based translation(2).pdf}
}

@inproceedings{koh2017UnderstandingBlackboxPredictions,
  title = {Understanding {{Black-box Predictions}} via {{Influence Functions}}},
  booktitle = {{{ICML}}},
  author = {Koh, P. W. and Liang, P.},
  date = {2017},
  eprint = {1703.04730},
  eprinttype = {arxiv},
  issn = {1938-7228},
  url = {http://arxiv.org/abs/1703.04730},
  abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
  archiveprefix = {arXiv},
  annotation = {992 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y6NUNQI8/Koh, Liang - 2017 - Understanding Black-box Predictions via Influence Functions(2).pdf}
}

@misc{kojima2022LargeLanguageModels,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  date = {2022-06-09},
  number = {arXiv:2205.11916},
  eprint = {2205.11916},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.11916},
  urldate = {2022-07-28},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with 175B parameter InstructGPT model, as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UL33QIQT/Kojima et al. - 2022 - Large Language Models are Zero-Shot Reasoners.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6QLHWQRK/2205.html}
}

@inproceedings{kolluru2020IMoJIEIterativeMemoryBased,
  title = {{{IMoJIE}}: {{Iterative Memory-Based Joint Open Information Extraction}}},
  shorttitle = {{{IMoJIE}}},
  booktitle = {{{ACL}}},
  author = {Kolluru, Keshav and Aggarwal, Samarth and Rathore, Vipul and Mausam and Chakrabarti, Soumen},
  date = {2020-05-17},
  eprint = {2005.08178},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.08178},
  urldate = {2021-03-16},
  abstract = {While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task. Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et. al., 2018). Our analysis reveals that CopyAttention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information. We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples. This approach overcomes both shortcomings of CopyAttention, resulting in a variable number of diverse extractions per sentence. We train IMoJIE on training data bootstrapped from extractions of several non-neural systems, which have been automatically filtered to reduce redundancy and noise. IMoJIE outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task.},
  archiveprefix = {arXiv},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2ZBAEHP2/Kolluru et al. - 2020 - IMoJIE Iterative Memory-Based Joint Open Informat.pdf;/home/hiaoxui/.local/share/zotero_files/storage/8TAXNCDY/2005.html}
}

@inproceedings{koncel-kedziorski2014MultiResolutionLanguageGrounding,
  title = {Multi-{{Resolution Language Grounding}} with {{Weak Supervision}}},
  booktitle = {{{EMNLP}}},
  author = {Koncel-Kedziorski, R. and Hajishirzi, H. and Farhadi, A.},
  date = {2014},
  pages = {386--396},
  isbn = {978-1-937284-96-1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VWJHJTY3/Koncel-Kedziorski, Hajishirzi, Farhadi - 2014 - Multi-Resolution Language Grounding with Weak Supervision(2).pdf}
}

@inproceedings{kong2014DependencyParserTweets,
  title = {A {{Dependency Parser}} for {{Tweets}}},
  booktitle = {{{EMNLP}}},
  author = {Kong, L. and Schneider, N. and Swayamdipta, S. and Bhatia, A. and Dyer, C. and Smith, N. A.},
  date = {2014},
  doi = {10.3115/v1/d14-1108},
  abstract = {We describe a new dependency parser for English tweets, TWEEBOPARSER. The parser builds on several contributions: new syntactic annotations for a corpus of tweets (TWEEBANK), with conventions informed by the domain; adaptations to a statistical parsing algorithm; and a new approach to exploiting out-of-domain Penn Treebank data. Our experiments show that the parser achieves over 80\% unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contribu- tions.},
  annotation = {195 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WK8Z48QL/Kong et al. - 2014 - A Dependency Parser for Tweets(2).pdf}
}

@inproceedings{konstas2012ConcepttotextGenerationDiscriminative,
  title = {Concept-to-Text {{Generation}} via {{Discriminative Reranking}}},
  booktitle = {{{ACL}}},
  author = {Konstas, I. and Lapata, M.},
  date = {2012},
  pages = {369--378},
  abstract = {This paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. A key insight in our ap-proach is to reduce the tasks of content se-lection (" what to say ") and surface realization (" how to say ") into a common parsing prob-lem. We define a probabilistic context-free grammar that describes the structure of the in-put (a corpus of database records and text de-scribing some of them) and represent it com-pactly as a weighted hypergraph. The hyper-graph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. We propose a novel decoding algorithm for finding the best scoring derivation and generating in this set-ting. Experimental evaluation on the ATIS do-main shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study.},
  isbn = {978-1-937284-24-4},
  issue = {July},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W6IHWRFP/Konstas, Lapata - 2012 - Concept-to-text Generation via Discriminative Reranking(2).pdf}
}

@article{konstas2013GlobalModelConcepttotext,
  title = {A Global Model for Concept-to-Text Generation},
  author = {Konstas, I. and Lapata, M.},
  date = {2013},
  journaltitle = {JAIR},
  volume = {48},
  pages = {305--346},
  issn = {10769757},
  doi = {10.1613/jair.4025},
  abstract = {Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. We present a joint model that captures content selection (" what to say ") and surface realization (" how to say ") in an unsupervised domain-independent fashion. Rather than breaking up the generation process into a sequence of local decisions, we define a probabilis-tic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). We recast generation as the task of finding the best derivation tree for a set of database records and describe an algorithm for decoding in this framework that allows to intersect the grammar with additional information capturing fluency and syntactic well-formedness constraints. Experimental evaluation on several domains achieves re-sults competitive with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.},
  annotation = {78 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D6M2SF2Y/Konstas, Lapata - 2013 - A global model for concept-to-text generation(2).pdf}
}

@inproceedings{koppel2019PairwiseLearningRank,
  title = {Pairwise {{Learning}} to {{Rank}} by {{Neural Networks Revisited}}: {{Reconstruction}}, {{Theoretical Analysis}} and {{Practical Performance}}},
  shorttitle = {Pairwise {{Learning}} to {{Rank}} by {{Neural Networks Revisited}}},
  booktitle = {{{ECML PKDD}}},
  author = {Köppel, Marius and Segner, Alexander and Wagener, Martin and Pensel, Lukas and Karwath, Andreas and Kramer, Stefan},
  date = {2019-09-06},
  eprint = {1909.02768},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.02768},
  urldate = {2021-06-25},
  abstract = {We present a pairwise learning to rank approach based on a neural net, called DirectRanker, that generalizes the RankNet architecture. We show mathematically that our model is reflexive, antisymmetric, and transitive allowing for simplified training and improved performance. Experimental results on the LETOR MSLR-WEB10K, MQ2007 and MQ2008 datasets show that our model outperforms numerous state-of-the-art methods, while being inherently simpler in structure and using a pairwise approach only.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-06-25]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JQIH2EMM/Köppel et al. - 2019 - Pairwise Learning to Rank by Neural Networks Revis.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KRINVI4B/1909.html}
}

@inproceedings{korattikara2015BayesianDarkKnowledge,
  title = {Bayesian {{Dark Knowledge}}},
  booktitle = {{{NeurIPS}}},
  author = {Korattikara, A. and Rathod, V. and Murphy, K. and Welling, M.},
  date = {2015},
  pages = {1--9},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/899BQBWB/Korattikara et al. - 2015 - Bayesian Dark Knowledge(2).pdf}
}

@inproceedings{kornblith2019SimilarityNeuralNetwork,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  booktitle = {{{ICML}}},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  date = {2019-07-19},
  eprint = {1905.00414},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.00414},
  urldate = {2020-10-21},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  annotation = {132 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MIXBQQKL/Kornblith et al. - 2019 - Similarity of Neural Network Representations Revis.pdf}
}

@inproceedings{kottur2017NaturalLanguageDoes,
  title = {Natural {{Language Does Not Emerge}} '{{Naturally}}' in {{Multi-Agent Dialog}}},
  booktitle = {{{EMNLP}}},
  author = {Kottur, S. and Moura, J. M. F. and Lee, S. and Batra, D.},
  date = {2017},
  eprint = {1706.08502},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.08502},
  abstract = {A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {109 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M4GUG746/Kottur et al. - 2017 - Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog(2).pdf}
}

@inproceedings{kovatchev2019QualitativeEvaluationFramework,
  title = {A Qualitative Evaluation Framework for Paraphrase Identification},
  booktitle = {Recent {{Advances}} in {{Natural Language Processing}}},
  author = {Kovatchev, V. and Antònia Martí, M. and Salamó, M. and Beltran, J.},
  date = {2019},
  pages = {568--577},
  issn = {13138502},
  doi = {10.26615/978-954-452-056-4_067},
  abstract = {In this paper, we present a new approach for the evaluation, error analysis, and interpretation of supervised and unsupervised Paraphrase Identification (PI) systems. Our evaluation framework makes use of a PI corpus annotated with linguistic phenomena to provide a better understanding and interpretation of the performance of various PI systems. Our approach allows for a qualitative evaluation and comparison of the PI models using human interpretable categories. It does not require modification of the training objective of the systems and does not place additional burden on the developers. We replicate several popular supervised and unsupervised PI systems. Using our evaluation framework we show that: 1) Each system performs differently with respect to a set of linguistic phenomena and makes qualitatively different kinds of errors; 2) Some linguistic phenomena are more challenging than others across all systems.},
  isbn = {978-954-452-055-7},
  annotation = {2 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IMC4KLUZ/Kovatchev et al. - 2019 - A qualitative evaluation framework for paraphrase identification(2).pdf}
}

@inproceedings{krifka2007ApproximateInterpretationsNumber,
  title = {Approximate {{Interpretations}} of {{Number Words}}: {{A}} Case for Strategic Communication},
  booktitle = {Cognitive Foundations of Interpretation},
  author = {Krifka, M.},
  date = {2007},
  pages = {1--16},
  abstract = {This paper gives an explanation of the well-known phenomenon that round numbers in measure terms (like one hundred meters) are interpreted in a more approximate way than non-round numbers (like one hundred and three meters). Several possible explanations are considered: First, a preference for short expressions and approximate interpretations; second, a conditional preference for short expressions under approximate interpretations; third, an explanation in terms of strategic communication that makes use of the fact that approximate interpretations, even if not favored initially, turn out to be more likely once the probability of the reported values are factored in. These explanations are shown to be flawed, in particular because the complexity of expressions does not always matter. The theory that is put forward makes use of scales that differ insofar as they are more or less fine-grained, and proposes a principle that a number expression is interpreted on the most coarse-grained scale that it occurs on. This principle can be motivated by strategic communication that factors in the overall likelihood of the message. The emerging theory is refined in various ways. In particular, it will be shown that complexity of expressions is important after all, but mainly on the evolutionary level, where it can be shown to lead to characteristic patterns of language change. The paper ends with the discussion of some surprising facts about the influence that the number system of a language has on which numbers are actually expressed in that language.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NAUPGI3I/Krifka - 2007 - Approximate Interpretations of Number Words A case for strategic communication(2).pdf}
}

@misc{kriman2021JointDetectionCoreference,
  title = {Joint {{Detection}} and {{Coreference Resolution}} of {{Entities}} and {{Events}} with {{Document-level Context Aggregation}}},
  author = {Kriman, Samuel and Ji, Heng},
  date = {2021},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IPY7ERIK/Kriman and Ji - Joint Detection and Coreference Resolution of Enti.pdf}
}

@inproceedings{krishna2021HurdlesProgressLongform,
  title = {Hurdles to {{Progress}} in {{Long-form Question Answering}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Krishna, Kalpesh and Roy, Aurko and Iyyer, Mohit},
  date = {2021-03-10},
  eprint = {2103.06332},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.06332},
  urldate = {2021-04-17},
  abstract = {The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system's generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / test overlap, as at least 81\% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We provide suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-04-17]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2R7CWNLQ/Krishna et al. - 2021 - Hurdles to Progress in Long-form Question Answerin.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ILU4DWBQ/2103.html}
}

@inproceedings{krishnamurthy2012WeaklySupervisedTraining,
  title = {Weakly {{Supervised Training}} of {{Semantic Parsers}}},
  booktitle = {{{EMNLP}}},
  author = {Krishnamurthy, J. and Mitchell, T. M.},
  date = {2012},
  pages = {754--765},
  abstract = {We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependency- parsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-the- art accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80\% precision and 56\% recall, despite never having seen an annotated logical form.},
  isbn = {978-1-937284-43-5},
  issue = {July},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D6U7DLBG/Krishnamurthy, Mitchell - 2012 - Weakly Supervised Training of Semantic Parsers(2).pdf}
}

@inproceedings{krishnamurthy2013JointlyLearningParse,
  title = {Jointly {{Learning}} to {{Parse}} and {{Perceive}}: {{Connecting Natural Language}} to the {{Physical World}}},
  booktitle = {{{ACL}}},
  author = {Krishnamurthy, J. and Kollar, T.},
  date = {2013},
  volume = {1},
  pages = {193--206},
  issn = {2307-387X},
  abstract = {This paper introduces Logical Semantics with Perception (LSP), a model for grounded lan- guage acquisition that learns to map natu- ral language statements to their referents in a physical environment. For example, given an image, LSP can map the statement “blue mug on the table” to the set of image seg- ments showing blue mugs on tables. LSP learns physical representations for both cate- gorical (“blue,” “mug”) and relational (“on”) language, and also learns to compose these representations to produce the referents of en- tire statements. We further introduce a weakly supervised training procedure that estimates LSP's parameters using annotated referents for entire statements, without annotated ref- erents for individual words or the parse struc- ture of the statement. We perform experiments on two applications: scene understanding and geographical question answering. We find that LSP outperforms existing, less expressive models that cannot represent relational lan- guage. We further find that weakly supervised training is competitive with fully supervised training while requiring significantly less an- notation effort.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LZE9QR2V/Krishnamurthy, Kollar - 2013 - Jointly Learning to Parse and Perceive Connecting Natural Language to the Physical World(2).pdf}
}

@inproceedings{kshirsagar2015FramesemanticRoleLabeling,
  title = {Frame-Semantic Role Labeling with Heterogeneous Annotations},
  booktitle = {{{ACL-IJCNLP}}},
  author = {Kshirsagar, M. and Thomson, S. and Schneider, N. and Carbonell, J. and Smith, N. A. and Dyer, C.},
  date = {2015},
  pages = {218--224},
  doi = {10.3115/v1/p15-2036},
  abstract = {We consider the task of identifying and la-beling the semantic arguments of a predi-cate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach aug-ments an existing model with features de-rived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4\% absolute in-crease in F1versus the original model.},
  isbn = {978-1-941643-73-0},
  annotation = {61 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F954X5QG/Kshirsagar et al. - 2015 - Frame-semantic role labeling with heterogeneous annotations(2).pdf}
}

@inproceedings{kumar2002MinimumBayesRiskDecoding,
  title = {Minimum {{Bayes-Risk Decoding}} for {{Statistical Machine Translation}}},
  booktitle = {{{HLT-NAACL}}},
  author = {Kumar, S. and Byrne, W.},
  date = {2002},
  number = {0121285},
  isbn = {0-00-140110-6},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AVVB7844/Kumar, Byrne - 2002 - Minimum Bayes-Risk Decoding for Statistical Machine Translation(2).pdf}
}

@inproceedings{kumar2016AskMeAnything,
  title = {Ask {{Me Anything}}: {{Dynamic Memory Networks}} for {{Natural Language Processing}}},
  shorttitle = {Ask {{Me Anything}}},
  booktitle = {{{ICML}}},
  author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  date = {2016-03-05},
  eprint = {1506.07285},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.07285},
  urldate = {2021-04-21},
  abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {880 citations (Semantic Scholar/arXiv) [2021-04-21]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9Y72TGVT/Kumar et al. - 2016 - Ask Me Anything Dynamic Memory Networks for Natur.pdf;/home/hiaoxui/.local/share/zotero_files/storage/3DLA5M9E/1506.html}
}

@inproceedings{kumar2022FineTuningCanDistort,
  title = {Fine-{{Tuning}} Can {{Distort Pretrained Features}} and {{Underperform Out-of-Distribution}}},
  booktitle = {{{ICLR}}},
  author = {Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy},
  date = {2022-02-21},
  eprint = {2202.10054},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.10054},
  urldate = {2022-05-15},
  abstract = {When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \$\textbackslash to\$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2\% higher accuracy ID but 7\% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1\% better ID, 10\% better OOD than full fine-tuning).},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TEJDC8DH/Kumar et al. - 2022 - Fine-Tuning can Distort Pretrained Features and Un.pdf;/home/hiaoxui/.local/share/zotero_files/storage/W3P2TI6I/2202.html}
}

@inproceedings{kuncoro2017WhatRecurrentNeural,
  title = {What {{Do Recurrent Neural Network Grammars Learn About Syntax}} ?},
  booktitle = {{{EACL}}},
  author = {Kuncoro, A. and Ballesteros, M. and Kong, L. and Dyer, C. and Neubig, G. and Smith, N. A.},
  date = {2017},
  volume = {1},
  pages = {1249--1258},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MVZS5KAK/Kuncoro et al. - 2017 - What Do Recurrent Neural Network Grammars Learn About Syntax(2).pdf}
}

@inproceedings{kuru2016CharNERCharacterLevelNamed,
  title = {{{CharNER}} : {{Character-Level Named Entity Recognition}}},
  booktitle = {{{COLING}}},
  author = {Kuru, O. and Can, O. A. and Deniz, Y.},
  date = {2016},
  pages = {911--921},
  abstract = {We describe and evaluate a character-level tagger for language-independent Named Entity Recognition (NER). Instead of words, a sentence is represented as a sequence of characters. The model consists of stacked bidirectional LSTMs which inputs characters and outputs tag probabilities for each character. These probabilities are then converted to consistent word level named entity tags using a Viterbi decoder. We are able to achieve close to state-of-the-art NER performance in seven languages with the same basic model using only labeled NER data and no hand-engineered features or other external resources like syntactic taggers or Gazetteers.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K5PFRAY4/Kuru, Can, Deniz - 2016 - CharNER Character-Level Named Entity Recognition(2).pdf}
}

@inproceedings{kushman2013UsingSemanticUnification,
  title = {Using {{Semantic Unification}} to {{Generate Regular Expressions}} from {{Natural Language}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Kushman, N. and Barzilay, R.},
  date = {2013},
  pages = {826--836},
  url = {http://www.aclweb.org/anthology/N13-1103},
  abstract = {We consider the problem of translating natural language text queries into regular expres- sions which represent their meaning. The mis- match in the level of abstraction between the natural language representation and the regu- lar expression representation make this a novel and challenging problem. However, a given regular expression can be written in many se- mantically equivalent forms, and we exploit this flexibility to facilitate translation by find- ing a form which more directly corresponds to the natural language. We evaluate our tech- nique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a state- of-the-art semantic parsing baseline, yielding a 29\% absolute improvement in accuracy.},
  isbn = {978-1-937284-47-3},
  issue = {June},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MPPSRLKA/Kushman, Barzilay - 2013 - Using Semantic Unification to Generate Regular Expressions from Natural Language(2).pdf}
}

@inproceedings{kwiatkowski2010InducingProbabilisticCCG,
  title = {Inducing {{Probabilistic CCG Grammars}} from {{Logical Form}} with {{Higher-Order Unification}}},
  booktitle = {{{EMNLP}}},
  author = {Kwiatkowski, T. and Zettlemoyer, L. S. and Goldwater, S. and Steedman, M.},
  date = {2010},
  pages = {1223--1233},
  abstract = {This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously es- timating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 1},
  isbn = {1-932432-86-8},
  issue = {October},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VEETF5NQ/Kwiatkowski et al. - 2010 - Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification(2).pdf}
}

@inproceedings{kwiatkowski2013ScalingSemanticParsers,
  title = {Scaling {{Semantic Parsers}} with {{On-the-fly Ontology Matching}}},
  booktitle = {{{EMNLP}}},
  author = {Kwiatkowski, T. and Choi, E. and Artzi, Y. and Zettlemoyer, L. S.},
  date = {2013},
  pages = {1545--1556},
  url = {http://www.aclweb.org/anthology/D13-1161},
  abstract = {We consider the challenge of learning seman- tic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to rep- resent in a fixed target ontology. For ex- ample, even simple phrases such as ‘daugh- ter’ and ‘number of people living in’ can- not be directly represented in Freebase, whose ontology instead encodes facts about gen- der, parenthood, and population. In this pa- per, we introduce a new semantic parsing ap- proach that learns to resolve such ontologi- cal mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logical- form meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art per- formance on two benchmark semantic parsing datasets, including a nine point accuracy im- provement on a recent Freebase QA corpus.},
  isbn = {978-1-937284-97-8},
  issue = {October},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UG2KD72R/Kwiatkowski et al. - 2013 - Scaling Semantic Parsers with On-the-fly Ontology Matching(2).pdf}
}

@article{kwiatkowski2019NaturalQuestionsBenchmark,
  title = {Natural {{Questions}}: {{A Benchmark}} for {{Question Answering Research}}},
  shorttitle = {Natural {{Questions}}},
  author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
  date = {2019-11},
  journaltitle = {TACL},
  number = {7},
  pages = {452--466},
  doi = {10.1162/tacl_a_00276},
  url = {https://direct.mit.edu/tacl/article/43518},
  urldate = {2022-01-18},
  abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R3U5MTAA/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf}
}

@inproceedings{laban2021CanTransformerModels,
  title = {Can {{Transformer Models Measure Coherence In Text}}? {{Re-Thinking}} the {{Shuffle Test}}},
  shorttitle = {Can {{Transformer Models Measure Coherence In Text}}?},
  booktitle = {{{ACL}}},
  author = {Laban, Philippe and Dai, Luke and Bandarkar, Lucas and Hearst, Marti A.},
  date = {2021-07-07},
  eprint = {2107.03448},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.03448},
  urldate = {2021-07-23},
  abstract = {The Shuffle Test is the most common task to evaluate whether NLP models can measure coherence in text. Most recent work uses direct supervision on the task; we show that by simply finetuning a RoBERTa model, we can achieve a near perfect accuracy of 97.8\%, a state-of-the-art. We argue that this outstanding performance is unlikely to lead to a good model of text coherence, and suggest that the Shuffle Test should be approached in a Zero-Shot setting: models should be evaluated without being trained on the task itself. We evaluate common models in this setting, such as Generative and Bi-directional Transformers, and find that larger architectures achieve high-performance out-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of the original by increasing the size of blocks shuffled. Even though human reader performance remains high (around 95\% accuracy), model performance drops from 94\% to 78\% as block size increases, creating a conceptually simple challenge to benchmark NLP models. Code available: https://github.com/tingofurro/shuffle\_test/},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QE48VURN/Laban et al. - 2021 - Can Transformer Models Measure Coherence In Text .pdf;/home/hiaoxui/.local/share/zotero_files/storage/F77VGCG7/2107.html}
}

@inproceedings{labeau2019ExperimentingPowerDivergences,
  title = {Experimenting with {{Power Divergences}} for {{Language Modeling}}},
  booktitle = {{{EMNLP}}},
  author = {Labeau, M. and Cohen, S. B.},
  date = {2019},
  number = {2018},
  pages = {4104--4114},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WTQ64CPZ/Labeau, Cohen - 2019 - Experimenting with Power Divergences for Language Modeling(2).pdf}
}

@inproceedings{lai2017RACELargescaleReAding,
  title = {{{RACE}}: {{Large-scale ReAding Comprehension Dataset From Examinations}}},
  booktitle = {{{EMNLP}}},
  author = {Lai, G. and Xie, Q. and Liu, H. and Yang, Y. and Hovy, E.},
  date = {2017},
  eprint = {1704.04683},
  eprinttype = {arxiv},
  pages = {785--794},
  doi = {10.18653/v1/d17-1082},
  abstract = {We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43\%) and the ceiling human performance (95\%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/\textasciitilde glai1/data/race/ and the code is available at https://github.com/qizhex/RACE\_AR\_baselines.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-83-8},
  annotation = {394 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B5MIA5BK/Lai et al. - 2017 - RACE Large-scale ReAding Comprehension Dataset From Examinations(2).pdf}
}

@inproceedings{lai2020ContextAnalysisPretrained,
  title = {Context {{Analysis}} for {{Pre-trained Masked Language Models}}},
  booktitle = {{{EMNLP}}},
  author = {Lai, Yi-An and Lalwani, Garima and Zhang, Yi},
  date = {2020},
  pages = {3789--3804},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.338},
  url = {https://www.aclweb.org/anthology/2020.findings-emnlp.338},
  urldate = {2022-02-10},
  abstract = {Pre-trained language models that learn contextualized word representations from a large unannotated corpus have become a standard component for many state-of-the-art NLP systems. Despite their successful applications in various downstream NLP tasks, the extent of contextual impact on the word representation has not been explored. In this paper, we present a detailed analysis of contextual impact in Transformer- and BiLSTM-based masked language models. We follow two different approaches to evaluate the impact of context: a masking based approach that is architecture agnostic, and a gradient based approach that requires back-propagation through networks. The findings suggest significant differences on the contextual impact between the two model architectures. Through further breakdown of analysis by syntactic categories, we find the contextual impact in Transformer-based MLM aligns well with linguistic intuition. We further explore the Transformer attention pruning based on our findings in contextual analysis.},
  eventtitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YYXK49YZ/Lai et al. - 2020 - Context Analysis for Pre-trained Masked Language M.pdf}
}

@misc{lai2021EndtoendNeuralCoreference,
  title = {End-to-End {{Neural Coreference Resolution Revisited}}: {{A Simple}} yet {{Effective Baseline}}},
  shorttitle = {End-to-End {{Neural Coreference Resolution Revisited}}},
  author = {Lai, Tuan Manh and Bui, Trung and Kim, Doo Soon},
  date = {2021-07-13},
  eprint = {2107.01700},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.01700},
  urldate = {2021-07-23},
  abstract = {Since the first end-to-end neural coreference resolution model was introduced, many extensions to the model have been proposed, ranging from using higher-order inference to directly optimizing evaluation metrics using reinforcement learning. Despite improving the coreference resolution performance by a large margin, these extensions add a lot of extra complexity to the original model. Motivated by this observation and the recent advances in pre-trained Transformer language models, we propose a simple yet effective baseline for coreference resolution. Our model is a simplified version of the original neural coreference resolution model, however, it achieves impressive performance, outperforming all recent extended works on the public English OntoNotes benchmark. Our work provides evidence for the necessity of carefully justifying the complexity of existing or newly proposed models, as introducing a conceptual or practical simplification to an existing model can still yield competitive results.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6HJ6HNC6/Lai et al. - 2021 - End-to-end Neural Coreference Resolution Revisited.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ZV2QDXE3/2107.html}
}

@article{lake2015HumanlevelConceptLearning,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, B. M. and Salakhutdinov, R. and Tenenbaum, J. B.},
  date = {2015},
  journaltitle = {Science},
  volume = {350},
  number = {6266},
  eprint = {26659050},
  eprinttype = {pmid},
  pages = {1332--1338},
  issn = {10959203},
  doi = {10.1126/science.aab3050},
  abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation.We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
  isbn = {0036-8075},
  annotation = {1512 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9L7Y6SRK/Lake, Salakhutdinov, Tenenbaum - 2015 - Human-level concept learning through probabilistic program induction(2).pdf}
}

@inproceedings{lakkaraju2016InterpretableDecisionSets,
  title = {Interpretable {{Decision Sets}}: {{A Joint Framework}} for {{Description}} and {{Prediction}}},
  booktitle = {{{KDD}}},
  author = {Lakkaraju, H. and Bach, S. H. and Jure, L.},
  date = {2016},
  volume = {1},
  eprint = {27853627},
  eprinttype = {pmid},
  pages = {1675--1684},
  issn = {2154-817X},
  doi = {10.1145/2939672.2939874},
  abstract = {One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.},
  isbn = {2154817X (Linking)},
  annotation = {353 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DDFC5ELP/Lakkaraju, Bach, Jure - 2016 - Interpretable Decision Sets A Joint Framework for Description and Prediction(2).pdf}
}

@misc{lamm2020QEDFrameworkDataset,
  title = {{{QED}}: {{A Framework}} and {{Dataset}} for {{Explanations}} in {{Question Answering}}},
  shorttitle = {{{QED}}},
  author = {Lamm, Matthew and Palomaki, Jennimaria and Alberti, Chris and Andor, Daniel and Choi, Eunsol and Soares, Livio Baldini and Collins, Michael},
  date = {2020-09-08},
  eprint = {2009.06354},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.06354},
  urldate = {2021-07-23},
  abstract = {A question answering system that in addition to providing an answer provides an explanation of the reasoning that leads to that answer has potential advantages in terms of debuggability, extensibility and trust. To this end, we propose QED, a linguistically informed, extensible framework for explanations in question answering. A QED explanation specifies the relationship between a question and answer according to formal semantic notions such as referential equality, sentencehood, and entailment. We describe and publicly release an expert-annotated dataset of QED explanations built upon a subset of the Google Natural Questions dataset, and report baseline models on two tasks -- post-hoc explanation generation given an answer, and joint question answering and explanation generation. In the joint setting, a promising result suggests that training on a relatively small amount of QED data can improve question answering. In addition to describing the formal, language-theoretic motivations for the QED approach, we describe a large user study showing that the presence of QED explanations significantly improves the ability of untrained raters to spot errors made by a strong neural QA baseline.},
  archiveprefix = {arXiv},
  annotation = {9 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U5VVTF5K/Lamm et al. - 2020 - QED A Framework and Dataset for Explanations in Q.pdf;/home/hiaoxui/.local/share/zotero_files/storage/FHTAEXZ6/2009.html}
}

@inproceedings{lample2016NeuralArchitecturesNamed,
  title = {Neural {{Architectures}} for {{Named Entity Recognition}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
  date = {2016},
  pages = {260--270},
  publisher = {{Association for Computational Linguistics}},
  location = {{San Diego, California}},
  doi = {10.18653/v1/N16-1030},
  url = {http://aclweb.org/anthology/N16-1030},
  urldate = {2020-11-21},
  eventtitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  annotation = {2224 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LCFPWBDY/Lample et al. - 2016 - Neural Architectures for Named Entity Recognition.pdf}
}

@inproceedings{lample2018PhraseBasedNeuralUnsupervised,
  title = {Phrase-{{Based}} \& {{Neural Unsupervised Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Lample, G. and Ott, M. and Conneau, A. and Denoyer, L. and Ranzato, M.},
  date = {2018},
  eprint = {20829068},
  eprinttype = {pmid},
  issn = {1532-8422},
  doi = {10.1053/j.jvca.2010.06.032},
  url = {http://arxiv.org/abs/1804.07755},
  abstract = {Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT'14 English-French and WMT'16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.},
  archiveprefix = {arXiv},
  isbn = {0892-0915 (Print)\textbackslash r0892-0915 (Linking)},
  keywords = {unread},
  annotation = {42 citations (Semantic Scholar/DOI) [2021-03-26] 383 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GM49DQY7/Lample et al. - 2018 - Phrase-Based & Neural Unsupervised Machine Translation(2).pdf}
}

@inproceedings{lample2018UnsupervisedMachineTranslation,
  title = {Unsupervised {{Machine Translation Using Monolingual Corpora Only}}},
  booktitle = {{{ICLR}}},
  author = {Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  date = {2018-04-13},
  eprint = {1711.00043},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.00043},
  urldate = {2020-12-01},
  abstract = {Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  annotation = {582 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7898VADW/Lample et al. - 2018 - Unsupervised Machine Translation Using Monolingual.pdf}
}

@inproceedings{lapata2003ProbabilisticTextStructuring,
  title = {Probabilistic Text Structuring: Experiments with Sentence Ordering},
  booktitle = {{{ACL}}},
  author = {Lapata, M.},
  date = {2003},
  pages = {545--552},
  doi = {10.3115/1075096.1075165},
  url = {http://portal.acm.org/citation.cfm?id=1075165},
  abstract = {Ordering information is a critical task for natural language generation applications. In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation. We describe a model that learns constraints on sentence order from a corpus of domain-specific texts and an algorithm that yields the most likely order among several alternatives. We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model's task. We also assess the appropriateness of such a model for multidocument summarization.},
  annotation = {292 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZI93RWDN/Lapata - 2003 - Probabilistic text structuring experiments with sentence ordering(2).pdf}
}

@inproceedings{law2019LorentzianDistanceLearning,
  title = {Lorentzian {{Distance Learning}} for {{Hyperbolic Representations}}},
  booktitle = {{{ICML}}},
  author = {Law, M. and Liao, R. and Snell, J. and Zemel, R.},
  date = {2019},
  volume = {97},
  pages = {3672--3681},
  url = {http://proceedings.mlr.press/v97/law19a.html},
  abstract = {We introduce an approach to learn representations based on the Lorentzian distance in hyperbolic geometry. Hyperbolic geometry is especially suited to hierarchically-structured datasets, which are prevalent in the real world. Current hyperbolic representation learning methods compare examples with the Poincaré distance. They try to minimize the distance of each node in a hierarchy with its descendants while maximizing its distance with other nodes. This formulation produces node representations close to the centroid of their descendants. To obtain efficient and interpretable algorithms, we exploit the fact that the centroid w.r.t the squared Lorentzian distance can be written in closed-form. We show that the Euclidean norm of such a centroid decreases as the curvature of the hyperbolic space decreases. This property makes it appropriate to represent hierarchies where parent nodes minimize the distances to their descendants and have smaller Euclidean norm than their children. Our approach obtains state-of-the-art results in retrieval and classification tasks on different datasets.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/56JVA7E8/Law et al. - 2019 - Lorentzian Distance Learning for Hyperbolic Representations(2).pdf}
}

@article{lawrence2020ArgumentMiningSurvey,
  title = {Argument {{Mining}}: {{A Survey}}},
  shorttitle = {Argument {{Mining}}},
  author = {Lawrence, John and Reed, Chris},
  date = {2020},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {45},
  number = {4},
  pages = {765--818},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00364},
  abstract = {Argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. Understanding argumentative structure makes it possible to determine not only what positions people are adopting, but also why they hold the opinions they do, providing valuable insights in domains as diverse as financial market prediction and public relations. This survey explores the techniques that establish the foundations for argument mining, provides a review of recent advances in argument mining techniques, and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.},
  langid = {english},
  annotation = {43 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MS5XDNX2/Lawrence and Reed - 2020 - Argument Mining A Survey.pdf}
}

@inproceedings{lazaridou2021MindGapAssessing,
  title = {Mind the {{Gap}}: {{Assessing Temporal Generalization}} in {{Neural Language Models}}},
  booktitle = {{{NeurIPS}}},
  author = {Lazaridou, Angeliki and Kuncoro, Adhiguna and Gribovskaya, Elena and Agrawal, Devang and Liška, Adam and Terzi, Tayfun and Gimenez, Mai},
  date = {2021},
  pages = {20},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HE6NQEF9/Lazaridou et al. - Mind the Gap Assessing Temporal Generalization in.pdf}
}

@article{lazer2009ComputationalSocialScience,
  title = {Computational {{Social Science}}},
  author = {Lazer, D. and Pentland, A. and Adamic, L. and Aral, S. and Barabási, A. and Brewer, D. and Christakis, N. and Contractor, N. and Fowler, J. and Gutmann, M. and Jebara, T. and King, G. and Macy, M. and Roy, D. and Van Alstyne, M.},
  date = {2009},
  journaltitle = {Science},
  volume = {323},
  issn = {10015728},
  doi = {10.13618/j.issn.1001-5728.2014.06.012},
  isbn = {0265-1491},
  issue = {February},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2I2FFK32/Lazer et al. - 2009 - Computational Social Science(2).pdf}
}

@inproceedings{lebret2016NeuralTextGeneration,
  title = {Neural {{Text Generation}} from {{Structured Data}} with {{Application}} to the {{Biography Domain}}},
  booktitle = {{{EMNLP}}},
  author = {Lebret, R. and Grangier, D. and Auli, M.},
  date = {2016},
  eprint = {1603.07771},
  eprinttype = {arxiv},
  doi = {10.18653/v1/D16-1128},
  url = {http://arxiv.org/abs/1603.07771},
  abstract = {This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {220 citations (Semantic Scholar/DOI) [2021-03-26] 24 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3I943P6R/Lebret, Grangier, Auli - 2016 - Neural Text Generation from Structured Data with Application to the Biography Domain(2).pdf}
}

@inproceedings{lee-thorp2022FNetMixingTokens,
  title = {{{FNet}}: {{Mixing Tokens}} with {{Fourier Transforms}}},
  shorttitle = {{{FNet}}},
  booktitle = {{{NAACL}}},
  author = {Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  date = {2022-05-26},
  eprint = {2105.03824},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.03824},
  urldate = {2022-07-14},
  abstract = {We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that "mix" input tokens. These linear mixers, along with standard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text classification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97\% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80\% faster on GPUs and 70\% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the "efficient" Transformers on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GKY6R2L5/Lee-Thorp et al. - 2022 - FNet Mixing Tokens with Fourier Transforms.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ZX4DP6NT/2105.html}
}

@inproceedings{lee2004SupervisedWordSense,
  title = {Supervised Word Sense Disambiguation with Support Vector Machines and Multiple Knowledge Sources},
  booktitle = {{{ACL}}},
  author = {Lee, Y. K. and Ng, H. T. and Chia, T. K.},
  date = {2004},
  pages = {137--140},
  issue = {July},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2PXJYXR9/Lee, Ng, Chia - 2004 - Supervised word sense disambiguation with support vector machines and multiple knowledge sources(2).pdf}
}

@inproceedings{lee2016GlobalNeuralCCG,
  title = {Global {{Neural CCG Parsing}} with {{Optimality Guarantees}}},
  booktitle = {{{EMNLP}}},
  author = {Lee, K. and Lewis, M. and Zettlemoyer, L. S.},
  date = {2016},
  eprint = {1607.01432},
  eprinttype = {arxiv},
  pages = {2366--2376},
  doi = {10.18653/v1/d16-1262},
  abstract = {We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9\% of held-out sentences, exploring on average only 190 subtrees.},
  archiveprefix = {arXiv},
  annotation = {29 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BTSJYGXI/Lee, Lewis, Zettlemoyer - 2016 - Global Neural CCG Parsing with Optimality Guarantees(2).pdf}
}

@inproceedings{lee2018DeterministicNonAutoregressiveNeural,
  title = {Deterministic {{Non-Autoregressive Neural Sequence Modeling}} by {{Iterative Refinement}}},
  booktitle = {{{EMNLP}}},
  author = {Lee, J. and Mansimov, E. and Cho, K.},
  date = {2018},
  eprint = {1802.06901},
  eprinttype = {arxiv},
  pages = {1173--1182},
  doi = {10.18653/v1/d18-1149},
  abstract = {We propose a conditional non-autoregressive neural sequence model based on iterative refinement. The proposed model is designed based on the principles of latent variable models and denoising autoencoders, and is generally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En-De and En-Ro) and image caption generation, and observe that it significantly speeds up decoding while maintaining the generation quality comparable to the autoregressive counterpart.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {172 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PXNQXW3N/Lee, Mansimov, Cho - 2018 - Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement(2).pdf}
}

@inproceedings{lee2018HigherorderCoreferenceResolution,
  title = {Higher-Order {{Coreference Resolution}} with {{Coarse-to-fine Inference}}},
  booktitle = {{{NAACL}}},
  author = {Lee, Kenton and He, Luheng and Zettlemoyer, Luke},
  date = {2018-04-15},
  eprint = {1804.05392},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.05392},
  urldate = {2021-03-10},
  abstract = {We introduce a fully differentiable approximation to higher-order inference for coreference resolution. Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations. This enables the model to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting accuracy. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient.},
  archiveprefix = {arXiv},
  annotation = {161 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KLLG2F4U/Lee et al. - 2018 - Higher-order Coreference Resolution with Coarse-to.pdf;/home/hiaoxui/.local/share/zotero_files/storage/EWFMF4JC/1804.html}
}

@inproceedings{lee2019LatentRetrievalWeakly,
  title = {Latent {{Retrieval}} for {{Weakly Supervised Open Domain Question Answering}}},
  booktitle = {{{ACL}}},
  author = {Lee, K. and Chang, M. and Toutanova, K.},
  date = {2019},
  eprint = {1906.00300},
  eprinttype = {arxiv},
  pages = {6086--6096},
  doi = {10.18653/v1/p19-1612},
  abstract = {Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {148 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DF5HZNP3/Lee, Chang, Toutanova - 2019 - Latent Retrieval for Weakly Supervised Open Domain Question Answering(2).pdf}
}

@inproceedings{lee2019SetTransformerFramework,
  title = {Set {{Transformer}}: {{A Framework}} for {{Attention-based Permutation-Invariant Neural Networks}}},
  booktitle = {{{ICML}}},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R and Choi, Seungjin and Teh, Yee Whye},
  date = {2019},
  pages = {10},
  abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition and fewshot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D8KRXRVA/Lee et al. - Set Transformer A Framework for Attention-based P.pdf}
}

@misc{lee2019WideNeuralNetworks,
  title = {Wide {{Neural Networks}} of {{Any Depth Evolve}} as {{Linear Models Under Gradient Descent}}},
  author = {Lee, J. and Xiao, L. and Schoenholz, S. S. and Bahri, Y. and Sohl-Dickstein, J. and Pennington, J.},
  date = {2019},
  eprint = {1902.06720},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1902.06720},
  abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {294 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SQ4CA5UM/Lee et al. - 2019 - Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent(2).pdf}
}

@inproceedings{lee2021RobustifyingMultihopQA,
  title = {Robustifying {{Multi-hop QA}} through {{Pseudo-Evidentiality Training}}},
  booktitle = {{{ACL}}},
  author = {Lee, Kyungjae and Hwang, Seung-won and Han, Sang-eun and Lee, Dohyeon},
  date = {2021-07-07},
  eprint = {2107.03242},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.03242},
  urldate = {2021-07-23},
  abstract = {This paper studies the bias problem of multi-hop question answering models, of answering correctly without correct reasoning. One way to robustify these models is by supervising to not only answer right, but also with right reasoning chains. An existing direction is to annotate reasoning chains to train models, requiring expensive additional annotations. In contrast, we propose a new approach to learn evidentiality, deciding whether the answer prediction is supported by correct evidences, without such annotations. Instead, we compare counterfactual changes in answer confidence with and without evidence sentences, to generate "pseudo-evidentiality" annotations. We validate our proposed model on an original set and challenge set in HotpotQA, showing that our method is accurate and robust in multi-hop reasoning.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SYNU4WIR/Lee et al. - 2021 - Robustifying Multi-hop QA through Pseudo-Evidentia.pdf;/home/hiaoxui/.local/share/zotero_files/storage/99CYFH6Z/2107.html}
}

@inproceedings{lee2022MetaLearningNatural,
  title = {Meta {{Learning}} for {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Meta {{Learning}} for {{Natural Language Processing}}},
  booktitle = {{{NAACL}}},
  author = {Lee, Hung-yi and Li, Shang-Wen and Vu, Ngoc Thang},
  date = {2022-07-02},
  eprint = {2205.01500},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.01500},
  urldate = {2022-07-15},
  abstract = {Deep learning has been the mainstream technique in natural language processing (NLP) area. However, the techniques require many labeled data and are less generalizable across domains. Meta-learning is an arising field in machine learning studying approaches to learn better learning algorithms. Approaches aim at improving algorithms in various aspects, including data efficiency and generalizability. Efficacy of approaches has been shown in many NLP tasks, but there is no systematic survey of these approaches in NLP, which hinders more researchers from joining the field. Our goal with this survey paper is to offer researchers pointers to relevant meta-learning works in NLP and attract more attention from the NLP community to drive future innovation. This paper first introduces the general concepts of meta-learning and the common approaches. Then we summarize task construction settings and application of meta-learning for various NLP problems and review the development of meta-learning in NLP community.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NAN3QITY/Lee et al. - 2022 - Meta Learning for Natural Language Processing A S.pdf;/home/hiaoxui/.local/share/zotero_files/storage/UHZ73P3B/2205.html}
}

@inproceedings{lei2015MoldingCNNsText,
  title = {Molding {{CNNs}} for Text: Non-Linear, Non-Consecutive Convolutions},
  booktitle = {{{EMNLP}}},
  author = {Lei, T. and Barzilay, R. and Jaakkola, T. S.},
  date = {2015},
  eprint = {1508.04112},
  eprinttype = {arxiv},
  pages = {1565--1575},
  url = {http://arxiv.org/abs/1508.04112},
  abstract = {The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2\% accuracy on the fine-grained sentiment classification task.},
  archiveprefix = {arXiv},
  isbn = {978-1-941643-32-7},
  issue = {September},
  annotation = {123 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R8Q5AVMZ/Lei, Barzilay, Jaakkola - 2015 - Molding CNNs for text non-linear, non-consecutive convolutions(2).pdf}
}

@thesis{lei2017InterpretableNeuralModels,
  title = {Interpretable {{Neural Models}} for {{Natural Language Processing}}},
  author = {Lei, T.},
  date = {2017},
  journaltitle = {Massachusetts Institute of Technology},
  abstract = {Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2017.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TD6AKJ95/Lei - 2017 - Interpretable Neural Models for Natural Language Processing(2).pdf}
}

@inproceedings{lei2021WhenAttentionMeets,
  title = {When {{Attention Meets Fast Recurrence}}: {{Training Language Models}} with {{Reduced Compute}}},
  shorttitle = {When {{Attention Meets Fast Recurrence}}},
  booktitle = {{{EMNLP}}},
  author = {Lei, Tao},
  date = {2021-09-14},
  eprint = {2102.12459},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.12459},
  urldate = {2021-11-25},
  abstract = {Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FL2KNTEV/Lei - 2021 - When Attention Meets Fast Recurrence Training Lan.pdf}
}

@inproceedings{levesque2013OurBestBehaviour,
  title = {On Our Best Behaviour.},
  booktitle = {{{IJCAI}}},
  author = {Levesque, H. J.},
  date = {2013},
  eprint = {966205},
  eprinttype = {pmid},
  issn = {0035-8797},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q2QPY5CS/Levesque - 2013 - On our best behaviour(2).pdf}
}

@inproceedings{levy2014DependencyBasedWordEmbeddings,
  title = {Dependency-{{Based Word Embeddings}}},
  booktitle = {{{ACL}}},
  author = {Levy, O. and Goldberg, Y.},
  date = {2014},
  pages = {302--308},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QPPWW98I/Levy, Goldberg - 2014 - Dependency-Based Word Embeddings(2).pdf}
}

@inproceedings{levy2018LongShortTermMemory,
  title = {Long {{Short-Term Memory As}} a {{Dynamically Computed Element-Wise Weighted Sum}}},
  booktitle = {{{ACL}}},
  author = {Levy, O. and Lee, K. and FitzGerald, N. and Zettlemoyer, L. S.},
  date = {2018},
  eprint = {1805.03716},
  eprinttype = {arxiv},
  pages = {1--9},
  abstract = {We present an alternate view to explain the success of LSTMs: the gates themselves are powerful recurrent models that provide more representational power than previ-ously appreciated. We do this by showing that much of the LSTM's architecture can be removed, producing a restricted class of RNNs where the main recurrence computes an element-wise weighted sum of context-independent functions of the inputs. Experiments on a range of challenging NLP problems demonstrate that the simplified models work as well as the original LSTMs, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CFVIXPH2/Levy et al. - 2018 - Long Short-Term Memory As a Dynamically Computed Element-Wise Weighted Sum(2).pdf}
}

@inproceedings{lewis2015JointCCGParsing,
  title = {Joint {{A}}∗{{CCG}} Parsing and Semantic Role Labeling},
  booktitle = {{{EMNLP}}},
  author = {Lewis, M. and He, L. and Zettlemoyer, L. S.},
  date = {2015},
  pages = {1444--1454},
  abstract = {Joint models of syntactic and semantic parsing have the potential to improve performance on both tasks-but to date, the best results have been achieved with pipelines. We introduce a joint model using CCG, which is motivated by the close link between CCG syntax and semantics. Semantic roles are recovered by labelling the deep dependency structures produced by the grammar. Furthermore, because CCG is lexicalized, we show it is possible to factor the parsing model over words and introduce a new A∗parsing algorithm-which we demonstrate is faster and more accurate than adaptive supertagging. Our joint model is the first to substantially improve both syntactic and semantic accuracy over a comparable pipeline, and also achieves state-of-the-art results for a nonensemble semantic role labelling model.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L57HNLJU/Lewis, He, Zettlemoyer - 2015 - Joint A∗CCG parsing and semantic role labeling(2).pdf}
}

@inproceedings{lewis2022BoostedDenseRetriever,
  title = {Boosted {{Dense Retriever}}},
  booktitle = {{{NAACL}}},
  author = {Lewis, Patrick and Oğuz, Barlas and Xiong, Wenhan and Petroni, Fabio and Yih, Wen-tau and Riedel, Sebastian},
  date = {2022},
  eprint = {2112.07771},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.07771},
  urldate = {2022-07-15},
  abstract = {We propose DrBoost, a dense retrieval ensemble inspired by boosting. DrBoost is trained in stages: each component model is learned sequentially and specialized by focusing only on retrieval mistakes made by the current ensemble. The final representation is the concatenation of the output vectors of all the component models, making it a drop-in replacement for standard dense retrievers at test time. DrBoost enjoys several advantages compared to standard dense retrieval models. It produces representations which are 4x more compact, while delivering comparable retrieval results. It also performs surprisingly well under approximate search with coarse quantization, reducing latency and bandwidth needs by another 4x. In practice, this can make the difference between serving indices from disk versus from memory, paving the way for much cheaper deployments.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BIQ8BHXR/Lewis et al. - 2021 - Boosted Dense Retriever.pdf;/home/hiaoxui/.local/share/zotero_files/storage/C4BVZ78N/2112.html}
}

@article{li2006SketchAlgorithmEstimating,
  title = {A Sketch Algorithm for Estimating Two-Way and Multi-Way Associations},
  author = {Li, P. and Church, K. W.},
  date = {2006},
  journaltitle = {Computational Linguistics},
  volume = {33},
  number = {3},
  pages = {305--354},
  issn = {08912017},
  doi = {10.1162/coli.2007.33.3.305},
  abstract = {We should not have to look at the entire corpus (e.g., the Web) to know if two (or more) words are strongly associated or not. One can often obtain estimates of associations from a small sample. We develop a sketch-based algorithm that constructs a contingency table for a sample. One can estimate the contingency table for the entire population using straightforward scaling. However, one can do better by taking advantage of the margins (also known as document frequencies). The proposed method cuts the errors roughly in half over Broder's sketches.},
  keywords = {unread},
  annotation = {53 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y6B4YSF4/Li, Church - 2006 - A sketch algorithm for estimating two-way and multi-way associations(2).pdf}
}

@inproceedings{li2009IntervalEventStream,
  title = {Interval Event Stream Processing},
  booktitle = {{{DEBS}}},
  author = {Li, M. and Mani, M. and Rundensteiner, E. A. and Wang, D. and Lin, T.},
  date = {2009},
  doi = {10.1145/1619258.1619302},
  abstract = {Event stream processing (ESP) has become increasingly important in modern applications, ranging from supply chain management to real-time intrusion detection. Existing ESP engines have focused on detecting temporal patterns from instantaneous events, that is, events with no duration. Under such a model, an event instance can only be happening "before", "after" or "at the same time as" another event instance. However, such sequential patterns are inadequate to express the complex temporal relationships in domains such as medical, finance and meteorology, where the events' durations could play an important role.},
  isbn = {978-1-60558-665-6},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GQ4VMFLH/Li et al. - 2009 - Interval event stream processing(2).pdf}
}

@inproceedings{li2014ScalingDistributedMachine,
  title = {Scaling {{Distributed Machine Learning}} with the {{Parameter Server}}},
  booktitle = {Symposium on {{Operating Systems Design}} and {{Implementation}}},
  author = {Li, M. and Andersen, D. G. and Park, J. W. and Ahmed, A. and Josifovski, V. and Long, J. and Shekita, E. J. and Su, B.},
  date = {2014},
  pages = {583--598},
  url = {http://www.mzi.gov.si/si/medijsko_sredisce/novica/article/799/8867/},
  abstract = {We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters, represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance. To demonstrate the scalability of the proposed frame-work, we show experimental results on petabytes of real data with billions of examples and parameters on prob-lems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching.},
  isbn = {978-1-931971-16-4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/73JZUD3G/Li et al. - 2014 - Scaling Distributed Machine Learning with the Parameter Server(2).pdf}
}

@inproceedings{li2015FastAccuratePrediction,
  title = {Fast and {{Accurate Prediction}} of {{Sentence Specificity}}},
  booktitle = {{{AAAI}}},
  author = {Li, J. J. and Nenkova, A.},
  date = {2015},
  pages = {2281--2287},
  issn = {1895104X},
  doi = {10.2478/s11535-006-0039-x},
  abstract = {Soybean kernels of cultivars Bosa and ZPS 015 were used in the experiment. The contents of available lysine as well as water and salt soluble proteins, were analysed in fresh soybean kernels, soybean products made after the processes of dry extrusion, micronisation, microwave toasting and autoclaving. Utilizing a technological procedure of processing, kernels were exposed to temperatures from 57 to 150°C. The duration of exposure of the soybean kernels to the increased temperatures, ranged from 25-30 seconds in dry extrusion to 30 minutes in autoclaving. All treatments were subjected to different sources of heat, causing different thermodynamic processes to take place in kernels and change their chemical composition; i.e. nutritive quality. The content of water and salt soluble proteins decreased under the influence of higher temperatures in the course of all treatments of processing. The drop of solubility already was drastically effected by temperatures of 100°C in dry extrusion, while there was a gradual decrease in other treatments. The content of available lysine was determined by the modified Carpenter methods with DNFB. The processes of micronisation and microwave toasting showed the greatest effect on the reduction of lysine availability. Dry extrusion and autoclaving, performed within closed systems — in which the increased moisture content has a special effect — resulted in significantly smaller changes of the available lysine content.},
  isbn = {978-1-57735-701-8},
  keywords = {unread},
  annotation = {26 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YBCDTYZ3/Li, Nenkova - 2015 - Fast and Accurate Prediction of Sentence Specificity(2).pdf}
}

@inproceedings{li2015HierarchicalNeuralAutoencoder,
  title = {A {{Hierarchical Neural Autoencoder}} for {{Paragraphs}} and {{Documents}}},
  booktitle = {{{ACL}}},
  author = {Li, J. and Luong, M. and Jurafsky, D.},
  date = {2015},
  eprint = {1506.01057},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.01057},
  abstract = {Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization\textbackslash footnote\{Code for the three models described in this paper can be found at www.stanford.edu/\textasciitilde jiweil/ .\vphantom\}},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {482 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/24IT72DF/Li, Luong, Jurafsky - 2015 - A Hierarchical Neural Autoencoder for Paragraphs and Documents(2).pdf}
}

@inproceedings{li2016DeepReinforcementLearning,
  title = {Deep {{Reinforcement Learning}} for {{Dialogue Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Li, J. and Monroe, W. and Ritter, A. and Galley, M. and Gao, J. and Jurafsky, D.},
  date = {2016},
  number = {4},
  eprint = {1606.01541},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.01541},
  abstract = {Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.},
  archiveprefix = {arXiv},
  annotation = {808 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y9J3KZST/Li et al. - 2016 - Deep Reinforcement Learning for Dialogue Generation(2).pdf}
}

@inproceedings{li2016PersonaBasedNeuralConversation,
  title = {A {{Persona-Based Neural Conversation Model}}},
  booktitle = {{{ACL}}},
  author = {Li, J. and Galley, M. and Brockett, C. and Spithourakis, G. P. and Gao, J. and Dolan, B.},
  date = {2016},
  eprint = {1603.06155},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1603.06155},
  abstract = {We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-2758-5},
  keywords = {unread},
  annotation = {646 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RQALIXNZ/Li et al. - 2016 - A Persona-Based Neural Conversation Model(2).pdf}
}

@inproceedings{li2017VisualizingUnderstandingNeural,
  title = {Visualizing and {{Understanding Neural Models}} in {{NLP}}},
  booktitle = {{{ACL}}},
  author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
  date = {2017},
  eprint = {1506.01066},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.01066},
  urldate = {2022-02-12},
  abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve \{\textbackslash em compositionality\}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's \{\textbackslash em salience\}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/25CIB7Q9/Li et al. - 2016 - Visualizing and Understanding Neural Models in NLP.pdf;/home/hiaoxui/.local/share/zotero_files/storage/R5AF6X69/1506.html}
}

@inproceedings{li2018LearningIncompleteLabels,
  title = {Learning with Incomplete Labels},
  booktitle = {{{AAAI}}},
  author = {Li, Y. and Xu, Z. and Zhang, Z.},
  date = {2018},
  pages = {3588--3595},
  abstract = {For many real-world tagging problems, training labels are usually obtained through social tagging and are notoriously incomplete. Consequently, handling data with incomplete labels has become a difficult challenge, which usually leads to a degenerated performance on label prediction. To improve the generalization performance, in this paper, we first propose the Improved Cross-View learning (referred as ICVL) model, which considers both global and local patterns of label relationship to enrich the original label set. Further, by extending the ICVL model with an outlier detection mechanism, we introduce the Improved Cross-View learning with Outlier Detection (referred as ICVL-OD) model to remove the abnormal tags resulting from label enrichment. Extensive evaluations on three benchmark datasets demonstrate that ICVL and ICVL-OD outstand with superior performances in comparison with the competing methods.},
  isbn = {978-1-57735-800-8},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GDK3AP5A/Li, Xu, Zhang - 2018 - Learning with incomplete labels(2).pdf}
}

@inproceedings{li2018VisualizingLossLandscape,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  booktitle = {{{NeurIPS}}},
  author = {Li, H. and Xu, Z. and Taylor, G. and Studer, C. and Goldstein, T.},
  date = {2018},
  eprint = {1712.09913},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1712.09913},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comp arisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archiveprefix = {arXiv},
  annotation = {479 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KVE3A82C/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets(2).pdf}
}

@inproceedings{li2019CNMInterpretableComplexvalued,
  title = {{{CNM}}: {{An Interpretable Complex-valued Network}} for {{Matching}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Li, Q. and Wang, B. and Melucci, M.},
  date = {2019},
  eprint = {1904.05298},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.05298},
  abstract = {This paper seeks to model human language by the mathematical framework of quantum physics. With the well-designed mathematical formulations in quantum physics, this framework unifies different linguistic units in a single complex-valued vector space, e.g. words as particles in quantum states and sentences as mixed systems. A complex-valued network is built to implement this framework for semantic matching. With well-constrained complex-valued components, the network admits interpretations to explicit physical meanings. The proposed complex-valued network for matching (CNM) achieves comparable performances to strong CNN and RNN baselines on two benchmarking question answering (QA) datasets.},
  archiveprefix = {arXiv},
  annotation = {17 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/364Y2KZ4/Li, Wang, Melucci - 2019 - CNM An Interpretable Complex-valued Network for Matching(2).pdf}
}

@inproceedings{li2019DependencySpanEndtoEnd,
  title = {Dependency or {{Span}}, {{End-to-End Uniform Semantic Role Labeling}}},
  booktitle = {{{AAAI}}},
  author = {Li, Zuchao and He, Shexia and Zhao, Hai and Zhang, Yiqing and Zhang, Zhuosheng and Zhou, Xi and Zhou, Xiang},
  date = {2019-07-17},
  volume = {33},
  pages = {6730--6737},
  doi = {10.1609/aaai.v33i01.33016730},
  abstract = {Semantic role labeling (SRL) aims to discover the predicateargument structure of a sentence. End-to-end SRL without syntactic input has received great attention. However, most of them focus on either span-based or dependency-based semantic representation form and only show specific model optimization respectively. Meanwhile, handling these two SRL tasks uniformly was less successful. This paper presents an end-to-end model for both dependency and span SRL with a unified argument representation to deal with two different types of argument annotations in a uniform fashion. Furthermore, we jointly predict all predicates and arguments, especially including long-term ignored predicate identification subtask. Our single model achieves new state-of-the-art results on both span (CoNLL 2005, 2012) and dependency (CoNLL 2008, 2009) SRL benchmarks.},
  langid = {english},
  annotation = {49 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KFDXNWTE/Li et al. - 2019 - Dependency or Span, End-to-End Uniform Semantic Ro.pdf}
}

@article{li2019GenerativeModelPunctuation,
  title = {A {{Generative Model}} for {{Punctuation}} in {{Dependency Trees}}},
  author = {Li, X. L. and Wang, D. and Eisner, J. M.},
  date = {2019},
  journaltitle = {TACL},
  volume = {7},
  eprint = {1906.11298v1},
  eprinttype = {arxiv},
  pages = {357--373},
  doi = {10.1162/tacl_a_00273},
  abstract = {Treebanks traditionally treat punctuation marks as ordinary words, but linguists have suggested that a tree’s “true” punctuation marks are not observed (Nunberg, 1990). These latent “underlying” marks serve to delimit or separate constituents in the syntax tree. When the tree’s yield is rendered as a written sentence, a string rewriting mechanism transduces the underlying marks into “surface” marks, which are part of the observed (surface) string but should not be regarded as part of the tree. We formalize this idea in a generative model of punctuation that admits efficient dynamic programming. We train it without observing the underlying marks, by locally maximizing the incomplete data likelihood (similarly to the EM algorithm). When we use the trained model to reconstruct the tree’s underlying punctuation, the results appear plausible across 5 languages, and in particular are consistent with Nunberg’s analysis of English. We show that our generative model can be used to beat baselines on punctuation restoration. Also, our reconstruction of a sentence’s underlying punctuation lets us appropriately render the surface punctuation (via our trained underlying-to-surface mechanism) when we syntactically transform the sentence.},
  archiveprefix = {arXiv},
  annotation = {2 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KWHWFMXF/Li, Wang, Eisner - 2019 - A Generative Model for Punctuation in Dependency Trees(2).pdf}
}

@misc{li2019PosteriorControlBlackbox,
  title = {Posterior {{Control}} of {{Blackbox Generation}}},
  author = {Li, L. X. and Rush, A. M.},
  date = {2019},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HVLXVLFT/Li, Rush - 2019 - Posterior Control of Blackbox Generation(2).pdf}
}

@inproceedings{li2019SpecializingWordEmbeddings,
  title = {Specializing {{Word Embeddings}} ( for {{Parsing}} ) by {{Information Bottleneck}}},
  booktitle = {{{EMNLP}}},
  author = {Li, X. L. and Eisner, J. M.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QUJDBMKM/Li, Eisner - 2019 - Specializing Word Embeddings ( for Parsing ) by Information Bottleneck(2).pdf}
}

@article{li2019TutorialDirichletProcess,
  title = {A Tutorial on {{Dirichlet}} Process Mixture Modeling},
  author = {Li, Y. and Schofield, E. and Gönen, M.},
  date = {2019},
  journaltitle = {Journal of Mathematical Psychology},
  volume = {91},
  pages = {128--144},
  publisher = {{Elsevier Inc.}},
  issn = {10960880},
  doi = {10.1016/j.jmp.2019.04.004},
  abstract = {Bayesian nonparametric (BNP) models are becoming increasingly important in psychology, both as theoretical models of cognition and as analytic tools. However, existing tutorials tend to be at a level of abstraction largely impenetrable by non-technicians. This tutorial aims to help beginners understand key concepts by working through important but often omitted derivations carefully and explicitly, with a focus on linking the mathematics with a practical computation solution for a Dirichlet Process Mixture Model (DPMM)—one of the most widely used BNP methods. Abstract concepts are made explicit and concrete to non-technical readers by working through the theory that gives rise to them. A publicly accessible computer program written in the statistical language R is explained line-by-line to help readers understand the computation algorithm. The algorithm is also linked to a construction method called the Chinese Restaurant Process in an accessible tutorial in this journal (Gershman and Blei, 2012). The overall goals are to help readers understand more fully the theory and application so that they may apply BNP methods in their own work and leverage the technical details in this tutorial to develop novel methods.},
  annotation = {5 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A58ACDX6/Li, Schofield, Gönen - 2019 - A tutorial on Dirichlet process mixture modeling(2).pdf}
}

@inproceedings{li2020DiceLossDataimbalanced,
  title = {Dice {{Loss}} for {{Data-imbalanced NLP Tasks}}},
  booktitle = {{{ACL}}},
  author = {Li, X. and Sun, X. and Meng, Y. and Liang, J. and Wu, F. and Li, J.},
  date = {2020},
  eprint = {1911.02855},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.02855},
  abstract = {Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of background examples (or easy-negative examples) overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sorensen-Dice coefficient or Tversky index, which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples.Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {17 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8YP4V27T/Li et al. - 2020 - Dice Loss for Data-imbalanced NLP Tasks(2).pdf}
}

@inproceedings{li2020EfficientRuemannianOptimization,
  title = {Efficient {{Ruemannian Optimization}} on the {{Stiefel Manifold}} via the {{Cayley Transform}}},
  booktitle = {{{ICLR}}},
  author = {Li, J. and Fuxin, L. and Todorovic, S.},
  date = {2020},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z565T75I/Li, Fuxin, Todorovic - 2020 - Efficient Ruemannian Optimization on the Stiefel Manifold via the Cayley Transform(2).pdf}
}

@misc{li2020EmpiricalAnalysisUnlabeled,
  title = {Empirical {{Analysis}} of {{Unlabeled Entity Problem}} in {{Named Entity Recognition}}},
  author = {Li, Yangming and Liu, Lemao and Shi, Shuming},
  date = {2020-12-13},
  eprint = {2012.05426},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.05426},
  urldate = {2020-12-30},
  abstract = {In many scenarios, named entity recognition (NER) models severely suffer from unlabeled entity problem, where the entities of a sentence may not be fully annotated. Through empirical studies performed on synthetic datasets, we find two causes of the performance degradation. One is the reduction of annotated entities and the other is treating unlabeled entities as negative instances. The first cause has less impact than the second one and can be mitigated by adopting pretraining language models. The second cause seriously misguides a model in training and greatly affects its performances. Based on the above observations, we propose a general approach that is capable of eliminating the misguidance brought by unlabeled entities. The core idea is using negative sampling to keep the probability of training with unlabeled entities at a very low level. Experiments on synthetic datasets and real-world datasets show that our model is robust to unlabeled entity problem and surpasses prior baselines. On well-annotated datasets, our model is competitive with state-of-the-art method.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JBZ7YRPV/Li et al. - 2020 - Empirical Analysis of Unlabeled Entity Problem in .pdf;/home/hiaoxui/.local/share/zotero_files/storage/JCP5J6E8/2012.html}
}

@inproceedings{li2020SentenceEmbeddingsPretrained,
  title = {On the {{Sentence Embeddings}} from {{Pre-trained Language Models}}},
  booktitle = {{{EMNLP}}},
  author = {Li, Bohan and Zhou, Hao and He, Junxian and Wang, Mingxuan and Yang, Yiming and Li, Lei},
  date = {2020-11-02},
  eprint = {2011.05864},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2011.05864},
  urldate = {2022-10-03},
  abstract = {Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W9SYAKTU/Li et al. - 2020 - On the Sentence Embeddings from Pre-trained Langua.pdf;/home/hiaoxui/.local/share/zotero_files/storage/Y9FJ4LN4/2011.html}
}

@inproceedings{li2020UnsupervisedCrosslingualAdaptation,
  title = {Unsupervised {{Cross-lingual Adaptation}} for {{Sequence Tagging}} and {{Beyond}}},
  booktitle = {{{EMNLP}}},
  author = {Li, Xin and Bing, Lidong and Zhang, Wenxuan and Li, Zheng and Lam, Wai},
  date = {2020-10-23},
  eprint = {2010.12405},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.12405},
  urldate = {2020-11-09},
  abstract = {Cross-lingual adaptation with multilingual pre-trained language models (mPTLMs) mainly consists of two lines of works: zero-shot approach and translation-based approach, which have been studied extensively on the sequence-level tasks. We further verify the efficacy of these cross-lingual adaptation approaches by evaluating their performances on more fine-grained sequence tagging tasks. After re-examining their strengths and drawbacks, we propose a novel framework to consolidate the zero-shot approach and the translation-based approach for better adaptation performance. Instead of simply augmenting the source data with the machine-translated data, we tailor-make a warm-up mechanism to quickly update the mPTLMs with the gradients estimated on a few translated data. Then, the adaptation approach is applied to the refined parameters and the cross-lingual transfer is performed in a warm-start way. The experimental results on nine target languages demonstrate that our method is beneficial to the cross-lingual adaptation of various sequence tagging tasks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CU95FDQP/Li et al. - 2020 - Unsupervised Cross-lingual Adaptation for Sequence.pdf;/home/hiaoxui/.local/share/zotero_files/storage/25BRLM2L/2010.html}
}

@misc{li2021DeepLearningSchemabased,
  title = {Deep {{Learning Schema-based Event Extraction}}: {{Literature Review}} and {{Current Trends}}},
  shorttitle = {Deep {{Learning Schema-based Event Extraction}}},
  author = {Li, Qian and Peng, Hao and Li, Jianxin and Hei, Yiming and Sun, Rui and Sheng, Jiawei and Guo, Shu and Wang, Lihong and Yu, Philip S.},
  date = {2021-07-22},
  eprint = {2107.02126},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.02126},
  urldate = {2021-07-23},
  abstract = {Schema-based event extraction is a critical technique to apprehend the essential content of events promptly. With the rapid development of deep learning technology, event extraction technology based on deep learning has become a research hotspot. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches, focusing on deep learning-based models. We summarize the task definition, paradigm, and models of schema-based event extraction and then discuss each of these in detail. We introduce benchmark datasets that support tests of predictions and evaluation metrics. A comprehensive comparison between different techniques is also provided in this survey. Finally, we conclude by summarizing future research directions facing the research area.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IZ8VFLP8/Li et al. - 2021 - Deep Learning Schema-based Event Extraction Liter.pdf;/home/hiaoxui/.local/share/zotero_files/storage/UY39DS46/2107.html}
}

@inproceedings{li2021DocumentLevelEventArgument,
  title = {Document-{{Level Event Argument Extraction}} by {{Conditional Generation}}},
  booktitle = {{{NAACL}}},
  author = {Li, Sha and Ji, Heng and Han, Jiawei},
  date = {2021},
  pages = {894--908},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.69},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.69},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EGADYNDI/Li et al. - 2021 - Document-Level Event Argument Extraction by Condit.pdf}
}

@inproceedings{li2021ImplicitRepresentationsMeaning,
  title = {Implicit {{Representations}} of {{Meaning}} in {{Neural Language Models}}},
  booktitle = {{{ACL}}},
  author = {Li, Belinda Z. and Nye, Maxwell and Andreas, Jacob},
  date = {2021-06-01},
  eprint = {2106.00737},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.00737},
  urldate = {2021-06-04},
  abstract = {Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as models of entities and situations as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity's current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data. Code and data are available at https://github.com/belindal/state-probes .},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-06-03]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4AUVBL95/Li et al. - 2021 - Implicit Representations of Meaning in Neural Lang.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QPGZ8PNY/2106.html}
}

@misc{li2021PrefixTuningOptimizingContinuous,
  title = {Prefix-{{Tuning}}: {{Optimizing Continuous Prompts}} for {{Generation}}},
  shorttitle = {Prefix-{{Tuning}}},
  author = {Li, Xiang Lisa and Liang, Percy},
  date = {2021-01-01},
  eprint = {2101.00190},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.00190},
  urldate = {2021-01-07},
  abstract = {Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\textbackslash\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.},
  archiveprefix = {arXiv},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8SQ2B42Q/Li and Liang - 2021 - Prefix-Tuning Optimizing Continuous Prompts for G.pdf;/home/hiaoxui/.local/share/zotero_files/storage/RSYWW5BH/2101.html}
}

@inproceedings{li2021SpanBasedModelJoint,
  title = {A {{Span-Based Model}} for {{Joint Overlapped}} and {{Discontinuous Named Entity Recognition}}},
  booktitle = {{{ACL}}},
  author = {Li, Fei and Lin, Zhichao and Zhang, Meishan and Ji, Donghong},
  date = {2021-06-27},
  eprint = {2106.14373},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.14373},
  urldate = {2021-06-29},
  abstract = {Research on overlapped and discontinuous named entity recognition (NER) has received increasing attention. The majority of previous work focuses on either overlapped or discontinuous entities. In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly. The model includes two major steps. First, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized. Second, we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession. In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. As a whole, our model can be regarded as a relation extraction paradigm essentially. Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-06-29]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TTXVJJHW/Li et al. - 2021 - A Span-Based Model for Joint Overlapped and Discon.pdf;/home/hiaoxui/.local/share/zotero_files/storage/TUS8FJJT/2106.html}
}

@inproceedings{li2022ApproximationPropertiesRecurrent,
  title = {On the Approximation Properties of Recurrent Encoder-Decoder Architectures},
  booktitle = {{{ICLR}}},
  author = {Li, Zhong and Li, Qianxiao and Jiang, Haotian},
  date = {2022},
  pages = {31},
  abstract = {Encoder-decoder architectures have recently gained popularity in sequence to sequence modelling, featuring in state-of-the-art models such as transformers. However, a mathematical understanding of their working principles still remains limited. In this paper, we study the approximation properties of recurrent encoder-decoder architectures. Prior work established theoretical results for RNNs in the linear setting, where approximation capabilities can be related to smoothness and memory of target temporal relationships. Here, we uncover that the encoder and decoder together form a particular “temporal product structure” which determines the approximation efficiency. Moreover, the encoder-decoder architecture generalises RNNs with the capability to learn time-inhomogeneous relationships. Our results provide the theoretical understanding of approximation properties of the recurrent encoder-decoder architecture, which precisely characterises, in the considered setting, the types of temporal relationships that can be efficiently learned.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DQPSETRF/Li et al. - 2022 - On the approximation properties of recur- rent enc.pdf}
}

@misc{li2022BranchTrainMergeEmbarrassinglyParallel,
  title = {Branch-{{Train-Merge}}: {{Embarrassingly Parallel Training}} of {{Expert Language Models}}},
  shorttitle = {Branch-{{Train-Merge}}},
  author = {Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
  date = {2022-08-05},
  number = {arXiv:2208.03306},
  eprint = {2208.03306},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.03306},
  urldate = {2022-08-12},
  abstract = {We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AP6J4F55/Li et al. - 2022 - Branch-Train-Merge Embarrassingly Parallel Traini.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6RV6FTNN/2208.html}
}

@inproceedings{li2022DQBARTEfficientSequencetoSequence,
  title = {{{DQ-BART}}: {{Efficient Sequence-to-Sequence Model}} via {{Joint Distillation}} and {{Quantization}}},
  shorttitle = {{{DQ-BART}}},
  booktitle = {{{ACL}}},
  author = {Li, Zheng and Wang, Zijian and Tan, Ming and Nallapati, Ramesh and Bhatia, Parminder and Arnold, Andrew and Xiang, Bing and Roth, Dan},
  date = {2022-03-21},
  eprint = {2203.11239},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.11239},
  urldate = {2022-03-23},
  abstract = {Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/966GBNME/Li et al. - 2022 - DQ-BART Efficient Sequence-to-Sequence Model via .pdf;/home/hiaoxui/.local/share/zotero_files/storage/XRINCID6/2203.html}
}

@inproceedings{li2022LearningTransferPrompts,
  title = {Learning to {{Transfer Prompts}} for {{Text Generation}}},
  booktitle = {{{NAACL}}},
  author = {Li, Junyi and Tang, Tianyi and Nie, Jian-Yun and Wen, Ji-Rong and Zhao, Wayne Xin},
  date = {2022-05-15},
  eprint = {2205.01543},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.01543},
  urldate = {2022-07-15},
  abstract = {Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text generation in a transferable setting. First, PTG learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks. To consider both task- and instance-level information, we design an adaptive attention mechanism to derive the target prompts. For each data instance, PTG learns a specific target prompt by attending to highly relevant source prompts. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We release our source prompts as an open resource, where users can add or reuse them to improve new text generation tasks for future research. Code and data can be available at https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5IY2DMXV/Li et al. - 2022 - Learning to Transfer Prompts for Text Generation.pdf;/home/hiaoxui/.local/share/zotero_files/storage/XJE6QDX9/2205.html}
}

@inproceedings{li2022ODETransformerOrdinary,
  title = {{{ODE Transformer}}: {{An Ordinary Differential Equation-Inspired Model}} for {{Sequence Generation}}},
  shorttitle = {{{ODE Transformer}}},
  booktitle = {{{ACL}}},
  author = {Li, Bei and Du, Quan and Zhou, Tao and Jing, Yi and Zhou, Shuhan and Zeng, Xin and Xiao, Tong and Zhu, JingBo and Liu, Xuebo and Zhang, Min},
  date = {2022-03-17},
  eprint = {2203.09176},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.09176},
  urldate = {2022-03-23},
  abstract = {Residual networks are an Euler discretization of solutions to Ordinary Differential Equations (ODE). This paper explores a deeper relationship between Transformer and numerical ODE methods. We first show that a residual block of layers in Transformer can be described as a higher-order solution to ODE. Inspired by this, we design a new architecture, \{\textbackslash it ODE Transformer\}, which is analogous to the Runge-Kutta method that is well motivated in ODE. As a natural extension to Transformer, ODE Transformer is easy to implement and efficient to use. Experimental results on the large-scale machine translation, abstractive summarization, and grammar error correction tasks demonstrate the high genericity of ODE Transformer. It can gain large improvements in model performance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the WMT'14 English-German and English-French benchmarks) at a slight cost in inference efficiency.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5HUTYI38/Li et al. - 2022 - ODE Transformer An Ordinary Differential Equation.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JPCDMG5U/2203.html}
}

@article{li2022UltrafineEntityTyping,
  title = {Ultra-Fine {{Entity Typing}} with {{Indirect Supervision}} from {{Natural Language Inference}}},
  author = {Li, Bangzheng and Yin, Wenpeng and Chen, Muhao},
  date = {2022-02-12},
  journaltitle = {TACL},
  volume = {10},
  eprint = {2202.06167},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {607--622},
  url = {http://arxiv.org/abs/2202.06167},
  urldate = {2022-07-15},
  abstract = {The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large amount of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics since types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability, by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZZEDETWR/Li et al. - 2022 - Ultra-fine Entity Typing with Indirect Supervision.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7R9M64BP/2202.html}
}

@inproceedings{li2022WhatHappensSGD,
  title = {What {{Happens}} after {{SGD Reaches Zero Loss}}? –{{A Mathematical Framework}}},
  booktitle = {{{ICLR}}},
  author = {Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
  date = {2022},
  pages = {56},
  abstract = {Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function L can form a manifold. Intuitively, with a sufficiently small learning rate η, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the gradient noise prevents further convergence. In such a regime, Blanc et al. (2020) proved that SGD with label noise locally decreases a regularizer-like term, the sharpness of loss, tr[∇2L]. The current paper gives a general framework for such analysis by adapting ideas from Katzenberger (1991). It allows in principle a complete characterization for the regularization effect of SGD around such manifold—i.e., the ”implicit bias”—using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance. This yields some new results: (1) a global analysis of the implicit bias valid for η−2 steps, in contrast to the local analysis of Blanc et al. (2020) that is only valid for η−1.6 steps and (2) allowing arbitrary noise covariance. As an application, we show with arbitrary large initialization, label noise SGD can always escape the kernel regime and only requires O(κ ln d) samples for learning an κ-sparse overparametrized linear model in Rd (Woodworth et al., 2020), while GD initialized in the kernel regime requires Ω(d) samples. This upper bound is minimax optimal and improves the previous O(κ2) upper bound (HaoChen et al., 2020).},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B9SZBZ2V/Li et al. - What Happens after SGD Reaches Zero Loss –A Mathe.pdf}
}

@inproceedings{liang2006AlignmentAgreement,
  title = {Alignment by Agreement},
  booktitle = {{{NAACL}}},
  author = {Liang, P. and Taskar, B. and Klein, D.},
  date = {2006},
  pages = {104--111},
  doi = {10.3115/1220835.1220849},
  url = {http://portal.acm.org/citation.cfm?doid=1220835.1220849},
  abstract = {We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32\% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29\% reduction in AER over symmetrized IBM model 4 predictions.},
  issue = {June},
  keywords = {unread},
  annotation = {487 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T5ZPHX8R/Liang, Taskar, Klein - 2006 - Alignment by agreement(2).pdf}
}

@inproceedings{liang2008StructureCompilationTrading,
  title = {Structure {{Compilation}} : {{Trading Structure}} for {{Features}}},
  booktitle = {{{ICML}}},
  author = {Liang, P. and Daumé III, H. and Klein, D.},
  date = {2008},
  pages = {592--599},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9RKVAL7J/Liang, Daumé III, Klein - 2008 - Structure Compilation Trading Structure for Features(2).pdf}
}

@inproceedings{liang2009LearningSemanticCorrespondences,
  title = {Learning {{Semantic Correspondences}} with {{Less Supervision}}},
  booktitle = {{{ACL-IJCNLP}}},
  author = {Liang, P. and Jordan, M. I. and Klein, D.},
  date = {2009},
  pages = {91--99},
  doi = {10.3115/1687878.1687893},
  abstract = {A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.},
  isbn = {978-1-932432-45-9},
  issue = {August},
  annotation = {259 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DBTGSKUH/Liang, Jordan, Klein - 2009 - Learning Semantic Correspondences with Less Supervision(2).pdf}
}

@inproceedings{liang2011LearningDependencyBasedCompositional,
  title = {Learning {{Dependency-Based Compositional Semantics}}},
  booktitle = {{{ACL}}},
  author = {Liang, P. and Jordan, M. I. and Klein, D.},
  date = {2011},
  volume = {39},
  number = {2},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {389--446},
  issn = {0891-2017},
  doi = {10.1162/COLI_a_00127},
  url = {http://www.mitpressjournals.org/doi/10.1162/COLI_a_00127},
  abstract = {Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.},
  archiveprefix = {arXiv},
  isbn = {978-1-932432-87-9},
  annotation = {519 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ASINTK9T/Liang, Jordan, Klein - 2011 - Learning Dependency-Based Compositional Semantics(2).pdf}
}

@misc{liang2013DependencyBasedCompositionalSemantics,
  title = {Dependency-{{Based Compositional Semantics}}},
  author = {Liang, P.},
  date = {2013},
  volume = {39},
  number = {2},
  eprint = {25246403},
  eprinttype = {pmid},
  issn = {0891-2017},
  doi = {10.1162/COLI_a_00127},
  url = {http://www.mitpressjournals.org/doi/10.1162/COLI_a_00127},
  abstract = {Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.},
  archiveprefix = {arXiv},
  isbn = {9781932432879},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BEWMXZXM/Liang - 2013 - Lambda Dependency-Based Compositional Semantics(2).pdf}
}

@misc{liang2013LambdaDependencyBasedCompositional,
  title = {Lambda {{Dependency-Based Compositional Semantics}}},
  author = {Liang, P.},
  date = {2013},
  eprint = {22251136},
  eprinttype = {pmid},
  issn = {04194217},
  doi = {10.1162/COLI},
  url = {http://arxiv.org/abs/1309.4408},
  abstract = {This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.},
  archiveprefix = {arXiv},
  isbn = {9781608459858},
  annotation = {93 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AB934KHV/Liang - 2013 - Lambda Dependency-Based Compositional Semantics(2).pdf}
}

@article{liang2014TalkingComputersNatural,
  title = {Talking to Computers in Natural Language},
  author = {Liang, P.},
  date = {2014},
  journaltitle = {Crossroads},
  volume = {21},
  number = {1},
  pages = {18--21},
  issn = {15284972},
  doi = {10.1145/2659831},
  url = {http://dl.acm.org/citation.cfm?doid=2677339.2659831},
  abstract = {Intended for a wide circle of specialists in automated systems. Above all, however, it is intended for those who work on systems for communicating with machines.},
  entrysubtype = {magazine},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RCJQAHTS/Liang - 2014 - Talking to computers in natural language(2).pdf}
}

@article{liang2015BringingMachineLearning,
  title = {Bringing {{Machine Learning}} and {{Compositional Semantics Together}}},
  author = {Liang, P. and Potts, C.},
  date = {2015},
  journaltitle = {Annual Reviews of Linguistics},
  volume = {1},
  number = {1},
  pages = {355--376},
  issn = {2333-9683},
  doi = {10.1146/annurev-linguist-030514-125312},
  url = {http://www.annualreviews.org/doi/10.1146/annurev-linguist-030514-125312},
  abstract = {Computational semantics has long been considered a field divided between logical and statistical approaches, but this divide is rapidly eroding with the development of statistical models that learn compositional semantic theories from corpora and databases. This review presents a simple discriminative learning framework for defining such models and relating them to logical theories. Within this framework, we discuss the task of learning to map utterances to logical forms (semantic parsing) and the task of learning from denotations with logical forms as latent variables. We also consider models that use distributed (e.g., vector) representations rather than logical ones, showing that these can be considered part of the same overall framework for understanding meaning and structural complexity.},
  isbn = {2333-9683},
  annotation = {62 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZWNN8DAM/Liang, Potts - 2015 - Bringing Machine Learning and Compositional Semantics Together(2).pdf}
}

@article{liang2016LearningExecutableSemantic,
  title = {Learning {{Executable Semantic Parsers}} for {{Natural Language Understanding}}},
  author = {Liang, P.},
  date = {2016},
  journaltitle = {Communications of the ACM},
  eprint = {1603.06677},
  eprinttype = {arxiv},
  issn = {00010782},
  doi = {10.1145/2866568},
  url = {http://arxiv.org/abs/1603.06677},
  abstract = {For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are interested in learning semantic parsers from data, which introduces a new layer of statistical and computational issues. This article lays out the components of a statistical semantic parser, highlighting the key challenges. We will see that semantic parsing is a rich fusion of the logical and the statistical world, and that this fusion will play an integral role in the future of natural language understanding systems.},
  archiveprefix = {arXiv},
  annotation = {87 citations (Semantic Scholar/DOI) [2021-03-26] 87 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YRRT2GP8/Liang - 2016 - Learning Executable Semantic Parsers for Natural Language Understanding(2).pdf}
}

@inproceedings{liang2019LearningKwayDdimensional,
  title = {Learning {{K-way D-dimensional}} Discrete Embedding for Hierarchical Data Visualization and Retrieval},
  booktitle = {{{IJCAI}}},
  author = {Liang, X. and Min, M. R. and Guo, H. and Wang, G.},
  date = {2019},
  pages = {2966--2972},
  doi = {10.24963/ijcai.2019/411},
  abstract = {Traditional embedding approaches associate a real-valued embedding vector with each symbol or data point, which is equivalent to applying a linear transformation to “one-hot” encoding of discrete symbols or data objects. Despite simplicity, these methods generate storage-inefficient representations and fail to effectively encode the internal semantic structure of data, especially when the number of symbols or data points and the dimensionality of the real-valued embedding vectors are large. In this paper, we propose a regularized autoencoder framework to learn compact Hierarchical K-way D-dimensional (HKD) discrete embedding of symbols or data points, aiming at capturing essential semantic structures of data. Experimental results on synthetic and real-world datasets show that our proposed HKD embedding can effectively reveal the semantic structure of data via hierarchical data visualization and greatly reduce the search space of nearest neighbor retrieval while preserving high accuracy.},
  isbn = {978-0-9992411-4-1},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S4FCQGH2/Liang et al. - 2019 - Learning K-way D-dimensional discrete embedding for hierarchical data visualization and retrieval(2).pdf}
}

@misc{liao2020EfficientGraphGeneration,
  title = {Efficient {{Graph Generation}} with {{Graph Recurrent Attention Networks}}},
  author = {Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Nash, Charlie and Hamilton, William L. and Duvenaud, David and Urtasun, Raquel and Zemel, Richard S.},
  date = {2020-07-17},
  eprint = {1910.00760},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.00760},
  urldate = {2020-10-09},
  abstract = {We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. To the best of our knowledge, GRAN is the first deep graph generative model that can scale to this size. Our code is released at: https://github.com/lrjconan/GRAN.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {41 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4FYYYCRA/Liao et al. - 2020 - Efficient Graph Generation with Graph Recurrent At.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6VJJDPMK/1910.html}
}

@inproceedings{likhosherstov2021SubLinearMemoryHow,
  title = {Sub-{{Linear Memory}}: {{How}} to {{Make Performers SLiM}}},
  shorttitle = {Sub-{{Linear Memory}}},
  booktitle = {{{NeurIPS}}},
  author = {Likhosherstov, Valerii and Choromanski, Krzysztof and Davis, Jared and Song, Xingyou and Weller, Adrian},
  date = {2021},
  eprint = {2012.11346},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.11346},
  urldate = {2021-12-07},
  abstract = {The Transformer architecture has revolutionized deep learning on sequential data, becoming ubiquitous in state-of-the-art solutions for a wide variety of applications. Yet vanilla Transformers are notoriously resource-expensive, requiring O(L2) in serial time and memory as functions of input length L. Recent works proposed various linear self-attention mechanisms, scaling only as O(L) for serial computation. We perform a thorough analysis of recent Transformer mechanisms with linear self-attention, Performers, in terms of overall computational complexity. We observe a remarkable computational flexibility: forward and backward propagation can be performed with no approximations using sublinear memory as a function of L (in addition to negligible storage for the input sequence), at a cost of greater time complexity in the parallel setting. In the extreme case, a Performer consumes only O(1) memory during training, and still requires O(L) time. This discovered time-memory tradeoff can be used for training or, due to complete backwardcompatibility, for fine-tuning on a low-memory device, e.g. a smartphone or an earlier-generation GPU, thus contributing towards decentralized and democratized deep learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7QSXAGCH/Likhosherstov et al. - 2020 - Sub-Linear Memory How to Make Performers SLiM.pdf}
}

@article{lim2016NonparametricBayesianTopic,
  title = {Nonparametric {{Bayesian}} Topic Modelling with the Hierarchical {{Pitman}}–{{Yor}} Processes},
  author = {Lim, K. W. and Buntine, W. and Chen, C. and Du, L.},
  date = {2016},
  journaltitle = {International Journal of Approximate Reasoning},
  volume = {78},
  eprint = {1609.06783},
  eprinttype = {arxiv},
  pages = {172--191},
  issn = {0888613X},
  doi = {10.1016/j.ijar.2016.07.007},
  abstract = {The Dirichlet process and its extension, the Pitman–Yor process, are stochastic processes that take probability distributions as a parameter. These processes can be stacked up to form a hierarchical nonparametric Bayesian model. In this article, we present efficient methods for the use of these processes in this hierarchical context, and apply them to latent variable models for text analytics. In particular, we propose a general framework for designing these Bayesian models, which are called topic models in the computer science community. We then propose a specific nonparametric Bayesian topic model for modelling text from social media. We focus on tweets (posts on Twitter) in this article due to their ease of access. We find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications.},
  archiveprefix = {arXiv},
  annotation = {24 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/23U97WAV/Lim et al. - 2016 - Nonparametric Bayesian topic modelling with the hierarchical Pitman–Yor processes(2).pdf}
}

@inproceedings{lin2003AutomaticEvaluationSummaries,
  title = {Automatic Evaluation of Summaries Using {{N-gram}} Co-Occurrence Statistics},
  booktitle = {{{NAACL}}},
  author = {Lin, C. and Hovy, E.},
  date = {2003},
  volume = {2003},
  pages = {71--78},
  doi = {10.3115/1073445.1073465},
  url = {http://portal.acm.org/citation.cfm?doid=1073445.1073465},
  abstract = {Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.},
  issue = {June},
  keywords = {unread},
  annotation = {1518 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M3H2DB2A/Lin, Hovy - 2003 - Automatic evaluation of summaries using N-gram co-occurrence statistics(2).pdf}
}

@inproceedings{lin2004ROUGEPackageAutomatic,
  title = {{{ROUGE}}: {{A Package}} for {{Automatic Evaluation}} of {{Summaries}}},
  booktitle = {Text {{Summarization Branches Out}}},
  author = {Lin, Chin-Yew},
  date = {2004},
  pages = {8},
  abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4FXVWB78/Lin - ROUGE A Package for Automatic Evaluation of Summa.pdf}
}

@inproceedings{lin2015UnsupervisedPOSInduction,
  title = {Unsupervised {{POS Induction}} with {{Word Embeddings}}},
  booktitle = {{{NAACL}}},
  author = {Lin, C. and Ammar, W. and Dyer, C. and Levin, L.},
  date = {2015},
  eprint = {1503.06760},
  eprinttype = {arxiv},
  pages = {1311--1316},
  url = {http://arxiv.org/abs/1503.06760},
  abstract = {Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsupervised problems has been less thoroughly explored. In this paper, we show that embeddings can likewise add value to the problem of unsupervised POS induction. In two representative models of POS induction, we replace multinomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages. We also analyze the effect of various choices while inducing word embeddings on "downstream" POS induction results.},
  archiveprefix = {arXiv},
  isbn = {978-1-941643-49-5},
  keywords = {unread},
  annotation = {59 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XAC8BHFM/Lin et al. - 2015 - Unsupervised POS Induction with Word Embeddings(2).pdf}
}

@inproceedings{lin2017BilinearCNNModels,
  title = {Bilinear {{CNN Models}} for {{Fine-grained Visual Recognition}}},
  booktitle = {{{ICCV}}},
  author = {Lin, T. and RoyChowdhury, A. and Maji, S.},
  date = {2017},
  eprint = {1504.07889v3},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4VJYRM6K/Lin, RoyChowdhury, Maji - 2017 - Bilinear CNN Models for Fine-grained Visual Recognition(2).pdf}
}

@inproceedings{lin2017ListonlyEntityLinking,
  title = {List-Only {{Entity Linking}}},
  booktitle = {{{ACL}}},
  author = {Lin, Y. and Lin, C. and Ji, H.},
  date = {2017},
  pages = {536--541},
  doi = {10.18653/v1/P17-2085},
  url = {http://aclweb.org/anthology/P17-2085},
  abstract = {Traditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the KB may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call it as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which can be used as seed mentions to bridge other mentions and the uninformative entities. In this work, we select most linkable mentions as seed mentions and disambiguate other mentions by comparing them with the seed mentions rather than directly with the entities. Our experiments on linking mentions to seven automatically mined lists show promising results and demonstrate the effectiveness of our approach.},
  annotation = {13 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KYVX3SBM/Lin, Lin, Ji - 2017 - List-only Entity Linking(2).pdf}
}

@inproceedings{lin2017StructuredSelfattentiveSentence,
  title = {A {{Structured Self-attentive Sentence Embedding}}},
  booktitle = {{{ICLR}}},
  author = {Lin, Zhouhan and Feng, Minwei and dos Santos, Cicero Nogueira and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  date = {2017-03-08},
  eprint = {1703.03130},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.03130},
  urldate = {2022-03-30},
  abstract = {This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YXNW7TTR/Lin et al. - 2017 - A Structured Self-attentive Sentence Embedding.pdf;/home/hiaoxui/.local/share/zotero_files/storage/I5MWNJSZ/1703.html}
}

@inproceedings{lin2018NeuralParticleSmoothing,
  title = {Neural {{Particle Smoothing}} for {{Sampling}} from {{Conditional Sequence Models}}},
  booktitle = {{{NAACL}}},
  author = {Lin, C. and Eisner, J. M.},
  date = {2018},
  volume = {21218},
  number = {3},
  eprint = {1804.10747},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.10747},
  abstract = {We introduce neural particle smoothing, a sequential Monte Carlo method for sampling annotations of an input string from a given probability model. In contrast to conventional particle filtering algorithms, we train a proposal distribution that looks ahead to the end of the input string by means of a right-to-left LSTM. We demonstrate that this innovation can improve the quality of the sample. To motivate our formal choices, we explain how our neural model and neural sampler can be viewed as low-dimensional but nonlinear approximations to working with HMMs over very large state spaces.},
  archiveprefix = {arXiv},
  annotation = {6 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UWDCDNNU/Lin, Eisner - 2018 - Neural Particle Smoothing for Sampling from Conditional Sequence Models(2).pdf}
}

@inproceedings{lin2019HierarchicalPointerNet,
  title = {Hierarchical {{Pointer Net Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Lin, X. and Joty, S. and Han, S. and Bing, L.},
  date = {2019},
  eprint = {1908.11571v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y5Z6KQVP/Lin et al. - 2019 - Hierarchical Pointer Net Parsing(2).pdf}
}

@inproceedings{lin2019KagNetKnowledgeAwareGraph,
  title = {{{KagNet}}: {{Knowledge-Aware Graph Networks}} for {{Commonsense Reasoning}}},
  booktitle = {{{EMNLP}}},
  author = {Lin, B. Y. and Chen, X. and Chen, J. and Ren, X.},
  date = {2019},
  eprint = {1909.02151},
  eprinttype = {arxiv},
  pages = {2829--2839},
  doi = {10.18653/v1/d19-1282},
  abstract = {Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {103 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4C2Q6JBD/Lin et al. - 2019 - KagNet Knowledge-Aware Graph Networks for Commonsense Reasoning(2).pdf}
}

@inproceedings{lin2019NeuralFiniteStateTransducers,
  title = {Neural {{Finite-State Transducers}}: {{Beyond Rational Relations}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Lin, C. and Zhu, H. and Gormley, M. R. and Eisner, J. M.},
  date = {2019},
  pages = {272--283},
  doi = {10.18653/v1/n19-1024},
  abstract = {We introduce neural finite state transducers (NFSTs), a family of string transduction models defining joint and conditional probability distributions over pairs of strings. The probability of a string pair is obtained by marginalizing over all its accepting paths in a finite state transducer. In contrast to ordinary weighted FSTs, however, each path is scored using an arbitrary function such as a recurrent neural network, which breaks the usual conditional independence assumption (Markov property). NFSTs are more powerful than previous finite-state models with neural features (Rastogi et al., 2016.) We present training and inference algorithms for locally and globally normalized variants of NFSTs. In experiments on different transduction tasks, they compete favorably against seq2seq models while offering interpretable paths that correspond to hard monotonic alignments.},
  keywords = {unread},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T7F6CXLQ/Lin et al. - 2019 - Neural Finite-State Transducers Beyond Rational Relations(2).pdf}
}

@misc{lin2019SubnormalizedSequenceModels,
  title = {Subnormalized {{Sequence Models}}},
  author = {Lin, C. and Eisner, J. M.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F3IHMCAK/Lin, Eisner - 2019 - Subnormalized Sequence Models(2).pdf}
}

@inproceedings{lin2020JointNeuralModel,
  title = {A {{Joint Neural Model}} for {{Information Extraction}} with {{Global Features}}},
  booktitle = {{{ACL}}},
  author = {Lin, Ying and Ji, Heng and Huang, Fei and Wu, Lingfei},
  date = {2020},
  pages = {7999--8009},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.713},
  url = {https://www.aclweb.org/anthology/2020.acl-main.713},
  urldate = {2021-01-27},
  abstract = {Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a VICTIM of a DIE event is likely to be a VICTIM of an ATTACK event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, ONEIE, that aims to extract the globally optimal IE result as a graph from an input sentence. ONEIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state-of-the-art on all subtasks. As ONEIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner. Our code and models for English, Spanish and Chinese are publicly available for research purpose 1.},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {unread},
  annotation = {19 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HZRA54KR/Lin et al. - 2020 - A Joint Neural Model for Information Extraction wi.pdf}
}

@article{lindsten2014ParticleGibbsAncestor,
  title = {Particle {{Gibbs}} with {{Ancestor Sampling}}},
  author = {Lindsten, F. and Jordan, M. I. and Schön, T. B.},
  date = {2014},
  journaltitle = {JMLR},
  volume = {15},
  eprint = {1401.0604},
  eprinttype = {arxiv},
  pages = {2145--2184},
  issn = {15337928},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9NXQZS7A/Lindsten, Jordan, Schön - 2014 - Particle Gibbs with Ancestor Sampling(2).pdf}
}

@inproceedings{ling2015FindingFunctionForm,
  title = {Finding {{Function}} in {{Form}}: {{Compositional Character Models}} for {{Open Vocabulary Word Representation}}},
  booktitle = {{{EMNLP}}},
  author = {Ling, W. and Luís, T. and Marujo, L. and Astudillo, R. F. and Amir, S. and Dyer, C. and Black, A. W. and Trancoso, I.},
  date = {2015},
  eprint = {1508.02096},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1508.02096},
  abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
  archiveprefix = {arXiv},
  annotation = {521 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F2NF4H42/Ling et al. - 2015 - Finding Function in Form Compositional Character Models for Open Vocabulary Word Representation(2).pdf}
}

@inproceedings{ling2016LatentPredictorNetworks,
  title = {Latent {{Predictor Networks}} for {{Code Generation}}},
  booktitle = {{{ACL}}},
  author = {Ling, W. and Grefenstette, E. and Hermann, K. M. and Kočiský, T. and Senior, A. and Wang, F. and Blunsom, P.},
  date = {2016},
  eprint = {1603.06744},
  eprinttype = {arxiv},
  pages = {599--609},
  url = {http://arxiv.org/abs/1603.06744},
  abstract = {Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-2758-5},
  keywords = {unread},
  annotation = {220 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4ZKDALZR/Ling et al. - 2016 - Latent Predictor Networks for Code Generation(2).pdf}
}

@inproceedings{lipton2018TroublingTrendsMachine,
  title = {Troubling {{Trends}} in {{Machine Learning Scholarship}}},
  booktitle = {{{ICML}}},
  author = {Lipton, Z. C. and Steinhardt, J.},
  date = {2018},
  eprint = {1807.03341},
  eprinttype = {arxiv},
  pages = {1--15},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DKJKQFUR/Lipton, Steinhardt - 2018 - Troubling Trends in Machine Learning Scholarship(2).pdf}
}

@inproceedings{listgarten2005MultipleAlignmentContinuous,
  title = {Multiple {{Alignment}} of {{Continuous Time Series}}},
  booktitle = {{{NeurIPS}}},
  author = {Listgarten, J. and Neal, R. M. and Roweis, S. T. and Emili, A.},
  date = {2005},
  pages = {817--824},
  issn = {10495258},
  abstract = {Page 1. of Continuous Jennifer Listgarten † , Radford M. Neal † , Sam T. Roweis † and Andrew Emili ‡},
  isbn = {0-262-19534-8},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KCR9XBNG/Listgarten et al. - 2005 - Multiple Alignment of Continuous Time Series(2).pdf}
}

@inproceedings{litbank,
  title = {An {{Annotated Dataset}} of {{Coreference}} in {{English Literature}}},
  booktitle = {{{LREC}}},
  author = {Bamman, David and Lewke, Olivia and Mansoor, Anya},
  date = {2020-05-15},
  eprint = {1912.01140},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1912.01140},
  urldate = {2022-04-01},
  abstract = {We present in this work a new dataset of coreference annotations for works of literature in English, covering 29,103 mentions in 210,532 tokens from 100 works of fiction. This dataset differs from previous coreference datasets in containing documents whose average length (2,105.3 words) is four times longer than other benchmark datasets (463.7 for OntoNotes), and contains examples of difficult coreference problems common in literature. This dataset allows for an evaluation of cross-domain performance for the task of coreference resolution, and analysis into the characteristics of long-distance within-document coreference.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4NGFF2XN/Bamman et al. - 2020 - An Annotated Dataset of Coreference in English Lit.pdf;/home/hiaoxui/.local/share/zotero_files/storage/VRGQ59BW/1912.html}
}

@inproceedings{liu2016HowNOTEvaluate,
  title = {How {{NOT To Evaluate Your Dialogue System}}: {{An Empirical Study}} of {{Unsupervised Evaluation Metrics}} for {{Dialogue Response Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Liu, C. and Lowe, R. and Serban, I. V. and Noseworthy, M. and Charlin, L. and Pineau, J.},
  date = {2016},
  eprint = {1603.08023},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1603.08023},
  abstract = {We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.},
  archiveprefix = {arXiv},
  annotation = {710 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FBMY5R9G/Liu et al. - 2016 - How NOT To Evaluate Your Dialogue System An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Respo(2).pdf}
}

@inproceedings{liu2017DeepHypersphericalLearning,
  title = {Deep Hyperspherical Learning},
  booktitle = {{{NeurIPS}}},
  author = {Liu, W. and Zhang, Y. M. and Li, X. and Yu, Z. and Dai, B. and Zhao, T. and Song, L.},
  date = {2017},
  eprint = {1711.03189},
  eprinttype = {arxiv},
  pages = {3951--3961},
  issn = {10495258},
  abstract = {Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7FV5S3EP/Liu et al. - 2017 - Deep hyperspherical learning(2).pdf}
}

@inproceedings{liu2017SoftlabelMethodNoisetolerant,
  title = {A {{Soft-label Method}} for {{Noise-tolerant Distantly Supervised Relation Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Liu, T. and Wang, K. and Chang, B. and Sui, Z.},
  date = {2017},
  pages = {1790--1795},
  doi = {10.18653/v1/D17-1189},
  url = {http://aclweb.org/anthology/D17-1189},
  abstract = {Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels relational facts with knowledge bases. Previous sentence level denoise models don’t achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training. To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During training, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms other state-of-the-art systems. Author\{4\}\{Affiliation\}},
  annotation = {85 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6BWLJSRL/Liu et al. - 2017 - A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction(2).pdf}
}

@inproceedings{liu2018LearningMinimumHyperspherical,
  title = {Learning towards Minimum Hyperspherical Energy},
  booktitle = {{{NeurIPS}}},
  author = {Liu, W. and Lin, R. and Liu, Z. and Liu, L. and Yu, Z. and Dai, B. and Song, L.},
  date = {2018},
  eprint = {1805.09298},
  eprinttype = {arxiv},
  pages = {6222--6233},
  issn = {10495258},
  abstract = {Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics - Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KEG2B85R/Liu et al. - 2018 - Learning towards minimum hyperspherical energy(2).pdf}
}

@inproceedings{liu2019IncorporatingContextualSyntactic,
  title = {Incorporating {{Contextual}} and {{Syntactic Structures Improves Semantic Similarity Modeling}}},
  booktitle = {{{EMNLP}}},
  author = {Liu, L. and Yang, W. and Rao, J. and Tang, R. and Lin, J.},
  date = {2019},
  pages = {1204--1209},
  url = {https://github.com/likicode/spwim},
  abstract = {Semantic similarity modeling is central to many NLP problems such as natural language inference and question answering. Syntactic structures interact closely with semantics in learning compositional representations and alleviating long-range dependency issues. However , such structure priors have not been well exploited in previous work for semantic mod-eling. To examine their effectiveness, we start with the Pairwise Word Interaction Model, one of the best models according to a recent reproducibility study, then introduce components for modeling context and structure using multi-layer BiLSTMs and TreeLSTMs. In addition, we introduce residual connections to the deep convolutional neural network component of the model. Extensive evaluations on eight benchmark datasets show that incorporating structural information contributes to consistent improvements over strong base-lines.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GJKB5GFQ/Liu et al. - 2019 - Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling(2).pdf}
}

@inproceedings{liu2019InoculationFineTuningMethod,
  title = {Inoculation by {{Fine-Tuning}}: {{A Method}} for {{Analyzing Challenge Datasets}}},
  booktitle = {{{NAACL}}},
  author = {Liu, N. F. and Schwartz, R. and Smith, N. A.},
  date = {2019},
  eprint = {1904.02668},
  eprinttype = {arxiv},
  pages = {2171--2179},
  doi = {10.18653/v1/n19-1225},
  abstract = {Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks. While model performance on these challenge datasets is significantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal. For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model's specific training set. We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI "stress tests" (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.},
  archiveprefix = {arXiv},
  annotation = {52 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4UJXYS8Y/Liu, Schwartz, Smith - 2019 - Inoculation by Fine-Tuning A Method for Analyzing Challenge Datasets(2).pdf}
}

@inproceedings{liu2019KnowledgeAugmentedLanguageModel,
  title = {Knowledge-{{Augmented Language Model}} and {{Its Application}} to {{Unsupervised Named-Entity Recognition}}},
  booktitle = {{{NAACL}}},
  author = {Liu, Angli and Du, Jingfei and Stoyanov, Veselin},
  date = {2019},
  pages = {1142--1150},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1117},
  url = {http://aclweb.org/anthology/N19-1117},
  urldate = {2020-10-16},
  abstract = {Traditional language models are unable to efficiently model entity names observed in text. All but the most popular named entities appear infrequently in text providing insufficient context. Recent efforts have recognized that context can be generalized between entity names that share the same type (e.g., person or location) and have equipped language models with access to an external knowledge base (KB). Our Knowledge-Augmented Language Model (KALM) continues this line of work by augmenting a traditional model with a KB. Unlike previous methods, however, we train with an end-to-end predictive objective optimizing the perplexity of text. We do not require any additional information such as named entity tags. In addition to improving language modeling performance, KALM learns to recognize named entities in an entirely unsupervised way by using entity type information latent in the model. On a Named Entity Recognition (NER) task, KALM achieves performance comparable with state-of-the-art supervised models. Our work demonstrates that named entities (and possibly other types of world knowledge) can be modeled successfully using predictive learning and training on large corpora of text without any additional information.},
  eventtitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  langid = {english},
  keywords = {unread},
  annotation = {16 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NDAJI3WX/Liu et al. - 2019 - Knowledge-Augmented Language Model and Its Applica.pdf}
}

@inproceedings{liu2019ReferentialReaderRecurrent,
  title = {The {{Referential Reader}}: {{A Recurrent Entity Network}} for {{Anaphora Resolution}}},
  shorttitle = {The {{Referential Reader}}},
  booktitle = {{{ACL}}},
  author = {Liu, Fei and Zettlemoyer, Luke and Eisenstein, Jacob},
  date = {2019-07-09},
  eprint = {1902.01541},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1902.01541},
  urldate = {2021-03-29},
  abstract = {We present a new architecture for storing and accessing entity mentions during online text processing. While reading the text, entity references are identified, and may be stored by either updating or overwriting a cell in a fixed-length memory. The update operation implies coreference with the other mentions that are stored in the same cell; the overwrite operation causes these mentions to be forgotten. By encoding the memory operations as differentiable gates, it is possible to train the model end-to-end, using both a supervised anaphora resolution objective as well as a supplementary language modeling objective. Evaluation on a dataset of pronoun-name anaphora demonstrates strong performance with purely incremental text processing.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {8 citations (Semantic Scholar/arXiv) [2021-03-29]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SLKQBJZ5/Liu et al. - 2019 - The Referential Reader A Recurrent Entity Network.pdf;/home/hiaoxui/.local/share/zotero_files/storage/EE9BG8JB/1902.html}
}

@misc{liu2020OrthogonalOverParameterizedTraining,
  title = {Orthogonal {{Over-Parameterized Training}}},
  author = {Liu, W. and Lin, R. and Liu, Z. and Rehg, J. M. and Xiong, L. and Weller, A. and Song, L.},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TMLHZWLQ/Anonymous - 2020 - Orthogonal Over-Parameterized Training(6).pdf}
}

@inproceedings{liu2021DynamicSlidingWindow,
  title = {Dynamic {{Sliding Window}} for {{Meeting Summarization}}},
  booktitle = {{{SIGDial}}},
  author = {Liu, Zhengyuan and Chen, Nancy F.},
  date = {2021-08-31},
  eprint = {2108.13629},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.13629},
  urldate = {2021-09-05},
  abstract = {Recently abstractive spoken language summarization raises emerging research interest, and neural sequence-to-sequence approaches have brought significant performance improvement. However, summarizing long meeting transcripts remains challenging. Due to the large length of source contents and targeted summaries, neural models are prone to be distracted on the context, and produce summaries with degraded quality. Moreover, pre-trained language models with input length limitations cannot be readily applied to long sequences. In this work, we first analyze the linguistic characteristics of meeting transcripts on a representative corpus, and find that the sentences comprising the summary correlate with the meeting agenda. Based on this observation, we propose a dynamic sliding window strategy for meeting summarization. Experimental results show that performance benefit from the proposed method, and outputs obtain higher factual consistency than the base model.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-05]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MDRGRRA2/Liu and Chen - 2021 - Dynamic Sliding Window for Meeting Summarization.pdf;/home/hiaoxui/.local/share/zotero_files/storage/A4FE69DR/2108.html}
}

@inproceedings{liu2021FastEffectiveSelfSupervised,
  title = {Fast, {{Effective}}, and {{Self-Supervised}}: {{Transforming Masked Language Models}} into {{Universal Lexical}} and {{Sentence Encoders}}},
  booktitle = {{{EMNLP}}},
  author = {Liu, Fangyu and Vulić, Ivan and Korhonen, Anna and Collier, Nigel},
  date = {2021},
  pages = {18},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YZJXWG3T/Liu et al. - Fast, Effective, and Self-Supervised Transforming.pdf}
}

@misc{liu2021GPTUnderstandsToo,
  title = {{{GPT Understands}}, {{Too}}},
  author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  date = {2021-03-18},
  eprint = {2103.10385},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.10385},
  urldate = {2021-04-09},
  abstract = {While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning -- which employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64\textbackslash\% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we find that P-tuning also improves BERTs' performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, P-tuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-04-09]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BT3XYGMK/Liu et al. - 2021 - GPT Understands, Too.pdf;/home/hiaoxui/.local/share/zotero_files/storage/4TXNRHHE/2103.html}
}

@inproceedings{liu2021SwinTransformerHierarchical,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  booktitle = {{{CVPR}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-03-25},
  eprint = {2103.14030},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.14030},
  urldate = {2021-03-26},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The code and models will be made publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WYXTISB8/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf;/home/hiaoxui/.local/share/zotero_files/storage/W6SAFYU8/2103.html}
}

@inproceedings{liu2021VisuallyGroundedReasoning,
  title = {Visually {{Grounded Reasoning}} across {{Languages}} and {{Cultures}}},
  booktitle = {{{EMNLP}}},
  author = {Liu, Fangyu and Bugliarello, Emanuele and Ponti, Edoardo Maria and Reddy, Siva and Collier, Nigel and Elliott, Desmond},
  date = {2021},
  pages = {19},
  abstract = {The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DX7Z9SGI/Liu et al. - Visually Grounded Reasoning across Languages and C.pdf}
}

@misc{liu2022RetroMAEPretrainingRetrievaloriented,
  title = {{{RetroMAE}}: {{Pre-training Retrieval-oriented Transformers}} via {{Masked Auto-Encoder}}},
  shorttitle = {{{RetroMAE}}},
  author = {Liu, Zheng and Shao, Yingxia},
  date = {2022-05-24},
  number = {arXiv:2205.12035},
  eprint = {2205.12035},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.12035},
  urldate = {2022-10-03},
  abstract = {Pre-trained models have demonstrated superior power on many important tasks. However, it is still an open problem of designing effective pre-training strategies so as to promote the models' usability on dense retrieval. In this paper, we propose a novel pre-training framework for dense retrieval based on the Masked Auto-Encoder, known as RetroMAE. Our proposed framework is highlighted for the following critical designs: 1) a MAE based pre-training workflow, where the input sentence is polluted on both encoder and decoder side with different masks, and original sentence is reconstructed based on both sentence embedding and masked sentence; 2) asymmetric model architectures, with a large-scale expressive transformer for sentence encoding and a extremely simplified transformer for sentence reconstruction; 3) asymmetric masking ratios, with a moderate masking on the encoder side (15\%) and an aggressive masking ratio on the decoder side (50\textasciitilde 90\%). We pre-train a BERT like encoder on English Wikipedia and BookCorpus, where it notably outperforms the existing pre-trained models on a wide range of dense retrieval benchmarks, like MS MARCO, Open-domain Question Answering, and BEIR.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GJ4FED65/Liu and Shao - 2022 - RetroMAE Pre-training Retrieval-oriented Transfor.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6KHIPS4N/2205.html}
}

@inproceedings{locatello2019ChallengingCommonAssumptions,
  title = {Challenging {{Common Assumptions}} in the {{Unsupervised Learning}} of {{Disentangled Representations}}},
  booktitle = {{{ICML}}},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Rätsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier},
  date = {2019-06-18},
  eprint = {1811.12359},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1811.12359},
  urldate = {2020-10-28},
  abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {390 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R4BBWWW4/Locatello et al. - 2019 - Challenging Common Assumptions in the Unsupervised.pdf;/home/hiaoxui/.local/share/zotero_files/storage/CPPSS98E/1811.html}
}

@inproceedings{logan2019BarackWifeHillary,
  title = {Barack’s {{Wife Hillary}}: {{Using Knowledge Graphs}} for {{Fact-Aware Language Modeling}}},
  shorttitle = {Barack’s {{Wife Hillary}}},
  booktitle = {{{ACL}}},
  author = {Logan, Robert and Liu, Nelson F. and Peters, Matthew E. and Gardner, Matt and Singh, Sameer},
  date = {2019},
  pages = {5962--5971},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1598},
  url = {https://www.aclweb.org/anthology/P19-1598},
  urldate = {2020-10-16},
  abstract = {Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText2 dataset,1 a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark (Merity et al., 2017). In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language models’ ability to complete sentences requiring factual knowledge, and show that the KGLM outperforms even very large language models in generating facts.},
  eventtitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {unread},
  annotation = {68 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WFE8KTTX/Logan et al. - 2019 - Barack’s Wife Hillary Using Knowledge Graphs for .pdf}
}

@inproceedings{lou2021MLBiNetCrossSentenceCollective,
  title = {{{MLBiNet}}: {{A Cross-Sentence Collective Event Detection Network}}},
  shorttitle = {{{MLBiNet}}},
  booktitle = {{{ACl}}},
  author = {Lou, Dongfang and Liao, Zhilin and Deng, Shumin and Zhang, Ningyu and Chen, Huajun},
  date = {2021-05-23},
  eprint = {2105.09458},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.09458},
  urldate = {2021-05-27},
  abstract = {We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-05-27]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AUQ4ZWRJ/Lou et al. - 2021 - MLBiNet A Cross-Sentence Collective Event Detecti.pdf;/home/hiaoxui/.local/share/zotero_files/storage/D6ZJEZ52/2105.html}
}

@inproceedings{louis2012CoherenceModelBased,
  title = {A {{Coherence Model Based}} on {{Syntactic Patterns}}},
  booktitle = {{{EMNLP}}},
  author = {Louis, A. and Nenkova, A.},
  date = {2012},
  pages = {1157--1168},
  abstract = {We introduce a model of coherence which captures the intentional discourse structure in text. Our work is based on the hypothesis that syntax provides a proxy for the communica- tive goal of a sentence and therefore the se- quence of sentences in a coherent discourse should exhibit detectable structural patterns. Results show that our method has high dis- criminating power for separating out coherent and incoherent news articles reaching accura- cies of up to 90\%. We also show that our syn- tactic patterns are correlated with manual an- notations of intentional structure for academic conference articles and can successfully pre- dict the coherence of abstract, introduction and related work sections of these articles. 1},
  isbn = {978-1-937284-43-5},
  issue = {July},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A2T4MUKT/Louis, Nenkova - 2012 - A Coherence Model Based on Syntactic Patterns(2).pdf}
}

@inproceedings{louizos2017BayesianCompressionDeep,
  title = {Bayesian {{Compression}} for {{Deep Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Louizos, C. and Ullrich, K. and Welling, M.},
  date = {2017},
  eprint = {1705.08665},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.08665},
  abstract = {Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {286 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VVXAL7N6/Louizos, Ullrich, Welling - 2017 - Bayesian Compression for Deep Learning(2).pdf}
}

@misc{loynd2019WorkingMemoryGraphs,
  title = {Working {{Memory Graphs}}},
  author = {Loynd, R. and Fernandez, R. and Celikyilmaz, A. and Swaminathan, A. and Hausknecht, M.},
  date = {2019},
  eprint = {1911.07141},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.07141},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/58TL8DIH/Loynd et al. - 2019 - Working Memory Graphs(2).pdf}
}

@inproceedings{lu2008GenerativeModelParsing,
  title = {A Generative Model for Parsing Natural Language to Meaning Representations},
  booktitle = {{{EMNLP}}},
  author = {Lu, W. and Ng, H. T. and Lee, W. S. and Zettlemoyer, L. S.},
  date = {2008},
  pages = {783--792},
  doi = {10.3115/1613715.1613815},
  url = {http://dl.acm.org/citation.cfm?id=1613815},
  abstract = {In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. © 2008 Association for Computational Linguistics.},
  annotation = {146 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LR7EHXG3/Lu et al. - 2008 - A generative model for parsing natural language to meaning representations(2).pdf}
}

@inproceedings{lu2011ProbabilisticForesttoStringModel,
  title = {A {{Probabilistic Forest-to-String Model}} for {{Language Generation}} from {{Typed Lambda Calculus Expressions}}},
  booktitle = {{{EMNLP}}},
  author = {Lu, W. and Ng, H. T.},
  date = {2011},
  pages = {1611--1622},
  url = {http://www.aclweb.org/anthology/D11-1149%5Cnhttp://dl.acm.org/citation.cfm?id=2145605},
  abstract = {"This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation."},
  isbn = {1-937284-11-5},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DF7MWRHZ/Lu, Ng - 2011 - A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions(2).pdf}
}

@inproceedings{lu2019LookupAdaptOneshot,
  title = {Look-up and {{Adapt}}: {{A One-shot Semantic Parser}}},
  booktitle = {{{EMNLP}}},
  author = {Lu, Z. and Arabshahi, F. and Labutov, I. and Mitchell, T.},
  date = {2019},
  eprint = {1910.12197},
  eprinttype = {arxiv},
  pages = {1129--1139},
  doi = {10.18653/v1/d19-1104},
  abstract = {Computing devices have recently become capable of interacting with their end users via natural language. However, they can only operate within a limited "supported" domain of discourse and fail drastically when faced with an out-of-domain utterance, mainly due to the limitations of their semantic parser. In this paper, we propose a semantic parser that generalizes to out-of-domain examples by learning a general strategy for parsing an unseen utterance through adapting the logical forms of seen utterances, instead of learning to generate a logical form from scratch. Our parser maintains a memory consisting of a representative subset of the seen utterances paired with their logical forms. Given an unseen utterance, our parser works by looking up a similar utterance from the memory and adapting its logical form until it fits the unseen utterance. Moreover, we present a data generation strategy for constructing utterance-logical form pairs from different domains. Our results show an improvement of up to 68.8\% on one-shot parsing under two different evaluation settings compared to the baselines.},
  archiveprefix = {arXiv},
  annotation = {2 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HIN5PQWX/Lu et al. - 2019 - Look-up and Adapt A One-shot Semantic Parser(2).pdf}
}

@inproceedings{lu2021ConundrumsEventCoreference,
  title = {Conundrums in {{Event Coreference Resolution}}: {{Making Sense}} of the {{State}} of the {{Art}}},
  booktitle = {{{EMNLP}}},
  author = {Lu, Jing and Ng, Vincent},
  date = {2021},
  pages = {13},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2K678RDD/Lu and Ng - Conundrums in Event Coreference Resolution Making.pdf}
}

@inproceedings{lu2021Text2EventControllableSequencetoStructure,
  title = {{{Text2Event}}: {{Controllable Sequence-to-Structure Generation}} for {{End-to-end Event Extraction}}},
  shorttitle = {{{Text2Event}}},
  booktitle = {{{ACL}}},
  author = {Lu, Yaojie and Lin, Hongyu and Xu, Jin and Han, Xianpei and Tang, Jialong and Li, Annan and Sun, Le and Liao, Meng and Chen, Shaoyi},
  date = {2021-06-17},
  eprint = {2106.09232},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.09232},
  urldate = {2021-06-22},
  abstract = {Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-06-22]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R4CBPZSA/Lu et al. - 2021 - Text2Event Controllable Sequence-to-Structure Gen.pdf;/home/hiaoxui/.local/share/zotero_files/storage/K8N8EZ2Q/2106.html}
}

@inproceedings{lu2022NeuroLogicEsqueDecoding,
  title = {{{NeuroLogic A}}*esque {{Decoding}}: {{Constrained Text Generation}} with {{Lookahead Heuristics}}},
  shorttitle = {{{NeuroLogic A}}*esque {{Decoding}}},
  booktitle = {{{NAACL}}},
  author = {Lu, Ximing and Welleck, Sean and West, Peter and Jiang, Liwei and Kasai, Jungo and Khashabi, Daniel and Bras, Ronan Le and Qin, Lianhui and Yu, Youngjae and Zellers, Rowan and Smith, Noah A. and Choi, Yejin},
  date = {2022},
  eprint = {2112.08726},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.08726},
  urldate = {2022-07-14},
  abstract = {The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A* search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop efficient lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MWRU6973/Lu et al. - 2021 - NeuroLogic Aesque Decoding Constrained Text Gene.pdf;/home/hiaoxui/.local/share/zotero_files/storage/NEEIAV66/2112.html}
}

@inproceedings{lu2022UnifiedStructureGeneration,
  title = {Unified {{Structure Generation}} for {{Universal Information Extraction}}},
  booktitle = {{{ACL}}},
  author = {Lu, Yaojie and Liu, Qing and Dai, Dai and Xiao, Xinyan and Lin, Hongyu and Han, Xianpei and Sun, Le and Wu, Hua},
  date = {2022-03-23},
  eprint = {2203.12277},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.12277},
  urldate = {2022-04-08},
  abstract = {Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism - structural schema instructor, and captures the common IE abilities via a large-scale pre-trained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DXNPKH83/Lu et al. - 2022 - Unified Structure Generation for Universal Informa.pdf;/home/hiaoxui/.local/share/zotero_files/storage/BDR3JZSJ/2203.html}
}

@inproceedings{luo2005CoreferenceResolutionPerformance,
  title = {On {{Coreference Resolution Performance Metrics}}},
  booktitle = {{{EMNLP}}},
  author = {Luo, Xiaoqiang},
  date = {2005},
  pages = {8},
  abstract = {The paper proposes a Constrained EntityAlignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widelyknown MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TNIX3ACU/Luo - On Coreference Resolution Performance Metrics.pdf}
}

@inproceedings{luong2015AddressingRareWord,
  title = {Addressing the {{Rare Word Problem}} in {{Neural Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Luong, M. and Sutskever, I. and Le, Q. V. and Vinyals, O. and Zaremba, W.},
  date = {2015},
  eprint = {1410.8206},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1410.8206},
  abstract = {Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {607 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KWJ4WMTL/Luong et al. - 2015 - Addressing the Rare Word Problem in Neural Machine Translation(2).pdf}
}

@inproceedings{luong2015EffectiveApproachesAttentionbased,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Luong, M. and Pham, H. and Manning, C. D.},
  date = {2015},
  eprint = {1508.04025},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1508.04025},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  archiveprefix = {arXiv},
  annotation = {4589 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3JZSKDEJ/Luong, Pham, Manning - 2015 - Effective Approaches to Attention-based Neural Machine Translation(2).pdf}
}

@inproceedings{luu2022TimeWaitsNo,
  title = {Time {{Waits}} for {{No One}}! {{Analysis}} and {{Challenges}} of {{Temporal Misalignment}}},
  booktitle = {{{NAACL}}},
  author = {Luu, Kelvin and Khashabi, Daniel and Gururangan, Suchin and Mandyam, Karishma and Smith, Noah A.},
  date = {2022-07-01},
  eprint = {2111.07408},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.07408},
  urldate = {2022-07-14},
  abstract = {When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain-specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal misalignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously reported. We also find that, while temporal adaptation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YJPFXQQL/Luu et al. - 2022 - Time Waits for No One! Analysis and Challenges of .pdf;/home/hiaoxui/.local/share/zotero_files/storage/HI5KCQVD/2111.html}
}

@inproceedings{ma2016EndtoendSequenceLabeling,
  title = {End-to-End {{Sequence Labeling}} via {{Bi-directional LSTM-CNNs-CRF}}},
  booktitle = {{{ACL}}},
  author = {Ma, X. and Hovy, E.},
  date = {2016},
  eprint = {25246403},
  eprinttype = {pmid},
  issn = {1098-6596},
  doi = {10.18653/v1/P16-1101},
  url = {http://arxiv.org/abs/1603.01354},
  abstract = {State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\textbackslash\% accuracy for POS tagging and 91.21\textbackslash\% F1 for NER.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-2758-5},
  annotation = {1531 citations (Semantic Scholar/DOI) [2021-03-26] 1531 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XN5WMPW6/Ma, Hovy - 2016 - End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF(2).pdf}
}

@inproceedings{ma2018BagofwordsTargetNeural,
  title = {Bag-of-Words as Target for Neural Machine Translation},
  booktitle = {{{ACL}}},
  author = {Ma, S. and Sun, X. and Wang, Y. and Lin, J.},
  date = {2018},
  eprint = {1805.04871},
  eprinttype = {arxiv},
  pages = {332--338},
  doi = {10.18653/v1/p18-2053},
  abstract = {A sentence can be translated into more than one correct sentences. However, most of the existing neural machine translation models only use one of the correct translations as the targets, and the other correct sentences are punished as the incorrect sentences in the training stage. Since most of the correct translations for one sentence share the similar bag-of-words, it is possible to distinguish the correct translations from the incorrect ones by the bag-of-words. In this paper, we propose an approach that uses both the sentences and the bag-of-words as targets in the training stage, in order to encourage the model to generate the potentially correct sentences that are not appeared in the training set. We evaluate our model on a Chinese-English translation dataset, and experiments show our model outperforms the strong baselines by the BLEU score of 4.55.},
  archiveprefix = {arXiv},
  isbn = {978-1-948087-34-6},
  annotation = {50 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F2DSBVXT/Ma et al. - 2018 - Bag-of-words as target for neural machine translation(2).pdf}
}

@inproceedings{ma2019FlowSeqNonAutoregressiveConditional,
  title = {{{FlowSeq}}: {{Non-Autoregressive Conditional Sequence Generation}} with {{Generative Flow}}},
  booktitle = {{{IJCNLP}}},
  author = {Ma, X. and Zhou, C. and Li, X. and Neubig, G. and Hovy, E.},
  date = {2019},
  eprint = {1909.02480},
  eprinttype = {arxiv},
  pages = {4273--4283},
  doi = {10.18653/v1/d19-1437},
  abstract = {Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.},
  archiveprefix = {arXiv},
  annotation = {63 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IID8AA9V/Ma et al. - 2019 - FlowSeq Non-Autoregressive Conditional Sequence Generation with Generative Flow(2).pdf}
}

@inproceedings{ma2021ContributionsTransformerAttention,
  title = {Contributions of {{Transformer Attention Heads}} in {{Multi-}} and {{Cross-lingual Tasks}}},
  booktitle = {{{ACL}}},
  author = {Ma, Weicheng and Zhang, Kai and Lou, Renze and Wang, Lili and Vosoughi, Soroush},
  date = {2021},
  eprint = {2108.08375},
  eprinttype = {arxiv},
  pages = {1956--1966},
  doi = {10.18653/v1/2021.acl-long.152},
  url = {http://arxiv.org/abs/2108.08375},
  urldate = {2021-09-05},
  abstract = {This paper studies the relative importance of attention heads in Transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks. Prior research has found that only a few attention heads are important in each mono-lingual Natural Language Processing (NLP) task and pruning the remaining heads leads to comparable or improved performance of the model. However, the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks. Through extensive experiments, we show that (1) pruning a number of attention heads in a multi-lingual Transformer-based model has, in general, positive effects on its performance in cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments. Our experiments focus on sequence labeling tasks, with potential applicability on other cross-lingual and multi-lingual tasks. For comprehensiveness, we examine two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and XLM-R, on three tasks across 9 languages each. We also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-05] 0 citations (Semantic Scholar/DOI) [2021-09-05]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G2M35HG4/Ma et al. - 2021 - Contributions of Transformer Attention Heads in Mu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QEMVYKUZ/2108.html}
}

@inproceedings{ma2022TemplatefreePromptTuning,
  title = {Template-Free {{Prompt Tuning}} for {{Few-shot NER}}},
  booktitle = {{{NAACL}}},
  author = {Ma, Ruotian and Zhou, Xin and Gui, Tao and Tan, Yiding and Li, Linyang and Zhang, Qi and Huang, Xuanjing},
  date = {2022-04-13},
  eprint = {2109.13532},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.13532},
  urldate = {2022-07-14},
  abstract = {Prompt-based methods have been successfully applied in sentence-level few-shot learning tasks, mostly owing to the sophisticated design of templates and label words. However, when applied to token-level labeling tasks such as NER, it would be time-consuming to enumerate the template queries over all potential entity spans. In this work, we propose a more elegant method to reformulate NER tasks as LM problems without any templates. Specifically, we discard the template construction process while maintaining the word prediction paradigm of pre-training models to predict a class-related pivot word (or label word) at the entity position. Meanwhile, we also explore principled ways to automatically search for appropriate label words that the pre-trained models can easily adapt to. While avoiding complicated template-based process, the proposed LM objective also reduces the gap between different objectives used in pre-training and fine-tuning, thus it can better benefit the few-shot performance. Experimental results demonstrate the effectiveness of the proposed method over bert-tagger and template-based method under few-shot setting. Moreover, the decoding speed of the proposed method is up to 1930.12 times faster than the template-based method.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KRNB9E43/Ma et al. - 2022 - Template-free Prompt Tuning for Few-shot NER.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JCF6UC8M/2109.html}
}

@misc{mackenzie2021WackyWeightsLearned,
  title = {Wacky {{Weights}} in {{Learned Sparse Representations}} and the {{Revenge}} of {{Score-at-a-Time Query Evaluation}}},
  author = {Mackenzie, Joel and Trotman, Andrew and Lin, Jimmy},
  date = {2021-10-27},
  number = {arXiv:2110.11540},
  eprint = {2110.11540},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.11540},
  urldate = {2022-06-03},
  abstract = {Recent advances in retrieval models based on learned sparse representations generated by transformers have led us to, once again, consider score-at-a-time query evaluation techniques for the top-k retrieval problem. Previous studies comparing document-at-a-time and score-at-a-time approaches have consistently found that the former approach yields lower mean query latency, although the latter approach has more predictable query latency. In our experiments with four different retrieval models that exploit representational learning with bags of words, we find that transformers generate "wacky weights" that appear to greatly reduce the opportunities for skipping and early exiting optimizations that lie at the core of standard document-at-a-time techniques. As a result, score-at-a-time approaches appear to be more competitive in terms of query evaluation latency than in previous studies. We find that, if an effectiveness loss of up to three percent can be tolerated, a score-at-a-time approach can yield substantial gains in mean query latency while at the same time dramatically reducing tail latency.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XKDFSKNQ/Mackenzie et al. - 2021 - Wacky Weights in Learned Sparse Representations an.pdf;/home/hiaoxui/.local/share/zotero_files/storage/H4S3TEGK/2110.html}
}

@inproceedings{madaan2021NeuralLanguageModeling,
  title = {Neural {{Language Modeling}} for {{Contextualized Temporal Graph Generation}}},
  booktitle = {{{NAACL}}},
  author = {Madaan, Aman and Yang, Yiming},
  date = {2021},
  pages = {864--881},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.67},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.67},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6TZV993G/Madaan and Yang - 2021 - Neural Language Modeling for Contextualized Tempor.pdf}
}

@inproceedings{maddison2017ConcreteDistributionContinuous,
  title = {The {{Concrete Distribution}}: {{A Continuous Relaxation}} of {{Discrete Random Variables}}},
  booktitle = {{{ICLR}}},
  author = {Maddison, C. J. and Mnih, A. and Teh, Y. W.},
  date = {2017},
  eprint = {1611.00712v3},
  eprinttype = {arxiv},
  pages = {1--20},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XHE9CTFI/Maddison, Mnih, Teh - 2017 - The Concrete Distribution A Continuous Relaxation of Discrete Random Variables(2).pdf}
}

@inproceedings{magar2022DataContaminationMemorization,
  title = {Data {{Contamination}}: {{From Memorization}} to {{Exploitation}}},
  shorttitle = {Data {{Contamination}}},
  booktitle = {{{ACL}}},
  author = {Magar, Inbal and Schwartz, Roy},
  date = {2022-03-15},
  eprint = {2203.08242},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.08242},
  urldate = {2022-03-23},
  abstract = {Pretrained language models are typically trained on massive web-based datasets, which are often "contaminated" with downstream test sets. It is not clear to what extent models exploit the contaminated data for downstream tasks. We present a principled method to study this question. We pretrain BERT models on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune them on the relevant task. Comparing performance between samples seen and unseen during pretraining enables us to define and quantify levels of memorization and exploitation. Experiments with two models and three downstream tasks show that exploitation exists in some cases, but in others the models memorize the contaminated data, but do not exploit it. We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CPZVDLHY/Magar and Schwartz - 2022 - Data Contamination From Memorization to Exploitat.pdf;/home/hiaoxui/.local/share/zotero_files/storage/Z6T6SZPC/2203.html}
}

@misc{majewska2020VerbKnowledgeInjection,
  title = {Verb {{Knowledge Injection}} for {{Multilingual Event Processing}}},
  author = {Majewska, Olga and Vulić, Ivan and Glavaš, Goran and Ponti, Edoardo M. and Korhonen, Anna},
  date = {2020-12-30},
  eprint = {2012.15421},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.15421},
  urldate = {2021-01-20},
  abstract = {In parallel to their overwhelming success across NLP tasks, language ability of deep Transformer networks, pretrained via language modeling (LM) objectives has undergone extensive scrutiny. While probing revealed that these models encode a range of syntactic and semantic properties of a language, they are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks, rather than leverage deeper linguistic knowledge. In this paper, we target one such area of their deficiency, verbal reasoning. We investigate whether injecting explicit information on verbs' semantic-syntactic behaviour improves the performance of LM-pretrained Transformers in event extraction tasks -- downstream tasks for which accurate verb processing is paramount. Concretely, we impart the verb knowledge from curated lexical resources into dedicated adapter modules (dubbed verb adapters), allowing it to complement, in downstream tasks, the language knowledge obtained during LM-pretraining. We first demonstrate that injecting verb knowledge leads to performance gains in English event extraction. We then explore the utility of verb adapters for event extraction in other languages: we investigate (1) zero-shot language transfer with multilingual Transformers as well as (2) transfer via (noisy automatic) translation of English verb-based lexical constraints. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when verb adapters are trained on noisily translated constraints.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JQ7MNQYR/Majewska et al. - 2020 - Verb Knowledge Injection for Multilingual Event Pr.pdf;/home/hiaoxui/.local/share/zotero_files/storage/3F6AJDGL/2012.html}
}

@misc{majumder2021RationaleInspiredNaturalLanguage,
  title = {Rationale-{{Inspired Natural Language Explanations}} with {{Commonsense}}},
  author = {Majumder, Bodhisattwa Prasad and Camburu, Oana-Maria and Lukasiewicz, Thomas and McAuley, Julian},
  date = {2021-06-25},
  eprint = {2106.13876},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.13876},
  urldate = {2021-06-29},
  abstract = {Explainable machine learning models primarily justify predicted labels using either extractive rationales (i.e., subsets of input features) or free-text natural language explanations (NLEs) as abstractive justifications. While NLEs can be more comprehensive than extractive rationales, machine-generated NLEs have been shown to sometimes lack commonsense knowledge. Here, we show that commonsense knowledge can act as a bridge between extractive rationales and NLEs, rendering both types of explanations better. More precisely, we introduce a unified framework, called RExC (Rationale-Inspired Explanations with Commonsense), that (1) extracts rationales as a set of features responsible for machine predictions, (2) expands the extractive rationales using available commonsense resources, and (3) uses the expanded knowledge to generate natural language explanations. Our framework surpasses by a large margin the previous state-of-the-art in generating NLEs across five tasks in both natural language processing and vision-language understanding, with human annotators consistently rating the explanations generated by RExC to be more comprehensive, grounded in commonsense, and overall preferred compared to previous state-of-the-art models. Moreover, our work shows that commonsense-grounded explanations can enhance both task performance and rationales extraction capabilities.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BNYWSGBE/Majumder et al. - 2021 - Rationale-Inspired Natural Language Explanations w.pdf}
}

@inproceedings{malinin2019ReverseKLDivergenceTraining,
  title = {Reverse {{KL-Divergence Training}} of {{Prior Networks}}: {{Improved Uncertainty}} and {{Adversarial Robustness}}},
  booktitle = {{{NeurIPS}}},
  author = {Malinin, A. and Gales, M.},
  date = {2019},
  eprint = {1905.13472},
  eprinttype = {arxiv},
  issn = {1049-5258},
  url = {http://arxiv.org/abs/1905.13472},
  abstract = {Ensemble approaches for uncertainty estimation have recently been applied to the tasks of misclassification detection, out-of-distribution input detection and adversarial attack detection. Prior Networks have been proposed as an approach to efficiently \textbackslash emph\{emulate\} an ensemble of models for classification by parameterising a Dirichlet prior distribution over output distributions. These models have been shown to outperform alternative ensemble approaches, such as Monte-Carlo Dropout, on the task of out-of-distribution input detection. However, scaling Prior Networks to complex datasets with many classes is difficult using the training criteria originally proposed. This paper makes two contributions. First, we show that the appropriate training criterion for Prior Networks is the \textbackslash emph\{reverse\} KL-divergence between Dirichlet distributions. This addresses issues in the nature of the training data target distributions, enabling prior networks to be successfully trained on classification tasks with arbitrarily many classes, as well as improving out-of-distribution detection performance. Second, taking advantage of this new training criterion, this paper investigates using Prior Networks to detect adversarial attacks and proposes a generalized form of adversarial training. It is shown that the construction of successful \textbackslash emph\{adaptive\} whitebox attacks, which affect the prediction and evade detection, against Prior Networks trained on CIFAR-10 and CIFAR-100 using the proposed approach requires a greater amount of computational effort than against networks defended using standard adversarial training or MC-dropout.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {29 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7A4KX2X9/Malinin, Gales - 2019 - Reverse KL-Divergence Training of Prior Networks Improved Uncertainty and Adversarial Robustness(2).pdf}
}

@inproceedings{malinin2020EbsembleDistributionDistillation,
  title = {Ebsemble {{Distribution Distillation}}},
  booktitle = {{{ICLR}}},
  author = {Malinin, A. and Mlodozeniec, B and Gales, M.},
  date = {2020},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9AJLGZPL/Malinin, Mlodozeniec, Gales - 2020 - Ebsemble Distribution Distillation(2).pdf}
}

@misc{malinin2020UncertaintyStructuredPrediction,
  title = {Uncertainty in {{Structured Prediction}}},
  author = {Malinin, A. and Gales, M.},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BJWMTW6C/Malinin, Gales - 2020 - Uncertainty in Structured Prediction(2).pdf;/home/hiaoxui/.local/share/zotero_files/storage/QWDQKDJZ/Anonymous - 2017 - Uncertainty in Structured Prediction(3).pdf}
}

@inproceedings{malisiewicz2011EnsembleExemplarSVMs,
  title = {Ensemble of {{Exemplar SVMs}} for {{Object Detection}} and {{Beyond}}},
  booktitle = {{{ICCV}}},
  author = {Malisiewicz, T. and Gupta, A. and Efros, A. A.},
  date = {2011},
  pages = {89--96},
  abstract = {This paper proposes a conceptually simple but surpris- ingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspon- dence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar- SVMs is thus defined by a single positive instance and mil- lions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generaliza- tion. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computa- tional cost increase. But the central benefit of our approach is that it creates an explicit association between each de- tection and a single training exemplar. Because most de- tections show good alignment to their associated exemplar, it is possible to transfer any available exemplar meta-data (segmentation, geometric structure, 3D model, etc.) directly onto the detections, which can then be used as part of over- all scene understanding.},
  isbn = {978-1-4577-1102-2},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E9UR7ZYM/Malisiewicz, Gupta, Efros - 2011 - Ensemble of Exemplar SVMs for Object Detection and Beyond(2).pdf}
}

@misc{malkin2021BoostingCoherenceLanguage,
  title = {Boosting Coherence of Language Models},
  author = {Malkin, Nikolay and Wang, Zhen and Jojic, Nebojsa},
  date = {2021-10-15},
  eprint = {2110.08294},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.08294},
  urldate = {2021-10-29},
  abstract = {Naturality of long-term information structure -- coherence -- remains a challenge in language generation. Large language models have insufficiently learned such structure, as their long-form generations differ from natural text in measures of coherence. To alleviate this divergence, we propose coherence boosting, an inference procedure that increases the effect of distant context on next-token prediction. We show the benefits of coherence boosting with pretrained models by distributional analyses of generated ordinary text and dialog responses. We also find that coherence boosting with state-of-the-art models for various zero-shot NLP tasks yields performance gains with no additional training.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MUI5FER4/Malkin et al. - 2021 - Boosting coherence of language models.pdf;/home/hiaoxui/.local/share/zotero_files/storage/AKWIYQYG/2110.html}
}

@inproceedings{malkin2022BalancedDataApproach,
  title = {A {{Balanced Data Approach}} for {{Evaluating Cross-Lingual Transfer}}: {{Mapping}} the {{Linguistic Blood Bank}}},
  shorttitle = {A {{Balanced Data Approach}} for {{Evaluating Cross-Lingual Transfer}}},
  booktitle = {{{NAACL}}},
  author = {Malkin, Dan and Limisiewicz, Tomasz and Stanovsky, Gabriel},
  date = {2022-05-09},
  eprint = {2205.04086},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.04086},
  urldate = {2022-07-14},
  abstract = {We show that the choice of pretraining languages affects downstream cross-lingual transfer for BERT-based models. We inspect zero-shot performance in balanced data conditions to mitigate data size confounds, classifying pretraining languages that improve downstream performance as donors, and languages that are improved in zero-shot performance as recipients. We develop a method of quadratic time complexity in the number of languages to estimate these relations, instead of an exponential exhaustive computation of all possible combinations. We find that our method is effective on a diverse set of languages spanning different linguistic features and two downstream tasks. Our findings can inform developers of large-scale multilingual language models in choosing better pretraining configurations.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H587PGNC/Malkin et al. - 2022 - A Balanced Data Approach for Evaluating Cross-Ling.pdf;/home/hiaoxui/.local/share/zotero_files/storage/BCGUJ7TC/2205.html}
}

@inproceedings{manakul2021SparsitySentenceStructure,
  title = {Sparsity and {{Sentence Structure}} in {{Encoder-Decoder Attention}} of {{Summarization Systems}}},
  booktitle = {{{EMNLP}}},
  author = {Manakul, Potsawee and Gales, Mark J. F.},
  date = {2021-09-08},
  eprint = {2109.03888},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.03888},
  urldate = {2021-09-12},
  abstract = {Transformer models have achieved state-of-the-art results in a wide range of NLP tasks including summarization. Training and inference using large transformer models can be computationally expensive. Previous work has focused on one important bottleneck, the quadratic self-attention mechanism in the encoder. Modified encoder architectures such as LED or LoBART use local attention patterns to address this problem for summarization. In contrast, this work focuses on the transformer's encoder-decoder attention mechanism. The cost of this attention becomes more significant in inference or training approaches that require model-generated histories. First, we examine the complexity of the encoder-decoder attention. We demonstrate empirically that there is a sparse sentence structure in document summarization that can be exploited by constraining the attention mechanism to a subset of input sentences, whilst maintaining system performance. Second, we propose a modified architecture that selects the subset of sentences to constrain the encoder-decoder attention. Experiments are carried out on abstractive summarization tasks, including CNN/DailyMail, XSum, Spotify Podcast, and arXiv.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-12]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6QMMK5CN/Manakul and Gales - 2021 - Sparsity and Sentence Structure in Encoder-Decoder.pdf;/home/hiaoxui/.local/share/zotero_files/storage/YL3EH7LE/2109.html}
}

@inproceedings{manhaeve2018DeepProbLogNeuralProbabilistic,
  title = {{{DeepProbLog}} : {{Neural Probabilistic Logic Programming}}},
  booktitle = {{{NeurIPS}}},
  author = {Manhaeve, R. and Kimmig, A. and Dumančić, S. and Demeester, T. and De Raedt, L.},
  date = {2018},
  eprint = {1805.10872v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8BCTSEQU/Manhaeve et al. - 2018 - DeepProbLog Neural Probabilistic Logic Programming(2).pdf}
}

@inproceedings{mani2006MachineLearningTemporal,
  title = {Machine Learning of Temporal Relations},
  booktitle = {{{COLING}}},
  author = {Mani, I. and Verhagen, M. and Wellner, B. and Lee, C. M. and Pustejovsky, J.},
  date = {2006},
  pages = {753--760},
  abstract = {This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93\% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions. © 2006 Association for Computational Linguistics.},
  isbn = {1-932432-65-5},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H4AN9ARC/Mani et al. - 2006 - Machine learning of temporal relations(2).pdf}
}

@misc{manning2000IntroductionFormalComputational,
  title = {An {{Introduction}} to {{Formal Computational Semantics}}},
  author = {Manning, C. D.},
  date = {2000},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IRLZZBUR/Manning - 2000 - An Introduction to Formal Computational Semantics(2).pdf}
}

@inproceedings{manning2013ComputationalLinguisticsDeep,
  title = {Computational {{Linguistics}} and {{Deep Learning}}},
  booktitle = {{{COLING}}},
  author = {Manning, C. D.},
  date = {2013},
  volume = {38},
  eprint = {22251136},
  eprinttype = {pmid},
  pages = {41--51},
  issn = {01272713},
  doi = {10.1162/COLI},
  abstract = {This paper evaluates the investment performance of Malaysian-based international equity funds. The results on the overall fund performance using Jensen's (1968) model indicate that, on average, international funds have significant negative risk-adjusted returns over the study period from 2008-2010. Since the model ignores market timing activity, it implicitly attributes the overall negative return to manager's poor stock selection ability. However, the performance breakdown results on managerial expertise using the models of Treynor and Mazuy (1966) and Henriksson and Merton (1981) show evidence of positive selectivity and negative market timing returns. Taken together, the highly significant negative timing returns suggest that, on average, international fund managers have perverse market timing ability. The paper finds little evidence that Malaysian investors achieve diversification benefits from investing in overseas equity markets.},
  archiveprefix = {arXiv},
  isbn = {978-1-60845-985-8},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2B24YJ48/Manning - 2013 - Computational Linguistics and Deep Learning(2).pdf}
}

@inproceedings{manning2014StanfordCoreNLPNatural,
  title = {The {{Stanford CoreNLP Natural Language Processing Toolkit}}},
  booktitle = {{{ACL}}},
  author = {Manning, C. D. and Surdeanu, M. and Bauer, J. and Finkel, J. and Bethard, S. and McClosky, D.},
  date = {2014},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {55--60},
  issn = {1098-6596},
  doi = {10.3115/v1/P14-5010},
  url = {http://aclweb.org/anthology/P14-5010},
  abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
  archiveprefix = {arXiv},
  isbn = {978-1-941643-00-6},
  annotation = {5124 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9NUII5HC/Manning et al. - 2014 - The Stanford CoreNLP Natural Language Processing Toolkit(2).pdf}
}

@inproceedings{marcheggiani2017EncodingSentencesGraph,
  title = {Encoding {{Sentences}} with {{Graph Convolutional Networks}} for {{Semantic Role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Marcheggiani, Diego and Titov, Ivan},
  date = {2017},
  pages = {1506--1515},
  location = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1159},
  abstract = {Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-theart LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.},
  langid = {english},
  annotation = {413 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GQFQS82H/Marcheggiani and Titov - 2017 - Encoding Sentences with Graph Convolutional Networ.pdf}
}

@misc{marcheggiani2017SimpleAccurateSyntaxAgnostic,
  title = {A {{Simple}} and {{Accurate Syntax-Agnostic Neural Model}} for {{Dependency-based Semantic Role Labeling}}},
  author = {Marcheggiani, D. and Frolov, A. and Titov, I.},
  date = {2017},
  eprint = {1701.02593},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1701.02593},
  abstract = {We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves competitive performance on English, even without any kind of syntactic information and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the English CoNLL-2009 dataset. We also consider Chinese, Czech and Spanish where our approach also achieves competitive results. Syntactic parsers are unreliable on out-of-domain data, so standard (i.e., syntactically-informed) SRL models are hindered when tested in this setting. Our syntax-agnostic model appears more robust, resulting in the best reported results on standard out-of-domain test sets.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {84 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WAXQP8WN/Marcheggiani, Frolov, Titov - 2017 - A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling(2).pdf}
}

@misc{marchisio2020WhenDoesUnsupervised,
  title = {When {{Does Unsupervised Machine Translation Work}}?},
  author = {Marchisio, Kelly and Duh, Kevin and Koehn, Philipp},
  date = {2020-04-14},
  eprint = {2004.05516},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.05516},
  urldate = {2020-10-23},
  abstract = {Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which these methods succeed, and where they fail. We conduct an extensive empirical evaluation of unsupervised MT using dissimilar language pairs, dissimilar domains, diverse datasets, and authentic low-resource languages. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that random word embedding initialization can dramatically affect downstream translation performance. We additionally find that unsupervised MT performance declines when source and target languages use different scripts, and observe very poor performance on authentic low-resource language pairs. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms.},
  archiveprefix = {arXiv},
  annotation = {10 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZZKPHUE7/Marchisio et al. - 2020 - When Does Unsupervised Machine Translation Work.pdf;/home/hiaoxui/.local/share/zotero_files/storage/FTBQTKSC/2004.html}
}

@article{marcus1993BuildingLargeAnnotated,
  title = {Building a {{Large Annotated Corpus}} of {{English}}: {{The Penn Treebank}}},
  author = {Marcus, M. and Santorini, B. and Marcinkiewicz, M. A.},
  date = {1993},
  journaltitle = {Computational Linguistics},
  volume = {19},
  number = {2},
  pages = {313},
  issn = {0891-2017},
  abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/44R8PYCY/Marcus, Santorini, Marcinkiewicz - 1993 - Building a Large Annotated Corpus of English The Penn Treebank(2).pdf}
}

@inproceedings{martins2016SoftmaxSparsemaxSparse,
  title = {From {{Softmax}} to {{Sparsemax}}: {{A Sparse Model}} of {{Attention}} and {{Multi-Label Classification}}},
  booktitle = {{{ICML}}},
  author = {Martins, A. F. T. and Astudillo, R. F.},
  date = {2016},
  volume = {48},
  eprint = {18267787},
  eprinttype = {pmid},
  issn = {19410093},
  doi = {10.1109/72.279181},
  url = {http://arxiv.org/abs/1602.02068},
  abstract = {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-2900-8},
  annotation = {4997 citations (Semantic Scholar/DOI) [2021-03-26] 255 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MMPINAD9/Martins, Astudillo - 2016 - From Softmax to Sparsemax A Sparse Model of Attention and Multi-Label Classification(2).pdf}
}

@misc{martins2021InftyFormerInfinite,
  title = {\$\textbackslash infty\$-Former: {{Infinite Memory Transformer}}},
  shorttitle = {\$\textbackslash infty\$-Former},
  author = {Martins, Pedro Henrique and Marinho, Zita and Martins, André F. T.},
  date = {2021-09-01},
  eprint = {2109.00301},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.00301},
  urldate = {2021-09-05},
  abstract = {Transformers struggle when attending to long contexts, since the amount of computation grows with the context length, and therefore they cannot model long-term memories effectively. Several variations have been proposed to alleviate this problem, but they all have a finite memory capacity, being forced to drop old information. In this paper, we propose the \$\textbackslash infty\$-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \$\textbackslash infty\$-former's attention complexity becomes independent of the context length. Thus, it is able to model arbitrarily long contexts and maintain "sticky memories" while keeping a fixed computation budget. Experiments on a synthetic sorting task demonstrate the ability of the \$\textbackslash infty\$-former to retain information from long sequences. We also perform experiments on language modeling, by training a model from scratch and by fine-tuning a pre-trained language model, which show benefits of unbounded long-term memories.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-05]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7XXDF23J/Martins et al. - 2021 - $infty$-former Infinite Memory Transformer.pdf;/home/hiaoxui/.local/share/zotero_files/storage/88WR3I4D/2109.html}
}

@inproceedings{matuszek2012JointModelLanguage,
  title = {A {{Joint Model}} of {{Language}} and {{Perception}} for {{Grounded Attribute Learning}}},
  booktitle = {{{ICML}}},
  author = {Matuszek, C. and FitzGerald, N. and Zettlemoyer, L. S. and Liefeng, B. and Fox, D.},
  date = {2012},
  eprint = {1206.6423},
  eprinttype = {arxiv},
  pages = {1671--1678},
  url = {http://arxiv.org/abs/1206.6423},
  abstract = {As robots become more ubiquitous and ca- pable, it becomes ever more important for untrained users to easily interact with them. Recently, this has led to study of the lan- guage grounding problem, where the goal is to extract representations of the mean- ings of natural language tied to the physi- cal world. We present an approach for joint learning of language and perception models for grounded attribute induction. The per- ception model includes classifiers for phys- ical characteristics and a language model based on a probabilistic categorial grammar that enables the construction of composi- tional meaning representations. We evaluate on the task of interpreting sentences that de- scribe sets of objects in a physical workspace, and demonstrate accurate task performance and effective latent-variable concept induc- tion in physical grounded scenes. 1.},
  archiveprefix = {arXiv},
  isbn = {978-1-4503-1285-1},
  keywords = {unread},
  annotation = {274 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A7EWQKD2/Matuszek et al. - 2012 - A Joint Model of Language and Perception for Grounded Attribute Learning(2).pdf}
}

@inproceedings{matuszek2013LearningParseNatural,
  title = {Learning to {{Parse Natural Language Commands}} to a {{Robot Control System}}},
  booktitle = {{{ISER}}},
  author = {Matuszek, C. and Herbst, E. and Zettlemoyer, L. S. and Fox, D.},
  date = {2013},
  eprint = {19886812},
  eprinttype = {pmid},
  pages = {403--415},
  issn = {21530858},
  doi = {10.1007/978-3-319-00065-7_28},
  url = {http://link.springer.com/10.1007/978-3-319-00065-7_28},
  abstract = {As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor envi- ronment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our for- mal representation allows a robot to interpret route instructions online while moving through a previously unknown environment.},
  archiveprefix = {arXiv},
  isbn = {978-3-319-00064-0},
  keywords = {unread},
  annotation = {318 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FDTMSBT6/Matuszek et al. - 2013 - Learning to Parse Natural Language Commands to a Robot Control System(2).pdf}
}

@inproceedings{maurya2022MetaXNLGMetaLearning,
  title = {Meta-{{X}}\$\_\{\vphantom\}{{NLG}}\vphantom\{\}\$: {{A Meta-Learning Approach Based}} on {{Language Clustering}} for {{Zero-Shot Cross-Lingual Transfer}} and {{Generation}}},
  shorttitle = {Meta-{{X}}\$\_\{\vphantom\}{{NLG}}\vphantom\{\}\$},
  booktitle = {{{ACL}}},
  author = {Maurya, Kaushal Kumar and Desarkar, Maunendra Sankar},
  date = {2022-03-19},
  eprint = {2203.10250},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.10250},
  urldate = {2022-03-23},
  abstract = {Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-X\$\_\{NLG\}\$) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y2J8WH2Y/Maurya and Desarkar - 2022 - Meta-X$_ NLG $ A Meta-Learning Approach Based on .pdf;/home/hiaoxui/.local/share/zotero_files/storage/TRR4TADL/2203.html}
}

@inproceedings{maveli2022CotrainingUnsupervisedConstituency,
  title = {Co-Training an {{Unsupervised Constituency Parser}} with {{Weak Supervision}}},
  booktitle = {{{ACL}}},
  author = {Maveli, Nickil and Cohen, Shay B.},
  date = {2022-03-18},
  eprint = {2110.02283},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.02283},
  urldate = {2022-03-23},
  abstract = {We introduce a method for unsupervised parsing that relies on bootstrapping classifiers to identify if a node dominates a specific span in a sentence. There are two types of classifiers, an inside classifier that acts on a span, and an outside classifier that acts on everything outside of a given span. Through self-training and co-training with the two classifiers, we show that the interplay between them helps improve the accuracy of both, and as a result, effectively parse. A seed bootstrapping technique prepares the data to train these classifiers. Our analyses further validate that such an approach in conjunction with weak supervision using prior branching knowledge of a known language (left/right-branching) and minimal heuristics injects strong inductive bias into the parser, achieving 63.1 F\$\_1\$ on the English (PTB) test set. In addition, we show the effectiveness of our architecture by evaluating on treebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art results. Our code and pre-trained models are available at https://github.com/Nickil21/weakly-supervised-parsing.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4A2VW7R9/Maveli and Cohen - 2022 - Co-training an Unsupervised Constituency Parser wi.pdf;/home/hiaoxui/.local/share/zotero_files/storage/Z4UTEBQU/2110.html}
}

@inproceedings{may2014ParticleFilterRejuvenation,
  title = {Particle Filter Rejuvenation and Latent {{Dirichlet}} Allocation},
  booktitle = {{{ACL}}},
  author = {May, C. and Clemmer, A. and Van Durme, B.},
  date = {2014},
  pages = {446--451},
  doi = {10.3115/v1/p14-2073},
  abstract = {Previous research has established several methods of online learning for latent Dirichlet allocation (LDA). However, streaming learning for LDA - allowing only one pass over the data and constant storage complexity - is not as well explored. We use reservoir sampling to reduce the storage complexity of a previously-studied online algorithm, namely the particle filter, to constant. We then show that a simpler particle filter implementation performs just as well, and that the quality of the initialization dominates other factors of performance. ?? 2014 Association for Computational Linguistics.},
  isbn = {978-1-937284-73-2},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HJJ6SD3V/May, Clemmer, Van Durme - 2014 - Particle filter rejuvenation and latent Dirichlet allocation(2).pdf}
}

@inproceedings{may2019DownstreamPerformanceCompressed,
  title = {On the {{Downstream Performance}} of {{Compressed Word Embeddings}}},
  booktitle = {{{NeurIPS}}},
  author = {May, A. and Ré, C.},
  date = {2019},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/77YKN4LB/May, Ré - 2019 - On the Downstream Performance of Compressed Word Embeddings(2).pdf}
}

@misc{mccarthy2020ImprovedVariationalNeural,
  title = {Improved {{Variational Neural Machine Translation}} by {{Promoting Mutual Information}}},
  author = {McCarthy, A. D. and Li, X. and Gu, J. and Dong, N.},
  date = {2020},
  eprint = {1909.09237},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.09237},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/82GSR7PK/McCarthy et al. - 2020 - Improved Variational Neural Machine Translation by Promoting Mutual Information(2).pdf}
}

@inproceedings{mcdonald2013UniversalDependencyAnnotation,
  title = {Universal {{Dependency Annotation}} for {{Multilingual Parsing}}},
  booktitle = {{{ACL}}},
  author = {McDonald, R. and Nivre, J. and Quirmbach-Brundage, Y. and Goldberg, Y. and Das, D. and Ganchev, K. and Hall, K. and Petrov, S. and Zhang, H. and Täckström, O. and Bedini, C. and Castelló, N. B. and Lee, J.},
  date = {2013},
  volume = {82},
  number = {4},
  pages = {92--97},
  issn = {00094846},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E7FTY5EM/McDonald et al. - 2013 - Universal Dependency Annotation for Multilingual Parsing(2).pdf}
}

@inproceedings{meek2014LearningGraphicalCausal,
  title = {Toward Learning Graphical and Causal Process Models},
  booktitle = {Uncertainty in {{Artificial Intelligence Workshop}} on {{Causal Inference}}: {{Learning}} and {{Prediction}}},
  author = {Meek, C.},
  date = {2014},
  volume = {1274},
  pages = {43--48},
  issn = {16130073},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I2TASA77/Meek - 2014 - Toward learning graphical and causal process models(2).pdf}
}

@inproceedings{mei2016NeuralHawkesProcess,
  title = {The {{Neural Hawkes Process}}: {{A Neurally Self-Modulating Multivariate Point Process}}},
  booktitle = {{{NeurIPS}}},
  author = {Mei, H. and Eisner, J. M.},
  date = {2016},
  eprint = {1612.09328},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1612.09328},
  abstract = {Many events occur in the world. Some event types are stochastically excited or inhibited---in the sense of having their probabilities elevated or decreased---by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.},
  archiveprefix = {arXiv},
  annotation = {198 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KYAEPZ48/Mei, Eisner - 2016 - The Neural Hawkes Process A Neurally Self-Modulating Multivariate Point Process(2).pdf}
}

@inproceedings{mei2016WhatTalkHow,
  title = {What to Talk about and How? {{Selective Generation}} Using {{LSTMs}} with {{Coarse-to-Fine Alignment}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Mei, H. and Bansal, M. and Walter, M. R.},
  date = {2016},
  eprint = {1509.00838},
  eprinttype = {arxiv},
  pages = {720--730},
  doi = {10.18653/v1/N16-1086},
  url = {http://arxiv.org/abs/1509.00838},
  abstract = {We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59\% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.},
  archiveprefix = {arXiv},
  isbn = {978-1-941643-91-4},
  annotation = {219 citations (Semantic Scholar/DOI) [2021-03-26] 219 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9Y85SCIK/Mei, Bansal, Walter - 2016 - What to talk about and how Selective Generation using LSTMs with Coarse-to-Fine Alignment(2).pdf}
}

@inproceedings{mei2019ImputingMissingEvents,
  title = {Imputing {{Missing Events}} in {{Continuous-Time Event Streams}}},
  booktitle = {{{ICML}}},
  author = {Mei, H. and Qin, G. and Eisner, J. M.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WGSFL4G6/Mei, Qin, Eisner - 2019 - Imputing Missing Events in Continuous-Time Event Streams(2).pdf}
}

@inproceedings{mei2020NeuralDatalogTime,
  title = {Neural {{Datalog Through Time}}: {{Imformed Temporal Modeling}} via {{Logical Specification}}},
  booktitle = {{{ICML}}},
  author = {Mei, H. and Qin, G. and Xu, M. and Eisner, J. M.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5EX76939/Mei et al. - 2020 - Neural Datalog Through Time Imformed Temporal Modeling via Logical Specification(2).pdf}
}

@article{meister2020BestFirstBeamSearch,
  title = {Best-{{First Beam Search}}},
  author = {Meister, C. and Cotterell, R. and Vieira, T.},
  date = {2020},
  journaltitle = {TACL},
  eprint = {2007.03909},
  eprinttype = {arxiv},
  abstract = {Decoding for many NLP tasks requires a heuristic algorithm for approximating exact search since the full search space is often intractable if not simply too large to traverse efficiently. The default algorithm for this job is beam search--a pruned version of breadth-first search--which in practice, returns better results than exact inference due to beneficial search bias. In this work, we show that standard beam search is a computationally inefficient choice for many decoding tasks; specifically, when the scoring function is a monotonic function in sequence length, other search algorithms can be used to reduce the number of calls to the scoring function (e.g., a neural network), which is often the bottleneck computation. We propose best-first beam search, an algorithm that provably returns the same set of results as standard beam search, albeit in the minimum number of scoring function calls to guarantee optimality (modulo beam size). We show that best-first beam search can be used with length normalization and mutual information decoding, among other rescoring functions. Lastly, we propose a memory-reduced variant of best-first beam search, which has a similar search bias in terms of downstream performance, but runs in a fraction of the time.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7X7GWLG6/Meister, Cotterell, Vieira - 2020 - Best-First Beam Search(2).pdf}
}

@misc{meister2020GeneralizedEntropyRegularization,
  title = {Generalized {{Entropy Regularization}} or: {{There}}'s {{Nothing Special}} about {{Label Smoothing}}},
  author = {Meister, C. and Salesky, E. and Cotterell, R.},
  date = {2020},
  eprint = {2005.00820},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.00820},
  abstract = {Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. This class of techniques, of which label smoothing is one, has a connection to entropy regularization. Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored. We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks. We also find that variance in model performance can be explained largely by the resulting entropy of the model. Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place.},
  archiveprefix = {arXiv},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N3BGAZYY/Meister, Salesky, Cotterell - 2020 - Generalized Entropy Regularization or There's Nothing Special about Label Smoothing(2).pdf}
}

@inproceedings{meister2021ConditionalPoissonStochastic,
  title = {Conditional {{Poisson Stochastic Beams}}},
  booktitle = {{{EMNLP}}},
  author = {Meister, Clara and Amini, Afra and Vieira, Tim and Cotterell, Ryan},
  date = {2021},
  pages = {18},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A7UIGYZ4/Meister et al. - Conditional Poisson Stochastic Beams.pdf}
}

@misc{meng2021COCOLMCorrectingContrasting,
  title = {{{COCO-LM}}: {{Correcting}} and {{Contrasting Text Sequences}} for {{Language Model Pretraining}}},
  shorttitle = {{{COCO-LM}}},
  author = {Meng, Yu and Xiong, Chenyan and Bajaj, Payal and Tiwary, Saurabh and Bennett, Paul and Han, Jiawei and Song, Xia},
  date = {2021-02-16},
  eprint = {2102.08473},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.08473},
  urldate = {2021-02-27},
  abstract = {We present COCO-LM, a new self-supervised learning framework that pretrains Language Models by COrrecting challenging errors and COntrasting text sequences. COCO-LM employs an auxiliary language model to mask-and-predict tokens in original text sequences. It creates more challenging pretraining inputs, where noises are sampled based on their likelihood in the auxiliary language model. COCO-LM then pretrains with two tasks: The first task, corrective language modeling, learns to correct the auxiliary model's corruptions by recovering the original tokens. The second task, sequence contrastive learning, ensures that the language model generates sequence representations that are invariant to noises and transformations. In our experiments on the GLUE and SQuAD benchmarks, COCO-LM outperforms recent pretraining approaches in various pretraining settings and few-shot evaluations, with higher pretraining efficiency. Our analyses reveal that COCO-LM's advantages come from its challenging training signals, more contextualized token representations, and regularized sequence representations.},
  archiveprefix = {arXiv},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NKND72WJ/Meng et al. - 2021 - COCO-LM Correcting and Contrasting Text Sequences.pdf;/home/hiaoxui/.local/share/zotero_files/storage/GUBP4ZHI/2102.html}
}

@inproceedings{meng2022GNNLMLanguageModeling,
  title = {{{GNN-LM}}: {{Language Modeling}} Based on {{Global Contexts}} via {{GNN}}},
  booktitle = {{{ICLR}}},
  author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
  date = {2022},
  pages = {13},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5ULWBYBN/Meng et al. - 2022 - GNN-LM LANGUAGE MODELING BASED ON GLOBAL CONTEXTS.pdf}
}

@inproceedings{merity2017PointerSentinelMixture,
  title = {Pointer {{Sentinel Mixture Models}}},
  booktitle = {{{ICLR}}},
  author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  date = {2017},
  eprint = {1609.07843},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1609.07843},
  urldate = {2022-02-09},
  abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R98HLQLX/Merity et al. - 2016 - Pointer Sentinel Mixture Models.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KL9W4BL3/1609.html}
}

@inproceedings{mesquita2019KnowledgeNetBenchmarkDataset,
  title = {{{KnowledgeNet}}: {{A Benchmark Dataset}} for {{Knowledge Base Population}}},
  booktitle = {{{EMNLP}}},
  author = {Mesquita, F. and Cannaviccio, M. and Schmidek, J. and Mirza, P. and Barbosa, D.},
  date = {2019},
  pages = {749--758},
  doi = {10.18653/v1/d19-1069},
  abstract = {KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., entity linking, relation extraction). We discuss five baseline approaches , where the best approach achieves an F1 score of 0.50, significantly outperforming a traditional approach by 79\% (0.28). However, our best baseline is far from reaching human performance (0.82), indicating our dataset is challenging. The KnowledgeNet dataset and baselines are available at https://github. com/diffbot/knowledge-net},
  keywords = {unread},
  annotation = {13 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/57EN3FBT/Mesquita et al. - 2019 - KnowledgeNet A Benchmark Dataset for Knowledge Base Population(2).pdf}
}

@inproceedings{miao2015NeuralVariationalInference,
  title = {Neural {{Variational Inference}} for {{Text Processing}}},
  booktitle = {{{ICML}}},
  author = {Miao, Y. and Yu, L. and Blunsom, P.},
  date = {2015},
  volume = {48},
  eprint = {1511.06038},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.06038},
  abstract = {Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.},
  archiveprefix = {arXiv},
  isbn = {0-89871-600-4},
  issue = {Mcmc},
  keywords = {unread},
  annotation = {362 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4VMRLJJ4/Miao, Blunsom - 2016 - Language as a Latent Variable Discrete Generative Models for Sentence Compression(2).pdf}
}

@inproceedings{miao2016LanguageLatentVariable,
  title = {Language as a {{Latent Variable}}: {{Discrete Generative Models}} for {{Sentence Compression}}},
  booktitle = {{{EMNLP}}},
  author = {Miao, Y. and Blunsom, P.},
  date = {2016},
  eprint = {26353135},
  eprinttype = {pmid},
  pages = {319--328},
  issn = {978-3-319-10589-5},
  doi = {10.1007/978-3-319-10590-1_53},
  url = {http://arxiv.org/abs/1609.07317},
  abstract = {In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.},
  archiveprefix = {arXiv},
  isbn = {978-3-319-10589-5},
  keywords = {unread},
  annotation = {9170 citations (Semantic Scholar/DOI) [2021-03-26] 166 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VSEN8C6Y/Miao, Blunsom - 2016 - Language as a Latent Variable Discrete Generative Models for Sentence Compression(2).pdf}
}

@inproceedings{miao2022LearningPruningFriendlyNetworks,
  title = {Learning {{Pruning-Friendly Networks}} via {{Frank-Wolfe}}: {{One-Shot}}, {{Any-Sparsity}}, {{And No Retraining}}},
  booktitle = {{{ICLR}}},
  author = {Miao, L. and Luo., X. and Chen, T. and Chen, W. and Liu, D. and Wang, Z.},
  date = {2022},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EQ4S37AT/learning_pruning_friendly_netw.pdf}
}

@misc{miceli-barone2021DistributionallyRobustRecurrent,
  title = {Distributionally {{Robust Recurrent Decoders}} with {{Random Network Distillation}}},
  author = {Miceli-Barone, Antonio Valerio and Birch, Alexandra and Sennrich, Rico},
  date = {2021-10-25},
  eprint = {2110.13229},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.13229},
  urldate = {2021-10-29},
  abstract = {Neural machine learning models can successfully model language that is similar to their training distribution, but they are highly susceptible to degradation under distribution shift, which occurs in many practical applications when processing out-of-domain (OOD) text. This has been attributed to "shortcut learning": relying on weak correlations over arbitrary large contexts. We propose a method based on OOD detection with Random Network Distillation to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning towards a less expressive but more robust model as the data becomes more OOD while retaining its full context capability when operating in-distribution. We apply our method to a GRU architecture, demonstrating improvements on multiple language modeling (LM) datasets.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4W8EJHWN/Miceli-Barone et al. - 2021 - Distributionally Robust Recurrent Decoders with Ra.pdf;/home/hiaoxui/.local/share/zotero_files/storage/4WPBC7ZZ/2110.html}
}

@inproceedings{miculicich2018DocumentLevelNeuralMachine,
  title = {Document-{{Level Neural Machine Translation}} with {{Hierarchical Attention Networks}}},
  booktitle = {{{EMNLP}}},
  author = {Miculicich, Lesly and Ram, Dhananjay and Pappas, Nikolaos and Henderson, James},
  date = {2018-10-01},
  eprint = {1809.01576},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1809.01576},
  urldate = {2021-03-08},
  abstract = {Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.},
  archiveprefix = {arXiv},
  annotation = {111 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9KI4EADN/Miculicich et al. - 2018 - Document-Level Neural Machine Translation with Hie.pdf;/home/hiaoxui/.local/share/zotero_files/storage/UJPFEJWN/1809.html}
}

@inproceedings{miculicich2019PartiallysupervisedMentionDetection,
  title = {Partially-Supervised {{Mention Detection}}},
  booktitle = {Workshop on {{Computational Models}} of {{Reference}}, {{Anaphora}} and {{Coreference}}},
  author = {Miculicich, Lesly and Henderson, James},
  date = {2019-08-26},
  eprint = {1908.09507},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1908.09507},
  urldate = {2021-03-08},
  abstract = {Learning to detect entity mentions without using syntactic information can be useful for integration and joint optimization with other tasks. However, it is common to have partially annotated data for this problem. Here, we investigate two approaches to deal with partial annotation of mentions: weighted loss and soft-target classification. We also propose two neural mention detection approaches: a sequence tagging, and an exhaustive search. We evaluate our methods with coreference resolution as a downstream task, using multitask learning. The results show that the recall and F1 score improve for all methods.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EDENMVZ5/Miculicich and Henderson - 2019 - Partially-supervised Mention Detection.pdf;/home/hiaoxui/.local/share/zotero_files/storage/CCJR465W/1908.html}
}

@inproceedings{mielke2018SpellOnceSummon,
  title = {Spell {{Once}}, {{Summon Anywhere}}: {{A Two-Level Open-Vocabulary Language Model}}},
  booktitle = {{{AAAI}}},
  author = {Mielke, S. J. and Eisner, J. M.},
  date = {2018},
  number = {3},
  eprint = {1804.08205},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.08205},
  abstract = {We show how the spellings of known words can help us deal with unknown words in open-vocabulary NLP tasks. The method we propose can be used to extend any closed-vocabulary generative model, but in this paper we specifically consider the case of neural language modeling. Our Bayesian generative story combines a standard RNN language model (generating the word tokens in each sentence) with an RNN-based spelling model (generating the letters in each word type). These two RNNs respectively capture sentence structure and word structure, and are kept separate as in linguistics. By invoking the second RNN to generate spellings for novel words in context, we obtain an open-vocabulary language model. For known words, embeddings are naturally inferred by combining evidence from type spelling and token context. Comparing to baselines (including a novel strong baseline), we beat previous work and establish state-of-the-art results on multiple datasets.},
  archiveprefix = {arXiv},
  annotation = {20 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UZV7ZV6P/Mielke, Eisner - 2018 - Spell Once, Summon Anywhere A Two-Level Open-Vocabulary Language Model(2).pdf}
}

@inproceedings{mielke2019WhatKindLanguage,
  title = {What {{Kind}} of {{Language Is Hard}} to {{Language-Model}}?},
  booktitle = {{{ACL}}},
  author = {Mielke, S. J. and Cotterell, R. and Gorman, K. and Roark, B. and Eisner, J. M.},
  date = {2019},
  eprint = {1906.04726},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1906.04726},
  abstract = {How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that "translationese" is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {20 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YXWQ8DGD/Mielke et al. - 2019 - What Kind of Language Is Hard to Language-Model(2).pdf}
}

@inproceedings{mihalcea2007CharacterizingHumourExploration,
  title = {Characterizing {{Humour}} : {{An Exploration}} of {{Features}} in {{Humorous Texts}}},
  booktitle = {{{CICLing}}},
  author = {Mihalcea, R. and Pulman, S.},
  date = {2007},
  pages = {337--347},
  issn = {03029743},
  abstract = {This paper investigates the problem of automatic humour recognition, and provides and in-depth analysis of two of the most frequently observed features of humorous text: human-centeredness and negative polarity. Through experiments performed on two collections of humorous texts, we show that these properties of verbal humour are consistent across different data sets. Springet-Verlag Berlin Heidelberg 2001.},
  isbn = {3-540-70938-X},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CXRRIPNZ/Mihalcea, Pulman - 2007 - Characterizing Humour An Exploration of Features in Humorous Texts(2).pdf}
}

@inproceedings{mikolov2013DistributedRepresentationsWords,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and {{Their Compositionality}}},
  booktitle = {{{NeurIPS}}},
  author = {Mikolov, T. and Chen, K. and Corrado, G. and Dean, J.},
  date = {2013},
  eprint = {903},
  eprinttype = {pmid},
  pages = {1--9},
  issn = {10495258},
  doi = {10.1162/jmlr.2003.3.4-5.951},
  archiveprefix = {arXiv},
  isbn = {2150-8097},
  annotation = {405 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LBWF9CFX/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and Their Compositionality(2).pdf}
}

@misc{mikolov2013EfficientEstimationWord,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, T. and Corrado, G. and Chen, K. and Dean, J.},
  date = {2013},
  eprint = {1301.3781v3},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JV2V3RDJ/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space(2).pdf}
}

@article{miller1995WordNetLexicalDatabase,
  title = {{{WordNet}}: {{A Lexical Database}} for {{English}}},
  author = {Miller, G. A.},
  date = {1995},
  journaltitle = {Communications of the ACM},
  volume = {38},
  number = {11},
  pages = {39--41},
  issn = {15577317},
  doi = {10.1145/219717.219748},
  abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonims that are in turn link through semantic relations that determine word definitions.},
  annotation = {9994 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KQYXPW7V/Miller - 1995 - WordNet A Lexical Database for English(2).pdf}
}

@inproceedings{miller1996FullyStatisticalApproach,
  title = {A {{Fully Statistical Approach}} to {{Natural Language Interfaces}}},
  booktitle = {{{ACL}}},
  author = {Miller, S. and Stallard, D. and Bobrow, R. and Schwartz, R.},
  date = {1996},
  pages = {55--61},
  doi = {10.3115/981863.981871},
  url = {http://www.aclweb.org/anthology/P96-1008},
  keywords = {unread},
  annotation = {152 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G7M6JGF6/Miller et al. - 1996 - A Fully Statistical Approach to Natural Language Interfaces(2).pdf}
}

@inproceedings{miltenburg2021PreregisteringNLPResearch,
  title = {Preregistering {{NLP Research}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Miltenburg, van Emiel and Lee, van der Chris and Krahmer, Emiel},
  date = {2021-03-23},
  eprint = {2103.06944},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.06944},
  urldate = {2021-06-03},
  abstract = {Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study. This practice is increasingly common in medicine and psychology, but is rarely discussed in NLP. This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which could provide firmer grounds for slow science in NLP research. The goal of this paper is to elicit a discussion in the NLP community, which we hope to synthesise into a general NLP preregistration form in future research.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-06-03]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9HJFFTF5/van Miltenburg et al. - 2021 - Preregistering NLP Research.pdf;/home/hiaoxui/.local/share/zotero_files/storage/CSB9PY4R/2103.html}
}

@misc{min2020KnowledgeGuidedText,
  title = {Knowledge {{Guided Text Retrieval}} and {{Reading}} for {{Open Domain Question Answering}}},
  author = {Min, Sewon and Chen, Danqi and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  date = {2020-04-13},
  eprint = {1911.03868},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.03868},
  urldate = {2021-06-08},
  abstract = {We introduce an approach for open-domain question answering (QA) that retrieves and reads a passage graph, where vertices are passages of text and edges represent relationships that are derived from an external knowledge base or co-occurrence in the same article. Our goals are to boost coverage by using knowledge-guided retrieval to find more relevant passages than text-matching methods, and to improve accuracy by allowing for better knowledge-guided fusion of information across related passages. Our graph retrieval method expands a set of seed keyword-retrieved passages by traversing the graph structure of the knowledge base. Our reader extends a BERT-based architecture and updates passage representations by propagating information from related passages and their relations, instead of reading each passage in isolation. Experiments on three open-domain QA datasets, WebQuestions, Natural Questions and TriviaQA, show improved performance over non-graph baselines by 2-11\% absolute. Our approach also matches or exceeds the state-of-the-art in every case, without using an expensive end-to-end training regime.},
  archiveprefix = {arXiv},
  annotation = {33 citations (Semantic Scholar/arXiv) [2021-06-08]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XTCJ8AIQ/Min et al. - 2020 - Knowledge Guided Text Retrieval and Reading for Op.pdf;/home/hiaoxui/.local/share/zotero_files/storage/YGDVGMWR/1911.html}
}

@inproceedings{minard2016MEANTIMENewsReaderMultilingual,
  title = {{{MEANTIME}}, the {{NewsReader}} Multilingual Event and {{TIME}} Corpus},
  booktitle = {{{LREC}}},
  author = {Minard, A. L. and Speranza, M. and Urizar, R. and Altuna, B. and Van Erp, M. and Schoen, A. and Van Son, C.},
  date = {2016},
  pages = {4417--4422},
  abstract = {In this paper, we present the NewsReaderMEANTIMEcorpus, a semantically annotated corpus ofWikinews articles. The corpus consists of 480 news articles, i.e. 120 English news articles and their translations in Spanish, Italian, and Dutch. MEANTIME contains anno- tations at different levels. The document-level annotation includes markables (e.g. entity mentions, event mentions, time expressions, and numerical expressions), relations between markables (modeling, for example, temporal information and semantic role labeling), and entity and event intra-document coreference. The corpus-level annotation includes entity and event cross-document coreference. Semantic annotation on the English section was performed manually; for the annotation in Italian, Spanish, and (partially) Dutch, a pro- cedure was devised to automatically project the annotations on the English texts onto the translated texts, based on the manual alignment of the annotated elements; this enabled us not only to speed up the annotation process but also provided cross-lingual coreference. The English section of the corpus was extended with timeline annotations for the SemEval 2015 TimeLine shared task. The First CLIN Dutch Shared Task at CLIN26 was based on the Dutch section, while the EVALITA 2016 FactA (Event Factuality Annotation) shared task, based on the Italian section, is currently being organized},
  isbn = {978-2-9517408-9-1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TA4FY3G2/Minard et al. - 2016 - MEANTIME, the NewsReader multilingual event and TIME corpus(2).pdf}
}

@inproceedings{mirza2016CATENACAusalTemporal,
  title = {{{CATENA}}: {{CAusal}} and Temporal Relation Extraction from Natural Language Texts},
  booktitle = {{{COLING}}},
  author = {Mirza, P. and Tonelli, S.},
  date = {2016},
  pages = {64--75},
  abstract = {We present CATENA, a sieve-based system to perform temporal and causal relation extraction and classification from English texts, exploiting the interaction between the temporal and the causal model. We evaluate the performance of each sieve, showing that the rule-based, the machine-learned and the reasoning components all contribute to achieving state-of-the-art performance on TempEval-3 and TimeBank-Dense data. Although causal relations are much sparser than temporal ones, the architecture and the selected features are mostly suitable to serve both tasks. The effects of the interaction between the temporal and the causal components, although limited, yield promising results and confirm the tight connection between the temporal and the causal dimension of texts.},
  isbn = {978-4-87974-702-0},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XJZEPZZI/Mirza, Tonelli - 2016 - CATENA CAusal and temporal relation extraction from natural language texts(2).pdf}
}

@inproceedings{misra2017MappingInstructionsVisual,
  title = {Mapping {{Instructions}} and {{Visual Observations}} to {{Actions}} with {{Reinforcement Learning}}},
  booktitle = {{{EMNLP}}},
  author = {Misra, D. and Langford, J. and Artzi, Y.},
  date = {2017},
  eprint = {1704.08795},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1704.08795},
  abstract = {We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent's exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {126 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ANQ8ECIG/Misra, Langford, Artzi - 2017 - Mapping Instructions and Visual Observations to Actions with Reinforcement Learning(2).pdf}
}

@article{miyahara2020QuantumExpectationmaximizationAlgorithm,
  title = {Quantum Expectation-Maximization Algorithm},
  author = {Miyahara, H. and Aihara, K. and Lechner, W.},
  date = {2020},
  journaltitle = {Physical Review A},
  volume = {101},
  number = {1},
  eprint = {1908.06655},
  eprinttype = {arxiv},
  issn = {24699934},
  doi = {10.1103/PhysRevA.101.012326},
  abstract = {Clustering algorithms are a cornerstone of machine learning applications. Recently, a quantum algorithm for clustering based on the k-means algorithm has been proposed by Kerenidis, Landman, Luongo, and Prakash. Based on their work, we propose a quantum expectation-maximization algorithm for Gaussian mixture models (GMMs). The robustness and quantum speedup of the algorithm are shown. We also show numerically the advantage of GMM over k-means algorithm for nontrivial cluster data.},
  archiveprefix = {arXiv},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PGU9S4FM/Miyahara, Aihara, Lechner - 2020 - Quantum expectation-maximization algorithm(2).pdf}
}

@misc{modi2017InScriptNarrativeTexts,
  title = {{{InScript}}: {{Narrative}} Texts Annotated with Script Information},
  shorttitle = {{{InScript}}},
  author = {Modi, Ashutosh and Anikina, Tatjana and Ostermann, Simon and Pinkal, Manfred},
  date = {2017-03-15},
  eprint = {1703.05260},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.05260},
  urldate = {2020-10-21},
  abstract = {This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  annotation = {28 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PS9HHBN2/Modi et al. - 2017 - InScript Narrative texts annotated with script in.pdf}
}

@inproceedings{mohebbi2021ExploringRoleBERT,
  title = {Exploring the {{Role}} of {{BERT Token Representations}} to {{Explain Sentence Probing Results}}},
  booktitle = {{{EMNLP}}},
  author = {Mohebbi, Hosein and Modarressi, Ali and Pilehvar, Mohammad Taher},
  date = {2021},
  pages = {15},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/INXJ7ZCL/Mohebbi et al. - Exploring the Role of BERT Token Representations t.pdf}
}

@article{mohri1997FiniteStateTransducersLanguage,
  title = {Finite-{{State Transducers}} in {{Language}} and {{Speech Processing}}},
  author = {Mohri, M.},
  date = {1997},
  journaltitle = {Computational Linguistics},
  volume = {23},
  number = {2},
  pages = {268--311},
  issn = {08912017},
  abstract = {Finite-state machines have been used in various domains of natural language processing. We consider here the use of a type of transducer that supports very efficient programs: sequential transducers. We recall classical theorems and give new ones characterizing sequential string-to-string transducers. Transducers that output weights also play an important role in language and speech processing. We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms. Some applications of these algorithms in speech recognition are described and illustrated.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/88BD8CPT/Mohri - 1997 - Finite-State Transducers in Language and Speech Processing(2).pdf}
}

@inproceedings{montero2021SentenceBottleneckAutoencoders,
  title = {Sentence {{Bottleneck Autoencoders}} from {{Transformer Language Models}}},
  booktitle = {{{EMNLP}}},
  author = {Montero, Ivan and Pappas, Nikolaos and Smith, Noah A},
  date = {2021},
  pages = {10},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MDD25JVU/Montero et al. - Sentence Bottleneck Autoencoders from Transformer .pdf}
}

@inproceedings{moore2004ImprovingIBMWordalignment,
  title = {Improving {{IBM}} Word-Alignment Model 1},
  booktitle = {{{ACL}}},
  author = {Moore, R. C.},
  date = {2004},
  pages = {518-es},
  doi = {10.3115/1218955.1219021},
  url = {http://portal.acm.org/citation.cfm?doid=1218955.1219021},
  abstract = {We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1. We demonstrate reduction in alignment error rate of approximately 30\% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.},
  keywords = {unread},
  annotation = {115 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QCPA3ZMM/Moore - 2004 - Improving IBM word-alignment model 1(2).pdf}
}

@misc{moradshahi2020LocalizingOpenOntologyQA,
  title = {Localizing {{Open-Ontology QA Semantic Parsers}} in a {{Day Using Machine Translation}}},
  author = {Moradshahi, Mehrad and Campagna, Giovanni and Semnani, Sina J. and Xu, Silei and Lam, Monica S.},
  date = {2020-10-10},
  eprint = {2010.05106},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.05106},
  urldate = {2020-10-28},
  abstract = {We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators. We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains. Our models achieve an overall test accuracy ranging between 61\% and 69\% for the hotels domain and between 64\% and 78\% for restaurants domain, which compares favorably to 69\% and 80\% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-the-art methodology by more than 30\% for hotels and 40\% for restaurants with localized ontologies for the subset of languages tested. Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JTE238V8/Moradshahi et al. - 2020 - Localizing Open-Ontology QA Semantic Parsers in a .pdf;/home/hiaoxui/.local/share/zotero_files/storage/3SF48TQL/2010.html}
}

@article{moro2014EntityLinkingMeets,
  title = {Entity {{Linking}} Meets {{Word Sense Disambiguation}}: A {{Unified Approach}}},
  author = {Moro, A. and Raganato, A. and Navigli, R.},
  date = {2014},
  journaltitle = {TACL},
  volume = {2},
  number = {0},
  pages = {231--244},
  issn = {2307-387X},
  abstract = {Entity Linking (EL) and Word Sense Disambiguation (WSD) both address the lexical ambiguity of language. But while the two tasks are pretty similar, they differ in a fundamental respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while in WSD there is a perfect match between the word form (better, its lemma) and a suitable word sense. In this paper we present Babelfy, a unified graph-based approach to EL and WSD based on a loose identification of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence semantic interpretations. Our experiments show state-of- the-art performances on both tasks on 6 different datasets, including a multilingual setting. Babelfy is online at http://babelfy.org},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/47P7V447/Moro, Raganato, Navigli - 2014 - Entity Linking meets Word Sense Disambiguation a Unified Approach(2).pdf}
}

@misc{muennighoff2022SGPTGPTSentence,
  title = {{{SGPT}}: {{GPT Sentence Embeddings}} for {{Semantic Search}}},
  shorttitle = {{{SGPT}}},
  author = {Muennighoff, Niklas},
  date = {2022-08-05},
  number = {arXiv:2202.08904},
  eprint = {2202.08904},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.08904},
  urldate = {2022-10-03},
  abstract = {Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7\% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ANI2A6PW/Muennighoff - 2022 - SGPT GPT Sentence Embeddings for Semantic Search.pdf;/home/hiaoxui/.local/share/zotero_files/storage/VV7WUM5V/2202.html}
}

@inproceedings{mukherjee2018ARMDNAssociativeRecurrent,
  title = {{{AR-MDN}}: {{Associative}} and {{Recurrent Mixture Density Networks}} for {{eRetail Demand Forecasting}}},
  booktitle = {Very {{Large Data Bases}}},
  author = {Mukherjee, S. and Shankar, D. and Ghosh, A. and Tathawadekar, N. and Kompalli, P. and Sarawagi, S. and Chaudhury, K.},
  date = {2018},
  eprint = {1803.03800},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.03800},
  abstract = {Accurate demand forecasts can help on-line retail organizations better plan their supply-chain processes. The challenge, however, is the large number of associative factors that result in large, non-stationary shifts in demand, which traditional time series and regression approaches fail to model. In this paper, we propose a Neural Network architecture called AR-MDN, that simultaneously models associative factors, time-series trends and the variance in the demand. We first identify several causal features and use a combination of feature embeddings, MLP and LSTM to represent them. We then model the output density as a learned mixture of Gaussian distributions. The AR-MDN can be trained end-to-end without the need for additional supervision. We experiment on a dataset of an year's worth of data over tens-of-thousands of products from Flipkart. The proposed architecture yields a significant improvement in forecasting accuracy when compared with existing alternatives.},
  archiveprefix = {arXiv},
  annotation = {13 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RN59VVI8/Mukherjee et al. - 2018 - AR-MDN Associative and Recurrent Mixture Density Networks for eRetail Demand Forecasting(2).pdf}
}

@inproceedings{mukherjee2020UncertaintyawareSelftrainingFewshot,
  title = {Uncertainty-Aware {{Self-training}} for {{Few-shot Text Classiﬁcation}}},
  booktitle = {{{NeurIPS}}},
  author = {Mukherjee, Subhabrata and Awadallah, Ahmed Hassan},
  date = {2020},
  pages = {14},
  abstract = {Recent success of pre-trained language models crucially hinges on fine-tuning them on large amounts of labeled data for the downstream task, that are typically expensive to acquire or difficult to access for many applications. We study selftraining as one of the earliest semi-supervised learning approaches to reduce the annotation bottleneck by making use of large-scale unlabeled data for the target task. Standard self-training mechanism randomly samples instances from the unlabeled pool to generate pseudo-labels and augment labeled data. We propose an approach to improve self-training by incorporating uncertainty estimates of the underlying neural network leveraging recent advances in Bayesian deep learning. Specifically, we propose (i) acquisition functions to select instances from the unlabeled pool leveraging Monte Carlo (MC) Dropout, and (ii) learning mechanism leveraging model confidence for self-training. As an application, we focus on text classification with five benchmark datasets. We show our methods leveraging only 20-30 labeled samples per class for each task for training and for validation perform within 3\% of fully supervised pre-trained language models fine-tuned on thousands of labels with an aggregate accuracy of 91\% and improvement of up to 12\% over baselines.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BVM7G2RX/Mukherjee and Awadallah - Uncertainty-aware Self-training for Few-shot Text .pdf}
}

@inproceedings{mukherjee2021CLUESFewShotLearning,
  title = {{{CLUES}}: {{Few-Shot Learning Evaluation}} in {{Natural Language Understanding}}},
  shorttitle = {{{CLUES}}},
  booktitle = {{{NeurIPS}}},
  author = {Mukherjee, Subhabrata and Liu, Xiaodong and Zheng, Guoqing and Hosseini, Saghar and Cheng, Hao and Yang, Greg and Meek, Christopher and Awadallah, Ahmed Hassan and Gao, Jianfeng},
  date = {2021-11-03},
  eprint = {2111.02570},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.02570},
  urldate = {2021-11-15},
  abstract = {Most recent progress in natural language understanding (NLU) has been driven, in part, by benchmarks such as GLUE, SuperGLUE, SQuAD, etc. In fact, many NLU models have now matched or exceeded “human-level” performance on many tasks in these benchmarks. Most of these benchmarks, however, give models access to relatively large amounts of labeled data for training. As such, the models are provided far more data than required by humans to achieve strong performance. That has motivated a line of work that focuses on improving few-shot learning performance of NLU models. However, there is a lack of standardized evaluation benchmarks for few-shot NLU resulting in different experimental settings in different papers. To help accelerate this line of work, we introduce CLUES1, a benchmark for evaluating the few-shot learning capabilities of NLU models. We demonstrate that while recent models reach human performance when they have access to large amounts of labeled data, there is a huge gap in performance in the few-shot setting for most tasks. We also demonstrate differences between alternative model families and adaptation techniques in the few shot setting. Finally, we discuss several principles and choices in designing the experimental settings for evaluating the true few-shot learning performance and suggest a unified standardized approach to few-shot learning evaluation. We aim to encourage research on NLU models that can generalize to new tasks with a small number of examples. Code and data for CLUES are available at https://github.com/microsoft/CLUES.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y7Y4YLJT/Mukherjee et al. - 2021 - CLUES Few-Shot Learning Evaluation in Natural Lan.pdf}
}

@inproceedings{murakami2017LearningGenerateMarket,
  title = {Learning to {{Generate Market Comments}} from {{Stock Prices}}},
  booktitle = {{{ACL}}},
  author = {Murakami, S. and Watanabe, A. and Miyazawa, A. and Goshima, K. and Yanase, T. and Takamura, H. and Miyao, Y.},
  date = {2017},
  pages = {1374--1384},
  doi = {10.18653/v1/P17-1126},
  url = {https://doi.org/10.18653/v1/P17-1126},
  abstract = {This paper presents a novel encoder-decoder model for automatically generat-ing market comments from stock prices. The model first encodes both short-and long-term series of stock prices so that it can mention short-and long-term changes in stock prices. In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic op-eration such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best model generates market comments at the fluency and the informativeness approach-ing human-generated reference texts.},
  isbn = {978-1-945626-75-3},
  annotation = {32 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T533VHJW/Murakami et al. - 2017 - Learning to Generate Market Comments from Stock Prices(2).pdf}
}

@inproceedings{murdoch2017AutomaticRuleExtraction,
  title = {Automatic Rule Extraction from Long Short Term Memory Networks},
  booktitle = {{{ICLR}}},
  author = {Murdoch, J. and Szlam, A.},
  date = {2017},
  volume = {4},
  number = {2},
  eprint = {1702.02540},
  eprinttype = {arxiv},
  pages = {221--226},
  issn = {23208430},
  doi = {10.5121/ijci.2015.4221},
  abstract = {Although deep learning models have proven effective at solving problems in natu-ral language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of repre-sentative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WTRYTYHV/Murdoch, Szlam - 2017 - Automatic rule extraction from long short term memory networks(2).pdf}
}

@inproceedings{mvdoc2022,
  title = {Multi-{{View Document Representation Learning}} for {{Open-Domain Dense Retrieval}}},
  booktitle = {{{ACL}}},
  author = {Zhang, Shunyu and Liang, Yaobo and Gong, Ming and Jiang, Daxin and Duan, Nan},
  date = {2022-03-15},
  eprint = {2203.08372},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.08372},
  urldate = {2022-03-23},
  abstract = {Dense retrieval has achieved impressive advances in first-stage retrieval from a large-scale document collection, which is built on bi-encoder architecture to produce single vector representation of query and document. However, a document can usually answer multiple potential queries from different views. So the single vector representation of a document is hard to match with multi-view queries, and faces a semantic mismatch problem. This paper proposes a multi-view document representation learning framework, aiming to produce multi-view embeddings to represent documents and enforce them to align with different queries. First, we propose a simple yet effective method of generating multiple embeddings through viewers. Second, to prevent multi-view embeddings from collapsing to the same one, we further propose a global-local loss with annealed temperature to encourage the multiple viewers to better align with different potential queries. Experiments show our method outperforms recent works and achieves state-of-the-art results.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GSILUWIJ/Zhang et al. - 2022 - Multi-View Document Representation Learning for Op.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KM7JHVG6/2203.html}
}

@inproceedings{mysore2022MultiVectorModelsTextual,
  title = {Multi-{{Vector Models}} with {{Textual Guidance}} for {{Fine-Grained Scientific Document Similarity}}},
  booktitle = {{{NAACL}}},
  author = {Mysore, Sheshera and Cohan, Arman and Hope, Tom},
  date = {2022-05-04},
  eprint = {2111.08366},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.08366},
  urldate = {2022-07-15},
  abstract = {We present a new scientific document similarity model based on matching fine-grained aspects of texts. To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations). Such co-citations not only reflect close paper relatedness, but also provide textual descriptions of how the co-cited papers are related. This novel form of textual supervision is used for learning to match aspects across papers. We develop multi-vector representations where vectors correspond to sentence-level aspects of documents, and present two methods for aspect matching: (1) A fast method that only matches single aspects, and (2) a method that makes sparse multiple matches with an Optimal Transport mechanism that computes an Earth Mover's Distance between aspects. Our approach improves performance on document similarity tasks in four datasets. Further, our fast single-match method achieves competitive results, paving the way for applying fine-grained similarity to large scientific corpora. Code, data, and models available at: https://github.com/allenai/aspire},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WGL7GF7P/Mysore et al. - 2022 - Multi-Vector Models with Textual Guidance for Fine.pdf;/home/hiaoxui/.local/share/zotero_files/storage/DP4AUYI6/2111.html}
}

@inproceedings{naik2018StressTestEvaluation,
  title = {Stress {{Test Evaluation}} for {{Natural Language Inference}}},
  booktitle = {{{COLING}}},
  author = {Naik, A. and Ravichander, A. and Sadeh, N. and Rose, C. and Neubig, G.},
  date = {2018},
  eprint = {1806.00692},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1806.00692},
  abstract = {Natural language inference (NLI) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. NLI was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for NLI, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed "stress tests" that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {112 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3APKKDAR/Naik et al. - 2018 - Stress Test Evaluation for Natural Language Inference(2).pdf}
}

@inproceedings{nallapati2016AbstractiveTextSummarization,
  title = {Abstractive {{Text Summarization Using Sequence-to-Sequence RNNs}} and {{Beyond}}},
  booktitle = {{{CoNLL}}},
  author = {Nallapati, Ramesh and Zhou, Bowen and dos {santos}, Cicero Nogueira and Gulcehre, Caglar and Xiang, Bing},
  date = {2016-08-26},
  eprint = {1602.06023},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1602.06023},
  urldate = {2022-03-01},
  abstract = {In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZBH34BL3/Nallapati et al. - 2016 - Abstractive Text Summarization Using Sequence-to-S.pdf;/home/hiaoxui/.local/share/zotero_files/storage/Y85VPSZC/1602.html}
}

@inproceedings{namysl2020NATNoiseAwareTraining,
  title = {{{NAT}}: {{Noise-Aware Training}} for {{Robust Neural Sequence Labeling}}},
  shorttitle = {{{NAT}}},
  booktitle = {{{ACL}}},
  author = {Namysl, Marcin and Behnke, Sven and Köhler, Joachim},
  date = {2020-05-14},
  eprint = {2005.07162},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.07162},
  urldate = {2021-03-13},
  abstract = {Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs - as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. We employ a vanilla noise model at training time. For evaluation, we use both the original data and its variants perturbed with real OCR errors and misspellings. Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input. We make our code and data publicly available for the research community.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M28DJHKU/Namysl et al. - 2020 - NAT Noise-Aware Training for Robust Neural Sequen.pdf;/home/hiaoxui/.local/share/zotero_files/storage/LXMX3FVQ/2005.html}
}

@inproceedings{namysl2021EmpiricalErrorModeling,
  title = {Empirical {{Error Modeling Improves Robustness}} of {{Noisy Neural Sequence Labeling}}},
  booktitle = {{{ACL-IJCNLP}}},
  author = {Namysl, Marcin and Behnke, Sven and Köhler, Joachim},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/33KEEF39/288_file_Paper.pdf;/home/hiaoxui/.local/share/zotero_files/storage/AIU9H36X/2105.11872.pdf}
}

@inproceedings{napoles2012AnnotatedGigaword,
  title = {Annotated {{Gigaword}}},
  booktitle = {Joint {{Workshop}} on {{Automatic Knowledge Base Construction}} and {{Web-scale Knowledge Extraction}}},
  author = {Napoles, C. and Gormley, M. and Van Durme, B.},
  date = {2012},
  pages = {95--100},
  url = {http://dl.acm.org/citation.cfm?id=2391218},
  abstract = {We have created layers of annotation on the English Gigaword v.5 corpus to render it useful as a standardized corpus for knowledge extraction and distributional semantics. Most existing large-scale work is based on inconsistent corpora which often have needed to be re-annotated by research teams independently, each time introducing biases that manifest as results that are only comparable at a high level. We provide to the community a public reference set based on current state-of-the-art syntactic analysis and coreference resolution, along with an interface for programmatic access. Our goal is to enable broader involvement in large-scale knowledge-acquisition efforts by researchers that otherwise may not have had the ability to produce such a resource on their own.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SVS5LU4K/Napoles, Gormley, Van Durme - 2012 - Annotated Gigaword(2).pdf}
}

@inproceedings{narisawa2013204CmMan,
  title = {Is a 204 Cm {{Man Tall}} or {{Small}}? {{Acquisition}} of {{Numerical Common Sense}} from the {{Web}}.},
  booktitle = {{{ACL}}},
  author = {Narisawa, K. and Watanabe, Y. and Mizuno, J. and Okazaki, N. and Inui, K.},
  date = {2013},
  pages = {382--391},
  isbn = {978-1-937284-50-3},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZHV2TKHQ/Narisawa et al. - 2013 - Is a 204 cm Man Tall or Small Acquisition of Numerical Common Sense from the Web(2).pdf}
}

@inproceedings{natarajan2008LogicalHierarchicalHidden,
  title = {Logical {{Hierarchical Hidden Markov Models For Modeling User Activities}}},
  booktitle = {Conference on {{Inductive Logic Programming}}},
  author = {Natarajan, S. and Bui, H. H. and Tadepalli, P. and Kersting, K.},
  date = {2008},
  pages = {192--209},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZV5P84FK/Natarajan et al. - 2008 - Logical Hierarchical Hidden Markov Models For Modeling User Activities(2).pdf}
}

@article{navigli2007WordSenseDisambiguation,
  title = {Word {{Sense Disambiguation}}: {{A Survey}}},
  author = {Navigli, R.},
  date = {2007},
  journaltitle = {ACM Computing Surveys},
  volume = {41},
  number = {2},
  eprint = {18353985},
  eprinttype = {pmid},
  pages = {1725--1730},
  issn = {10450823},
  doi = {10.1145/1459352.1459355},
  abstract = {Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data and/or do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networksâ€™ links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.},
  archiveprefix = {arXiv},
  isbn = {0360-0300},
  annotation = {1726 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8UC9NVNA/Navigli - 2007 - Word Sense Disambiguation A Survey(2).pdf}
}

@inproceedings{navigli2019GameTheoryMeets,
  title = {Game {{Theory Meets Embeddings}} : A {{Unified Framework}} for {{Word Sense Disambiguation}}},
  booktitle = {{{EMNLP}}},
  author = {Navigli, R.},
  date = {2019},
  pages = {88--99},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z3U6MRN8/Navigli - 2019 - Game Theory Meets Embeddings a Unified Framework for Word Sense Disambiguation(2).pdf}
}

@misc{nawrot2021HierarchicalTransformersAre,
  title = {Hierarchical {{Transformers Are More Efficient Language Models}}},
  author = {Nawrot, Piotr and Tworkowski, Szymon and Tyrolski, Michał and Kaiser, Łukasz and Wu, Yuhuai and Szegedy, Christian and Michalewski, Henryk},
  date = {2021-10-26},
  eprint = {2110.13711},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.13711},
  urldate = {2021-12-15},
  abstract = {Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/67HM353D/Nawrot et al. - 2021 - Hierarchical Transformers Are More Efficient Langu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/HKL4S87S/2110.html}
}

@incollection{neal1998ViewEmAlgorithm,
  title = {A {{View}} of the {{Em Algorithm}} That {{Justifies Incremental}}, {{Sparse}}, and Other {{Variants}}},
  booktitle = {Learning in {{Graphical Models}}},
  author = {Neal, R. M. and Hinton, G. E.},
  date = {1998},
  eprint = {15991970},
  eprinttype = {pmid},
  pages = {355--368},
  issn = {978-1-932432-41-1},
  doi = {10.1007/978-94-011-5014-9_12},
  url = {http://link.springer.com/10.1007/978-94-011-5014-9_12},
  abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
  archiveprefix = {arXiv},
  isbn = {0-262-60032-3},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2ZHVE692/Neal, Hinton - 1998 - A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants(2).pdf}
}

@inproceedings{neelakantan2014EfficientNonparametricEstimation,
  title = {Efficient {{Non-parametric Estimation}} of {{Multiple Embeddings}} per {{Word}} in {{Vector Space}}},
  booktitle = {{{EMNLP}}},
  author = {Neelakantan, A. and Shankar, J. and Passos, A. and McCallum, A.},
  date = {2014},
  eprint = {1504.06654},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1504.06654},
  abstract = {There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.},
  archiveprefix = {arXiv},
  annotation = {379 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2IQ3CHI2/Neelakantan et al. - 2014 - Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space(2).pdf}
}

@inproceedings{nenkova2004EvaluatingContentSelection,
  title = {Evaluating Content Selection in Summarization: {{The}} Pyramid Method},
  booktitle = {{{NAACL-HLT}}},
  author = {Nenkova, A. and Passonneau, R.},
  date = {2004},
  pages = {145--152},
  url = {papers2://publication/uuid/DC675E84-0A45-48B7-A26C-F08B4B9398D3},
  abstract = {We present an empirically grounded method for evaluating content selection in summariza- tion. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative im- portance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus im- proves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VC7W2AZM/Nenkova, Passonneau - 2004 - Evaluating content selection in summarization The pyramid method(2).pdf}
}

@article{ng1997CorpusBasedApproachesSemantic,
  title = {Corpus-{{Based Approaches}} to {{Semantic Interpretation}} in {{Natural Language Processing}}},
  author = {Ng, H. T. and Zelle, J.},
  date = {1997},
  journaltitle = {AI Magazine},
  volume = {18},
  number = {4},
  pages = {45--64},
  issn = {0738-4602},
  doi = {10.1609/aimag.v18i4.1321},
  abstract = {In recent years, there has been a flurry of research into empirical, corpus-based learning approaches to natural language processing (NLP). Most empir- ical NLP work to date has focused on relatively low-level language processing such as part-of- speech tagging, text segmentation, and syntactic parsing. The success of these approaches has stim- ulated research in using empirical learning tech- niques in other facets of NLP, including semantic analysis—uncovering the meaning of an utter- ance. This article is an introduction to some of the emerging research in the application of corpus- based learning techniques to problems in semantic interpretation. In particular, we focus on two im- portant problems in semantic interpretation, namely, word-sense disambiguation and semantic parsing.},
  keywords = {unread},
  annotation = {39 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ABE6IZYF/Ng, Zelle - 1997 - Corpus-Based Approaches to Semantic Interpretation in Natural Language Processing(2).pdf}
}

@inproceedings{nguyen2021RSTParsingScratch,
  title = {{{RST Parsing}} from {{Scratch}}},
  booktitle = {{{NAACL}}},
  author = {Nguyen, Thanh-Tung and Nguyen, Xuan-Phi and Joty, Shafiq and Li, Xiaoli},
  date = {2021},
  pages = {1613--1625},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.128},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.128},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PZIRY682/Nguyen et al. - 2021 - RST Parsing from Scratch.pdf}
}

@inproceedings{nguyen2021SkimAttentionLearningFocus,
  title = {Skim-{{Attention}}: {{Learning}} to {{Focus}} via {{Document Layout}}},
  shorttitle = {Skim-{{Attention}}},
  booktitle = {{{EMNLP}}},
  author = {Nguyen, Laura and Scialom, Thomas and Staiano, Jacopo and Piwowarski, Benjamin},
  date = {2021-09-02},
  eprint = {2109.01078},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.01078},
  urldate = {2021-09-05},
  abstract = {Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2-dimensional position of the words in a document. Our experiments show that Skim-Attention obtains a lower perplexity than prior works, while being more computationally efficient. Skim-Attention can be further combined with long-range Transformers to efficiently process long documents. We also show how Skim-Attention can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Finally, we show the emergence of a document structure representation in Skim-Attention.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-05]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RMKX26Z7/Nguyen et al. - 2021 - Skim-Attention Learning to Focus via Document Lay.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KFPWR7DV/2109.html}
}

@misc{nguyen2022AreDiscreteUnits,
  title = {Are Discrete Units Necessary for {{Spoken Language Modeling}}?},
  author = {Nguyen, Tu Anh and Sagot, Benoit and Dupoux, Emmanuel},
  date = {2022-08-22},
  number = {arXiv:2203.05936},
  eprint = {2203.05936},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.05936},
  urldate = {2022-10-10},
  abstract = {Recent work in spoken language modeling shows the possibility of learning a language unsupervisedly from raw audio without any text labels. The approach relies first on transforming the audio into a sequence of discrete units (or pseudo-text) and then training a language model directly on such pseudo-text. Is such a discrete bottleneck necessary, potentially introducing irreversible errors in the encoding of the speech signal, or could we learn a language model without discrete units at all? In this work, we study the role of discrete versus continuous representations in spoken language modeling. We show that discretization is indeed essential for good results in spoken language modeling. We show that discretization removes linguistically irrelevant information from the continuous features, helping to improve language modeling performances. On the basis of this study, we train a language model on the discrete units of the HuBERT features, reaching new state-of-the-art results in the lexical, syntactic and semantic metrics of the Zero Resource Speech Challenge 2021 (Track 1 - Speech Only).},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y596FABK/Nguyen et al. - 2022 - Are discrete units necessary for Spoken Language M.pdf;/home/hiaoxui/.local/share/zotero_files/storage/INFS7527/2203.html}
}

@inproceedings{nickel2017PoincareEmbeddingsLearning,
  title = {Poincaré Embeddings for Learning Hierarchical Representations},
  booktitle = {{{NeurIPS}}},
  author = {Nickel, M. and Kiela, D.},
  date = {2017},
  eprint = {1705.08039},
  eprinttype = {arxiv},
  pages = {6339--6348},
  abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space - or more precisely into an n-dimensional Poincaré ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincaré embeddings can outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ELITD2KL/Nickel, Kiela - 2017 - Poincaré embeddings for learning hierarchical representations(2).pdf}
}

@inproceedings{niculae2018SparseMAPDifferentiableSparse,
  title = {{{SparseMAP}}: {{Differentiable Sparse Structured Inference}}},
  booktitle = {{{ICML}}},
  author = {Niculae, V. and Martins, A. F. T. and Blondel, M. and Cardie, C.},
  date = {2018},
  eprint = {1802.04223},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.04223},
  abstract = {Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function. SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns probability mass to all structures, including implausible ones. Importantly, SparseMAP can be computed using only calls to a MAP oracle, making it applicable to problems with intractable marginal inference, e.g., linear assignment. Sparsity makes gradient backpropagation efficient regardless of the structure, enabling us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.},
  archiveprefix = {arXiv},
  annotation = {57 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2DF96G92/Niculae et al. - 2018 - SparseMAP Differentiable Sparse Structured Inference(2).pdf}
}

@misc{nie2018IncorporatingConsistencyVerification,
  title = {Incorporating {{Consistency Verification}} into {{Neural Data-to-Document Generation}}},
  author = {Nie, F. and Chen, H. and Wang, J. and Yao, J. and Lin, C. and Pan, R.},
  date = {2018},
  eprint = {1808.05306},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1808.05306},
  abstract = {Recent neural models for data-to-document generation have achieved remarkable progress in producing fluent and informative texts. However, large proportions of generated texts do not actually conform to the input data. To address this issue, we propose a new training framework which attempts to verify the consistency between the generated texts and the input data to guide the training process. To measure the consistency, a relation extraction model is applied to check information overlaps between the input data and the generated texts. The non-differentiable consistency signal is optimized via reinforcement learning. Experimental results on a recently released challenging dataset ROTOWIRE show improvements from our framework in various metrics.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B9ZPP77T/Nie et al. - 2018 - Incorporating Consistency Verification into Neural Data-to-Document Generation(2).pdf}
}

@inproceedings{nie2018OperationsGuidedNeuralNetworks,
  title = {Operations-{{Guided Neural Networks}} for {{High Fidelity Data-To-Text Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Nie, F. and Wang, J. and Yao, J. and Pan, R. and Lin, C.},
  date = {2018},
  eprint = {1809.02735},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1809.02735},
  abstract = {Recent neural models for data-to-text generation are mostly based on data-driven end-to-end training over encoder-decoder networks. Even though the generated texts are mostly fluent and informative, they often generate descriptions that are not consistent with the input structured data. This is a critical issue especially in domains that require inference or calculations over raw data. In this paper, we attempt to improve the fidelity of neural data-to-text generation by utilizing pre-executed symbolic operations. We propose a framework called Operation-guided Attention-based sequence-to-sequence network (OpAtt), with a specifically designed gating mechanism as well as a quantization module for operation results to utilize information from pre-executed operations. Experiments on two sports datasets show our proposed method clearly improves the fidelity of the generated texts to the input structured data.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {27 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L5LZBANA/Nie et al. - 2018 - Operations-Guided Neural Networks for High Fidelity Data-To-Text Generation(2).pdf}
}

@inproceedings{nie2020AdversarialNLINew,
  title = {Adversarial {{NLI}}: {{A New Benchmark}} for {{Natural Language Understanding}}},
  shorttitle = {Adversarial {{NLI}}},
  booktitle = {{{ACl}}},
  author = {Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  date = {2020-05-06},
  eprint = {1910.14599},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.14599},
  urldate = {2022-03-01},
  abstract = {We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/72P3VT6A/Nie et al. - 2020 - Adversarial NLI A New Benchmark for Natural Langu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ZX9XI8J5/1910.html}
}

@inproceedings{niepert2021ImplicitMLEBackpropagating,
  title = {Implicit {{MLE}}: {{Backpropagating Through Discrete Exponential Family Distributions}}},
  shorttitle = {Implicit {{MLE}}},
  booktitle = {{{NeurIPS}}},
  author = {Niepert, Mathias and Minervini, Pasquale and Franceschi, Luca},
  date = {2021-10-27},
  eprint = {2106.01798},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.01798},
  urldate = {2022-03-07},
  abstract = {Combining discrete probability distributions and combinatorial optimization problems with neural network components has numerous applications but poses several challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a framework for end-to-end learning of models combining discrete exponential family distributions and differentiable neural components. I-MLE is widely applicable as it only requires the ability to compute the most probable states and does not rely on smooth relaxations. The framework encompasses several approaches such as perturbation-based implicit differentiation and recent methods to differentiate through black-box combinatorial solvers. We introduce a novel class of noise distributions for approximating marginals via perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood estimation when used in some recently studied learning settings that involve combinatorial solvers. Experiments on several datasets suggest that I-MLE is competitive with and often outperforms existing approaches which rely on problem-specific relaxations.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PFHL6ZTC/Niepert et al. - 2021 - Implicit MLE Backpropagating Through Discrete Exp.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6UHBFVP5/2106.html}
}

@inproceedings{nigam2000AnalyzingEffectivenessApplicability,
  title = {Analyzing the Effectiveness and Applicability of Co-Training},
  booktitle = {{{CIKM}}},
  author = {Nigam, K. and Ghani, R.},
  date = {2000},
  pages = {86--93},
  doi = {10.1145/354756.354805},
  abstract = {Recen tly there has been signi?can tin terest in supervised learning algorithms that com bine labeled and unlabeled data for text learning tasks? The co?training setting ??? applies to datasets that ha e a natural separation of their features in v to t o disjoin w t sets? We demonstrate that when learning from labeled and unlabeled data? algorithms explicitly lev eraging a natural independen t split of the features outperform al? gorithms that do not? When a natural split does not exist? co?training algorithms that man ufacture a feature split ma y out?perform algorithms not using a split? These results help explain wh y co?training algorithms are both discriminativ e in nature and robust to the assumptions of their em bedded classi?ers?},
  annotation = {1001 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/98PPJAY8/Nigam, Ghani - 2000 - Analyzing the effectiveness and applicability of co-training(2).pdf}
}

@inproceedings{ning2018JointReasoningTemporal,
  title = {Joint Reasoning for Temporal and Causal Relations},
  booktitle = {{{ACL}}},
  author = {Ning, Q. and Feng, Z. and Wu, H. and Roth, D.},
  date = {2018},
  pages = {2278--2288},
  abstract = {Understanding temporal and causal relations between events is a fundamental natural language understanding task. Because a cause must be before its effect in time, temporal and causal relations are closely related and one relation even dictates the other one in many cases. However, limited attention has been paid to studying these two relations jointly. This paper presents a joint inference framework for them using constrained conditional models (CCMs). Specifically, we formulate the joint problem as an integer linear programming (ILP) problem, enforcing constraints inherently in the nature of time and causality. We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text.},
  isbn = {978-1-948087-32-2},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KDRATEGP/Ning et al. - 2018 - Joint reasoning for temporal and causal relations(2).pdf}
}

@inproceedings{nokland2019TrainingNeuralNetworks,
  title = {Training {{Neural Networks}} with {{Local Error Signals}}},
  booktitle = {{{ICML}}},
  author = {Nøkland, A. and Eidnes, L. H.},
  date = {2019},
  eprint = {1901.06656},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.06656},
  abstract = {Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back-propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss functions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility. Code is available https://github.com/anokland/local-loss},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {54 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UFMJFI2R/Nøkland, Eidnes - 2019 - Training Neural Networks with Local Error Signals(2).pdf}
}

@inproceedings{nowozin2016FGANTrainingGenerative,
  title = {F-{{GAN}}: {{Training Generative Neural Samplers}} Using {{Variational Divergence Minimization}}},
  shorttitle = {F-{{GAN}}},
  booktitle = {{{NeurIPS}}},
  author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  date = {2016-06-02},
  eprint = {1606.00709},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.00709},
  urldate = {2021-03-16},
  abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
  archiveprefix = {arXiv},
  annotation = {890 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XF6KJK5H/Nowozin et al. - 2016 - f-GAN Training Generative Neural Samplers using V.pdf;/home/hiaoxui/.local/share/zotero_files/storage/V5YCVH9H/1606.html}
}

@inproceedings{och2003MinimumErrorRate,
  title = {Minimum {{Error Rate Training}} in {{Statistical Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Och, F. J.},
  date = {2003},
  pages = {160--167},
  doi = {10.3115/1075096.1075117},
  annotation = {3219 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AUWRCB93/Och - 2003 - Minimum Error Rate Training in Statistical Machine Translation(2).pdf}
}

@article{och2003SystematicComparisonVarious,
  title = {A {{Systematic Comparison}} of {{Various Statistical Alignment Models}}},
  author = {Och, F. J. and Ney, H.},
  date = {2003},
  journaltitle = {Computational Linguistics},
  volume = {29},
  number = {1},
  pages = {19--51},
  issn = {0891-2017},
  doi = {10.1162/089120103321337421},
  url = {http://www.mitpressjournals.org/doi/10.1162/089120103321337421},
  abstract = {We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.},
  isbn = {0891-2017},
  annotation = {4259 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2GXISXFF/Och, Ney - 2003 - A Systematic Comparison of Various Statistical Alignment Models(2).pdf}
}

@inproceedings{oconnor2021WhatContextFeatures,
  title = {What {{Context Features Can Transformer Language Models Use}}?},
  booktitle = {{{ACL}}},
  author = {O'Connor, Joe and Andreas, Jacob},
  date = {2021-06-15},
  eprint = {2106.08367},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.08367},
  urldate = {2021-09-07},
  abstract = {Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations -- including shuffling word order within sentences and deleting all words other than nouns -- remove less than 15\% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.},
  archiveprefix = {arXiv},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8RFJ5EEP/O'Connor and Andreas - 2021 - What Context Features Can Transformer Language Mod.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QUK8QZMX/2106.html}
}

@inproceedings{ogorman2016RicherEventDescription,
  title = {Richer {{Event Description}}: {{Integrating}} Event Coreference with Temporal, Causal and Bridging Annotation},
  booktitle = {Workshop on {{Computing News Storylines}}},
  author = {O'Gorman, T. and Wright-Bettner, K. and Palmer, M.},
  date = {2016},
  pages = {47--56},
  doi = {10.18653/v1/w16-5706},
  abstract = {We examine the effect of industry life-cycle stages on within-industry acquisitions and capital expenditures by conglomerates and single-segment firms controlling for endogeneity of organizational form.We find greater differences in acquisitions than in capital expenditures, which are similar across organizational types. In particular, 36\% of the growth recorded by conglomerate segments in growth industries comes from acquisitions, versus 9\% for single-segment firms. In growth industries, the effect of fi- nancial dependence on acquisitions and plant openings is mitigated for conglomerate firms. Plants acquired by conglomerate firms increase in productivity. The results suggest that organizational forms’ comparative advantages differ across industry conditions.},
  keywords = {unread},
  annotation = {62 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T8MQCVLR/O'Gorman, Wright-Bettner, Palmer - 2016 - Richer Event Description Integrating event coreference with temporal, causal and bridging a(2).pdf}
}

@inproceedings{ontanon2022MakingTransformersSolve,
  title = {Making {{Transformers Solve Compositional Tasks}}},
  booktitle = {{{ACL}}},
  author = {Ontañón, Santiago and Ainslie, Joshua and Cvicek, Vaclav and Fisher, Zachary},
  date = {2022-03-03},
  eprint = {2108.04378},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.04378},
  urldate = {2022-03-23},
  abstract = {Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing. In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization. Through this exploration, we identified Transformer configurations that generalize compositionally significantly better than previously reported in the literature in a diverse set of compositional tasks, and that achieve state-of-the-art results in a semantic parsing compositional generalization benchmark (COGS), and a string edit operation composition benchmark (PCFG).},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V3NRATAA/Ontañón et al. - 2022 - Making Transformers Solve Compositional Tasks.pdf;/home/hiaoxui/.local/share/zotero_files/storage/XPWISB6F/2108.html}
}

@inproceedings{oren2019DistributionallyRobustLanguage,
  title = {Distributionally {{Robust Language Modeling}}},
  booktitle = {{{EMNLP}}},
  author = {Oren, Y. and Sagawa, S. and Hashimoto, T. B. and Liang, P.},
  date = {2019},
  volume = {2},
  eprint = {1909.02060},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.02060},
  abstract = {Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.},
  archiveprefix = {arXiv},
  annotation = {21 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CBYHWSKA/Oren et al. - 2019 - Distributionally Robust Language Modeling(2).pdf}
}

@misc{oroojlooyjadid2017DeepQNetworkBeer,
  title = {A {{Deep Q-Network}} for the {{Beer Game}}: {{A Reinforcement Learning}} Algorithm to {{Solve Inventory Optimization Problems}}},
  author = {Oroojlooyjadid, A. and Nazari, M. and Snyder, L. and Takáč, M.},
  date = {2017},
  eprint = {1708.05924},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1708.05924},
  abstract = {The beer game is a widely used in-class game that is played in supply chain management classes to demonstrate the bullwhip effect. The game is a decentralized, multi-agent, cooperative problem that can be modeled as a serial supply chain network in which agents cooperatively attempt to minimize the total cost of the network even though each agent can only observe its own local information. Each agent chooses order quantities to replenish its stock. Under some conditions, a base-stock replenishment policy is known to be optimal. However, in a decentralized supply chain in which some agents (stages) may act irrationally (as they do in the beer game), there is no known optimal policy for an agent wishing to act optimally. We propose a machine learning algorithm, based on deep Q-networks, to optimize the replenishment decisions at a given stage. When playing alongside agents who follow a base-stock policy, our algorithm obtains near-optimal order quantities. It performs much better than a base-stock policy when the other agents use a more realistic model of human ordering behavior. Unlike most other algorithms in the literature, our algorithm does not have any limits on the beer game parameter values. Like any deep learning algorithm, training the algorithm can be computationally intensive, but this can be performed ahead of time; the algorithm executes in real time when the game is played. Moreover, we propose a transfer learning approach so that the training performed for one agent and one set of cost coefficients can be adapted quickly for other agents and costs. Our algorithm can be extended to other decentralized multi-agent cooperative games with partially observed information, which is a common type of situation in real-world supply chain problems.},
  archiveprefix = {arXiv},
  annotation = {19 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ST7EP3PW/Oroojlooyjadid et al. - 2017 - A Deep Q-Network for the Beer Game A Reinforcement Learning algorithm to Solve Inventory Optimization (2).pdf}
}

@inproceedings{ouchi2018SpanSelectionModel,
  title = {A {{Span Selection Model}} for {{Semantic Role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Ouchi, Hiroki and Shindo, Hiroyuki and Matsumoto, Yuji},
  date = {2018-10-04},
  eprint = {1810.02245},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1810.02245},
  urldate = {2020-08-21},
  abstract = {We present a simple and accurate span-based model for semantic role labeling (SRL). Our model directly takes into account all possible argument spans and scores them for each label. At decoding time, we greedily select higher scoring labeled spans. One advantage of our model is to allow us to design and use spanlevel features, that are difficult to use in tokenbased BIO tagging approaches. Experimental results demonstrate that our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  annotation = {43 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/849763GQ/Ouchi et al. - 2018 - A Span Selection Model for Semantic Role Labeling.pdf}
}

@inproceedings{ouyang2021ERNIEMEnhancedMultilingual,
  title = {{{ERNIE-M}}: {{Enhanced Multilingual Representation}} by {{Aligning Cross-lingual Semantics}} with {{Monolingual Corpora}}},
  shorttitle = {{{ERNIE-M}}},
  booktitle = {{{EMNLP}}},
  author = {Ouyang, Xuan and Wang, Shuohuan and Pang, Chao and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  date = {2021},
  pages = {27--38},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.3},
  url = {https://aclanthology.org/2021.emnlp-main.3},
  urldate = {2022-03-15},
  eventtitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/93M6SRW8/Ouyang et al. - 2021 - ERNIE-M Enhanced Multilingual Representation by A.pdf}
}

@misc{paige2014AsynchronousAnytimeSequential,
  title = {Asynchronous {{Anytime Sequential Monte Carlo}}},
  author = {Paige, B. and Wood, F. and Doucet, A. and Teh, Y. W.},
  date = {2014},
  eprint = {1407.2864},
  eprinttype = {arxiv},
  issn = {10495258},
  url = {http://arxiv.org/abs/1407.2864},
  abstract = {We introduce a new sequential Monte Carlo algorithm we call the particle cascade. The particle cascade is an asynchronous, anytime alternative to traditional particle filtering algorithms. It uses no barrier synchronizations which leads to improved particle throughput and memory efficiency. It is an anytime algorithm in the sense that it can be run forever to emit an unbounded number of particles while keeping within a fixed memory budget. We prove that the particle cascade is an unbiased marginal likelihood estimator which means that it can be straightforwardly plugged into existing pseudomarginal methods.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {46 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HV3HWSJZ/Paige et al. - 2014 - Asynchronous Anytime Sequential Monte Carlo(2).pdf}
}

@article{palmer2004PropositionBankAnnotated,
  title = {The {{Proposition Bank}}: {{An Annotated Corpus}} of {{Semantic Roles}}},
  author = {Palmer, M. and Gildea, D. and Kingsbury, P.},
  date = {2004},
  journaltitle = {Computational Linguistics},
  volume = {31},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4UDBP4H6/Palmer, Gildea, Kingsbury - 2004 - The Proposition Bank An Annotated Corpus of Semantic Roles(2).pdf}
}

@misc{palmer2009SemLinkLinkingPropBank,
  title = {{{SemLink}} - {{Linking PropBank}} , {{VerbNet}}, {{FrameNet}}},
  author = {Palmer, M.},
  date = {2009},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R5QLPUDH/Palmer - 2009 - SemLink - Linking PropBank , VerbNet, FrameNet(2).pdf}
}

@inproceedings{pan2020LearningConstraintsStructured,
  title = {Learning {{Constraints}} for {{Structured Prediction Using Rectifier Networks}}},
  booktitle = {{{ACL}}},
  author = {Pan, X. and Mehta, M. and Srikumar, V.},
  date = {2020},
  eprint = {2006.01209},
  eprinttype = {arxiv},
  pages = {4843--4858},
  url = {http://arxiv.org/abs/2006.01209},
  abstract = {Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJRCKNZZ/Pan, Mehta, Srikumar - 2020 - Learning Constraints for Structured Prediction Using Rectifier Networks(2).pdf}
}

@misc{pan2021ImprovedTextClassification,
  title = {Improved {{Text Classification}} via {{Contrastive Adversarial Training}}},
  author = {Pan, Lin and Hang, Chung-Wei and Sil, Avirup and Potdar, Saloni and Yu, Mo},
  date = {2021},
  eprint = {2107.10137},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.10137},
  urldate = {2021-07-23},
  abstract = {We propose a simple and general method to regularize the fine-tuning of Transformer-based encoders for text classification tasks. Specifically, during fine-tuning we generate adversarial examples by perturbing the word embeddings of the model and perform contrastive learning on clean and adversarial examples in order to teach the model to learn noise-invariant representations. By training on both clean and adversarial examples along with the additional contrastive objective, we observe consistent improvement over standard fine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned BERT Large model outperforms BERT Large baseline by 1.7\% on average, and our fine-tuned RoBERTa Large improves over RoBERTa Large baseline by 1.3\%. We additionally validate our method in different domains using three intent classification datasets, where our fine-tuned RoBERTa Large outperforms RoBERTa Large baseline by 1-2\% on average.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3849Y2KC/Pan et al. - 2021 - Improved Text Classification via Contrastive Adver.pdf;/home/hiaoxui/.local/share/zotero_files/storage/8RNX4583/2107.html}
}

@inproceedings{pang2002ThumbsSentimentClassification,
  title = {Thumbs up? {{Sentiment Classification}} Using {{Machine Learning Techniques}}},
  booktitle = {{{EMNLP}}},
  author = {Pang, B. and Lee, L. and Vaithyanathan, S.},
  date = {2002},
  issn = {0003-5696},
  doi = {10.1515/9783110239171.151},
  abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G82SEUXI/Pang, Lee, Vaithyanathan - 2002 - Thumbs up Sentiment Classification using Machine Learning Techniques(2).pdf}
}

@inproceedings{pang2022QuALITYQuestionAnswering,
  title = {{{QuALITY}}: {{Question Answering}} with {{Long Input Texts}}, {{Yes}}!},
  booktitle = {{{NAACL}}},
  author = {Pang, Richard Yuanzhe and Parrish, A. and Joshi, Nitish and Nangia, N. and Phang, J. and Chen, A. and Padmakumar, V. and Ma, J. and Thompson, J. and He, H. and Bowman, S. R.},
  date = {2022},
  abstract = {To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Current models perform poorly on this task (55.4\%) and significantly lag behind human performance (93.5\%).},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5DADSG55/Pang et al. - QuALITY Question Answering with Long Input Texts,.pdf}
}

@inproceedings{papadimitriou2022WhenClassifyingGrammatical,
  title = {When Classifying Grammatical Role, {{BERT}} Doesn't Care about Word Order... except When It Matters},
  booktitle = {{{ACL}}},
  author = {Papadimitriou, Isabel and Futrell, Richard and Mahowald, Kyle},
  date = {2022-03-11},
  eprint = {2203.06204},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.06204},
  urldate = {2022-03-23},
  abstract = {Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey "The chef chopped the onion," not "The onion chopped the chef." Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations. To overcome this confound, we probe grammatical role representation in English BERT and GPT-2, on instances where lexical expectations are not sufficient, and word order knowledge is necessary for correct classification. Such non-prototypical instances are naturally occurring English sentences with inanimate subjects or animate objects, or sentences where we systematically swap the arguments to make sentences like "The onion chopped the chef". We find that, while early layer embeddings are largely lexical, word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions. Our experiments isolate the effect of word order on the contextualization process, and highlight how models use context in the uncommon, but critical, instances where it matters.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TSZJ7XEE/Papadimitriou et al. - 2022 - When classifying grammatical role, BERT doesn't ca.pdf;/home/hiaoxui/.local/share/zotero_files/storage/74FQX5CI/2203.html}
}

@inproceedings{papai2012SliceNormalizedDynamic,
  title = {Slice {{Normalized Dynamic Markov Logic Networks}}},
  booktitle = {{{NeurIPS}}},
  author = {Papai, T. and Kautz, H. and Stefankovic, D.},
  date = {2012},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ABRZYL77/Papai, Kautz, Stefankovic - 2012 - Slice Normalized Dynamic Markov Logic Networks(2).pdf}
}

@inproceedings{paperno2016LAMBADADatasetWord,
  title = {The {{LAMBADA}} Dataset: {{Word}} Prediction Requiring a Broad Discourse Context},
  shorttitle = {The {{LAMBADA}} Dataset},
  booktitle = {{{ACL}}},
  author = {Paperno, Denis and Kruszewski, Germán and Lazaridou, Angeliki and Pham, Ngoc Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fernandez, Raquel},
  date = {2016},
  pages = {1525--1534},
  publisher = {{Association for Computational Linguistics}},
  location = {{Berlin, Germany}},
  doi = {10.18653/v1/P16-1144},
  url = {http://aclweb.org/anthology/P16-1144},
  urldate = {2021-09-07},
  eventtitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  annotation = {110 citations (Semantic Scholar/DOI) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A97HYCGB/Paperno et al. - 2016 - The LAMBADA dataset Word prediction requiring a b.pdf}
}

@inproceedings{papineni2002BLEUMethodAutomatic,
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  booktitle = {{{ACL}}},
  author = {Papineni, K. and Roukos, S. and Ward, T. and Zhu, W.},
  date = {2002},
  eprint = {1702.00764},
  eprinttype = {arxiv},
  pages = {311--318},
  issn = {00134686},
  doi = {10.3115/1073083.1073135},
  url = {http://dl.acm.org/citation.cfm?id=1073135},
  abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
  archiveprefix = {arXiv},
  isbn = {1-55860-883-4},
  issue = {July},
  annotation = {9993 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X62S64IZ/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine translation(2).pdf}
}

@article{pappas2019GILEGeneralizedInputLabel,
  title = {{{GILE}}: {{A Generalized Input-Label Embedding}} for {{Text Classification}}},
  shorttitle = {{{GILE}}},
  author = {Pappas, Nikolaos and Henderson, James},
  date = {2019-11},
  journaltitle = {TACL},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {139--155},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00259},
  url = {https://direct.mit.edu/tacl/article/43491},
  urldate = {2021-04-05},
  abstract = {Neural text classification models typically treat output labels as categorical variables that lack description and semantics. This forces their parametrization to be dependent on the label set size, and, hence, they are unable to scale to large label sets and generalize to unseen ones. Existing joint input-label text models overcome these issues by exploiting label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels happen often at the expense of weak performance on the labels seen during training. In this paper, we propose a new input-label model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels. The model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance. We evaluate models on full-resource and low- or zero-resource text classification of multilingual news and biomedical text with a large label set. Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios.},
  langid = {english},
  keywords = {unread},
  annotation = {13 citations (Semantic Scholar/DOI) [2021-04-05]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y3455UYF/Pappas and Henderson - 2019 - GILE A Generalized Input-Label Embedding for Text.pdf}
}

@inproceedings{paranjape2017MotifsTemporalNetworks,
  title = {Motifs in Temporal Networks},
  booktitle = {{{WSDM}}},
  author = {Paranjape, A. and Benson, A. R. and Leskovec, J.},
  date = {2017},
  pages = {601--610},
  doi = {10.1145/3018661.3018731},
  abstract = {Networks are a fundamental tool for modeling complex systems in a variety of domains including social and communication networks as well as biology and neuroscience. Small subgraph patterns in networks, called network motifs, are crucial to understanding the structure and function of these systems. However, the role of network motifs in temporal networks, which contain many timestamped links between the nodes, is not yet well understood. Here we develop a notion of a temporal network motif as an elementary unit of temporal networks and provide a general methodology for counting such motifs. We define temporal network motifs as induced subgraphs on sequences of temporal edges, design fast algorithms for counting temporal motifs, and prove their runtime complexity. Our fast algorithms achieve up to 56.5x speedup compared to a baseline method. Furthermore, we use our algorithms to count temporal motifs in a variety of networks. Results show that networks from different domains have significantly different motif counts, whereas networks from the same domain tend to have similar motif counts. We also find that different motifs occur at different time scales, which provides further insights into structure and function of temporal networks.},
  isbn = {978-1-4503-4675-7},
  annotation = {262 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NNE24E73/Paranjape, Benson, Leskovec - 2017 - Motifs in temporal networks(2).pdf}
}

@inproceedings{parikh2016DecomposableAttentionModel,
  title = {A {{Decomposable Attention Model}} for {{Natural Language Inference}}},
  booktitle = {{{EMNLP}}},
  author = {Parikh, A. P. and Täckström, O. and Das, D. and Uszkoreit, J.},
  date = {2016},
  eprint = {1606.01933},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.01933},
  abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
  archiveprefix = {arXiv},
  annotation = {823 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZLVLBCDH/Parikh et al. - 2016 - A Decomposable Attention Model for Natural Language Inference(2).pdf}
}

@inproceedings{park2021FinetuningPretrainedTransformers,
  title = {Finetuning {{Pretrained Transformers}} into {{Variational Autoencoders}}},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Insights}} from {{Negative Results}} in {{NLP}}},
  author = {Park, Seongmin and Lee, Jihwa},
  date = {2021-11-23},
  eprint = {2108.02446},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.02446},
  urldate = {2022-04-28},
  abstract = {Text variational autoencoders (VAEs) are notorious for posterior collapse, a phenomenon where the model's decoder learns to ignore signals from the encoder. Because posterior collapse is known to be exacerbated by expressive decoders, Transformers have seen limited adoption as components of text VAEs. Existing studies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et al., 2021) mitigate posterior collapse using massive pretraining, a technique unavailable to most of the research community without extensive computing resources. We present a simple two-phase training scheme to convert a sequence-to-sequence Transformer into a VAE with just finetuning. The resulting language model is competitive with massively pretrained Transformer-based VAEs in some internal metrics while falling short on others. To facilitate training we comprehensively explore the impact of common posterior collapse alleviation techniques in the literature. We release our code for reproducability.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QD5D4UBB/Park and Lee - 2021 - Finetuning Pretrained Transformers into Variationa.pdf;/home/hiaoxui/.local/share/zotero_files/storage/SB7M5B2C/2108.html}
}

@inproceedings{park2022EfficientClassificationLong,
  title = {Efficient {{Classification}} of {{Long Documents Using Transformers}}},
  booktitle = {{{ACL}}},
  author = {Park, Hyunji Hayley and Vyas, Yogarshi and Shah, Kashif},
  date = {2022-03-21},
  eprint = {2203.11258},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.11258},
  urldate = {2022-03-23},
  abstract = {Several methods have been proposed for classifying long textual documents using Transformers. However, there is a lack of consensus on a benchmark to enable a fair comparison among different approaches. In this paper, we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets -- both in terms of accuracy as well as time and space overheads. Our datasets cover binary, multi-class, and multi-label classification tasks and represent various ways information is organized in a long text (e.g. information that is critical to making the classification decision is at the beginning or towards the end of the document). Our results show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets. These findings emphasize the need for future studies to consider comprehensive baselines and datasets that better represent the task of long document classification to develop robust models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9SNHFN6R/Park et al. - 2022 - Efficient Classification of Long Documents Using T.pdf;/home/hiaoxui/.local/share/zotero_files/storage/LD8ZATAA/2203.html}
}

@inproceedings{pasupat2014ZeroshotEntityExtraction,
  title = {Zero-Shot {{Entity Extraction}} from {{Web Pages}}},
  booktitle = {{{ACL}}},
  author = {Pasupat, P. and Liang, P.},
  date = {2014},
  pages = {391--401},
  abstract = {In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction sys-tems rely on seed examples or redundancy across multiple web pages. In this paper, we consider a new zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page. Our approach de-fines a log-linear model over latent extrac-tion predicates, which select lists of enti-ties from the web page. The main chal-lenge is to define features on widely vary-ing candidate entity lists. We tackle this by abstracting list elements and using aggre-gate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline.},
  isbn = {978-1-937284-72-5},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HXLJJKRX/Pasupat, Liang - 2014 - Zero-shot Entity Extraction from Web Pages(2).pdf}
}

@inproceedings{pasupat2015CompositionalSemanticParsing,
  title = {Compositional Semantic Parsing on Semi-Structured Tables},
  booktitle = {{{ACL-IJCNLP}}},
  author = {Pasupat, P. and Liang, P.},
  date = {2015},
  eprint = {1508.00305},
  eprinttype = {arxiv},
  pages = {1470--1480},
  doi = {10.3115/v1/p15-1142},
  url = {http://arxiv.org/abs/1508.00305},
  abstract = {Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: Answering complex questions on semi-structured tables using question-Answer pairs as supervision. The central challenge arises from two compounding factors: The broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.},
  archiveprefix = {arXiv},
  isbn = {978-1-941643-72-3},
  annotation = {277 citations (Semantic Scholar/DOI) [2021-03-26] 277 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WKRLGAKB/Pasupat, Liang - 2015 - Compositional semantic parsing on semi-structured tables(2).pdf}
}

@inproceedings{pasupat2018MappingNaturalLanguage,
  title = {Mapping Natural Language Commands to Web Elements},
  booktitle = {{{EMNLP}}},
  author = {Pasupat, P. and Jiang, T. and Liu, E. and Guu, K. and Liang, P.},
  date = {2018},
  eprint = {1808.09132v2},
  eprinttype = {arxiv},
  pages = {4970--4976},
  doi = {10.18653/v1/d18-1540},
  abstract = {The web provides a rich, open-domain environment with textual, structural, and spatial properties. We propose a new task for grounding language in this environment: given a natural language command (e.g., "click on the second article"), choose the correct element on the web page (e.g., a hyperlink or text box). We collected a dataset of over 50,000 commands that capture various phenomena such as functional references (e.g. "find who made this site"), relational reasoning (e.g. "article by john"), and visual reasoning (e.g. "top-most article"). We also implemented and analyzed three baseline models that capture different phenomena present in the dataset.},
  archiveprefix = {arXiv},
  annotation = {10 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G5NAWKAU/Pasupat et al. - 2018 - Mapping natural language commands to web elements(2).pdf}
}

@inproceedings{patel2022RevisitingCompositionalGeneralization,
  title = {Revisiting the {{Compositional Generalization Abilities}} of {{Neural Sequence Models}}},
  booktitle = {{{ACL}}},
  author = {Patel, Arkil and Bhattamishra, Satwik and Blunsom, Phil and Goyal, Navin},
  date = {2022-03-14},
  eprint = {2203.07402},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.07402},
  urldate = {2022-03-23},
  abstract = {Compositional generalization is a fundamental trait in humans, allowing us to effortlessly combine known phrases to form novel sentences. Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize. In this paper, we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark. We demonstrate that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated. We perform detailed empirical analysis of this phenomenon. Our results indicate that the generalization performance of models is highly sensitive to the characteristics of the training data which should be carefully considered while designing such benchmarks in future.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JYB2VYHU/Patel et al. - 2022 - Revisiting the Compositional Generalization Abilit.pdf;/home/hiaoxui/.local/share/zotero_files/storage/K7PXUWGI/2203.html}
}

@inproceedings{pauls2009ConsensusTrainingConsensus,
  title = {Consensus {{Training}} for {{Consensus Decoding}} in {{Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Pauls, A. and DeNero, J. and Klein, D.},
  date = {2009},
  doi = {10.3115/1699648.1699688},
  isbn = {978-1-932432-63-3},
  keywords = {unread},
  annotation = {31 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C88KRD9B/Pauls, DeNero, Klein - 2009 - Consensus Training for Consensus Decoding in Machine Translation(2).pdf}
}

@inproceedings{pei2019PersonalizedRerankingRecommendation,
  title = {Personalized Re-Ranking for Recommendation},
  booktitle = {{{ACM Conference}} on {{Recommender Systems}}},
  author = {Pei, Changhua and Zhang, Yi and Zhang, Yongfeng and Sun, Fei and Lin, Xiao and Sun, Hanxiao and Wu, Jian and Jiang, Peng and Ge, Junfeng and Ou, Wenwu and Pei, Dan},
  date = {2019-09-10},
  pages = {3--11},
  publisher = {{ACM}},
  location = {{Copenhagen Denmark}},
  doi = {10.1145/3298689.3347000},
  url = {https://dl.acm.org/doi/10.1145/3298689.3347000},
  urldate = {2021-06-25},
  eventtitle = {{{RecSys}} '19: {{Thirteenth ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-6243-6},
  langid = {english},
  keywords = {unread},
  annotation = {31 citations (Semantic Scholar/DOI) [2021-06-25]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LXBL3V5D/Pei et al. - 2019 - Personalized re-ranking for recommendation.pdf}
}

@inproceedings{peng2017AddressingDataSparsity,
  title = {Addressing the {{Data Sparsity Issue}} in {{Neural AMR Parsing}}},
  booktitle = {{{EACL}}},
  author = {Peng, X. and Wang, C. and Gildea, D. and Xue, N.},
  date = {2017},
  volume = {1},
  eprint = {1702.05053},
  eprinttype = {arxiv},
  pages = {366--375},
  url = {http://arxiv.org/abs/1702.05053},
  abstract = {Neural attention models have achieved great success in different NLP tasks. How- ever, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we de- scribe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural atten- tion model and our results are also compet- itive against state-of-the-art systems that do not use extra linguistic resources.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-3860-4},
  keywords = {unread},
  annotation = {52 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AKX8FRGV/Peng et al. - 2017 - Addressing the Data Sparsity Issue in Neural AMR Parsing(2).pdf}
}

@article{peng2017CrossSentenceNaryRelation,
  title = {Cross-{{Sentence N-ary Relation Extraction}} with {{Graph LSTMs}}},
  author = {Peng, N. and Poon, H. and Quirk, C. and Toutanova, K.},
  date = {2017},
  journaltitle = {TACL},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WYI69789/Peng et al. - 2017 - Cross-Sentence N-ary Relation Extraction with Graph LSTMs(2).pdf}
}

@inproceedings{peng2017DeepMultitaskLearning,
  title = {Deep Multitask Learning for Semantic Dependency Parsing},
  booktitle = {{{ACL}}},
  author = {Peng, H. and Thomson, S. and Smith, N. A.},
  date = {2017},
  eprint = {1704.06855},
  eprinttype = {arxiv},
  pages = {2037--2048},
  doi = {10.18653/v1/P17-1186},
  abstract = {We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches-one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at https://github.com/Noahs-ARK/NeurboParser.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-75-3},
  annotation = {112 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RKBA4KZP/Peng, Thomson, Smith - 2017 - Deep multitask learning for semantic dependency parsing(2).pdf}
}

@inproceedings{peng2018BackpropagatingStructuredArgmax,
  title = {Backpropagating through {{Structured Argmax}} Using a {{SPIGOT}}},
  booktitle = {{{ACL}}},
  author = {Peng, H. and Thomson, S. and Smith, N. A.},
  date = {2018},
  eprint = {1805.04658},
  eprinttype = {arxiv},
  pages = {1--11},
  url = {http://arxiv.org/abs/1805.04658},
  abstract = {We introduce the structured projection of intermediate gradients optimization technique (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks (Kim et al., 2017) and some reinforcement learning-inspired solutions (Yogatama et al., 2017). Like so-called straight-through estimators (Hinton, 2012), SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT's proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.},
  archiveprefix = {arXiv},
  annotation = {27 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4YH68767/Peng, Thomson, Smith - 2018 - Backpropagating through Structured Argmax using a SPIGOT(2).pdf}
}

@inproceedings{peng2018ControllableStoryGeneration,
  title = {Towards {{Controllable Story Generation}}},
  booktitle = {{{NAACL}}},
  author = {Peng, N. and Ghazvininejad, M. and May, J. and Knight, K.},
  date = {2018},
  pages = {43--49},
  abstract = {We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence 1 (Egidi and Gerrig, 2009) and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower per-plexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DKM24JBW/Peng et al. - 2018 - Towards Controllable Story Generation(2).pdf}
}

@inproceedings{peng2018LearningJointSemantic,
  title = {Learning {{Joint Semantic Parsers}} from {{Disjoint Data}}},
  booktitle = {{{NAACL}}},
  author = {Peng, H. and Thomson, S. and Swayamdipta, S. and Smith, N. A.},
  date = {2018},
  eprint = {1804.05990},
  eprinttype = {arxiv},
  pages = {1492--1502},
  doi = {10.18653/v1/n18-1135},
  abstract = {We present a new approach to learning semantic parsers from multiple datasets, even when the target semantic formalisms are drastically different, and the underlying corpora do not overlap. We handle such "disjoint" data by treating annotations for unobserved formalisms as latent structured variables. Building on state-of-the-art baselines, we show improvements both in frame-semantic parsing and semantic dependency parsing by modeling them jointly.},
  archiveprefix = {arXiv},
  annotation = {39 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2ISSUKKG/Peng et al. - 2018 - Learning Joint Semantic Parsers from Disjoint Data(2).pdf}
}

@inproceedings{peng2019PaLMHybridParser,
  title = {{{PaLM}}: {{A Hybrid Parser}} and {{Language Model}}},
  booktitle = {{{EMNLP}}},
  author = {Peng, H. and Schwartz, R. and Smith, N. A.},
  date = {2019},
  eprint = {1909.02134},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.02134},
  abstract = {We present PaLM, a hybrid parser and neural language model. Building on an RNN language model, PaLM adds an attention layer over text spans in the left context. An unsupervised constituency parser can be derived from its attention weights, using a greedy decoding algorithm. We evaluate PaLM on language modeling, and empirically show that it outperforms strong baselines. If syntactic annotations are available, the attention component can be trained in a supervised manner, providing syntactically-informed representations of the context, and further improving language modeling performance.},
  archiveprefix = {arXiv},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VEVDRUXH/Peng, Schwartz, Smith - 2019 - PaLM A Hybrid Parser and Language Model(2).pdf}
}

@inproceedings{peng2020RationalRecurrences,
  title = {Rational Recurrences},
  booktitle = {{{EMNLP}}},
  author = {Peng, H. and Schwartz, R. and Thomson, S. and Smith, N. A.},
  date = {2020},
  eprint = {1808.09357},
  eprinttype = {arxiv},
  pages = {1203--1214},
  doi = {10.18653/v1/d18-1152},
  abstract = {Despite the tremendous empirical success of neural models in natural language processing, many of them lack the strong intuitions that accompany classical machine learning approaches. Recently, connections have been shown between convolutional neural networks (CNNs) and weighted finite state automata (WFSAs), leading to new interpretations and insights. In this work, we show that some recurrent neural networks also share this connection to WFSAs. We characterize this connection formally, defining rational recurrences to be recurrent hidden state update functions that can be written as the Forward calculation of a finite set of WFSAs. We show that several recent neural models use rational recurrences. Our analysis provides a fresh view of these models and facilitates devising new neural architectures that draw inspiration from WFSAs. We present one such model, which performs better than two recent baselines on language modeling and text classification. Our results demonstrate that transferring intuitions from classical models like WFSAs can be an effective approach to designing and understanding neural models.},
  archiveprefix = {arXiv},
  annotation = {22 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z4ADHFF4/Peng et al. - 2020 - Rational recurrences(2).pdf}
}

@misc{peng2021ABCAttentionBoundedmemory,
  title = {{{ABC}}: {{Attention}} with {{Bounded-memory Control}}},
  shorttitle = {{{ABC}}},
  author = {Peng, Hao and Kasai, Jungo and Pappas, Nikolaos and Yogatama, Dani and Wu, Zhaofeng and Kong, Lingpeng and Schwartz, Roy and Smith, Noah A.},
  date = {2021-10-05},
  eprint = {2110.02488},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.02488},
  urldate = {2021-10-16},
  abstract = {Transformer architectures have achieved state-of-the-art results on a variety of sequence modeling tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights--an established approach (Wang et al., 2020b) previously thought to be not applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NPXMIHY9/Peng et al. - 2021 - ABC Attention with Bounded-memory Control.pdf;/home/hiaoxui/.local/share/zotero_files/storage/36UU49M2/2110.html}
}

@inproceedings{peng2021RandomFeatureAttention,
  title = {Random {{Feature Attention}}},
  booktitle = {{{ICLR}}},
  author = {Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A. and Kong, Lingpeng},
  date = {2021-03-19},
  eprint = {2103.02143},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.02143},
  urldate = {2021-03-26},
  abstract = {Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.},
  archiveprefix = {arXiv},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JBY7GGYU/Peng et al. - 2021 - Random Feature Attention.pdf;/home/hiaoxui/.local/share/zotero_files/storage/PEMLB8XB/2103.html}
}

@inproceedings{perez-beltrachini2018BootstrappingGeneratorsNoisy,
  title = {Bootstrapping {{Generators}} from {{Noisy Data}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Perez-Beltrachini, L. and Lapata, M.},
  date = {2018},
  eprint = {1804.06385},
  eprinttype = {arxiv},
  pages = {1516--1527},
  url = {http://arxiv.org/abs/1804.06385},
  abstract = {A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and associated texts. In this paper we aim to bootstrap generators from large scale datasets where the data (e.g., DBPedia facts) and related texts (e.g., Wikipedia abstracts) are loosely aligned. We tackle this challenging task by introducing a special-purpose content selection mechanism. We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {24 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XPAGMY8Q/Perez-Beltrachini, Lapata - 2018 - Bootstrapping Generators from Noisy Data(2).pdf}
}

@inproceedings{perez-mayos2021HowMuchPretraining,
  title = {How Much Pretraining Data Do Language Models Need to Learn Syntax?},
  booktitle = {{{EMNLP}}},
  author = {Pérez-Mayos, Laura and Ballesteros, Miguel and Wanner, Leo},
  date = {2021},
  pages = {12},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L9NAFLGR/Pérez-Mayos et al. - How much pretraining data do language models need .pdf}
}

@inproceedings{peters2018DeepContextualizedWord,
  title = {Deep Contextualized Word Representations},
  booktitle = {{{NAACL}}},
  author = {Peters, M. E. and Neumann, M. and Iyyer, M. and Gardner, M. and Clark, C. and Lee, K. and Zettlemoyer, L. S.},
  date = {2018},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.05365},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  annotation = {5314 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L3AN34WN/Peters et al. - 2018 - Deep contextualized word representations(2).pdf}
}

@inproceedings{peters2018DissectingContextualWord,
  title = {Dissecting {{Contextual Word Embeddings}}: {{Architecture}} and {{Representation}}},
  shorttitle = {Dissecting {{Contextual Word Embeddings}}},
  booktitle = {{{EMNLP}}},
  author = {Peters, Matthew E. and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  date = {2018-09-27},
  eprint = {1808.08949},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1808.08949},
  urldate = {2020-11-19},
  abstract = {Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {183 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NMAF59QS/Peters et al. - 2018 - Dissecting Contextual Word Embeddings Architectur.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7I5I2N99/1808.html}
}

@inproceedings{peters2019KnowledgeEnhancedContextual,
  title = {Knowledge {{Enhanced Contextual Word Representations}}},
  booktitle = {{{EMNLP}}},
  author = {Peters, M. E. and Neumann, M. and Logan, R. and Schwartz, R. and Joshi, V. and Singh, S. and Smith, N. A.},
  date = {2019},
  eprint = {1909.04164},
  eprinttype = {arxiv},
  pages = {43--54},
  doi = {10.18653/v1/d19-1005},
  abstract = {Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert's runtime is comparable to BERT's and it scales to large KBs.},
  archiveprefix = {arXiv},
  annotation = {142 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/98ZDBFLI/Peters et al. - 2019 - Knowledge Enhanced Contextual Word Representations(2).pdf}
}

@inproceedings{petroni2019LanguageModelsKnowledge,
  title = {Language {{Models}} as {{Knowledge Bases}}?},
  booktitle = {{{EMNLP}}},
  author = {Petroni, F. and Rocktäschel, T. and Lewis, P. and Bakhtin, A. and Wu, Y. and Miller, A. H. and Riedel, S.},
  date = {2019},
  eprint = {1909.01066},
  eprinttype = {arxiv},
  abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I9N7SNE6/Petroni et al. - 2019 - Language Models as Knowledge Bases(2).pdf}
}

@inproceedings{petrov2006LearningAccurateCompact,
  title = {Learning Accurate, Compact, and Interpretable Tree Annotation},
  booktitle = {{{ACL}}},
  author = {Petrov, S. and Barrett, L. and Thibaux, R. and Klein, D.},
  date = {2006},
  pages = {433--440},
  doi = {10.3115/1220175.1220230},
  url = {http://portal.acm.org/citation.cfm?doid=1220175.1220230},
  abstract = {We present an automatic approach to tree annota- tion in which basic nonterminal symbols are alter- nately split and merged to maximize the likelihood of a training treebank. Starting with a simple X- bar grammar, we learn a new grammar whose non- terminals are subsymbols of the original nontermi- nals. In contrast with previous work, we are able to split various terminals to different degrees, as ap- propriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more ac- curate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2\% on the Penn Treebank, higher than fully lexicalized systems.},
  isbn = {1-932432-65-5},
  issue = {July},
  annotation = {943 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PTZWAUA5/Petrov et al. - 2006 - Learning accurate, compact, and interpretable tree annotation(2).pdf}
}

@inproceedings{petrov2007ImprovedInferenceUnlexicalized,
  title = {Improved {{Inference}} for {{Unlexicalized Parsing}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Petrov, S. and Klein, D.},
  date = {2007},
  pages = {404--411},
  doi = {10.3115/1614164},
  abstract = {We present several improvements to unlexicalized parsing with hierarchically\textbackslash nstate-split PCFGs. First, we present a novel coarse-to-fine method\textbackslash nin which a grammar’s own hierarchical projections are used for incremental\textbackslash npruning, including a method for efficiently computing projections\textbackslash nof a grammar without a treebank. In our experiments, hierarchical\textbackslash npruning greatly accelerates parsing with no loss in empirical accuracy.\textbackslash nSecond, we compare various inference procedures for state-split PCFGs\textbackslash nfrom the standpoint of risk minimization, paying particular attention\textbackslash nto their practical tradeoffs. Finally, we present multilingual experiments\textbackslash nwhich show that parsing with hierarchical state-splitting is fast\textbackslash nand accurate in multiple languages and domains, even without any\textbackslash nlanguage-specific tuning.},
  issue = {April},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AI35RRBP/Petrov, Klein - 2007 - Improved Inference for Unlexicalized Parsing(2).pdf}
}

@inproceedings{piech2015LearningProgramEmbeddings,
  title = {Learning {{Program Embeddings}} to {{Propagate Feedback}} on {{Student Code}}},
  booktitle = {{{ICML}}},
  author = {Piech, C. and Huang, J. and Nguyen, A. and Phulsuksombati, M. and Sahami, M. and Guibas, L.},
  date = {2015},
  volume = {37},
  eprint = {1505.05969},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1505.05969},
  abstract = {Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University's CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.},
  archiveprefix = {arXiv},
  isbn = {9781510810587 (ISBN)},
  annotation = {112 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6D6J4ZHQ/Piech et al. - 2015 - Learning Program Embeddings to Propagate Feedback on Student Code(2).pdf}
}

@incollection{pierrehumberl1990MeaningIntonationalContours,
  title = {The {{Meaning}} of {{Intonational Contours}} in the {{Interpretation}} of {{Discourse}}},
  booktitle = {Intentions in {{Communication}}},
  author = {Pierrehumberl, J. and Hirschberg, J.},
  date = {1990},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LZALXJRC/Pierrehumberl, Hirschberg - 1990 - The Meaning of Intonational Contours in the Interpretation of Discourse(2).pdf}
}

@inproceedings{pimentel2020InformationTheoreticProbingLinguistic,
  title = {Information-{{Theoretic Probing}} for {{Linguistic Structure}}},
  booktitle = {{{ACL}}},
  author = {Pimentel, T. and Valvoda, J. and Maudslay, R. H. and Zmigrod, R. and Williams, A. and Cotterell, R.},
  date = {2020},
  eprint = {2004.03061},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.03061},
  abstract = {The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually ``know'' about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network's learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research---plus English---totalling eleven languages.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {36 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9S3N2XS4/Pimentel et al. - 2020 - Information-Theoretic Probing for Linguistic Structure(2).pdf}
}

@inproceedings{pitler2008EasilyIdentifiableDiscourse,
  title = {Easily {{Identifiable Discourse Relations}}},
  booktitle = {{{COLING}}},
  author = {Pitler, E. and Raghupathy, M. and Mehta, H. and Nenkova, A. and Lee, A. and Joshi, A.},
  date = {2008},
  pages = {87--90},
  url = {http://www.aclweb.org/anthology/C08-2022},
  abstract = {Knott (1996) provides an extensive of connectives and their properties13 It is important to select a word with some syntactic mo- tivation to an argument span, but due to the lack of consistent alignment between syntax and , we must},
  isbn = {978-1-905593-44-6},
  issue = {June},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MQYGF2JM/Pitler et al. - 2008 - Easily Identifiable Discourse Relations(2).pdf}
}

@inproceedings{poliak2018CollectingDiverseNatural,
  title = {Collecting {{Diverse Natural Language Inference Problems}} for {{Sentence Representation Evaluation}}},
  booktitle = {{{EMNLP}}},
  author = {Poliak, A. and Haldar, A. and Rudinger, R. and Hu, J. E. and Pavlick, E. and White, A. S. and Van Durme, B.},
  date = {2018},
  eprint = {1804.08207v2},
  eprinttype = {arxiv},
  pages = {67--81},
  doi = {10.18653/v1/D18-1007},
  url = {http://aclweb.org/anthology/D18-1007},
  abstract = {We present a large-scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation captures distinct types of reasoning. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. We refer to our collection as the DNC: Diverse Natural Language Inference Collection. The DNC is available online at https://www.decomp.net, and will grow over time as additional resources are recast and added from novel sources.},
  archiveprefix = {arXiv},
  annotation = {71 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/488X6QRK/Poliak et al. - 2018 - Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation(2).pdf}
}

@inproceedings{poliak2018EvaluationSemanticPhenomena,
  title = {On the {{Evaluation}} of {{Semantic Phenomena}} in {{Neural Machine Translation Using Natural Language Inference}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Poliak, A. and Belinkov, Y. and Glass, J. and Van Durme, B.},
  date = {2018},
  eprint = {1804.09779},
  eprinttype = {arxiv},
  pages = {513--523},
  url = {http://arxiv.org/abs/1804.09779},
  abstract = {We propose a process for investigating the extent to which sentence representations arising from neural machine translation (NMT) systems encode distinct semantic phenomena. We use these representations as features to train a natural language inference (NLI) classifier based on datasets recast from existing semantic annotations. In applying this process to a representative NMT system, we find its encoder appears most suited to supporting inferences at the syntax-semantics interface, as compared to anaphora resolution requiring world-knowledge. We conclude with a discussion on the merits and potential deficiencies of the existing process, and how it may be improved and extended as a broader framework for evaluating semantic coverage.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {26 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4G2DHLC2/Poliak et al. - 2018 - On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference(2).pdf}
}

@inproceedings{poliak2018HypothesisOnlyBaselines,
  title = {Hypothesis {{Only Baselines}} in {{Natural Language Inference}}},
  booktitle = {Joint {{Conference}} on {{Lexical}} and {{Computational Semantics}}},
  author = {Poliak, A. and Naradowsky, J. and Haldar, A. and Rudinger, R. and Van Durme, B.},
  date = {2018},
  eprint = {1805.01042v1},
  eprinttype = {arxiv},
  pages = {180--191},
  doi = {10.18653/v1/s18-2023},
  archiveprefix = {arXiv},
  annotation = {211 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R2NHZSUD/Poliak et al. - 2018 - Hypothesis Only Baselines in Natural Language Inference(2).pdf}
}

@misc{poliak2020SurveyRecognizingTextual,
  title = {A {{Survey}} on {{Recognizing Textual Entailment}} as an {{NLP Evaluation}}},
  author = {Poliak, Adam},
  date = {2020},
  abstract = {Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the reasoning capabilities of NLP systems. We then focus our discussion on RTE by highlighting prominent RTE datasets as well as advances in RTE dataset that focus on specific linguistic phenomena that can be used to evaluate NLP systems on a fine-grained level. We conclude by arguing that when evaluating NLP systems, the community should utilize newly introduced RTE datasets that focus on specific linguistic phenomena.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WT5YJ6PE/Poliak - A Survey on Recognizing Textual Entailment as an N.pdf}
}

@inproceedings{ponti2019CrosslingualSemanticSpecialization,
  title = {Cross-Lingual {{Semantic Specialization}} via {{Lexical Relation Induction}}},
  booktitle = {{{EMNLP}}},
  author = {Ponti, E. M. and Vuli, I. and Glavas, G. and Reichart, R. and Korhonen, A.},
  date = {2019},
  pages = {2206--2217},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YWQR6FID/Ponti et al. - 2019 - Cross-lingual Semantic Specialization via Lexical Relation Induction(2).pdf}
}

@inproceedings{poon2009UnsupervisedSemanticParsing,
  title = {Unsupervised Semantic Parsing},
  booktitle = {{{EMNLP}}},
  author = {Poon, H. and Domingos, P.},
  date = {2009},
  volume = {1},
  pages = {1--10},
  doi = {10.3115/1699510.1699512},
  abstract = {We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.},
  issue = {August},
  annotation = {281 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CRIUFK2J/Poon, Domingos - 2009 - Unsupervised semantic parsing(2).pdf}
}

@inproceedings{poon2013GroundedUnsupervisedSemantic,
  title = {Grounded {{Unsupervised Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Poon, H.},
  date = {2013},
  number = {2010},
  pages = {933--943},
  doi = {10.3115/1699510.1699512},
  abstract = {We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM. To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84\%, effectively tying with the best published results by supervised approaches.},
  annotation = {281 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QJA4KFXR/Poon - 2013 - Grounded Unsupervised Semantic Parsing(2).pdf}
}

@inproceedings{poria2015DeepConvolutionalNeural,
  title = {Deep {{Convolutional Neural Network Textual Features}} and {{Multiple Kernel Learning}} for {{Utterance-Level Multimodal Sentiment Analysis}}},
  booktitle = {{{EMNLP}}},
  author = {Poria, S. and Cambria, E. and Gelbukh, A.},
  date = {2015},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QHZ6UU9B/Poria, Cambria, Gelbukh - 2015 - Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-Level (2).pdf}
}

@article{povey2011KaldiSpeechRecognition,
  title = {The {{Kaldi}} Speech Recognition Toolkit},
  author = {Povey, D. and Ghoshal, A. and Boulianne, G. and Burget, L. and Glembek, O. and Goel, N. and Hannemann, M. and Motlicek, P. and Qian, Y. and Schwarz, P. and Silovsky, J. and Stemmer, G. and Vesely, K.},
  date = {2011},
  journaltitle = {ASRU},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {1--4},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.},
  archiveprefix = {arXiv},
  isbn = {978-1-4673-0366-8},
  keywords = {unread},
  annotation = {1013 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KHMIEECW/Povey et al. - 2011 - The Kaldi speech recognition toolkit(2).pdf}
}

@article{power2012GeneratingNumericalApproximations,
  title = {Generating {{Numerical Approximations}}},
  author = {Power, R. and Williams, S.},
  date = {2012},
  journaltitle = {Computational Linguistics},
  volume = {38},
  number = {1},
  pages = {113--134},
  issn = {0891-2017},
  doi = {10.1162/COLI_a_00086},
  url = {http://www.mitpressjournals.org/doi/10.1162/COLI_a_00086},
  abstract = {We describe a computational model for planning phrases like “more than a quarter” and “25.9 per cent” which describe proportions at different levels of precision. The model lays out the key choices in planning a numerical description, using formal definitions of mathematical form (e.g., the distinction between fractions and percentages) and roundness adapted from earlier studies. The task is modeled as a constraint satisfaction problem, with solutions subsequently ranked by preferences (e.g., for roundness). Detailed constraints are based on a corpus of numerical expressions collected in the NUMGEN project, and evaluated through empirical studies in which subjects were asked to produce (or complete) numerical expressions in specified contexts.},
  annotation = {22 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T75GZWYL/Power, Williams - 2012 - Generating Numerical Approximations(2).pdf}
}

@inproceedings{pradhan2012CoNLL2012SharedTask,
  title = {{{CoNLL-2012 Shared Task}}: {{Modeling Multilingual Unrestricted Coreference}} in {{OntoNotes}}},
  booktitle = {{{CoNLL}}},
  author = {Pradhan, Sameer and Moschitti, Alessandro and Xue, Nianwen and Uryupina, Olga and Zhang, Yuchen},
  date = {2012},
  pages = {40},
  abstract = {The CoNLL-2012 shared task involved predicting coreference in English, Chinese, and Arabic, using the final version, v5.0, of the OntoNotes corpus. It was a follow-on to the English-only task organized in 2011. Until the creation of the OntoNotes corpus, resources in this sub-field of language processing were limited to noun phrase coreference, often on a restricted set of entities, such as the ACE entities. OntoNotes provides a largescale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types, and covers multiple languages. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, evaluation criteria, and presents and discusses the results achieved by the participating systems. The task of coreference has had a complex evaluation history. Potentially many evaluation conditions, have, in the past, made it difficult to judge the improvement in new algorithms over previously reported results. Having a standard test set and standard evaluation parameters, all based on a resource that provides multiple integrated annotation layers (syntactic parses, semantic roles, word senses, named entities and coreference) and in multiple languages could support joint modeling and help ground and energize ongoing research in the task of entity and event coreference.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NXI83F88/Pradhan et al. - CoNLL-2012 Shared Task Modeling Multilingual Unre.pdf}
}

@inproceedings{pradhan2013RobustLinguisticAnalysis,
  title = {Towards {{Robust Linguistic Analysis}} Using {{OntoNotes}}},
  booktitle = {{{CoNLL}}},
  author = {Pradhan, Sameer and Moschitti, Alessandro and Xue, Nianwen and Ng, Hwee Tou and Bjorkelund, Anders and Uryupina, Olga and Zhang, Yuchen and Zhong, Zhi},
  date = {2013},
  pages = {10},
  abstract = {Large-scale linguistically annotated corpora have played a crucial role in advancing the state of the art of key natural language technologies such as syntactic, semantic and discourse analyzers, and they serve as training data as well as evaluation benchmarks. Up till now, however, most of the evaluation has been done on monolithic corpora such as the Penn Treebank, the Proposition Bank. As a result, it is still unclear how the state-of-the-art analyzers perform in general on data from a variety of genres or domains. The completion of the OntoNotes corpus, a large-scale, multi-genre, multilingual corpus manually annotated with syntactic, semantic and discourse information, makes it possible to perform such an evaluation. This paper presents an analysis of the performance of publicly available, state-of-the-art tools on all layers and languages in the OntoNotes v5.0 corpus. This should set the benchmark for future development of various NLP components in syntax and semantics, and possibly encourage research towards an integrated system that makes use of the various layers jointly to improve overall performance.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YZELAYYH/Pradhan et al. - Towards Robust Linguistic Analysis using OntoNotes.pdf}
}

@inproceedings{press2020ImprovingTransformerModels,
  title = {Improving {{Transformer Models}} by {{Reordering}} Their {{Sublayers}}},
  booktitle = {{{ACL}}},
  author = {Press, Ofir and Smith, Noah A. and Levy, Omer},
  date = {2020},
  pages = {2996--3005},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.270},
  url = {https://www.aclweb.org/anthology/2020.acl-main.270},
  urldate = {2020-09-29},
  eventtitle = {{{ACL}}},
  langid = {english},
  keywords = {unread},
  annotation = {14 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7T4UB7AG/Press et al. - 2020 - Improving Transformer Models by Reordering their S.pdf}
}

@inproceedings{press2021ShortformerBetterLanguage,
  title = {Shortformer: {{Better Language Modeling}} Using {{Shorter Inputs}}},
  shorttitle = {Shortformer},
  booktitle = {{{ACL}}},
  author = {Press, Ofir and Smith, Noah A. and Lewis, Mike},
  date = {2021-06-02},
  eprint = {2012.15832},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.15832},
  urldate = {2022-02-10},
  abstract = {Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D9JRVZ6V/Press et al. - 2021 - Shortformer Better Language Modeling using Shorte.pdf;/home/hiaoxui/.local/share/zotero_files/storage/WHKD3RX5/2012.html}
}

@inproceedings{pseudoToken2022,
  title = {A {{Sentence}} Is {{Worth}} 128 {{Pseudo Tokens}}: {{A Semantic-Aware Contrastive Learning Framework}} for {{Sentence Embeddings}}},
  shorttitle = {A {{Sentence}} Is {{Worth}} 128 {{Pseudo Tokens}}},
  booktitle = {{{ACL}}},
  author = {Tan, Haochen and Shao, Wei and Wu, Han and Yang, Ke and Song, Linqi},
  date = {2022-03-11},
  eprint = {2203.05877},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.05877},
  urldate = {2022-03-23},
  abstract = {Contrastive learning has shown great potential in unsupervised sentence embedding tasks, e.g., SimCSE. However, We find that these existing solutions are heavily affected by superficial features like the length of sentences or syntactic structures. In this paper, we propose a semantics-aware contrastive learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT), which is able to exploit the pseudo-token space (i.e., latent semantic space) representation of a sentence while eliminating the impact of superficial features such as sentence length and syntax. Specifically, we introduce an additional pseudo token embedding layer independent of the BERT encoder to map each sentence into a sequence of pseudo tokens in a fixed length. Leveraging these pseudo sequences, we are able to construct same-length positive and negative pairs based on the attention mechanism to perform contrastive learning. In addition, we utilize both the gradient-updating and momentum-updating encoders to encode instances while dynamically maintaining an additional queue to store the representation of sentence embeddings, enhancing the encoder's learning performance for negative examples. Experiments show that our model outperforms the state-of-the-art baselines on six standard semantic textual similarity (STS) tasks. Furthermore, experiments on alignments and uniformity losses, as well as hard examples with different sentence lengths and syntax, consistently verify the effectiveness of our method.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JB7INPZ5/Tan et al. - 2022 - A Sentence is Worth 128 Pseudo Tokens A Semantic-.pdf;/home/hiaoxui/.local/share/zotero_files/storage/8PGQ52KJ/2203.html}
}

@article{pustejovsky2003TIMEBANKCorpus,
  title = {The {{TIMEBANK Corpus}}},
  author = {Pustejovsky, J. and Hanks, P. and Sauri, R. and See, A. and Gaizauskas, R. and Setzer, A. and Sundheim, B. and Radev, D. and Day, D. and Ferro, L. and Lazo, M.},
  date = {2003},
  journaltitle = {Corpus Linguistics},
  volume = {2003},
  pages = {647--656},
  issn = {0302-9743},
  doi = {10.1007/978-3-540-73351-5},
  url = {http://www.springerlink.com/content/c9313110264107m6},
  abstract = {The application of the multilingual knowledge encoded in Wikipedia to an open–domain Cross–Lingual Question Answering system\textbackslash n based on the Inter Lingual Index (ILI) module of EuroWordNet is proposed and evaluated. This strategy overcomes the problems\textbackslash n due to ILI’s low coverage on proper nouns (Named Entities). Moreover, as these are open class words (highly changing), using\textbackslash n a community–based up–to–date resource avoids the tedious maintenance of hand–coded bilingual dictionaries. A study reveals\textbackslash n the importance to translate Named Entities in CL–QA and the advantages of relying on Wikipedia over ILI for doing this. Tests\textbackslash n on questions from the Cross–Language Evaluation Forum (CLEF) justify our approach (20\% of these are correctly answered thanks\textbackslash n to Wikipedia’s Multilingual Knowledge).},
  isbn = {978-3-540-73350-8},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5BPCN5BD/Pustejovsky et al. - 2003 - The TIMEBANK Corpus(2).pdf}
}

@inproceedings{pyatkin2021AskingItAll,
  title = {Asking {{It All}}: {{Generating Contextualized Questions}} for Any {{Semantic Role}}},
  booktitle = {{{EMNLP}}},
  author = {Pyatkin, Valentina and Roit, Paul and Michael, Julian and Goldberg, Yoav and Tsarfaty, Reut and Dagan, Ido},
  date = {2021},
  pages = {13},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UIMSI763/Pyatkin et al. - Asking It All Generating Contextualized Questions.pdf}
}

@inproceedings{qi2019AnsweringComplexOpendomain,
  title = {Answering {{Complex Open-domain Questions Through Iterative Query Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Qi, P. and Lin, X. and Mehr, L. and Wang, Z. and Manning, C. D.},
  date = {2019},
  eprint = {1910.07000},
  eprinttype = {arxiv},
  pages = {2590--2602},
  doi = {10.18653/v1/d19-1261},
  abstract = {It is challenging for current one-step retrieve-and-read question answering (QA) systems to answer questions like "Which novel by the author of 'Armada' will be adapted as a feature film by Steven Spielberg?" because the question seldom contains retrievable clues about the missing entity (here, the author). Answering such a question requires multi-hop reasoning where one must gather information about the missing entity (or facts) to proceed with further reasoning. We present GoldEn (Gold Entity) Retriever, which iterates between reading context and retrieving more supporting documents to answer open-domain multi-hop questions. Instead of using opaque and computationally expensive neural retrieval models, GoldEn Retriever generates natural language search queries given the question and available context, and leverages off-the-shelf information retrieval systems to query for missing entities. This allows GoldEn Retriever to scale up efficiently for open-domain multi-hop reasoning while maintaining interpretability. We evaluate GoldEn Retriever on the recently proposed open-domain multi-hop QA dataset, HotpotQA, and demonstrate that it outperforms the best previously published model despite not using pretrained language models such as BERT.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {38 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YAKEXRA7/Qi et al. - 2019 - Answering Complex Open-domain Questions Through Iterative Query Generation(2).pdf}
}

@inproceedings{qin2018LearningLatentSemantic,
  title = {Learning {{Latent Semantic Annotations}} for {{Grounding Natural Language}} to {{Structured Data}}},
  booktitle = {{{EMNLP}}},
  author = {Qin, G. and Yao, J. and Wang, X. and Wang, J. and Lin, C.},
  date = {2018},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TY6VJAR7/Qin et al. - 2018 - Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data(2).pdf}
}

@inproceedings{qin2021BackFutureUnsupervised,
  title = {Back to the {{Future}}: {{Unsupervised Backprop-based Decoding}} for {{Counterfactual}} and {{Abductive Commonsense Reasoning}}},
  shorttitle = {Back to the {{Future}}},
  booktitle = {{{EMNLP}}},
  author = {Qin, Lianhui and Shwartz, Vered and West, Peter and Bhagavatula, Chandra and Hwang, Jena and Bras, Ronan Le and Bosselut, Antoine and Choi, Yejin},
  date = {2021-08-02},
  eprint = {2010.05906},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.05906},
  urldate = {2022-04-08},
  abstract = {Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling. In this paper, we propose DeLorean, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision. The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters. By alternating between forward and backward propagation, DeLorean can decode the output representation that reflects both the left and right contexts. We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text generation and counterfactual story revision, where DeLorean outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QVYQZ6QI/Qin et al. - 2021 - Back to the Future Unsupervised Backprop-based De.pdf;/home/hiaoxui/.local/share/zotero_files/storage/MIQTV89S/2010.html}
}

@inproceedings{qin2022KnowledgeInheritancePretrained,
  title = {Knowledge {{Inheritance}} for {{Pre-trained Language Models}}},
  booktitle = {{{NAACL}}},
  author = {Qin, Yujia and Lin, Yankai and Yi, Jing and Zhang, Jiajie and Han, Xu and Zhang, Zhengyan and Su, Yusheng and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  date = {2022-04-26},
  eprint = {2105.13880},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.13880},
  urldate = {2022-07-15},
  abstract = {Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing PLMs benefit training large-scale PLMs in future. Specifically, we introduce a pre-training framework named "knowledge inheritance" (KI) and explore how could knowledge distillation serve as auxiliary supervision during pre-training to efficiently learn larger PLMs. Experimental results demonstrate the superiority of KI in training efficiency. We also conduct empirical analyses to explore the effects of teacher PLMs' pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI could be applied to domain adaptation and knowledge transfer.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WSWG7VLE/Qin et al. - 2022 - Knowledge Inheritance for Pre-trained Language Mod.pdf;/home/hiaoxui/.local/share/zotero_files/storage/WHKWHKSW/2105.html}
}

@misc{qin2022NLPTaskEffectiveness,
  title = {The {{NLP Task Effectiveness}} of {{Long-Range Transformers}}},
  author = {Qin, Guanghui and Feng, Yukun and Van Durme, Benjamin},
  date = {2022-02-15},
  eprint = {2202.07856},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.07856},
  urldate = {2022-02-18},
  abstract = {Transformer models cannot easily scale to long sequences due to their O(N\^2) time and space complexity. This has led to Transformer variants seeking to lessen computational complexity, such as Longformer and Performer. While such models have theoretically greater efficiency, their effectiveness on real NLP tasks has not been well studied. We benchmark 7 variants of Transformer models on 5 difficult NLP tasks and 7 datasets. We design experiments to isolate the effect of pretraining and hyperparameter settings, to focus on their capacity for long-range attention. Moreover, we present various methods to investigate attention behaviors, to illuminate model details beyond metric scores. We find that attention of long-range transformers has advantages on content selection and query-guided decoding, but they come with previously unrecognized drawbacks such as insufficient attention to distant tokens.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X6DNFHMF/Qin et al. - 2022 - The NLP Task Effectiveness of Long-Range Transform.pdf;/home/hiaoxui/.local/share/zotero_files/storage/PW4RJVSW/2202.html}
}

@inproceedings{qiu2020BlockwiseSelfAttentionLong,
  title = {Blockwise {{Self-Attention}} for {{Long Document Understanding}}},
  booktitle = {{{EMNLP}}},
  author = {Qiu, Jiezhong and Ma, Hao and Levy, Omer and Yih, Scott Wen-tau and Wang, Sinong and Tang, Jie},
  date = {2020-11-01},
  eprint = {1911.02972},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.02972},
  urldate = {2021-03-27},
  abstract = {We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1\% less memory and 12.0-25.1\% less time to learn the model. During testing, BlockBERT saves 27.8\% inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.},
  archiveprefix = {arXiv},
  annotation = {20 citations (Semantic Scholar/arXiv) [2021-03-27]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M2VWP3NT/Qiu et al. - 2020 - Blockwise Self-Attention for Long Document Underst.pdf;/home/hiaoxui/.local/share/zotero_files/storage/4NK757M2/1911.html}
}

@inproceedings{quirk2015LanguageCodeLearning,
  title = {Language to {{Code}}: {{Learning Semantic Parsers}} for {{If-This-Then-That Recipes}}},
  booktitle = {{{ACL-IJCNLP}}},
  author = {Quirk, C. and Mooney, R. and Galley, M.},
  date = {2015},
  pages = {878--888},
  doi = {10.3115/v1/P15-1085},
  abstract = {Using natural language to write programs is a touchstone problem for computational linguistics. We present an approach that learns to map natural-language descrip-tions of simple " if-then " rules to executable code. By training and testing on a large cor-pus of naturally-occurring programs (called " recipes ") and their natural language de-scriptions, we demonstrate the ability to effectively map language to code. We compare a number of semantic parsing ap-proaches on the highly noisy training data collected from ordinary users, and find that loosely synchronous systems perform best.},
  isbn = {978-1-941643-72-3},
  keywords = {unread},
  annotation = {113 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CRNK63Z6/Quirk, Mooney, Galley - 2015 - Language to Code Learning Semantic Parsers for If-This-Then-That Recipes(2).pdf}
}

@inproceedings{quirk2017DistantSupervisionRelation,
  title = {Distant {{Supervision}} for {{Relation Extraction}} beyond the {{Sentence Boundary}}},
  booktitle = {{{EACL}}},
  author = {Quirk, Chris and Poon, Hoifung},
  date = {2017-08-14},
  eprint = {1609.04873},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1609.04873},
  urldate = {2021-03-03},
  abstract = {The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross- sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.},
  archiveprefix = {arXiv},
  annotation = {108 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YTYFPEUJ/Quirk and Poon - 2017 - Distant Supervision for Relation Extraction beyond.pdf;/home/hiaoxui/.local/share/zotero_files/storage/FAL28947/1609.html}
}

@article{r.b.2003KnowledgeActionFrame,
  title = {Knowledge, {{Action}}, and the {{Frame Problem}}},
  author = {R. B., Scherl and H. J., Levesque},
  date = {2003},
  journaltitle = {Artificial Intelligence},
  volume = {144},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NDC5HJCK/aij-frame.pdf}
}

@misc{rabanser2017IntroductionTensorDecompositions,
  title = {Introduction to {{Tensor Decompositions}} and Their {{Applications}} in {{Machine Learning}}},
  author = {Rabanser, S. and Shchur, O. and Günnemann, S.},
  date = {2017},
  eprint = {1711.10781},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.10781},
  abstract = {Tensors are multidimensional arrays of numerical values and therefore generalize matrices to multiple dimensions. While tensors first emerged in the psychometrics community in the \$20\^\{\textbackslash text\{th\}\}\$ century, they have since then spread to numerous other disciplines, including machine learning. Tensors and their decompositions are especially beneficial in unsupervised learning settings, but are gaining popularity in other sub-disciplines like temporal and multi-relational data analysis, too. The scope of this paper is to give a broad overview of tensors, their decompositions, and how they are used in machine learning. As part of this, we are going to introduce basic tensor concepts, discuss why tensors can be considered more rigid than matrices with respect to the uniqueness of their decomposition, explain the most important factorization algorithms and their properties, provide concrete examples of tensor decomposition applications in machine learning, conduct a case study on tensor-based estimation of mixture models, talk about the current state of research, and provide references to available software libraries.},
  archiveprefix = {arXiv},
  annotation = {90 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NBZRAQDM/Rabanser, Shchur, Günnemann - 2017 - Introduction to Tensor Decompositions and their Applications in Machine Learning(2).pdf}
}

@article{rabiner2019TutorialHiddenMarkov,
  title = {A {{Tutorial}} on {{Hidden Markov Models}} and {{Selected Applications}} in {{Speech Recognition}}},
  author = {Rabiner, L. R.},
  date = {2019},
  journaltitle = {IEEE},
  volume = {77},
  number = {2},
  eprint = {18626},
  eprinttype = {pmid},
  pages = {257--286},
  issn = {00189219},
  doi = {10.1109/5.18626},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
  archiveprefix = {arXiv},
  isbn = {0018-9219},
  annotation = {9995 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F9SQVHNF/Rabiner - 2019 - A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition(2).pdf}
}

@inproceedings{rabinovich2017AbstractSyntaxNetworks,
  title = {Abstract Syntax Networks for Code Generation and Semantic Parsing},
  booktitle = {{{ACL}}},
  author = {Rabinovich, M. and Stern, M. and Klein, D.},
  date = {2017},
  eprint = {1704.07535},
  eprinttype = {arxiv},
  pages = {1139--1149},
  doi = {10.18653/v1/P17-1105},
  abstract = {Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark HEARTHSTONE dataset for code generation, our model obtains 79.2 BLEU and 22.7\% exact match accuracy, compared to previous state-ofthe-art values of 67.1 and 6.1\%. Furthermore, we perform competitively on the ATIS, JOBS, and GEO semantic parsing datasets with no task-specific engineering.},
  archiveprefix = {arXiv},
  annotation = {192 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/53RDRHE9/Rabinovich, Stern, Klein - 2017 - Abstract syntax networks for code generation and semantic parsing(2).pdf}
}

@misc{radford2019LanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  date = {2019},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q5VVEBN7/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@inproceedings{radford2021LearningTransferableVisual,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  booktitle = {{{ICML}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021},
  pages = {16},
  abstract = {SOTA computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers nontrivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PV395SJ7/Radford et al. - Learning Transferable Visual Models From Natural L.pdf}
}

@inproceedings{rae2020CompressiveTransformersLongRange,
  title = {Compressive {{Transformers}} for {{Long-Range Sequence Modelling}}},
  booktitle = {{{ICLR}}},
  author = {Rae, Jack W. and Potapenko, Anna and Jayakumar, Siddhant M. and Lillicrap, Timothy P.},
  date = {2020},
  eprint = {1911.05507},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.05507},
  urldate = {2022-02-09},
  abstract = {We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.},
  archiveprefix = {arXiv},
  eventtitle = {2020},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/53MPI5BQ/Rae et al. - 2019 - Compressive Transformers for Long-Range Sequence M.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ZFN9HGBF/1911.html}
}

@inproceedings{rae2020TransformersNeedDeep,
  title = {Do {{Transformers Need Deep Long-Range Memory}}},
  booktitle = {{{ACL}}},
  author = {Rae, Jack W. and Razavi, Ali},
  date = {2020-07-07},
  eprint = {2007.03356},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2007.03356},
  urldate = {2022-02-09},
  abstract = {Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL -- a Transformer augmented with a long-range memory of past activations -- has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3MVRV2IP/Rae and Razavi - 2020 - Do Transformers Need Deep Long-Range Memory.pdf;/home/hiaoxui/.local/share/zotero_files/storage/EVI2WR2A/2007.html}
}

@misc{raffel2019ExploringLimitsTransfer,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, C. and Shazeer, N. and Roberts, A. and Lee, K. and Narang, S. and Matena, M. and Zhou, Y. and Li, W. and Liu, P. J.},
  date = {2019},
  eprint = {1910.10683},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.10683},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.},
  archiveprefix = {arXiv},
  annotation = {965 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DE3A5GJM/Raffel et al. - 2019 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(2).pdf}
}

@inproceedings{raghunathan2018CertifiedDefensesAdversarial,
  title = {Certified {{Defenses}} against {{Adversarial Examples}}},
  booktitle = {{{ICLR}}},
  author = {Raghunathan, A. and Steinhardt, J. and Liang, P.},
  date = {2018},
  eprint = {1801.09344},
  eprinttype = {arxiv},
  pages = {1--15},
  url = {http://arxiv.org/abs/1801.09344},
  abstract = {While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \textbackslash epsilon = 0.1 can cause more than 35\% test error.},
  archiveprefix = {arXiv},
  annotation = {496 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PPQKWPBW/Raghunathan, Steinhardt, Liang - 2018 - Certified Defenses against Adversarial Examples(2).pdf}
}

@inproceedings{rahimi2007RegularizationMethodImmune,
  title = {Regularization Method and Immune Genetic Algorithm for Inverse Problems of Ship Maneuvering},
  booktitle = {{{NeurIPS}}},
  author = {Rahimi, A. and Recht, B.},
  date = {2007},
  issn = {10071172},
  doi = {10.1007/s12204-009-0467-7},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WZ4CH8QS/Rahimi, Recht - 2007 - Regularization method and immune genetic algorithm for inverse problems of ship maneuvering(2).pdf}
}

@inproceedings{rajani2019ExplainYourselfLeveraging,
  title = {Explain {{Yourself}}! {{Leveraging Language Models}} for {{Commonsense Reasoning}}},
  booktitle = {{{ACL}}},
  author = {Rajani, N. F. and McCann, B. and Xiong, C. and Socher, R.},
  date = {2019},
  eprint = {1906.02361},
  eprinttype = {arxiv},
  pages = {4932--4942},
  doi = {10.18653/v1/p19-1487},
  abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10\% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {94 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJY89XUK/Rajani et al. - 2019 - Explain Yourself! Leveraging Language Models for Commonsense Reasoning(2).pdf}
}

@misc{rajbhandari2021ZeROInfinityBreakingGPU,
  title = {{{ZeRO-Infinity}}: {{Breaking}} the {{GPU Memory Wall}} for {{Extreme Scale Deep Learning}}},
  shorttitle = {{{ZeRO-Infinity}}},
  author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  date = {2021-04-15},
  eprint = {2104.07857},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.07857},
  urldate = {2021-05-25},
  abstract = {In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model. In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs(40\% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed, a deep learning optimization library that makes distributed training easy, efficient, and effective.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-05-25]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LSZ4P7KU/Rajbhandari et al. - 2021 - ZeRO-Infinity Breaking the GPU Memory Wall for Ex.pdf;/home/hiaoxui/.local/share/zotero_files/storage/K39IYS2W/2104.html}
}

@inproceedings{rajpurkar2016SQuAD100000,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  booktitle = {{{EMNLP}}},
  author = {Rajpurkar, P. and Zhang, J. and Lopyrev, K. and Liang, P.},
  date = {2016},
  eprint = {299497},
  eprinttype = {pmid},
  issn = {9781941643327},
  doi = {10.18653/v1/D16-1264},
  url = {http://arxiv.org/abs/1606.05250},
  abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
  archiveprefix = {arXiv},
  isbn = {978-1-941643-32-7},
  issue = {ii},
  annotation = {2600 citations (Semantic Scholar/DOI) [2021-03-26] 2600 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZMRKV3LA/Rajpurkar et al. - 2016 - SQuAD 100,000 Questions for Machine Comprehension of Text(2).pdf}
}

@inproceedings{rajpurkar2018KnowWhatYou,
  title = {Know {{What You Don}}'t {{Know}}: {{Unanswerable Questions}} for {{SQuAD}}},
  booktitle = {{{ACL}}},
  author = {Rajpurkar, P. and Jia, R. and Liang, P.},
  date = {2018},
  eprint = {1806.03822},
  eprinttype = {arxiv},
  pages = {1--6},
  url = {http://arxiv.org/abs/1806.03822},
  abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
  archiveprefix = {arXiv},
  annotation = {773 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4FTUHXID/Rajpurkar, Jia, Liang - 2018 - Know What You Don't Know Unanswerable Questions for SQuAD(2).pdf}
}

@article{ramos-soto2015LinguisticDescriptionsAutomatic,
  title = {Linguistic Descriptions for Automatic Generation of Textual Short-Term Weather Forecasts on Real Prediction Data},
  author = {Ramos-Soto, A. and Bugarín, A. and Barro, S. and Taboada, J.},
  date = {2015},
  journaltitle = {TFS},
  volume = {23},
  number = {1},
  eprint = {1411.4925v1},
  eprinttype = {arxiv},
  pages = {44--57},
  issn = {10636706},
  doi = {10.1109/TFUZZ.2014.2328011},
  abstract = {We present in this paper an application that automatically generates textual short-term weather forecasts for every municipality in Galicia (NW Spain), using the real data provided by the Galician Meteorology Agency (MeteoGalicia). This solution combines in an innovative way computing with perceptions techniques and strategies for linguistic description of data, together with a natural language generation (NLG) system. The application, which is named GALiWeather, extracts relevant information from weather forecast input data and encodes it into intermediate descriptions using linguistic variables and temporal references. These descriptions are later translated into natural language texts by the NLG system. The obtained forecast results have been thoroughly validated by an expert meteorologist from MeteoGalicia using a quality assessment methodology, which covers two key dimensions of a text: the accuracy of its content and the correctness of its form. Following this validation, GALiWeather will be released as a real service, offering custom forecasts for a wide public.},
  archiveprefix = {arXiv},
  isbn = {9783642407680},
  annotation = {56 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C29RC8NE/Ramos-Soto et al. - 2015 - Linguistic descriptions for automatic generation of textual short-term weather forecasts on real predictio(2).pdf}
}

@inproceedings{ramzi2022SHINESHaringINverse,
  title = {{{SHINE}}: {{SHaring}} the {{INverse Estimate}} from the Forward Pass for Bi-Level Optimization and Implicit Models},
  booktitle = {{{ICLR}}},
  author = {Ramzi, Zaccharie and Mannel, Florian and Bai, Shaojie and Starck, Jean-Luc and Ciuciu, Philippe and Moreau, Thomas},
  date = {2022},
  pages = {20},
  abstract = {In recent years, implicit deep learning has emerged as a method to increase the effective depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models (DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by the iterative inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this computational bottleneck from which many bi-level problems suffer. The main idea is to use the quasi-Newton matrices from the forward pass to efficiently approximate the inverse Jacobian matrix in the direction needed for the gradient computation. We provide a theorem that motivates using our method with the original forward algorithms. In addition, by modifying these forward algorithms, we further provide theoretical guarantees that our method asymptotically estimates the true implicit gradient. We empirically study this approach and the recent Jacobian-Free method in different settings, ranging from hyperparameter optimization to large Multiscale DEQs (MDEQs) applied to CIFAR and ImageNet. Both methods reduce significantly the computational cost of the backward pass. While SHINE has a clear advantage on hyperparameter optimization problems, both methods attain similar computational performances for larger scale problems such as MDEQs at the cost of a limited performance drop compared to the original models.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJZZXS6J/Ramzi et al. - 2022 - SHINE SHARING THE INVERSE ESTIMATE FROM THE FORWA.pdf}
}

@thesis{ransom2001NewSearchTechniques,
  title = {New {{Search Techniques}} for {{Binary Pulsars}}},
  author = {Ransom, S. M.},
  date = {2001},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8WFB4RM2/Ransom - 2001 - New Search Techniques for Binary Pulsars(2).pdf}
}

@article{ransom2002FourierTechniquesVery,
  title = {Fourier {{Techniques}} for {{Very Long Astrophysical Time Series Analysis}}},
  author = {Ransom, S. M. and Eikenberry, S. S. and Middleditch, J.},
  date = {2002},
  journaltitle = {The Astronomical Journal},
  volume = {124},
  number = {3},
  eprint = {astro-ph/0204349},
  eprinttype = {arxiv},
  pages = {38},
  issn = {1538-3881},
  doi = {10.1086/342285},
  url = {http://arxiv.org/abs/astro-ph/0204349},
  abstract = {We present an assortment of both standard and advanced Fourier techniques that are useful in the analysis of astrophysical time series of very long duration-where the observation time is much greater than the time resolution of the individual data points. We begin by reviewing the operational characteristics of Fourier transforms of time-series data, including power-spectral statistics, discussing some of the differences between analyses of binned data, sampled data, and event data, and we briefly discuss algorithms for calculating discrete Fourier transforms (DFTs) of very long time series. We then discuss the response of DFTs to periodic signals and present techniques to recover Fourier amplitude ``lost'' during simple traditional analyses if the periodicities change frequency during the observation. These techniques include Fourier interpolation, which allows us to correct the response for signals that occur between Fourier frequency bins. We then present techniques for estimating additional signal properties such as the signal's centroid and duration in time, the first and second derivatives of the frequency, the pulsed fraction, and an overall estimate of the significance of a detection. Finally, we present a recipe for a basic but thorough Fourier analysis of a time series for well-behaved pulsations.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {209 citations (Semantic Scholar/DOI) [2021-03-26] 209 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5BE5X3V2/Ransom, Eikenberry, Middleditch - 2002 - Fourier Techniques for Very Long Astrophysical Time Series Analysis(2).pdf}
}

@article{ransom2003NewSearchTechnique,
  title = {A {{New Search Technique}} for {{Short Orbital Period Binary Pulsars}}},
  author = {Ransom, S. M. and Cordes, J. M. and Eikenberry, S. S.},
  date = {2003},
  journaltitle = {The Astrophysical Journal},
  volume = {589},
  number = {2},
  eprint = {astro-ph/0210010},
  eprinttype = {arxiv},
  pages = {911--920},
  issn = {0004-637X},
  doi = {10.1086/374806},
  url = {http://stacks.iop.org/0004-637X/589/i=2/a=911},
  abstract = {We describe a new and efficient technique, which we call sideband or phase-modulation searching, that allows one to detect short-period binary pulsars in observations longer than the orbital period. The orbital motion of the pulsar during long observations effectively modulates the phase of the pulsar signal, causing sidebands to appear around the pulsar spin frequency and its harmonics in the Fourier transform. For the majority of binary radio pulsars or low-mass X-ray binaries (LMXBs), large numbers of sidebands are present, allowing efficient searches using Fourier transforms of short portions of the original power spectrum. Analysis of the complex amplitudes and phases of the sidebands can provide enough information to solve for the Keplerian orbital parameters. This technique is particularly applicable to radio pulsar searches in globular clusters and searches for coherent X-ray pulsations from LMXBs and is complementary to more standard "acceleration" searches.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {66 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8W979E46/Ransom, Cordes, Eikenberry - 2003 - A New Search Technique for Short Orbital Period Binary Pulsars(2).pdf}
}

@inproceedings{ranzato2016SequenceLevelTraining,
  title = {Sequence Level Training with Recurrent Neural Networks},
  booktitle = {{{ICLR}}},
  author = {Ranzato, M. and Chopra, S. and Auli, M. and Zaremba, W.},
  date = {2016},
  eprint = {1511.06732},
  eprinttype = {arxiv},
  abstract = {Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CMNMFYIA/Ranzato et al. - 2016 - Sequence level training with recurrent neural networks(4).pdf}
}

@inproceedings{rao2015ParserAbstractMeaning,
  title = {Parser for {{Abstract Meaning Representation}} Using {{Learning}} to {{Search}}},
  booktitle = {{{SemEval}}},
  author = {Rao, S. and Vyas, Y. and Daumé III, H. and Resnik, P.},
  date = {2015},
  eprint = {1510.07586},
  eprinttype = {arxiv},
  abstract = {We develop a novel technique to parse English sentences into Abstract Meaning Representation (AMR) using SEARN, a Learning to Search approach, by modeling the concept and the relation learning in a unified framework. We evaluate our parser on multiple datasets from varied domains and show an absolute improvement of 2\% to 6\% over the state-of-the-art. Additionally we show that using the most frequent concept gives us a baseline that is stronger than the state-of-the-art for concept prediction. We plan to release our parser for public use.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/INW4694Y/Rao et al. - 2015 - Parser for Abstract Meaning Representation using Learning to Search(2).pdf}
}

@inproceedings{rao2016NoiseContrastiveEstimationAnswer,
  title = {Noise-{{Contrastive Estimation}} for {{Answer Selection}} with {{Deep Neural Networks}}},
  booktitle = {{{CIKM}}},
  author = {Rao, Jinfeng and He, Hua and Lin, Jimmy},
  date = {2016-10-24},
  pages = {1913--1916},
  publisher = {{ACM}},
  location = {{Indianapolis Indiana USA}},
  doi = {10.1145/2983323.2983872},
  url = {https://dl.acm.org/doi/10.1145/2983323.2983872},
  urldate = {2022-04-27},
  abstract = {We study answer selection for question answering, in which given a question and a set of candidate answer sentences, the goal is to identify the subset that contains the answer. Unlike previous work which treats this task as a straightforward pointwise classification problem, we model this problem as a ranking task and propose a pairwise ranking approach that can directly exploit existing pointwise neural network models as base components. We extend the NoiseContrastive Estimation approach with a triplet ranking loss function to exploit interactions in triplet inputs over the question paired with positive and negative examples. Experiments on TrecQA and WikiQA datasets show that our approach achieves state-of-the-art effectiveness without the need for external knowledge sources or feature engineering.},
  eventtitle = {{{CIKM}}'16: {{ACM Conference}} on {{Information}} and {{Knowledge Management}}},
  isbn = {978-1-4503-4073-1},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C8JZW8TN/Rao et al. - 2016 - Noise-Contrastive Estimation for Answer Selection .pdf}
}

@inproceedings{rao2018LearningAskGood,
  title = {Learning to {{Ask Good Questions}}: {{Ranking Clarification Questions}} Using {{Neural Expected Value}} of {{Perfect Information}}},
  booktitle = {{{ACL}}},
  author = {Rao, S. and Daumé III, H.},
  date = {2018},
  eprint = {1805.04655},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1805.04655},
  abstract = {Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. In this work, we build a neural network model for the task of ranking clarification questions. Our model is inspired by the idea of expected value of perfect information: a good question is one whose expected answer will be useful. We study this problem using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We create a dataset of clarification questions consisting of \textasciitilde 77K posts paired with a clarification question (and answer) from three domains of StackExchange: askubuntu, unix and superuser. We evaluate our model on 500 samples of this dataset against expert human judgments and demonstrate significant improvements over controlled baselines.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {54 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KR5KJF83/Rao, Daumé III - 2018 - Learning to Ask Good Questions Ranking Clarification Questions using Neural Expected Value of Perfect Informa(2).pdf}
}

@inproceedings{ratner2017SnorkelRapidTraining,
  title = {Snorkel: {{Rapid}} Training Data Creation with Weak Supervision},
  booktitle = {Very {{Large Data Bases}}},
  author = {Ratner, A. and Bach, S. H. and Ehrenberg, H. and Fries, J. and Wu, S. and Re, C.},
  date = {2017},
  volume = {11},
  number = {3},
  pages = {269--282},
  issn = {21508097},
  doi = {10.14778/3157794.3157797},
  abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8x faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8x speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
  keywords = {unread},
  annotation = {322 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9HIRDRPZ/Ratner et al. - 2017 - Snorkel Rapid training data creation with weak supervision(2).pdf}
}

@incollection{rausch2011PracticalMultipleSequence,
  title = {Practical {{Multiple Sequence Alignment}}},
  booktitle = {Problem {{Solving Handbook}} in {{Computational Biology}} and {{Bioinformatics}}},
  author = {Rausch, T. and Reinert, K.},
  date = {2011},
  doi = {10.1007/978-0-387-09760-2},
  url = {http://link.springer.com/10.1007/978-0-387-09760-2},
  isbn = {978-0-387-09759-6},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IMFLL93W/Rausch, Reinert - 2011 - Practical Multiple Sequence Alignment(2).pdf}
}

@misc{ravenscroft2021CD2CRCoreferenceResolution,
  title = {{{CD2CR}}: {{Co-reference Resolution Across Documents}} and {{Domains}}},
  shorttitle = {{{CD2CR}}},
  author = {Ravenscroft, James and Cattan, Arie and Clare, Amanda and Dagan, Ido and Liakata, Maria},
  date = {2021-01-29},
  eprint = {2101.12637},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.12637},
  urldate = {2021-02-27},
  abstract = {Cross-document co-reference resolution (CDCR) is the task of identifying and linking mentions to entities and concepts across many text documents. Current state-of-the-art models for this task assume that all documents are of the same type (e.g. news articles) or fall under the same theme. However, it is also desirable to perform CDCR across different domains (type or theme). A particular use case we focus on in this paper is the resolution of entities mentioned across scientific work and newspaper articles that discuss them. Identifying the same entities and corresponding concepts in both scientific articles and news can help scientists understand how their work is represented in mainstream media. We propose a new task and English language dataset for cross-document cross-domain co-reference resolution (CD\$\^2\$CR). The task aims to identify links between entities across heterogeneous document types. We show that in this cross-domain, cross-document setting, existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CD\$\^2\$CR. Our data set, annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FXEDUQX9/Ravenscroft et al. - 2021 - CD2CR Co-reference Resolution Across Documents an.pdf;/home/hiaoxui/.local/share/zotero_files/storage/75YMELK9/2101.html}
}

@inproceedings{ravishankar2022WordOrderDoes,
  title = {Word {{Order Does Matter}} ({{And Shuffled Language Models Know It}})},
  booktitle = {{{ACL}}},
  author = {Ravishankar, Vinit and Abdou, Mostafa and Kulmizev, Artur and Søgaard, Anders},
  date = {2022-03-21},
  eprint = {2203.10995},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.10995},
  urldate = {2022-03-23},
  abstract = {Recent studies have shown that language models pretrained and/or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE, putting into question the importance of word order information. Somewhat counter-intuitively, some of these studies also report that position embeddings appear to be crucial for models' good performance with shuffled text. We probe these language models for word order information and investigate what position embeddings learned from shuffled text encode, showing that these models retain information pertaining to the original, naturalistic word order. We show this is in part due to a subtlety in how shuffling is implemented in previous work -- before rather than after subword segmentation. Surprisingly, we find even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities. Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WY5TA4RU/Ravishankar et al. - 2022 - Word Order Does Matter (And Shuffled Language Mode.pdf;/home/hiaoxui/.local/share/zotero_files/storage/YGQ3ATKU/2203.html}
}

@inproceedings{reddi2018ConvergenceAdam,
  title = {On the Convergence of Adam and Beyond},
  booktitle = {{{ICLR}}},
  author = {Reddi, S. J. and Kale, S. and Kumar, S.},
  date = {2018},
  eprint = {1709.01507},
  eprinttype = {arxiv},
  pages = {1--23},
  issn = {08695652},
  doi = {10.1134/S0001434607010294},
  abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of ex-ponential moving averages of squared past gradients. It has been empirically ob-served that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous anal-ysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with " long-term memory " of past gradi-ents, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  archiveprefix = {arXiv},
  annotation = {19 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P76DAWIH/Reddi, Kale, Kumar - 2018 - On the convergence of adam and beyond(2).pdf}
}

@article{reddy2014LargescaleSemanticParsing,
  title = {Large-Scale {{Semantic Parsing}} without {{Question-Answer Pairs}}},
  author = {Reddy, S. and Lapata, M. and Steedman, M.},
  date = {2014},
  journaltitle = {TACL},
  volume = {2},
  eprint = {25810777},
  eprinttype = {pmid},
  pages = {377--392},
  issn = {2307-387X},
  doi = {10.1017/CBO9781107415324.004},
  url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/398},
  abstract = {In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the FREE 917 and WEBQUESTIONS benchmark datasets show our semantic parser improves over the state of the art.},
  archiveprefix = {arXiv},
  isbn = {9782951740877},
  keywords = {unread},
  annotation = {1013 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FSK5KAY8/Reddy, Lapata, Steedman - 2014 - Large-scale Semantic Parsing without Question-Answer Pairs(2).pdf}
}

@inproceedings{reddy2017UniversalSemanticParsing,
  title = {Universal {{Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Reddy, S. and Täckström, O. and Petrov, S. and Steedman, M. and Lapata, M.},
  date = {2017},
  eprint = {1702.03196},
  eprinttype = {arxiv},
  pages = {89--101},
  url = {http://arxiv.org/abs/1702.03196},
  abstract = {Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions. Our code and data can be downloaded at https://github.com/sivareddyg/udeplambda.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {61 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I3QPVD6Q/Reddy et al. - 2017 - Universal Semantic Parsing(2).pdf}
}

@inproceedings{reimers2020MakingMonolingualSentence,
  title = {Making {{Monolingual Sentence Embeddings Multilingual}} Using {{Knowledge Distillation}}},
  booktitle = {{{EMNLP}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  date = {2020-10-05},
  eprint = {2004.09813},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2004.09813},
  urldate = {2022-09-30},
  abstract = {We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y39UJCGS/Reimers and Gurevych - 2020 - Making Monolingual Sentence Embeddings Multilingua.pdf;/home/hiaoxui/.local/share/zotero_files/storage/4NGCWYSR/2004.html}
}

@article{reisinger2015SemanticProtoRoles,
  title = {Semantic {{Proto-Roles}}},
  author = {Reisinger, D. and Rudinger, R. and Ferraro, F. and Harman, C. and Rawlins, K. and Van Durme, B.},
  date = {2015},
  journaltitle = {TACL},
  volume = {3},
  pages = {475--488},
  doi = {10.1162/tacl_a_00152},
  abstract = {We present the first large-scale, corpus based verification of Dowty’s seminal theory of proto-roles. Our results demonstrate both the need for and the feasibility of a property-based annotation scheme of semantic relationships, as opposed to the currently dominant notion of categorical roles.},
  annotation = {55 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JUZT997G/Reisinger et al. - 2015 - Semantic Proto-Roles(2).pdf}
}

@article{reiter2005ChoosingWordsComputergenerated,
  title = {Choosing Words in Computer-Generated Weather Forecasts},
  author = {Reiter, E. and Sripada, S. and Hunter, J. and Yu, J. and Davy, I.},
  date = {2005},
  journaltitle = {Artificial Intelligence},
  volume = {167},
  number = {1-2},
  pages = {137--169},
  issn = {00043702},
  doi = {10.1016/j.artint.2005.06.006},
  abstract = {One of the main challenges in automatically generating textual weather forecasts is choosing appropriate English words to communicate numeric weather data. A corpus-based analysis of how humans write forecasts showed that there were major differences in how individual writers performed this task, that is, in how they translated data into words. These differences included both different preferences between potential near-synonyms that could be used to express information, and also differences in the meanings that individual writers associated with specific words. Because we thought these differences could confuse readers, we built our SumTime-Mousam weather-forecast generator to use consistent data-to-word rules, which avoided words which were only used by a few people, and words which were interpreted differently by different people. An evaluation by forecast users suggested that they preferred SumTime-Mousam's texts to human-generated texts, in part because of better word choice; this may be the first time that an evaluation has shown that nlg texts are better than human-authored texts. © 2005 Elsevier B.V. All rights reserved.},
  isbn = {0004-3702},
  annotation = {241 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FIFNGXPM/Reiter et al. - 2005 - Choosing words in computer-generated weather forecasts(2).pdf}
}

@article{reiter2009InvestigationValidityMetrics,
  title = {An {{Investigation}} into the {{Validity}} of {{Some Metrics}} for {{Automatically Evaluating Natural Language Generation Systems}}},
  author = {Reiter, E. and Belz, A.},
  date = {2009},
  journaltitle = {Computational Linguistics},
  volume = {35},
  number = {4},
  pages = {529--558},
  issn = {08912017},
  doi = {10.1162/coli.2009.35.4.35405},
  abstract = {There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies.},
  keywords = {unread},
  annotation = {144 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BRB2UT2R/Reiter, Belz - 2009 - An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Sys(2).pdf}
}

@article{reiter2018BLEUStructuredReview,
  title = {{{BLEU Structured Review}}: {{A Structured Review}} of the {{Validity}} of {{BLEU}}},
  author = {Reiter, E.},
  date = {2018},
  journaltitle = {Computational Linguistics},
  issue = {August 2017},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UXWNF2WB/Reiter - 2018 - BLEU Structured Review A Structured Review of the Validity of BLEU(2).pdf}
}

@misc{ren2021CombinerFullAttention,
  title = {Combiner: {{Full Attention Transformer}} with {{Sparse Computation Cost}}},
  shorttitle = {Combiner},
  author = {Ren, Hongyu and Dai, Hanjun and Dai, Zihang and Yang, Mengjiao and Leskovec, Jure and Schuurmans, Dale and Dai, Bo},
  date = {2021-07-12},
  eprint = {2107.05768},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.05768},
  urldate = {2021-07-23},
  abstract = {Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity \$\textbackslash mathcal\{O\}(L\^2)\$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost (\$\textbackslash mathcal\{O\}(L\textbackslash log(L))\$ or \$\textbackslash mathcal\{O\}(L\textbackslash sqrt\{L\})\$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HNZLWU2Q/Ren et al. - 2021 - Combiner Full Attention Transformer with Sparse C.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JM8I3B4Y/2107.html}
}

@inproceedings{rezende2014StochasticBackpropagationApproximate,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  booktitle = {{{ICML}}},
  author = {Rezende, D. J. and Mohamed, S. and Wierstra, D.},
  date = {2014},
  eprint = {1401.4082},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1401.4082},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  archiveprefix = {arXiv},
  annotation = {86 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/98HW4VH6/Rezende, Mohamed, Wierstra - 2014 - Stochastic Backpropagation and Approximate Inference in Deep Generative Models(2).pdf}
}

@inproceedings{rezende2015VariationalInferenceNormalizing,
  title = {Variational Inference with Normalizing Flows},
  booktitle = {{{ICML}}},
  author = {Rezende, D. J. and Mohamed, S.},
  date = {2015},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  pages = {1530--1538},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-1058-7},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SQPI36CN/Rezende, Mohamed - 2015 - Variational inference with normalizing flows(2).pdf}
}

@inproceedings{ri2022PretrainingArtificialLanguage,
  title = {Pretraining with {{Artificial Language}}: {{Studying Transferable Knowledge}} in {{Language Models}}},
  shorttitle = {Pretraining with {{Artificial Language}}},
  booktitle = {{{ACL}}},
  author = {Ri, Ryokan and Tsuruoka, Yoshimasa},
  date = {2022-03-22},
  eprint = {2203.10326},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.10326},
  urldate = {2022-03-23},
  abstract = {We investigate what kind of structural knowledge learned in neural network encoders is transferable to processing natural language. We design artificial languages with structural properties that mimic natural language, pretrain encoders on the data, and see how much performance the encoder exhibits on downstream tasks in natural language. Our experimental results show that pretraining with an artificial language with a nesting dependency structure provides some knowledge transferable to natural language. A follow-up probing analysis indicates that its success in the transfer is related to the amount of encoded contextual information and what is transferred is the knowledge of position-aware context dependence of language. Our results provide insights into how neural network encoders process human languages and the source of cross-lingual transferability of recent multilingual language models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T89RIVP9/Ri and Tsuruoka - 2022 - Pretraining with Artificial Language Studying Tra.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ISIAEWQB/2203.html}
}

@inproceedings{ribeiro2016WhyShouldTrust,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  booktitle = {{{KDD}}},
  author = {Ribeiro, M. T. and Singh, S. and Guestrin, C.},
  date = {2016},
  eprint = {214160309},
  eprinttype = {pmid},
  issn = {9781450321389},
  doi = {10.18653/v1/N16-3020},
  url = {http://arxiv.org/abs/1602.04938},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arXiv},
  isbn = {978-1-4503-2138-9},
  annotation = {244 citations (Semantic Scholar/DOI) [2021-03-26] 4115 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SG8XU69I/Ribeiro, Singh, Guestrin - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier(2).pdf}
}

@inproceedings{ribeiro2018SemanticallyEquivalentAdversarial,
  title = {Semantically Equivalent Adversarial Rules for Debugging {{NLP}} Models},
  booktitle = {{{ACL}}},
  author = {Ribeiro, M. T. and Singh, S. and Guestrin, C.},
  date = {2018},
  pages = {856--865},
  doi = {10.18653/v1/p18-1079},
  abstract = {Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) - semantic-preserving perturbations that induce changes in the model's predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) - simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.},
  annotation = {199 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CA3PEF5S/Ribeiro, Singh, Guestrin - 2018 - Semantically equivalent adversarial rules for debugging NLP models(4).pdf}
}

@inproceedings{ribeiro2019EnhancingAMRtoTextGeneration,
  title = {Enhancing {{AMR-to-Text Generation}} with {{Dual Graph Representations}}},
  booktitle = {{{EMNLP}}},
  author = {Ribeiro, L. F. R. and Gardent, C. and Gurevych, I.},
  date = {2019},
  eprint = {1909.00352},
  eprinttype = {arxiv},
  pages = {3181--3192},
  doi = {10.18653/v1/d19-1314},
  abstract = {Generating text from graph-based data, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a graph with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The model learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {25 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N9CDQY73/Ribeiro, Gardent, Gurevych - 2019 - Enhancing AMR-to-Text Generation with Dual Graph Representations(2).pdf}
}

@inproceedings{ribeiro2020AccuracyBehavioralTesting,
  title = {Beyond {{Accuracy}}: {{Behavioral Testing}} of {{NLP Models}} with {{C heckList}}},
  booktitle = {{{ACL}}},
  author = {Ribeiro, M. T. and Wu, T. and Guestrin, C. and Singh, S.},
  date = {2020},
  eprint = {2005.04118v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LYXS9KWU/Ribeiro et al. - 2020 - Beyond Accuracy Behavioral Testing of NLP Models with C heckList(2).pdf;/home/hiaoxui/.local/share/zotero_files/storage/U7ZJI3Q3/D18-1390.pdf}
}

@inproceedings{richardson2013MCTestChallengeDataset,
  title = {{{MCTest}}: {{A Challenge Dataset}} for the {{Open-Domain Machine Comprehension}} of {{Text}}},
  booktitle = {{{EMNLP}}},
  author = {Richardson, M. and Burges, C. J. C. and Renshaw, E.},
  date = {2013},
  abstract = {We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MH4BYURJ/Richardson, Burges, Renshaw - 2013 - MCTest A Challenge Dataset for the Open-Domain Machine Comprehension of Text(2).pdf}
}

@inproceedings{richardson2017LearningSemanticCorrespondences,
  title = {Learning {{Semantic Correspondences}} in {{Technical Documentation}}},
  booktitle = {{{ACL}}},
  author = {Richardson, K. and Kuhn, J.},
  date = {2017},
  pages = {1612--1622},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6IXXLNZP/Richardson, Kuhn - 2017 - Learning Semantic Correspondences in Technical Documentation(2).pdf}
}

@inproceedings{richardson2018PolyglotSemanticParsing,
  title = {Polyglot {{Semantic Parsing}} in {{APIs}}},
  booktitle = {{{NAACL}}},
  author = {Richardson, K. and Berant, J. and Kuhn, J.},
  date = {2018},
  eprint = {1803.06966v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G7BPHEE7/Richardson, Berant, Kuhn - 2018 - Polyglot Semantic Parsing in APIs(2).pdf}
}

@inproceedings{rillof1996AutomaticallyGeneratingExtraction,
  title = {Automatically {{Generating Extraction Patterns}} from {{Untagged Text}}},
  booktitle = {{{AAAI}}},
  author = {Rillof, E.},
  date = {1996},
  pages = {1044--1049},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DHV8MDVF/Rillof - 1996 - Automatically Generating Extraction Patterns from Untagged Text(2).pdf}
}

@inproceedings{riloff1999LearningDictionariesInformation,
  title = {Learning {{Dictionaries}} for {{Information Extraction}} by {{Multi-Level Bootstrapping}}},
  booktitle = {{{AAAI}}},
  author = {Riloff, E. and Jones, R.},
  date = {1999},
  pages = {474--479},
  url = {http://dl.acm.org/citation.cfm?id=315364},
  abstract = {Information extraction systems usually require two dictionaries: a semantic lexicon and a dictionary of extraction patterns for the domain. We present a multilevel bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. As input, our technique requires only unannotated training texts and a handful of seed words for a category. We use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap its extractions into the semantic lexicon, which is the basis for selecting the next extraction pattern. To make this approach more robust, we add a second level of bootstrapping (metabootstrapping) that retains only the most reliable lexicon entries produced by mutual bootstrapping and then restarts the process. We evaluated this multilevel bootstrapping technique on a collection of corporate web pages and a corpus of terrorism news articles. The algorithm produced high-quality dictionaries for several semantic categories.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2D4AYBJZ/Riloff, Jones - 1999 - Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping(2).pdf}
}

@inproceedings{riloff2013SarcasmContrastPositive,
  title = {Sarcasm as {{Contrast}} between a {{Positive Sentiment}} and {{Negative Situation}}},
  booktitle = {{{EMNLP}}},
  author = {Riloff, E. and Qadir, A. and Surve, P. and Silva, L. D. and Gilbert, N. and Huang, R.},
  date = {2013},
  url = {papers3://publication/uuid/9E81B37F-37A5-421F-A49F-5F1A107CC460},
  abstract = {A common form of sarcasm on Twitter con- sists of a positive sentiment contrasted with a negative situation. For example, many sarcas- tic tweets include a positive sentiment, such as “love” or “enjoy”, followed by an expression that describes an undesirable activity or state (e.g., “taking exams” or “being ignored”).We have developed a sarcasm recognizer to iden- tify this type of sarcasm in tweets. We present a novel bootstrapping algorithmthat automati- cally learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. We show that identifying contrast- ing contexts using the phrases learned through bootstrapping yields improved recall for sar- casm recognition. 1},
  isbn = {978-1-937284-97-8},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JXGUXNGI/Riloff et al. - 2013 - Sarcasm as Contrast between a Positive Sentiment and Negative Situation(2).pdf}
}

@article{riloff2018RetrospectiveMutualBootstrapping,
  title = {A {{Retrospective}} on {{Mutual Bootstrapping}}},
  author = {Riloff, E. and Jones, R.},
  date = {2018},
  journaltitle = {AI Magazine},
  volume = {51},
  pages = {51--61},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TT6XXQVN/Riloff, Jones - 2018 - A Retrospective on Mutual Bootstrapping(2).pdf}
}

@misc{ringner2009LawUnconsciousStatistician,
  title = {The Law of the Unconscious Statistician},
  author = {Ringner, B.},
  date = {2009},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A2W5LL46/Ringner - 2009 - The law of the unconscious statistician(2).pdf}
}

@inproceedings{ritter2010DataDrivenResponseGeneration,
  title = {Data-{{Driven Response Generation}} in {{Social Media}}},
  booktitle = {{{EMNLP}}},
  author = {Ritter, A. and Cherry, C. and Dolan, W. B.},
  date = {2010},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C92RWRTL/Ritter, Cherry, Dolan - 2010 - Data-Driven Response Generation in Social Media(2).pdf}
}

@misc{roberta2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  author = {Liu, Y. and Ott, M. and Goyal, N. and Du, J. and Joshi, M. and Chen, D. and Levy, O. and Lewis, M. and Zettlemoyer, L. S. and Stoyanov, V.},
  date = {2019},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1907.11692},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {2496 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UF9JZDUE/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach(2).pdf}
}

@inproceedings{rocktaschel2017EndtoendDifferentiableProving,
  title = {End-to-End Differentiable Proving},
  booktitle = {{{NeurIPS}}},
  author = {Rocktäschel, T. and Riedel, S.},
  date = {2017},
  eprint = {1705.11040},
  eprinttype = {arxiv},
  pages = {3789--3801},
  issn = {10495258},
  abstract = {We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PJGQCQ7S/Rocktäschel, Riedel - 2017 - End-to-end differentiable proving(2).pdf}
}

@article{rogers2020PrimerBERTologyWhat,
  title = {A {{Primer}} in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  date = {2020-11-09},
  journaltitle = {TACL},
  eprint = {2002.12327},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.12327},
  urldate = {2020-11-19},
  archiveprefix = {arXiv},
  annotation = {142 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D2W9YANK/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ICJXID3W/2002.html}
}

@misc{rojas1997TutorialIntroductionLambda,
  title = {A {{Tutorial Introduction}} to the {{Lambda Calculus}}},
  author = {Rojas, R.},
  date = {1997},
  volume = {58},
  eprint = {10512656},
  eprinttype = {pmid},
  issn = {00033472},
  doi = {10.1006/anbe.1999.1219},
  abstract = {This paper is a short and painless introduction to the λ calculus. Originally developed in order to study some mathematical properties of effectively com- putable functions, this formalism has provided a strong theoretical foundation for the family of functional programming languages. We show how to perform some arithmetical computations using the λ calculus and how to define recur- sive functions, even though functions in λ calculus are not given names and thus cannot refer explicitly to themselves.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NN5KTP3D/Rojas - 1997 - A Tutorial Introduction to the Lambda Calculus(2).pdf}
}

@inproceedings{romanov2019WhatNameReducing,
  title = {What's in a {{Name}}? {{Reducing Bias}} in {{Bios}} without {{Access}} to {{Protected Attributes}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Romanov, A. and De-Arteaga, M. and Wallach, H. and Chayes, J. and Borgs, C. and Chouldechova, A. and Geyik, S. and Kenthapadi, K. and Rumshisky, A. and Kalai, A. T.},
  date = {2019},
  eprint = {1904.05233},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.05233},
  abstract = {There is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual's true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals' names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale dataset of online biographies. We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier's overall true positive rate.},
  archiveprefix = {arXiv},
  annotation = {23 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QAYSC6BT/Romanov et al. - 2019 - What's in a Name Reducing Bias in Bios without Access to Protected Attributes(2).pdf}
}

@misc{rong2014Word2vecParameterLearning,
  title = {Word2vec {{Parameter Learning Explained}}},
  author = {Rong, X.},
  date = {2014},
  eprint = {1411.2738v4},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RW6SB74X/Rong - 2014 - word2vec Parameter Learning Explained(2).pdf}
}

@inproceedings{ross2019HowWellNLI,
  title = {How Well Do {{NLI}} Models Capture Verb Veridicality ?},
  booktitle = {{{EMNLP}}},
  author = {Ross, A. and Pavlick, E.},
  date = {2019},
  pages = {2230--2240},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9PEQYJ4A/Ross, Pavlick - 2019 - How well do NLI models capture verb veridicality(2).pdf}
}

@misc{rosset2020KnowledgeAwareLanguageModel,
  title = {Knowledge-{{Aware Language Model Pretraining}}},
  author = {Rosset, Corby and Xiong, Chenyan and Phan, Minh and Song, Xia and Bennett, Paul and Tiwary, Saurabh},
  date = {2020-06-29},
  eprint = {2007.00655},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2007.00655},
  urldate = {2020-10-16},
  abstract = {How much knowledge do pretrained language models hold? Recent research observed that pretrained transformers are adept at modeling semantics but it is unclear to what degree they grasp human knowledge, or how to ensure they do so. In this paper we incorporate knowledge-awareness in language model pretraining without changing the transformer architecture, inserting explicit knowledge layers, or adding external storage of semantic information. Rather, we simply signal the existence of entities to the input of the transformer in pretraining, with an entityextended tokenizer; and at the output, with an additional entity prediction task. Our experiments show that solely by adding these entity signals in pretraining, significantly more knowledge is packed into the transformer parameters: we observe improved language modeling accuracy, factual correctness in LAMA knowledge probing tasks, and semantics in the hidden representations through edge probing. We also show that our knowledge-aware language model (KALM) can serve as a drop-in replacement for GPT-2 models, significantly improving downstream tasks like zero-shot question-answering with no task-related training.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GSZQ7SEP/Rosset et al. - 2020 - Knowledge-Aware Language Model Pretraining.pdf}
}

@inproceedings{roth2004LinearProgrammingFormulation,
  title = {A {{Linear Programming Formulation}} for {{Global Inference}} in {{Natural Language Tasks}}},
  booktitle = {{{CoNLL}}},
  author = {Roth, D. and Yih, W.},
  date = {2004},
  pages = {3--10},
  issn = {00010782},
  doi = {10.3115/1690219.1690287},
  abstract = {In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.},
  isbn = {978-1-937284-43-5},
  issue = {July},
  annotation = {2063 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JLX2ZLHX/Roth, Yih - 2004 - A Linear Programming Formulation for Global Inference in Natural Language Tasks(2).pdf}
}

@inproceedings{roth2005IntegerLinearProgramming,
  title = {Integer {{Linear Programming Inference}} for {{Conditional Random Fields}}},
  booktitle = {{{ICML}}},
  author = {Roth, D. and Yih, W.},
  date = {2005},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YL4PC2MQ/Roth, Yih - 2005 - Integer Linear Programming Inference for Conditional Random Fields(2).pdf}
}

@article{roth2015ContextawareFrameSemanticRole,
  title = {Context-Aware {{Frame-Semantic Role Labeling}}},
  author = {Roth, M. and Lapata, M.},
  date = {2015},
  journaltitle = {TACL},
  volume = {3},
  pages = {449--460},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00150},
  abstract = {Frame semantic representations have been useful in several applications ranging from text-to-scene generation, to question answering and social network analysis. Predicting such representations from raw text is, however, a challenging task and corresponding models are typically only trained on a small set of sentence-level annotations. In this paper, we present a semantic role labeling system that takes into account sentence and discourse context. We introduce several new features which we motivate based on linguistic insights and experimentally demonstrate that they lead to significant improvements over the current state-of-the-art in FrameNet-based semantic role labeling.},
  annotation = {28 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RKC3SBFA/Roth, Lapata - 2015 - Context-aware Frame-Semantic Role Labeling(2).pdf}
}

@inproceedings{rothCompositionWordRepresentations2014,
  title = {Composition of Word Representations Improves Semantic Role Labelling},
  booktitle = {{{EMNLP}}},
  author = {Roth, M. and Woodsend, K.},
  date = {2014},
  pages = {407--413},
  doi = {10.3115/v1/d14-1045},
  abstract = {State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance. Unfortunately, such corpora are expensive to produce and often do not generalize well across domains. Even in domain, errors are often made where syntactic information does not provide sufficient cues. In this paper, we mitigate both of these problems by employing distributional word representations gathered from unlabelled data. While straight-forward word representations of predicates and arguments improve performance, we show that further gains are achieved by composing representations that model the interaction between predicate and argument, and capture full argument spans.},
  annotation = {53 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2ZT7RK7K/Roth, Woodsend - 2014 - Composition of word representations improves semantic role labelling(2).pdf}
}

@inproceedings{rothNeuralSemanticRole2016,
  title = {Neural Semantic Role Labeling with Dependency Path Embeddings},
  booktitle = {{{ACL}}},
  author = {Roth, M. and Lapata, M.},
  date = {2016},
  volume = {2},
  eprint = {1605.07515},
  eprinttype = {arxiv},
  pages = {1192--1202},
  doi = {10.18653/v1/p16-1113},
  abstract = {This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as subsequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method.},
  archiveprefix = {arXiv},
  annotation = {145 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G66RWLTB/Roth, Lapata - 2016 - Neural semantic role labeling with dependency path embeddings(2).pdf}
}

@article{roy2020EfficientContentBasedSparse,
  title = {Efficient {{Content-Based Sparse Attention}} with {{Routing Transformers}}},
  author = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  date = {2020-10-24},
  journaltitle = {TACL},
  eprint = {2003.05997},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2003.05997},
  urldate = {2021-06-29},
  abstract = {Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to \$O\textbackslash left(n\^\{1.5\}d\textbackslash right)\$ from \$O\textbackslash left(n\^2d\textbackslash right)\$ for sequence length \$n\$ and hidden dimension \$d\$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.},
  archiveprefix = {arXiv},
  annotation = {74 citations (Semantic Scholar/arXiv) [2021-06-29]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4IVD5KXB/Roy et al. - 2020 - Efficient Content-Based Sparse Attention with Rout.pdf;/home/hiaoxui/.local/share/zotero_files/storage/HC9ZQ9AV/2003.html}
}

@inproceedings{ru2021LearningLogicRules,
  title = {Learning {{Logic Rules}} for {{Document-Level Relation Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Ru, Dongyu and Sun, Changzhi and Feng, Jiangtao and Qiu, Lin and Zhou, Hao and Zhang, Weinan and Yu, Yong and Li, Lei},
  date = {2021},
  pages = {12},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7G7TC7PH/Ru et al. - Learning Logic Rules for Document-Level Relation E.pdf}
}

@inproceedings{ruan2022HiStructImprovingExtractive,
  title = {{{HiStruct}}+: {{Improving Extractive Text Summarization}} with {{Hierarchical Structure Information}}},
  shorttitle = {{{HiStruct}}+},
  booktitle = {{{ACL}}},
  author = {Ruan, Qian and Ostendorff, Malte and Rehm, Georg},
  date = {2022-03-17},
  eprint = {2203.09629},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.09629},
  urldate = {2022-03-30},
  abstract = {Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy. In addition, section titles usually indicate the common topic of their respective sentences. We propose a novel approach to formulate, extract, encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre-trained, encoder-only Transformer language model (HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on PubMed and arXiv substantially. Using various experimental settings on three datasets (i.e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model outperforms a strong baseline collectively, which differs from our model only in that the hierarchical structure information is not injected. It is also observed that the more conspicuous hierarchical structure the dataset has, the larger improvements our method gains. The ablation study demonstrates that the hierarchical position information is the main contributor to our model's SOTA performance.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/78IK4NDD/Ruan et al. - 2022 - HiStruct+ Improving Extractive Text Summarization.pdf;/home/hiaoxui/.local/share/zotero_files/storage/86GCKYQ9/2203.html}
}

@misc{rubin2020SmBoPSemiautoregressiveBottomup,
  title = {{{SmBoP}}: {{Semi-autoregressive Bottom-up Semantic Parsing}}},
  shorttitle = {{{SmBoP}}},
  author = {Rubin, Ohad and Berant, Jonathan},
  date = {2020-10-23},
  eprint = {2010.12412},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.12412},
  urldate = {2020-12-06},
  abstract = {The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step \$t\$ the top-\$K\$ sub-trees of height \$\textbackslash leq t\$. Our parser enjoys several benefits compared to top-down autoregressive parsing. First, since sub-trees in each decoding step are generated in parallel, the theoretical runtime is logarithmic rather than linear. Second, our bottom-up approach learns representations with meaningful semantic sub-programs at each step, rather than semantically vague partial trees. Last, SmBoP includes Transformer-based layers that contextualize sub-trees with one another, allowing us, unlike traditional beam-search, to score trees conditioned on other trees that have been previously explored. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP is competitive with top-down autoregressive parsing. On the test set, SmBoP obtains an EM score of \$60.5\textbackslash\%\$, similar to the best published score for a model that does not use database content, which is at \$60.6\textbackslash\%\$.},
  archiveprefix = {arXiv},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SLLQSNP3/Rubin and Berant - 2020 - SmBoP Semi-autoregressive Bottom-up Semantic Pars.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KIKPMRKL/2010.html}
}

@inproceedings{ruderStrongBaselinesNeural2018,
  title = {Strong Baselines for Neural Semi-Supervised Learning under Domain Shift},
  booktitle = {{{ACL}}},
  author = {Ruder, S. and Plank, B.},
  date = {2018},
  eprint = {1804.09530},
  eprinttype = {arxiv},
  pages = {1044--1054},
  doi = {10.18653/v1/p18-1096},
  abstract = {Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline.},
  archiveprefix = {arXiv},
  isbn = {978-1-948087-32-2},
  keywords = {unread},
  annotation = {72 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K9EXEFKD/Ruder, Plank - 2018 - Strong baselines for neural semi-supervised learning under domain shift(2).pdf}
}

@inproceedings{rudingerNeuralDavidsonianSemanticProtorole2018,
  title = {Neural-{{Davidsonian Semantic Proto-role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Rudinger, R. and Teichert, A. and Culkin, R. and Zhang, S. and Van Durme, B.},
  date = {2018},
  eprint = {1804.07976v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q6SXAJ87/Rudinger et al. - 2018 - Neural-Davidsonian Semantic Proto-role Labeling(2).pdf}
}

@inproceedings{rudingerNeuralModelsFactuality2018,
  title = {Neural {{Models}} of {{Factuality}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Rudinger, R. and White, A. S. and Van Durme, B.},
  date = {2018},
  eprint = {1804.02472v1},
  eprinttype = {arxiv},
  pages = {731--744},
  doi = {10.18653/v1/n18-1067},
  abstract = {We present two neural models for event factuality prediction, which yield significant performance gains over previous models on three event factuality datasets: FactBank, UW, and MEANTIME. We also present a substantial expansion of the It Happened portion of the Universal Decompositional Semantics dataset, yielding the largest event factuality dataset to date. We report model results on this extended factuality dataset as well.},
  archiveprefix = {arXiv},
  annotation = {36 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DFAT7QMI/Rudinger, White, Van Durme - 2018 - Neural Models of Factuality(2).pdf}
}

@inproceedings{rusch2022LongExpressiveMemory,
  title = {Long {{Expressive Memory}} for {{Sequence Modeling}}},
  booktitle = {{{ICLR}}},
  author = {Rusch, T. Konstantin and Mishra, Siddhartha and Erichson, N. Benjamin and Mahoney, Michael W.},
  date = {2022},
  eprint = {2110.04744},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.04744},
  urldate = {2022-02-24},
  abstract = {We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps. To derive LEM, we consider a system of multiscale ordinary differential equations, as well as a suitable time-discretization of this system. For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a well-known challenge for gradient-based recurrent sequential learning methods. We also prove that LEM can approximate a large class of dynamical systems to high accuracy. Our empirical results, ranging from image and time-series classification through dynamical systems prediction to speech recognition and language modeling, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LVG7BIS7/Rusch et al. - 2021 - Long Expressive Memory for Sequence Modeling.pdf;/home/hiaoxui/.local/share/zotero_files/storage/UPW8VM9J/2110.html}
}

@inproceedings{rushNeuralAttentionModel2015,
  title = {A {{Neural Attention Model}} for {{Abstractive Sentence Summarization}}},
  booktitle = {{{EMNLP}}},
  author = {Rush, A. M. and Chopra, S. and Weston, J.},
  date = {2015},
  eprint = {1509.00685},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1509.00685},
  abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
  archiveprefix = {arXiv},
  annotation = {1690 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JI4FUQKR/Rush, Chopra, Weston - 2015 - A Neural Attention Model for Abstractive Sentence Summarization(2).pdf}
}

@inproceedings{rushTorchStructDeepStructured2020,
  title = {Torch-{{Struct}}: {{Deep Structured Prediction Library}}},
  booktitle = {{{ACL}}},
  author = {Rush, Alexander},
  date = {2020},
  pages = {335--342},
  abstract = {The literature on structured prediction for NLP describes a rich collection of distributions and algorithms over sequences, segmentations, alignments, and trees; however, these algorithms are difficult to utilize in deep learning frameworks. We introduce Torch-Struct, a library for structured prediction designed to take advantage of and integrate with vectorized, auto-differentiation based frameworks. TorchStruct includes a broad collection of probabilistic structures accessed through a simple and flexible distribution-based API that connects to any deep learning model. The library utilizes batched, vectorized operations and exploits auto-differentiation to produce readable, fast, and testable code. Internally, we also include a number of general-purpose optimizations to provide cross-algorithm efficiency. Experiments show significant performance gains over fast baselines. Case studies demonstrate the benefits of the library. TorchStruct is available at https://github.com/ harvardnlp/pytorch-struct.},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M54JI2E6/Rush - 2020 - Torch-Struct Deep Structured Prediction Library.pdf}
}

@inproceedings{sabourDynamicRoutingCapsules2017,
  title = {Dynamic {{Routing Between Capsules}}},
  booktitle = {{{NeurIPS}}},
  author = {Sabour, S. and Frosst, N. and Hinton, G. E.},
  date = {2017},
  eprint = {1710.09829},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.09829},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  archiveprefix = {arXiv},
  issue = {Nips},
  keywords = {unread},
  annotation = {1915 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5PYW24GG/Sabour, Frosst, Hinton - 2017 - Dynamic Routing Between Capsules(2).pdf}
}

@misc{sachan2022ImprovingPassageRetrieval,
  title = {Improving {{Passage Retrieval}} with {{Zero-Shot Question Generation}}},
  author = {Sachan, Devendra Singh and Lewis, Mike and Joshi, Mandar and Aghajanyan, Armen and Yih, Wen-tau and Pineau, Joelle and Zettlemoyer, Luke},
  date = {2022-04-15},
  eprint = {2204.07496},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.07496},
  urldate = {2022-04-23},
  abstract = {We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned on a retrieved passage. This approach can be applied on top of any retrieval method (e.g. neural or keyword-based), does not require any domain- or task-specific training (and therefore is expected to generalize better to data distribution shifts), and provides rich cross-attention between query and passage (i.e. it must explain every token in the question). When evaluated on a number of open-domain retrieval datasets, our re-ranker improves strong unsupervised retrieval models by 6\%-18\% absolute and strong supervised models by up to 12\% in terms of top-20 passage retrieval accuracy. We also obtain new state-of-the-art results on full open-domain question answering by simply adding the new re-ranker to existing models with no further changes.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MNI3LTFV/Sachan et al. - 2022 - Improving Passage Retrieval with Zero-Shot Questio.pdf;/home/hiaoxui/.local/share/zotero_files/storage/LW3UR36W/2204.html}
}

@misc{saeed2009OverviewMultipleSequence,
  title = {An {{Overview}} of {{Multiple Sequence Alignment Systems}}},
  author = {Saeed, F. and Khokhar, A.},
  date = {2009},
  eprint = {0901.2747},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/0901.2747},
  abstract = {An overview of current multiple alignment systems to date are described.The useful algorithms, the procedures adopted and their limitations are presented.We also present the quality of the alignments obtained and in which cases(kind of alignments, kind of sequences etc) the particular systems are useful.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GSPPWD4N/Saeed, Khokhar - 2009 - An Overview of Multiple Sequence Alignment Systems(2).pdf}
}

@inproceedings{saeed2021RuleBERTTeachingSoft,
  title = {{{RuleBERT}}: {{Teaching Soft Rules}} to {{Pre-Trained Language Models}}},
  booktitle = {{{EMNLP}}},
  author = {Saeed, Mohammed and Ahmadi, Naser and Nakov, Preslav and Papotti, Paolo},
  date = {2021},
  pages = {17},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CRSLPQBA/Saeed et al. - RuleBERT Teaching Soft Rules to Pre-Trained Langu.pdf}
}

@misc{sahlgren2021SingletonFallacyWhy,
  title = {The {{Singleton Fallacy}}: {{Why Current Critiques}} of {{Language Models Miss}} the {{Point}}},
  shorttitle = {The {{Singleton Fallacy}}},
  author = {Sahlgren, Magnus and Carlsson, Fredrik},
  date = {2021-02-08},
  eprint = {2102.04310},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.04310},
  urldate = {2021-02-15},
  abstract = {This paper discusses the current critique against neural network-based Natural Language Understanding (NLU) solutions known as language models. We argue that much of the current debate rests on an argumentation error that we will refer to as the singleton fallacy: the assumption that language, meaning, and understanding are single and uniform phenomena that are unobtainable by (current) language models. By contrast, we will argue that there are many different types of language use, meaning and understanding, and that (current) language models are build with the explicit purpose of acquiring and representing one type of structural understanding of language. We will argue that such structural understanding may cover several different modalities, and as such can handle several different types of meaning. Our position is that we currently see no theoretical reason why such structural knowledge would be insufficient to count as "real" understanding.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DR996EDZ/Sahlgren and Carlsson - 2021 - The Singleton Fallacy Why Current Critiques of La.pdf;/home/hiaoxui/.local/share/zotero_files/storage/582E5EGQ/2102.html}
}

@inproceedings{sakaguchiErrorrepairDependencyParsing2017,
  title = {Error-Repair {{Dependency Parsing}} for {{Ungrammatical Texts}}},
  booktitle = {{{ACL}}},
  author = {Sakaguchi, K. and Post, M. and Van Durme, B.},
  date = {2017},
  pages = {189--195},
  doi = {10.18653/v1/p17-2030},
  abstract = {We propose a new dependency pars-ing scheme which jointly parses a sen-tence and repairs grammatical errors by extending the non-directional transition-based formalism of Goldberg and El-hadad (2010) with three additional ac-tions: SUBSTITUTE, DELETE, INSERT. Be-cause these actions may cause an infinite loop in derivation, we also introduce sim-ple constraints that ensure the parser ter-mination. We evaluate our model with re-spect to dependency accuracy and gram-maticality improvements for ungrammat-ical sentences, demonstrating the robust-ness and applicability of our scheme.},
  keywords = {unread},
  annotation = {5 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UPDZUFXI/Sakaguchi, Post, Van Durme - 2017 - Error-repair Dependency Parsing for Ungrammatical Texts(2).pdf}
}

@inproceedings{sakaguchiWinoGrandeAdversarialWinograd2019,
  title = {{{WinoGrande}}: {{An Adversarial Winograd Schema Challenge}} at {{Scale}}},
  booktitle = {{{AAAI}}},
  author = {Sakaguchi, K. and Bras, R. L. and Bhagavatula, C. and Choi, Y.},
  date = {2019},
  eprint = {1907.10641},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1907.10641},
  abstract = {The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90\% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on WinoGrande achieve 59.4-79.1\%, which are 15-35\% below human performance of 94.0\%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - WSC (90.1\%), DPR (93.1\%), COPA (90.6\%), KnowRef (85.6\%), and Winogender (97.1\%). These results have dual implications: on one hand, they demonstrate the effectiveness of WinoGrande when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {92 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3T4Y48CN/Sakaguchi et al. - 2019 - WinoGrande An Adversarial Winograd Schema Challenge at Scale(2).pdf}
}

@article{salakhutdinov2007LearningNonlinearEmbedding,
  title = {Learning a {{Nonlinear Embedding}} by {{Preserving Class Neighbourhood Structure}}},
  author = {Salakhutdinov, R. and Hinton, G. E.},
  date = {2007},
  journaltitle = {JMLR},
  number = {1},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9YCWIVBA/Salakhutdinov, Hinton - 2007 - Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure(2).pdf}
}

@inproceedings{salakhutdinovDeepBoltzmannMachines2009,
  title = {Deep {{Boltzmann Machines}}},
  booktitle = {{{AISTATS}}},
  author = {Salakhutdinov, R. and Hinton, G. E.},
  date = {2009},
  number = {3},
  pages = {448--455},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N3QHQSC2/Salakhutdinov, Hinton - 2009 - Deep Boltzmann Machines(2).pdf}
}

@inproceedings{salakhutdinovOptimizationEMExpectationconjugategradient2003,
  title = {Optimization with {{EM}} and Expectation-Conjugate-Gradient},
  booktitle = {{{ICML}}},
  author = {Salakhutdinov, R. and Roweis, S. T. and Ghahramani, Z.},
  date = {2003},
  volume = {20},
  number = {2},
  pages = {672},
  doi = {10.1145/ 1273496.1273497},
  abstract = {We show a close relationship between the Expectation- Maximization (EM) algorithm and direct optimization algorithms such as gradientbased methods for parameter learning. We identify analytic conditions under which EM exhibits Newton-like behavior, and conditions under which it possesses poor, first-order convergence. Based on this analysis, we propose two novel algorithms for maximum likelihood estimation of latent variable models, and report empirical results showing that, as predicted by theory, the proposed new algorithms can substantially outperform standard EM in terms of speed of convergence in certain cases. 1.},
  isbn = {1-57735-189-4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XSDTFB4A/Salakhutdinov, Roweis, Ghahramani - 2003 - Optimization with EM and expectation-conjugate-gradient(2).pdf}
}

@inproceedings{salimansImprovedTechniquesTraining2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  booktitle = {{{NeurIPS}}},
  author = {Salimans, T. and Goodfellow, I. and Zaremba, W. and Cheung, V. and Radford, A. and Chen, X.},
  date = {2016},
  eprint = {1606.03498},
  eprinttype = {arxiv},
  pages = {1--10},
  url = {http://arxiv.org/abs/1606.03498},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  archiveprefix = {arXiv},
  annotation = {4106 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6Y38Z7VR/Salimans et al. - 2016 - Improved Techniques for Training GANs(2).pdf}
}

@inproceedings{salimansMarkovChainMonte2015,
  title = {Markov {{Chain Monte Carlo}} and {{Variational Inference}}: {{Bridging}} the {{Gap}}},
  booktitle = {{{ICML}}},
  author = {Salimans, T. and Kingma, D. P. and Welling, M.},
  date = {2015},
  eprint = {1410.6460},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1410.6460},
  abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
  archiveprefix = {arXiv},
  isbn = {1410.6460},
  keywords = {unread},
  annotation = {330 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JDYG27I6/Salimans, Kingma, Welling - 2015 - Markov Chain Monte Carlo and Variational Inference Bridging the Gap(2).pdf}
}

@misc{salinas2017DeepARProbabilisticForecasting,
  title = {{{DeepAR}}: {{Probabilistic Forecasting}} with {{Autoregressive Recurrent Networks}}},
  author = {Salinas, D. and Flunkert, V. and Gasthaus, J.},
  date = {2017},
  eprint = {1704.04110},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1704.04110},
  abstract = {Probabilistic forecasting, i.e. estimating the probability distribution of a time series' future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an auto regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15\% compared to state-of-the-art methods.},
  archiveprefix = {arXiv},
  annotation = {195 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VQRCFCPT/Salinas, Flunkert, Gasthaus - 2017 - DeepAR Probabilistic Forecasting with Autoregressive Recurrent Networks(2).pdf}
}

@inproceedings{sanh2022MultitaskPromptedTraining,
  title = {Multitask {{Prompted Training Enables Zero-Shot Task Generalization}}},
  booktitle = {{{ICLR}}},
  author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Bers, Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
  date = {2022-03-17},
  eprint = {2110.08207},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.08207},
  urldate = {2022-07-28},
  abstract = {Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PR285BJE/Sanh et al. - 2022 - Multitask Prompted Training Enables Zero-Shot Task.pdf;/home/hiaoxui/.local/share/zotero_files/storage/SXGQT4VY/2110.html}
}

@inproceedings{sanhHierarchicalMultiTaskApproach2019,
  title = {A {{Hierarchical Multi-Task Approach}} for {{Learning Embeddings}} from {{Semantic Tasks}}},
  booktitle = {{{AAAI}}},
  author = {Sanh, V. and Wolf, T. and Ruder, S.},
  date = {2019},
  eprint = {1811.06031},
  eprinttype = {arxiv},
  pages = {6949--6956},
  issn = {2159-5399},
  doi = {10.1609/aaai.v33i01.33016949},
  abstract = {Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {87 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CKCF8GLC/Sanh, Wolf, Ruder - 2019 - A Hierarchical Multi-Task Approach for Learning Embeddings from Semantic Tasks(2).pdf}
}

@inproceedings{santhanam2022ColBERTv2EffectiveEfficient,
  title = {{{ColBERTv2}}: {{Effective}} and {{Efficient Retrieval}} via {{Lightweight Late Interaction}}},
  shorttitle = {{{ColBERTv2}}},
  booktitle = {{{NAACL}}},
  author = {Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
  date = {2022-07-10},
  eprint = {2112.01488},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.01488},
  urldate = {2022-07-15},
  abstract = {Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10\$\textbackslash times\$.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TSURUJVR/Santhanam et al. - 2022 - ColBERTv2 Effective and Efficient Retrieval via L.pdf;/home/hiaoxui/.local/share/zotero_files/storage/SWHAI5SS/2112.html}
}

@inproceedings{santhanam2022ColBERTv2EffectiveEfficienta,
  title = {{{ColBERTv2}}: {{Effective}} and {{Efficient Retrieval}} via {{Lightweight Late Interaction}}},
  shorttitle = {{{ColBERTv2}}},
  booktitle = {{{NAACL}}},
  author = {Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
  date = {2022-07-10},
  eprint = {2112.01488},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.01488},
  urldate = {2022-11-02},
  abstract = {Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10\$\textbackslash times\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9TU74YCV/Santhanam et al. - 2022 - ColBERTv2 Effective and Efficient Retrieval via L.pdf;/home/hiaoxui/.local/share/zotero_files/storage/IT4DP8TN/2112.html}
}

@inproceedings{santhanam2022PLAIDEfficientEngine,
  title = {{{PLAID}}: {{An Efficient Engine}} for {{Late Interaction Retrieval}}},
  shorttitle = {{{PLAID}}},
  booktitle = {{{CIKM}}},
  author = {Santhanam, Keshav and Khattab, Omar and Potts, Christopher and Zaharia, Matei},
  date = {2022-10-17},
  pages = {1747--1756},
  publisher = {{ACM}},
  location = {{Atlanta GA USA}},
  doi = {10.1145/3511808.3557325},
  url = {https://dl.acm.org/doi/10.1145/3511808.3557325},
  urldate = {2022-10-27},
  eventtitle = {{{CIKM}} '22: {{The}} 31st {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  isbn = {978-1-4503-9236-5},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NANPZPAH/Santhanam et al. - 2022 - PLAID An Efficient Engine for Late Interaction Re.pdf}
}

@inproceedings{sap2022AnnotatorsAttitudesHow,
  title = {Annotators with {{Attitudes}}: {{How Annotator Beliefs And Identities Bias Toxic Language Detection}}},
  shorttitle = {Annotators with {{Attitudes}}},
  booktitle = {{{NAACL}}},
  author = {Sap, Maarten and Swayamdipta, Swabha and Vianna, Laura and Zhou, Xuhui and Choi, Yejin and Smith, Noah A.},
  date = {2022-05-09},
  eprint = {2111.07997},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.07997},
  urldate = {2022-07-14},
  abstract = {The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HHVF96A4/Sap et al. - 2022 - Annotators with Attitudes How Annotator Beliefs A.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6SEUCWZ3/2111.html}
}

@misc{saparov2021GeneralNaturalLanguage,
  title = {Towards {{General Natural Language Understanding}} with {{Probabilistic Worldbuilding}}},
  author = {Saparov, Abulhair and Mitchell, Tom M.},
  date = {2021-12-20},
  number = {arXiv:2105.02486},
  eprint = {2105.02486},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.02486},
  urldate = {2022-07-27},
  abstract = {We introduce the Probabilistic Worldbuilding Model (PWM), a new fully-symbolic Bayesian model of semantic parsing and reasoning, as a first step in a research program toward more domain- and task-general NLU and AI. Humans create internal mental models of their observations which greatly aid in their ability to understand and reason about a large variety of problems. In PWM, the meanings of sentences, acquired facts about the world, and intermediate steps in reasoning are all expressed in a human-readable formal language, with the design goal of interpretability. PWM is Bayesian, designed specifically to be able to generalize to new domains and new tasks. We derive and implement an inference algorithm that reads sentences by parsing and abducing updates to its latent world model that capture the semantics of those sentences, and evaluate it on two out-of-domain question-answering datasets: (1) ProofWriter and (2) a new dataset we call FictionalGeoQA, designed to be more representative of real language but still simple enough to focus on evaluating reasoning ability, while being robust against heuristics. Our method outperforms baselines on both, thereby demonstrating its value as a proof-of-concept.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3SZQ3K42/Saparov and Mitchell - 2021 - Towards General Natural Language Understanding wit.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7VGM5S8A/2105.html}
}

@inproceedings{sapATOMICAtlasMachine2019,
  title = {{{ATOMIC}}: {{An Atlas}} of {{Machine Commonsense}} for {{If-Then Reasoning}}},
  booktitle = {{{AAAI}}},
  author = {Sap, M. and Bras, R. L. and Allaway, E. and Bhagavatula, C. and Lourie, N. and Rashkin, H. and Roof, B. and Smith, N. A. and Choi, Y.},
  date = {2019},
  eprint = {1811.00146},
  eprinttype = {arxiv},
  pages = {3027--3035},
  issn = {2159-5399},
  doi = {10.1609/aaai.v33i01.33013027},
  abstract = {We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., “if X pays Y a compliment, then Y will likely return the compliment”). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.},
  archiveprefix = {arXiv},
  annotation = {184 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L9JJEG33/Sap et al. - 2019 - ATOMIC An Atlas of Machine Commonsense for If-Then Reasoning(2).pdf}
}

@inproceedings{sapSOCIALIQACommonsense2019,
  title = {{{SOCIAL IQA}} : {{Commonsense Reasoning}} about {{Social Interactions Maarten Sap}}},
  booktitle = {{{EMNLP}}},
  author = {Sap, M. and Rashkin, H. and Chen, D. and Bras, R. L. and Choi, Y.},
  date = {2019},
  pages = {4453--4463},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/URXX24XS/Sap et al. - 2019 - SOCIAL IQA Commonsense Reasoning about Social Interactions Maarten Sap(2).pdf}
}

@inproceedings{sarawagiSemiMarkovConditionalRandom2005,
  title = {Semi-{{Markov Conditional Random Fields}} for {{Information Extraction}}},
  booktitle = {{{NeurIPS}}},
  author = {Sarawagi, S. and Cohen, W. W.},
  date = {2005},
  pages = {1185--1192},
  issn = {10495258},
  doi = {10.1.1.128.3524},
  url = {http://papers.nips.cc/paper/2648-semi-markov-conditional-random-fields-for-information-extraction},
  abstract = {We describe semi-Markov conditional random fields (semi-CRFs), a con-ditionally trained version of semi-Markov chains. Intuitively, a semi-CRF on an input sequence x outputs a " segmentation " of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements x i of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time—often only a small constant factor slower than conventional CRFs. In experiments on five named entity recognition problems, semi-CRFs generally outper-form conventional CRFs.},
  isbn = {0-262-19534-8},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F2GXR4S8/Sarawagi, Cohen - 2005 - Semi-Markov Conditional Random Fields for Information Extraction(2).pdf}
}

@inproceedings{sawhney2021HypMixHyperbolicInterpolative,
  title = {{{HypMix}}: {{Hyperbolic Interpolative Data Augmentation}}},
  booktitle = {{{EMNLP}}},
  author = {Sawhney, Ramit and Thakkar, Megh and Agarwal, Shivam and Jin, Di and Yang, Diyi and Flek, Lucie},
  date = {2021},
  pages = {11},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PC56YEYA/Sawhney et al. - HypMix Hyperbolic Interpolative Data Augmentation.pdf}
}

@inproceedings{sbert2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  booktitle = {{{EMNLP}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  date = {2019-08-27},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1908.10084},
  urldate = {2022-04-28},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FU7BLR7K/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/home/hiaoxui/.local/share/zotero_files/storage/46F3WZ46/1908.html}
}

@inproceedings{scao2021HowManyData,
  title = {How {{Many Data Points}} Is a {{Prompt Worth}}?},
  booktitle = {{{NAACL-HLT}}},
  author = {Scao, Teven Le and Rush, Alexander M.},
  date = {2021-04-06},
  eprint = {2103.08493},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.08493},
  urldate = {2021-05-27},
  abstract = {When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.},
  archiveprefix = {arXiv},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-05-27]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/63B5KATE/Scao and Rush - 2021 - How Many Data Points is a Prompt Worth.pdf;/home/hiaoxui/.local/share/zotero_files/storage/Q3T5P2KU/2103.html}
}

@misc{schick2021ExploitingClozeQuestions,
  title = {Exploiting {{Cloze Questions}} for {{Few Shot Text Classification}} and {{Natural Language Inference}}},
  author = {Schick, Timo and Schütze, Hinrich},
  date = {2021-01-25},
  eprint = {2001.07676},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2001.07676},
  urldate = {2021-04-09},
  abstract = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.},
  archiveprefix = {arXiv},
  annotation = {24 citations (Semantic Scholar/arXiv) [2021-04-09]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XG4YRXDK/Schick and Schütze - 2021 - Exploiting Cloze Questions for Few Shot Text Class.pdf;/home/hiaoxui/.local/share/zotero_files/storage/UU9MQXBF/2001.html}
}

@inproceedings{schick2021ItNotJust,
  title = {It's {{Not Just Size That Matters}}: {{Small Language Models Are Also Few-Shot Learners}}},
  shorttitle = {It's {{Not Just Size That Matters}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Schick, Timo and Schütze, Hinrich},
  date = {2021},
  eprint = {2009.07118},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.07118},
  urldate = {2021-04-09},
  abstract = {When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance on challenging natural language understanding benchmarks. In this work, we show that performance similar to GPT-3 can be obtained with language models whose parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain some form of task description, combined with gradient-based optimization; additionally exploiting unlabeled data gives further improvements. Based on our findings, we identify several key factors required for successful natural language understanding with small language models.},
  archiveprefix = {arXiv},
  annotation = {20 citations (Semantic Scholar/arXiv) [2021-04-09]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FG9F2D72/Schick and Schütze - 2020 - It's Not Just Size That Matters Small Language Mo.pdf;/home/hiaoxui/.local/share/zotero_files/storage/VTZH48DP/2009.html}
}

@inproceedings{schlangenGeneralAbstractModel2009,
  title = {A {{General}}, {{Abstract Model}} of {{Incremental Dialogue Processing}}},
  booktitle = {{{EACL}}},
  author = {Schlangen, D. and Skantze, G.},
  date = {2009},
  doi = {10.5087/dad.2011.105},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3GCNEIG8/Schlangen, Skantze - 2009 - A General, Abstract Model of Incremental Dialogue Processing(2).pdf}
}

@inproceedings{schnabelEvaluationMethodsUnsupervised2015,
  title = {Evaluation Methods for Unsupervised Word Embeddings},
  booktitle = {{{EMNLP}}},
  author = {Schnabel, T. and Labutov, I. and Mimno, D.},
  date = {2015},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E5GK4ED5/Schnabel, Labutov, Mimno - 2015 - Evaluation methods for unsupervised word embeddings(2).pdf}
}

@inproceedings{schneiderNLTKFrameNetAPI2017,
  title = {The {{NLTK FrameNet API}}: {{Designing}} for Discoverability with a Rich Linguistic Resource},
  booktitle = {{{EMNLP}}},
  author = {Schneider, N. and Wooters, C.},
  date = {2017},
  eprint = {1703.07438},
  eprinttype = {arxiv},
  doi = {10.18653/v1/d17-2001},
  abstract = {A new Python API, integrated within the NLTK suite, offers access to the FrameNet 1.7 lexical database. The lexicon (structured in terms of frames) as well as annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-97-5},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DYD73RBX/Schneider, Wooters - 2017 - The NLTK FrameNet API Designing for discoverability with a rich linguistic resource(2).pdf}
}

@article{scholkopf2021CausalRepresentationLearning,
  title = {Towards {{Causal Representation Learning}}},
  author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  date = {2021-02-22},
  journaltitle = {Advances in Machine Learning and Deep Neural Networks},
  eprint = {2102.11107},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.11107},
  urldate = {2021-04-07},
  abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {6 citations (Semantic Scholar/arXiv) [2021-04-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7VZQT27F/Schölkopf et al. - 2021 - Towards Causal Representation Learning.pdf;/home/hiaoxui/.local/share/zotero_files/storage/PDUR3XH6/2102.html}
}

@article{schroedl2005ImprovedSearchAlgorithm,
  title = {An Improved Search Algorithm for Optimal Multiple-Sequence Alignment},
  author = {Schroedl, S.},
  date = {2005},
  journaltitle = {JAIR},
  volume = {23},
  pages = {587--623},
  issn = {10769757},
  doi = {10.1613/jair.1534},
  abstract = {Multiple sequence alignment (MSA) is a ubiquitous problem in computational biology. Although it is NP-hard to find an optimal solution for an arbitrary number of sequences, due to the importance of this problem researchers are trying to push the limits of exact algorithms further. Since MSA can be cast as a classical path finding problem, it is attracting a growing number of AI researchers interested in heuristic search algorithms as a challenge with actual practical relevance. In this paper, we first review two previous, complementary lines of research. Based on Hirschberg's algorithm, Dynamic Programming needs O(kN\^(k-1)) space to store both the search frontier and the nodes needed to reconstruct the solution path, for k sequences of length N. Best first search, on the other hand, has the advantage of bounding the search space that has to be explored using a heuristic. However, it is necessary to maintain all explored nodes up to the final solution in order to prevent the search from re-expanding them at higher cost. Earlier approaches to reduce the Closed list are either incompatible with pruning methods for the Open list, or must retain at least the boundary of the Closed list. In this article, we present an algorithm that attempts at combining the respective advantages; like A* it uses a heuristic for pruning the search space, but reduces both the maximum Open and Closed size to O(kN\^(k-1)), as in Dynamic Programming. The underlying idea is to conduct a series of searches with successively increasing upper bounds, but using the DP ordering as the key for the Open priority queue. With a suitable choice of thresholds, in practice, a running time below four times that of A* can be expected. In our experiments we show that our algorithm outperforms one of the currently most successful algorithms for optimal multiple sequence alignments, Partial Expansion A*, both in time and memory. Moreover, we apply a refined heuristic based on optimal alignments not only of pairs of sequences, but of larger subsets. This idea is not new; however, to make it practically relevant we show that it is equally important to bound the heuristic computation appropriately, or the overhead can obliterate any possible gain. Furthermore, we discuss a number of improvements in time and space efficiency with regard to practical implementations. Our algorithm, used in conjunction with higher-dimensional heuristics, is able to calculate for the first time the optimal alignment for almost all of the problems in Reference 1 of the benchmark database BAliBASE.},
  annotation = {31 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/934DDEGS/Schroedl - 2005 - An improved search algorithm for optimal multiple-sequence alignment(2).pdf}
}

@inproceedings{sebastianDecisionTreeFields2011,
  title = {Decision {{Tree Fields}}},
  booktitle = {{{ICCV}}},
  author = {Sebastian, N. and Rother, C. and Bagon, S. and Sharp, T. and Yao, B. and Kohli, P.},
  date = {2011},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KRUUP5XB/Sebastian et al. - 2011 - Decision Tree Fields(2).pdf}
}

@inproceedings{seegerBayesianIntermittentDemand2016,
  title = {Bayesian Intermittent Demand Forecasting for Large Inventories},
  booktitle = {{{NeurIPS}}},
  author = {Seeger, M. and Salinas, D. and Flunkert, V.},
  date = {2016},
  pages = {4646--4654},
  issn = {10495258},
  abstract = {We present a scalable and robust Bayesian method for demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics. Inference is approximated by the Newton-Raphson algorithm, reduced to linear-time Kalman smoothing, which allows us to operate on several orders of magnitude larger problems than previous related work. In a study on large real-world sales datasets, our method outperforms competing approaches on fast and medium moving items.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TAQ77PR3/Seeger, Salinas, Flunkert - 2016 - Bayesian intermittent demand forecasting for large inventories(2).pdf}
}

@inproceedings{seeGetPointSummarization2017,
  title = {Get to the Point: {{Summarization}} with Pointer-Generator Networks},
  booktitle = {{{ACL}}},
  author = {See, A. and Liu, P. J. and Manning, C. D.},
  date = {2017},
  eprint = {1704.04368},
  eprinttype = {arxiv},
  pages = {1073--1083},
  doi = {10.18653/v1/P17-1099},
  abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-75-3},
  annotation = {1619 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WU8YKEDA/See, Liu, Manning - 2017 - Get to the point Summarization with pointer-generator networks(2).pdf}
}

@inproceedings{selmanPELCNFProbabilisticEvent2011,
  title = {{{PEL-CNF}} : {{Probabilistic}} Event Logic Conjunctive Normal Form for Video Interpretation},
  booktitle = {{{ICCV}}},
  author = {Selman, J. and Amer, M. and Fern, A. and Todorovic, S.},
  date = {2011},
  doi = {10.1109/ICCVW.2011.6130308},
  keywords = {unread},
  annotation = {12 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V3EKQTST/Selman et al. - 2011 - PEL-CNF Probabilistic event logic conjunctive normal form for video interpretation(2).pdf}
}

@inproceedings{selsamLearningSATSolver2018,
  title = {Learning a {{SAT Solver}} from {{Single-Bit Supervision}}},
  booktitle = {{{ICLR}}},
  author = {Selsam, D. and Lamm, M. and Bünz, B. and Liang, P. and de Moura, L. and Dill, D. L.},
  options = {useprefix=true},
  date = {2018},
  eprint = {1802.03685},
  eprinttype = {arxiv},
  pages = {1--11},
  url = {http://arxiv.org/abs/1802.03685},
  abstract = {We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.},
  archiveprefix = {arXiv},
  annotation = {173 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7J4NDBJX/Selsam et al. - 2018 - Learning a SAT Solver from Single-Bit Supervision(2).pdf}
}

@inproceedings{sennrich2016NeuralMachineTranslation,
  title = {Neural {{Machine Translation}} of {{Rare Words}} with {{Subword Units}}},
  booktitle = {{{ACL}}},
  author = {Sennrich, R. and Haddow, B. and Birch, A.},
  date = {2016},
  eprint = {1508.07909},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1508.07909},
  abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {3268 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z3FLMTPQ/Sennrich, Haddow, Birch - 2016 - Neural Machine Translation of Rare Words with Subword Units(2).pdf}
}

@inproceedings{sennrich2017HowGrammaticalCharacterlevel,
  title = {How Grammatical Is Character-Level Neural Machine Translation? {{Assessing}} Mt Quality with Contrastive Translation Pairs},
  booktitle = {{{EACL}}},
  author = {Sennrich, R.},
  date = {2017},
  eprint = {1612.04629},
  eprinttype = {arxiv},
  pages = {376--382},
  doi = {10.18653/v1/e17-2060},
  abstract = {Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of error. We present LingEval971, a large-scale data set of 97 000 contrastive translation pairs based on the WMT English!German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-3860-4},
  keywords = {unread},
  annotation = {99 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZGLBDP85/Sennrich - 2017 - How grammatical is character-level neural machine translation Assessing mt quality with contrastive translation pai(2).pdf}
}

@inproceedings{seo2017BidirectionalAttentionFlow,
  title = {Bidirectional {{Attention Flow}} for {{Machine Comprehension}}},
  booktitle = {{{ICLR}}},
  author = {Seo, M. and Kembhavi, A. and Farhadi, A. and Hajishirzi, H.},
  date = {2017},
  eprint = {1611.01603},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.01603},
  abstract = {Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {1295 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9Y9T5UUA/Seo et al. - 2017 - Bidirectional Attention Flow for Machine Comprehension(2).pdf}
}

@inproceedings{serrano2019AttentionInterpretable,
  title = {Is {{Attention Interpretable}}?},
  booktitle = {{{ACL}}},
  author = {Serrano, S. and Smith, N. A.},
  date = {2019},
  eprint = {1906.03731},
  eprinttype = {arxiv},
  pages = {2931--2951},
  doi = {10.18653/v1/p19-1282},
  abstract = {Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.},
  archiveprefix = {arXiv},
  annotation = {115 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J9ZITHCM/Serrano, Smith - 2019 - Is Attention Interpretable(2).pdf}
}

@misc{shaham2022SCROLLSStandardizedCompaRison,
  title = {{{SCROLLS}}: {{Standardized CompaRison Over Long Language Sequences}}},
  shorttitle = {{{SCROLLS}}},
  author = {Shaham, Uri and Segal, Elad and Ivgi, Maor and Efrat, Avia and Yoran, Ori and Haviv, Adi and Gupta, Ankit and Xiong, Wenhan and Geva, Mor and Berant, Jonathan and Levy, Omer},
  date = {2022-01-10},
  eprint = {2201.03533},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.03533},
  urldate = {2022-01-17},
  abstract = {NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and handpick ones where the text is naturally long, while prioritizing tasks that involve synthesizing information across the input. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. Initial baselines, including Longformer Encoder-Decoder, indicate that there is ample room for improvement on SCROLLS. We make all datasets available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UZLSQW9T/Shaham et al. - 2022 - SCROLLS Standardized CompaRison Over Long Languag.pdf}
}

@inproceedings{shang2015NeuralRespondingMachine,
  title = {Neural {{Responding Machine}} for {{Short-Text Conversation}}},
  booktitle = {{{ACL}}},
  author = {Shang, L. and Lu, Z. and Li, H.},
  date = {2015},
  eprint = {1503.02364},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1503.02364},
  abstract = {We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75\% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {822 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XU4VZ2YD/Shang, Lu, Li - 2015 - Neural Responding Machine for Short-Text Conversation(2).pdf}
}

@inproceedings{sharan2017LearningOvercompleteHMMs,
  title = {Learning {{Overcomplete HMMs}}},
  booktitle = {{{NeurIPS}}},
  author = {Sharan, V. and Kakade, S. and Liang, P. and Valiant, G.},
  date = {2017},
  eprint = {1711.02309},
  eprinttype = {arxiv},
  pages = {1--10},
  url = {http://arxiv.org/abs/1711.02309},
  abstract = {We study the problem of learning overcomplete HMMs---those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results---both positive and negative---which help define the boundaries between the tractable and intractable settings. Specifically, we show positive results for a large subclass of HMMs whose transition matrices are sparse, well-conditioned, and have small probability mass on short cycles. On the other hand, we show that learning is impossible given only a polynomial number of samples for HMMs with a small output alphabet and whose transition matrices are random regular graphs with large degree. We also discuss these results in the context of learning HMMs which can capture long-term dependencies.},
  archiveprefix = {arXiv},
  annotation = {10 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RZXHZB7S/Sharan et al. - 2017 - Learning Overcomplete HMMs(2).pdf}
}

@misc{sharma2021MetaXTMetaCrossTask,
  title = {{{MetaXT}}: {{Meta Cross-Task Transfer}} between {{Disparate Label Spaces}}},
  shorttitle = {{{MetaXT}}},
  author = {Sharma, Srinagesh and Zheng, Guoqing and Awadallah, Ahmed Hassan},
  date = {2021-09-09},
  eprint = {2109.04240},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.04240},
  urldate = {2021-09-12},
  abstract = {Albeit the universal representational power of pre-trained language models, adapting them onto a specific NLP task still requires a considerably large amount of labeled data. Effective task fine-tuning meets challenges when only a few labeled examples are present for the task. In this paper, we aim to the address of the problem of few shot task learning by exploiting and transferring from a different task which admits a related but disparate label space. Specifically, we devise a label transfer network (LTN) to transform the labels from source task to the target task of interest for training. Both the LTN and the model for task prediction are learned via a bi-level optimization framework, which we term as MetaXT. MetaXT offers a principled solution to best adapt a pre-trained language model to the target task by transferring knowledge from the source task. Empirical evaluations on cross-task transfer settings for four NLP tasks, from two different types of label space disparities, demonstrate the effectiveness of MetaXT, especially when the labeled data in the target task is limited.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-12]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HJF6RNI6/Sharma et al. - 2021 - MetaXT Meta Cross-Task Transfer between Disparate.pdf;/home/hiaoxui/.local/share/zotero_files/storage/EVQ9QKUN/2109.html}
}

@article{shelton2014TutorialStructuredContinuoustime,
  title = {Tutorial on Structured Continuous-Time {{Markov}} Processes},
  author = {Shelton, C. R. and Ciardo, G.},
  date = {2014},
  journaltitle = {JAIR},
  volume = {51},
  pages = {725--778},
  issn = {10769757},
  abstract = {A continuous-time Markov process (CTMP) is a collection of variables indexed by a continuous quantity, time. It obeys the Markov property that the distribution over a future variable is independent of past variables given the state at the present time. We introduce continuous-time Markov process representations and algorithms for filtering, smoothing, expected sufficient statistics calculations, and model estimation, assuming no prior knowledge of continuous-time processes but some basic knowledge of probability and statistics. We begin by describing "flat" or unstructured Markov processes and then move to structured Markov processes (those arising from state spaces consisting of assignments to variables) including Kronecker, decision-diagram, and continuous-time Bayesian network representations. We provide the first connection between decision-diagrams and continuous-time Bayesian networks.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6DJWC4Z3/Shelton, Ciardo - 2014 - Tutorial on structured continuous-time Markov processes(2).pdf}
}

@inproceedings{shen2016MinimumRiskTraining,
  title = {Minimum {{Risk Training}} for {{Neural Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Shen, S. and Cheng, Y. and He, Z. and He, W. and Wu, H. and Sun, M. and Liu, Y.},
  date = {2016},
  eprint = {1512.02433},
  eprinttype = {arxiv},
  pages = {1--9},
  url = {http://arxiv.org/abs/1512.02433},
  abstract = {We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. Experiments on Chinese-English and English-French translation show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system.},
  archiveprefix = {arXiv},
  annotation = {314 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CPTI39QY/Shen et al. - 2016 - Minimum Risk Training for Neural Machine Translation(2).pdf}
}

@inproceedings{shen2018NASHEndtoEndNeural,
  title = {{{NASH}}: {{Toward End-to-End Neural Architecture}} for {{Generative Semantic Hashing}}},
  booktitle = {{{ACL}}},
  author = {Shen, D. and Su, Q. and Chapfuwa, P. and Wang, W. and Wang, G. and Carin, L. and Henao, R.},
  date = {2018},
  eprint = {1805.05361},
  eprinttype = {arxiv},
  pages = {1--10},
  abstract = {Semantic hashing has become a power-ful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous tech-niques generally require two-stage train-ing, and the binary constraints are han-dled ad-hoc. In this paper, we present an end-to-end Neural Architecture for Se-mantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli la-tent variables. A neural variational in-ference framework is proposed for train-ing, where gradients are directly back-propagated through the discrete latent variable to optimize the hash function. We also draw connections between pro-posed method and rate-distortion the-ory, which provides a theoretical foun-dation for the effectiveness of the pro-posed framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsuper-vised and supervised scenarios.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C6GM5457/Shen et al. - 2018 - NASH Toward End-to-End Neural Architecture for Generative Semantic Hashing(2).pdf}
}

@inproceedings{shen2019OrderedNeuronsIntegrating,
  title = {Ordered {{Neurons}}: {{Integrating Tree Structures}} into {{Recurrent Neural Networks}}},
  booktitle = {{{ICLR}}},
  author = {Shen, Y. and Tan, S. and Sordoni, A. and Courville, A.},
  date = {2019},
  eprint = {1810.09536},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1810.09536},
  abstract = {Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents. This paper proposes to add such an inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated. Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {143 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5FSXHJH6/Shen et al. - 2019 - Ordered Neurons Integrating Tree Structures into Recurrent Neural Networks(2).pdf}
}

@inproceedings{sheng2021CasEEJointLearning,
  title = {{{CasEE}}: {{A Joint Learning Framework}} with {{Cascade Decoding}} for {{Overlapping Event Extraction}}},
  shorttitle = {{{CasEE}}},
  booktitle = {{{ACL}}},
  author = {Sheng, Jiawei and Guo, Shu and Yu, Bowen and Li, Qian and Hei, Yiming and Wang, Lihong and Liu, Tingwen and Xu, Hongbo},
  date = {2021-07-04},
  eprint = {2107.01583},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.01583},
  urldate = {2021-07-23},
  abstract = {Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Most existing methods assume that events appear in sentences without overlaps, which are not applicable to the complicated overlapping event extraction. This work systematically studies the realistic event overlapping problem, where a word may serve as triggers with several types or arguments with different roles. To tackle the above problem, we propose a novel joint learning framework with cascade decoding for overlapping event extraction, termed as CasEE. Particularly, CasEE sequentially performs type detection, trigger extraction and argument extraction, where the overlapped targets are extracted separately conditioned on the specific former prediction. All the subtasks are jointly learned in a framework to capture dependencies among the subtasks. The evaluation on a public event extraction benchmark FewFC demonstrates that CasEE achieves significant improvements on overlapping event extraction over previous competitive methods.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K7Z6CLIM/Sheng et al. - 2021 - CasEE A Joint Learning Framework with Cascade Dec.pdf;/home/hiaoxui/.local/share/zotero_files/storage/HCNNRLKX/2107.html}
}

@inproceedings{sherborne2022ZeroShotCrosslingualSemantic,
  title = {Zero-{{Shot Cross-lingual Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Sherborne, Tom and Lapata, Mirella},
  date = {2022-03-07},
  eprint = {2104.07554},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.07554},
  urldate = {2022-03-23},
  abstract = {Recent work in cross-lingual semantic parsing has successfully applied machine translation to localize parsers to new languages. However, these advances assume access to high-quality machine translation systems and word alignment tools. We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem, without parallel data (i.e., utterance-logical form pairs) for new languages. We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-logical form paired data and in-domain natural language corpora in each new language. Our model encourages language-agnostic encodings by jointly optimizing for logical-form generation with auxiliary objectives designed for cross-lingual latent representation alignment. Our parser performs significantly above translation-based baselines and, in some cases, competes with the supervised upper-bound.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TWGCFE2Y/Sherborne and Lapata - 2022 - Zero-Shot Cross-lingual Semantic Parsing.pdf;/home/hiaoxui/.local/share/zotero_files/storage/LK7JIEBE/2104.html}
}

@inproceedings{shi2018StructuredWordEmbedding,
  title = {Structured {{Word Embedding}} for {{Low Memory Neural Network Language Model}}},
  booktitle = {Interspeech},
  author = {Shi, K. and Yu, K.},
  date = {2018},
  pages = {1254--1258},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YNKUY64N/Shi, Yu - 2018 - Structured Word Embedding for Low Memory Neural Network Language Model(2).pdf}
}

@inproceedings{shi2019FastDirectSearch,
  title = {Fast {{Direct Search}} in an {{Optimally Compressed Continuous Target Space}} for {{Efficient Multi-Label Active Learning}}},
  booktitle = {{{ICML}}},
  author = {Shi, W. and Yu, Q.},
  date = {2019},
  volume = {97},
  pages = {5769--5778},
  url = {http://proceedings.mlr.press/v97/shi19b.html},
  abstract = {Active learning for multi-label classification poses fundamental challenges given the complex label correlations and a potentially large and sparse label space. We propose a novel CS-BPCA process that integrates compressed sensing and Bayesian principal component analysis to perform a two-level label transformation, resulting in an optimally compressed continuous target space. Besides leveraging correlation and sparsity of a large label space for effective compression, an optimal compressing rate and the relative importance of the resultant targets are automatically determined through Bayesian inference. Furthermore, the orthogonality of the transformed space completely decouples the correlations among targets, which significantly simplifies multi-label sampling in the target space. We define a novel sampling function that leverages a multi-output Gaussian Process (MOGP). Gradient-free optimization strategies are developed to achieve fast online hyper-parameter learning and model retraining for active learning. Experimental results over multiple real-world datasets and comparison with competitive multi-label active learning models demonstrate the effectiveness of the proposed framework.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S8A9XFIC/Shi, Yu - 2019 - Fast Direct Search in an Optimally Compressed Continuous Target Space for Efficient Multi-Label Active Learning(2).pdf}
}

@inproceedings{shi2019RetrofittingContextualizedWord,
  title = {Retrofitting {{Contextualized Word Embeddings}} with {{Paraphrases}}},
  booktitle = {{{EMNLP}}},
  author = {Shi, W. and Chen, M. and Zhou, P. and Chang, K.},
  date = {2019},
  eprint = {1909.09700},
  eprinttype = {arxiv},
  pages = {1198--1203},
  url = {http://arxiv.org/abs/1909.09700},
  abstract = {Contextualized word embedding models, such as ELMo, generate meaningful representations of words and their context. These models have been shown to have a great impact on downstream applications. However, in many cases, the contextualized embedding of a word changes drastically when the context is paraphrased. As a result, the downstream model is not robust to paraphrasing and other linguistic variations. To enhance the stability of contextualized word embedding models, we propose an approach to retrofitting contextualized embedding models with paraphrase contexts. Our method learns an orthogonal transformation on the input space, which seeks to minimize the variance of word representations on paraphrased contexts. Experiments show that the retrofitted model significantly outperforms the original ELMo on various sentence classification and language inference tasks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7NLTBFGR/Shi et al. - 2019 - Retrofitting Contextualized Word Embeddings with Paraphrases(2).pdf}
}

@misc{shi2019SimpleBERTModels,
  title = {Simple {{BERT Models}} for {{Relation Extraction}} and {{Semantic Role Labeling}}},
  author = {Shi, Peng and Lin, Jimmy},
  date = {2019-04-10},
  eprint = {1904.05255},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.05255},
  urldate = {2020-10-21},
  abstract = {We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {86 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9BENHYKV/Shi and Lin - 2019 - Simple BERT Models for Relation Extraction and Sem.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QCJ8HGRG/1904.html}
}

@inproceedings{shi2019VisuallyGroundedNeural,
  title = {Visually {{Grounded Neural Syntax Acquisition}}},
  booktitle = {{{ACL}}},
  author = {Shi, H. and Mao, J. and Gimpel, K. and Livescu, K.},
  date = {2019},
  eprint = {1906.02890},
  eprinttype = {arxiv},
  pages = {1842--1861},
  doi = {10.18653/v1/p19-1180},
  abstract = {We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without any explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees. We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data. We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {24 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZXYXT5B/Shi et al. - 2019 - Visually Grounded Neural Syntax Acquisition(2).pdf}
}

@misc{shi2020RobustnessVerificationTransformers,
  title = {Robustness {{Verification}} for {{Transformers}}},
  author = {Shi, Zhouxing and Zhang, Huan and Chang, Kai-Wei and Huang, Minlie and Hsieh, Cho-Jui},
  date = {2020-12-23},
  number = {arXiv:2002.06622},
  eprint = {2002.06622},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2002.06622},
  urldate = {2022-10-25},
  abstract = {Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. However, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. We resolve these challenges and develop the first robustness verification algorithm for Transformers. The certified robustness bounds computed by our method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of different words in sentiment analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PUY3JB4H/Shi et al. - 2020 - Robustness Verification for Transformers.pdf;/home/hiaoxui/.local/share/zotero_files/storage/RYDDG7LU/2002.html}
}

@inproceedings{shi2022RevisitingOverSmoothingBERT,
  title = {Revisiting {{Over-Smoothing}} in {{BERT From The Perspective}} of {{Graph}}},
  booktitle = {{{ICLR}}},
  author = {Shi, Han and Gao, Jiahui and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Kong, Lingpeng and Lee, Stephen M S and Kwok, James T},
  date = {2022},
  pages = {15},
  abstract = {Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6BKXIJ4A/Shi et al. - 2022 - REVISITING OVER-SMOOTHING IN BERT FROM THE PERSPEC.pdf}
}

@inproceedings{shi2022SamplingMirroredStein,
  title = {Sampling with {{Mirrored Stein Operators}}},
  booktitle = {{{ICLR}}},
  author = {Shi, Jiaxin and Liu, Chang and Mackey, Lester},
  date = {2022},
  eprint = {2106.12506},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.12506},
  urldate = {2022-02-24},
  abstract = {We introduce a new family of particle evolution samplers suitable for constrained domains and non-Euclidean geometries. Stein Variational Mirror Descent and Mirrored Stein Variational Gradient Descent minimize the Kullback-Leibler (KL) divergence to constrained target distributions by evolving particles in a dual space defined by a mirror map. Stein Variational Natural Gradient exploits non-Euclidean geometry to more efficiently minimize the KL divergence to unconstrained targets. We derive these samplers from a new class of mirrored Stein operators and adaptive kernels developed in this work. We demonstrate that these new samplers yield accurate approximations to distributions on the simplex, deliver valid confidence intervals in post-selection inference, and converge more rapidly than prior methods in large-scale unconstrained posterior inference. Finally, we establish the convergence of our new procedures under verifiable conditions on the target distribution.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JVEU6PLL/Shi et al. - 2021 - Sampling with Mirrored Stein Operators.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7SRKQ9LM/2106.html}
}

@article{shibuya2020NestedNamedEntity,
  title = {Nested {{Named Entity Recognition}} via {{Second-best Sequence Learning}} and {{Decoding}}},
  author = {Shibuya, Takashi and Hovy, Eduard},
  date = {2020-09},
  journaltitle = {TACL},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {605--620},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00334},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00334},
  urldate = {2021-01-16},
  abstract = {When an entity name contains other names within it, the identification of all combinations of names can become difficult and expensive. We propose a new method to recognize not only outermost named entities but also inner nested ones. We design an objective function for training a neural model that treats the tag sequence for nested entities as the second best path within the span of their parent entity. In addition, we provide the decoding method for inference that extracts entities iteratively from outermost ones to inner ones in an outsideto-inside way. Our method has no additional hyperparameters to the conditional random field based model widely used for flat named entity recognition tasks. Experiments demonstrate that our method performs better than or at least as well as existing methods capable of handling nested entities, achieving F1-scores of 85.82\%, 84.34\%, and 77.36\% on ACE2004, ACE-2005, and GENIA datasets, respectively.},
  langid = {english},
  annotation = {11 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4MP4RESG/Shibuya and Hovy - 2020 - Nested Named Entity Recognition via Second-best Se.pdf}
}

@misc{shih2019XLEditorPosteditingSentences,
  title = {{{XL-Editor}}: {{Post-editing Sentences}} with {{XLNet}}},
  author = {Shih, Y. and Chang, W. and Yang, Y.},
  date = {2019},
  eprint = {1910.10479},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.10479},
  abstract = {While neural sequence generation models achieve initial success for many NLP applications, the canonical decoding procedure with left-to-right generation order (i.e., autoregressive) in one-pass can not reflect the true nature of human revising a sentence to obtain a refined result. In this work, we propose XL-Editor, a novel training framework that enables state-of-the-art generalized autoregressive pretraining methods, XLNet specifically, to revise a given sentence by the variable-length insertion probability. Concretely, XL-Editor can (1) estimate the probability of inserting a variable-length sequence into a specific position of a given sentence; (2) execute post-editing operations such as insertion, deletion, and replacement based on the estimated variable-length insertion probability; (3) complement existing sequence-to-sequence models to refine the generated sequences. Empirically, we first demonstrate better post-editing capabilities of XL-Editor over XLNet on the text insertion and deletion tasks, which validates the effectiveness of our proposed framework. Furthermore, we extend XL-Editor to the unpaired text style transfer task, where transferring the target style onto a given sentence can be naturally viewed as post-editing the sentence into the target style. XL-Editor achieves significant improvement in style transfer accuracy and also maintains coherent semantic of the original sentence, showing the broad applicability of our method.},
  archiveprefix = {arXiv},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ME59ZJ87/Shih, Chang, Yang - 2019 - XL-Editor Post-editing Sentences with XLNet(2).pdf}
}

@inproceedings{shin2020AutoPromptElicitingKnowledge,
  title = {{{AutoPrompt}}: {{Eliciting Knowledge}} from {{Language Models}} with {{Automatically Generated Prompts}}},
  shorttitle = {{{AutoPrompt}}},
  booktitle = {{{EMNLP}}},
  author = {Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L. and Wallace, Eric and Singh, Sameer},
  date = {2020-11-07},
  eprint = {2010.15980},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.15980},
  urldate = {2020-11-10},
  abstract = {The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.},
  archiveprefix = {arXiv},
  annotation = {9 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U8I2BJ8W/Shin et al. - 2020 - AutoPrompt Eliciting Knowledge from Language Mode.pdf;/home/hiaoxui/.local/share/zotero_files/storage/WTNYEMLN/2010.html}
}

@misc{shin2021ConstrainedLanguageModels,
  title = {Constrained {{Language Models Yield Few-Shot Semantic Parsers}}},
  author = {Shin, Richard and Lin, Christopher H. and Thomson, Sam and Chen, Charles and Roy, Subhro and Platanios, Emmanouil Antonios and Pauls, Adam and Klein, Dan and Eisner, Jason and Van Durme, Benjamin},
  date = {2021-04-18},
  eprint = {2104.08768},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.08768},
  urldate = {2021-05-25},
  abstract = {We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. With a small amount of data and very little code to convert into English-like representations, we provide a blueprint for rapidly bootstrapping semantic parsers and demonstrate good performance on multiple tasks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-05-25]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V3UQTADZ/Shin et al. - 2021 - Constrained Language Models Yield Few-Shot Semanti.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QJNH4P6Z/2104.html}
}

@inproceedings{shiv2019NovelPositionalEncodings,
  title = {Novel {{Positional Encodings}} to {{Enable}} Tree-{{Structured}} Transformers},
  booktitle = {{{NeurIPS}}},
  author = {Shiv, V. L. and Quirk, C.},
  date = {2019},
  abstract = {With interest in program synthesis and similarly flavored problems rapidly increasing , neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6RUCJK3H/Shiv, Quirk - 2019 - Novel Positional Encodings to Enable tree-Structured transformers(2).pdf}
}

@inproceedings{shu2018CompressingWordEmbeddings,
  title = {Compressing {{Word Embeddings}} via {{Deep Computational Code Learning}}},
  booktitle = {{{ICLR}}},
  author = {Shu, R. and Nakayama, H.},
  date = {2018},
  eprint = {1711.01068v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GNNLMCRY/Shu, Nakayama - 2018 - Compressing Word Embeddings via Deep Computational Code Learning(2).pdf}
}

@inproceedings{shwartz2019StillPainNeck,
  title = {Still a {{Pain}} in the {{Neck}}: {{Evaluating Text Representations}} on {{Lexical Composition}}},
  booktitle = {{{EMNLP}}},
  author = {Shwartz, V. and Dagan, I.},
  date = {2019},
  volume = {7},
  eprint = {1902.10618},
  eprinttype = {arxiv},
  pages = {403--419},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00277},
  abstract = {Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans. Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {20 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HTBQPDDS/Shwartz, Dagan - 2019 - Still a Pain in the Neck Evaluating Text Representations on Lexical Composition(2).pdf}
}

@misc{shwartz2020YouAreGrounded,
  title = {"{{You}} Are Grounded!": {{Latent Name Artifacts}} in {{Pre-trained Language Models}}},
  author = {Shwartz, V. and Rudinger, R. and Tafjord, O.},
  date = {2020},
  eprint = {2004.03012},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.03012},
  abstract = {Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for "Donald is a" substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HWLCH3LZ/Shwartz, Rudinger, Tafjord - 2020 - You are grounded! Latent Name Artifacts in Pre-trained Language Models(2).pdf}
}

@inproceedings{siamese2014,
  title = {{{DeepFace}}: {{Closing}} the {{Gap}} to {{Human-Level Performance}} in {{Face Verification}}},
  shorttitle = {{{DeepFace}}},
  booktitle = {{{CVPR}}},
  author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  date = {2014-06},
  pages = {1701--1708},
  publisher = {{IEEE}},
  location = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.220},
  url = {https://ieeexplore.ieee.org/document/6909616},
  urldate = {2022-10-03},
  abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ⇒ align ⇒ represent ⇒ classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4,000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4799-5118-5},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G66UXGN7/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performan.pdf}
}

@article{silver2016MasteringGameGo,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, D. and Huang, A. and Maddison, C. J. and Guez, A. and Sifre, L. and Van Den Driessche, G. and Schrittwieser, J. and Antonoglou, I. and Panneershelvam, V. and Lanctot, M. and Dieleman, S. and Grewe, D. and Nham, J. and Kalchbrenner, N. and Sutskever, I. and Lillicrap, T. and Leach, M. and Kavukcuoglu, K. and Graepel, T. and Hassabis, D.},
  date = {2016},
  journaltitle = {Nature},
  volume = {529},
  number = {7587},
  eprint = {26819042},
  eprinttype = {pmid},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  issn = {14764687},
  doi = {10.1038/nature16961},
  url = {http://dx.doi.org/10.1038/nature16961},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  annotation = {8357 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BTYRW9LA/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search(2).pdf}
}

@inproceedings{simcse2021,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  booktitle = {{{EMNLP}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  date = {2021-09-09},
  eprint = {2104.08821},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.08821},
  urldate = {2022-04-28},
  abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to previous best results. We also show -- both theoretically and empirically -- that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
  archiveprefix = {arXiv},
  keywords = {constrastive},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BXIQLR22/Gao et al. - 2021 - SimCSE Simple Contrastive Learning of Sentence Em.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KKPNA9ZZ/2104.html}
}

@misc{simonyan2014DeepConvolutionalNetworks,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-04-19},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2022-02-07},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DLYT7AQA/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf;/home/hiaoxui/.local/share/zotero_files/storage/WA9UIN7J/1312.html}
}

@incollection{singh2002OpenMindCommon,
  title = {Open {{Mind Common Sense}}: {{Knowledge Acquisition}} from the {{General Public}}},
  shorttitle = {Open {{Mind Common Sense}}},
  booktitle = {On the {{Move}} to {{Meaningful Internet Systems}} 2002: {{CoopIS}}, {{DOA}}, and {{ODBASE}}},
  author = {Singh, Push and Lin, Thomas and Mueller, Erik T. and Lim, Grace and Perkins, Travell and Li Zhu, Wan},
  editor = {Meersman, Robert and Tari, Zahir},
  options = {useprefix=true},
  date = {2002},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {2519},
  pages = {1223--1237},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36124-3_77},
  url = {http://link.springer.com/10.1007/3-540-36124-3_77},
  urldate = {2020-11-14},
  abstract = {Open Mind Common Sense is a knowledge acquisition system designed to acquire commonsense knowledge from the general public over the web. We describe and evaluate our first fielded system, which enabled the construction of a 450,000 assertion commonsense knowledge base. We then discuss how our second-generation system addresses weaknesses discovered in the first. The new system acquires facts, descriptions, and stories by allowing participants to construct and fill in natural language templates. It employs word-sense disambiguation and methods of clarifying entered knowledge, analogical inference to provide feedback, and allows participants to validate knowledge and in turn each other.},
  editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
  editorbtype = {redactor},
  isbn = {978-3-540-00106-5 978-3-540-36124-4},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6KDCELET/Singh et al. - 2002 - Open Mind Common Sense Knowledge Acquisition from.pdf}
}

@misc{singh2018NaturalLanguageProcessing,
  title = {Natural {{Language Processing}} for {{Information Extraction}}},
  author = {Singh, S.},
  date = {2018},
  eprint = {1807.02383v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y4AC6ZLZ/Singh - 2018 - Natural Language Processing for Information Extraction(2).pdf}
}

@inproceedings{singh2020CopyNextExplicitSpan,
  title = {{{CopyNext}}: {{Explicit Span Copying}} and {{Alignment}} in {{Sequence}} to {{Sequence Models}}},
  shorttitle = {{{CopyNext}}},
  booktitle = {Workshop on {{Structured Prediction}} for {{NLP}}},
  author = {Singh, Abhinav and Xia, Patrick and Qin, Guanghui and Yarmohammadi, Mahsa and Van Durme, Benjamin},
  date = {2020-10-28},
  eprint = {2010.15266},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.15266},
  urldate = {2021-04-17},
  abstract = {Copy mechanisms are employed in sequence to sequence models (seq2seq) to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from. Further, they require contiguous token sequences from the input (spans) to be copied individually. We present a model with an explicit token-level copy operation and extend it to copying entire spans. Our model provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like information extraction. We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art accuracy with an order of magnitude increase in decoding speed.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-04-17]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WI6WHXIH/Singh et al. - 2020 - CopyNext Explicit Span Copying and Alignment in S.pdf;/home/hiaoxui/.local/share/zotero_files/storage/4MUNMPQR/2010.html}
}

@article{siskind1996ComputationalStudyCrosssituational,
  title = {A Computational Study of Cross-Situational Techniques for Learning Word-to-Meaning Mappings},
  author = {Siskind, J. M.},
  date = {1996},
  journaltitle = {Cognition},
  volume = {61},
  number = {1-2},
  eprint = {8990968},
  eprinttype = {pmid},
  pages = {39--91},
  issn = {00100277},
  doi = {10.1016/S0010-0277(96)00728-7},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027796007287},
  abstract = {This paper presents a computational study of part of the lexical-acquisition task faced by children, namely the acquisition of word-to-meaning mappings. It first approximates this task as a formal mathematical problem. It then presents an implemented algorithm for solving this problem, illustrating its operation on a small example. This algorithm offers one precise interpretation of the intuitive notions of cross-situational learning and the principle of contrast applied between words in an utterance. It robustly learns a homonymous lexicon despite noisy multi-word input, in the presence of referential uncertainty, with no prior knowledge that is specific to the language being learned. Computational simulations demonstrate the robustness of this algorithm and illustrate how algorithms based on cross-situational learning and the principle of contrast might be able to solve lexical-acquisition problems of the size faced by children, under weak, worst-case assumptions about the type and quantity of data available.},
  isbn = {0010-0277},
  keywords = {unread},
  annotation = {529 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U964HHSK/Siskind - 1996 - A computational study of cross-situational techniques for learning word-to-meaning mappings(2).pdf}
}

@article{skarding2021FoundationsModellingDynamic,
  title = {Foundations and Modelling of Dynamic Networks Using {{Dynamic Graph Neural Networks}}: {{A}} Survey},
  shorttitle = {Foundations and Modelling of Dynamic Networks Using {{Dynamic Graph Neural Networks}}},
  author = {Skarding, Joakim and Gabrys, Bogdan and Musial, Katarzyna},
  date = {2021},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {9},
  eprint = {2005.07496},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {79143--79168},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3082932},
  url = {http://arxiv.org/abs/2005.07496},
  urldate = {2022-07-01},
  abstract = {Dynamic networks are used in a wide range of fields, including social network analysis, recommender systems, and epidemiology. Representing complex networks as structures changing over time allow network models to leverage not only structural but also temporal patterns. However, as dynamic network literature stems from diverse fields and makes use of inconsistent terminology, it is challenging to navigate. Meanwhile, graph neural networks (GNNs) have gained a lot of attention in recent years for their ability to perform well on a range of network science tasks, such as link prediction and node classification. Despite the popularity of graph neural networks and the proven benefits of dynamic network models, there has been little focus on graph neural networks for dynamic networks. To address the challenges resulting from the fact that this research crosses diverse fields as well as to survey dynamic graph neural networks, this work is split into two main parts. First, to address the ambiguity of the dynamic network terminology we establish a foundation of dynamic networks with consistent, detailed terminology and notation. Second, we present a comprehensive survey of dynamic graph neural network models using the proposed terminology},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XGACLQVE/Skarding et al. - 2021 - Foundations and modelling of dynamic networks usin.pdf;/home/hiaoxui/.local/share/zotero_files/storage/B4FC38GX/2005.html}
}

@inproceedings{skipprop2017,
  title = {Skip-{{Prop}}: {{Representing Sentences}} with {{One Vector Per Proposition}}},
  booktitle = {{{IWCS}}},
  author = {Rudinger, Rachel and Duh, Kevin and Durme, Benjamin Van},
  date = {2017},
  pages = {7},
  abstract = {We introduce the notion of a multi-vector sentence representation based on a “one vector per proposition” philosophy, which we term skip-prop vectors. By representing each predicate-argument structure in a complex sentence as an individual vector, skip-prop is (1) a response to empirical evidence that single-vector sentence representations degrade with sentence length, and (2) a representation that maintains a semantically useful level of granularity. We demonstrate the feasibility of training skip-prop vectors, introducing a method adapted from skip-thought vectors, and compare skip-prop with “one vector per sentence” and “one vector per token” approaches.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P32SR6N3/Rudinger et al. - Skip-Prop Representing Sentences with One Vector .pdf}
}

@inproceedings{slobodkin2021MediatorsDeterminingWhat,
  title = {Mediators in {{Determining}} What {{Processing BERT Performs First}}},
  booktitle = {{{NAACL}}},
  author = {Slobodkin, Aviv and Choshen, Leshem and Abend, Omri},
  date = {2021},
  pages = {86--93},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.8},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.8},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GQQI8FG6/Slobodkin et al. - 2021 - Mediators in Determining what Processing BERT Perf.pdf}
}

@inproceedings{smiley2016WhenPlummetWhen,
  title = {When to {{Plummet}} and {{When}} to {{Soar}}: {{Corpus Based Verb Selection}} for {{Natural Language Generation}}},
  booktitle = {{{INLG}}},
  author = {Smiley, C. and Plachouras, V. and Schilder, F. and Bretz, H. and Leidner, J. L. and Song, D.},
  date = {2016},
  pages = {36--39},
  abstract = {For data-to-text tasks in Natural Language Generation (NLG), researchers are often faced with choices about the right words to express phenomena seen in the data. One common phenomenon centers around the description of trends between two data points and selecting the appropriate verb to express both the di-rection and intensity of movement. Our re-search shows that rather than simply select-ing the same verbs again and again, variation and naturalness can be achieved by quantify-ing writers' patterns of usage around verbs.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YRX6RT6H/Smiley et al. - 2016 - When to Plummet and When to Soar Corpus Based Verb Selection for Natural Language Generation(2).pdf}
}

@inproceedings{smith2005ContrastiveEstimationTraining,
  title = {Contrastive Estimation: {{Training Log-Linear Models}} on {{Unlabeled Data}}},
  booktitle = {{{ACL}}},
  author = {Smith, N. A. and Eisner, J. M.},
  date = {2005},
  pages = {354--362},
  doi = {10.3115/1219840.1219884},
  abstract = {Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.},
  isbn = {1-932432-51-5},
  issue = {June},
  keywords = {unread},
  annotation = {351 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SJP6IN36/Smith, Eisner - 2005 - Contrastive estimation Training Log-Linear Models on Unlabeled Data(2).pdf}
}

@inproceedings{smith2012UnsupervisedLearningApproximate,
  title = {Unsupervised Learning on an Approximate Corpus},
  booktitle = {{{NAACL-HLT}}},
  author = {Smith, J. and Eisner, J. M.},
  date = {2012},
  pages = {131--141},
  abstract = {Unsupervised learning techniques can take advantage of large amounts of unannotated text, but the largest text corpus (the Web) is not easy to use in its full form. Instead, we have statistics about this corpus in the form of n-gram counts (Brants and Franz, 2006). While n-gram counts do not directly provide sentences, a distribution over sentences can be estimated from them in the same way that ngram language models are estimated. We treat this distribution over sentences as an approximate corpus and show how unsupervised learning can be performed on such a corpus using variational inference. We compare hidden Markov model (HMM) training on exact and approximate corpora of various sizes, measuring speed and accuracy on unsupervised part-of-speech tagging.},
  isbn = {1-937284-20-4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DJI7SG9U/Smith, Eisner - 2012 - Unsupervised learning on an approximate corpus(2).pdf}
}

@inproceedings{smith2013CommonCrawl,
  title = {Dirt {{Cheap Web-Scale Parallel Text}} from the {{Common Crawl}}},
  booktitle = {{{ACL}}},
  author = {Smith, Jason R and Saint-Amand, Herve and Plamada, Magdalena and Koehn, Philipp and Callison-Burch, Chris and Lopez, Adam},
  date = {2013},
  pages = {10},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y3VLFASE/Smith et al. - Dirt Cheap Web-Scale Parallel Text from the Common.pdf}
}

@inproceedings{snow2008CheapFastIt,
  title = {Cheap and {{Fast}} - {{But}} Is It {{Good}} ? {{Evaluating Non-Expert Annotations}} for {{Natural Language Tasks}}},
  booktitle = {{{EMNLP}}},
  author = {Snow, R. and O'Connor, B. and Jurafsky, D. and Ng, A. Y.},
  date = {2008},
  eprint = {23259955},
  eprinttype = {pmid},
  pages = {254--263},
  issn = {09246495},
  doi = {10.1.1.142.8286},
  abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We ex-plore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: af-fect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechani-cal Turk non-expert annotations and existing gold standard labels provided by expert label-ers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effec-tive as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annota-tion quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
  archiveprefix = {arXiv},
  isbn = {978-1-4503-2922-4},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GFFFUXFG/Snow et al. - 2008 - Cheap and Fast - But is it Good Evaluating Non-Expert Annotations for Natural Language Tasks(2).pdf}
}

@inproceedings{snyder2007DatabasetextAlignmentStructured,
  title = {Database-Text Alignment via Structured Multilabel Classification},
  booktitle = {{{IJCAI}}},
  author = {Snyder, B. and Barzilay, R.},
  date = {2007},
  pages = {1713--1718},
  issn = {10450823},
  abstract = {This paper addresses the task of aligning a database with a corresponding text. The goal is to link individual database entries with sentences that verbalize the same information. By providing explicit semantics-to-text links, these alignments can aid the training of natural language generation and information extraction systems. Beyond these pragmatic benefits, the alignment problem is appealing from a modeling perspective: the mappings between database entries and text sentences exhibit rich structural dependencies, unique to this task. Thus, the key challenge is to make use of as many global dependencies as possible without sacrificing tractability. To this end, we cast text-database alignment as a structured multilabel classification task where each sentence is labeled with a subset of matching database entries. In contrast to existing multilabel classifiers, our approach operates over arbitrary global features of inputs and proposed labels. We compare our model with a baseline classifier that makes locally optimal decisions. Our results show that the proposed model yields a 15\% relative reduction in error, and compares favorably with human performance.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/STH6QN2L/Snyder, Barzilay - 2007 - Database-text alignment via structured multilabel classification(2).pdf}
}

@inproceedings{socher2013ParsingCompositionalVector,
  title = {Parsing with {{Compositional Vector Grammars}}},
  booktitle = {{{ACL}}},
  author = {Socher, R. and Bauer, J. and Manning, C. D. and Ng, A. Y.},
  date = {2013},
  volume = {80},
  number = {19},
  pages = {5080--5083},
  issn = {15205126},
  doi = {10.1021/ja01552a021},
  abstract = {Natural language parsing has typically been done with small sets of discrete categories such as \{NP\} and \{VP\}, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (\{CVG)\}, which combines \{PCFGs\} with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The \{CVG\} improves the \{PCFG\} of the Stanford Parser by 3.8 \% to obtain an F1 score of 90.4\%. It is fast to train and implemented approximately as an efficient reranker it is about 20 \% faster than the current Stanford factored parser. The \{CVG\} learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as \{PP\} attachments. 1},
  keywords = {unread},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2JQ5FQPY/Socher et al. - 2013 - Parsing with Compositional Vector Grammars(2).pdf}
}

@inproceedings{socher2013RecursiveDeepModels,
  title = {Recursive {{Deep Models}} for {{Semantic Compositionality Over}} a {{Sentiment Treebank}}},
  booktitle = {{{EMNLP}}},
  author = {Socher, R. and Perelygin, A. and Wu, J. Y. and Chuang, J. and Manning, C. D. and Ng, A. Y. and Potts, C.},
  date = {2013},
  eprint = {24086296},
  eprinttype = {pmid},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0073791},
  abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80\% up to 85.4\%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7\%, an improvement of 9.7\% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
  archiveprefix = {arXiv},
  isbn = {978-1-937284-97-8},
  annotation = {1068 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HP9LIUFT/Socher et al. - 2013 - Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank(2).pdf}
}

@inproceedings{softprompt2021,
  title = {Learning {{How}} to {{Ask}}: {{Querying LMs}} with {{Mixtures}} of {{Soft Prompts}}},
  booktitle = {{{NAACL}}},
  author = {Qin, G. and Eisner, J. M.},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SUEXINT7/final.pdf}
}

@inproceedings{song2013GeneralFrameworkRecognizing,
  title = {A {{General Framework}} for {{Recognizing Complex Events}} in {{Markov Logic}}},
  booktitle = {{{AAAI}}},
  author = {Song, Y. C. and Kautz, H. and Li, Y. and Luo, J.},
  date = {2013},
  pages = {68--73},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LPGE54LT/Song et al. - 2013 - A General Framework for Recognizing Complex Events in Markov Logic(2).pdf}
}

@inproceedings{song2019GenerativeModelingEstimating,
  title = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
  booktitle = {{{NeurIPS}}},
  author = {Song, Y. and Ermon, S.},
  date = {2019},
  eprint = {1907.05600},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1907.05600},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  archiveprefix = {arXiv},
  issue = {NeurIPS},
  keywords = {unread},
  annotation = {99 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PBMAHZZW/Song, Ermon - 2019 - Generative Modeling by Estimating Gradients of the Data Distribution(2).pdf}
}

@inproceedings{soricut2003SentenceLevelDiscourse,
  title = {Sentence Level Discourse Parsing Using Syntactic and Lexical Information},
  booktitle = {{{NAACL-HLT}}},
  author = {Soricut, R. and Marcu, D.},
  date = {2003},
  doi = {10.3115/1073445.1073475},
  abstract = {We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8\% over a state-ofthe- art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.},
  annotation = {457 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IK3J3CJA/Soricut, Marcu - 2003 - Sentence level discourse parsing using syntactic and lexical information(2).pdf}
}

@inproceedings{sorokin2017ContextAwareRepresentationsKnowledge,
  title = {Context-{{Aware Representations}} for {{Knowledge Base Relation Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Sorokin, Daniil and Gurevych, Iryna},
  date = {2017},
  pages = {1784--1789},
  publisher = {{Association for Computational Linguistics}},
  location = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1188},
  url = {http://aclweb.org/anthology/D17-1188},
  urldate = {2020-11-21},
  abstract = {We demonstrate that for sentence-level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation. Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence. We combine the context representations with an attention mechanism to make the final prediction.},
  langid = {english},
  keywords = {unread},
  annotation = {55 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YV9BL5I5/Sorokin and Gurevych - 2017 - Context-Aware Representations for Knowledge Base R.pdf}
}

@inproceedings{speer2017ConceptNetOpenMultilingual,
  title = {{{ConceptNet}} 5.5: {{An Open Multilingual Graph}} of {{General Knowledge}}},
  booktitle = {{{AAAI}}},
  author = {Speer, R. and Chin, J. and Havasi, C.},
  date = {2017},
  eprint = {1612.03975},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1612.03975},
  abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
  archiveprefix = {arXiv},
  annotation = {690 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XFHYKJFR/Speer, Chin, Havasi - 2017 - ConceptNet 5.5 An Open Multilingual Graph of General Knowledge(2).pdf}
}

@misc{sripada2003SumTimeMeteoParallelCorpus,
  title = {{{SumTime-Meteo}}: {{Parallel Corpus}} of {{Naturally Occurring Forecast Texts}} and {{Weather Data}}},
  author = {Sripada, S. G. and Reiter, E. and Hunter, J. and Yu, J.},
  date = {2003},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/54Q64VGQ/Sripada et al. - 2003 - SumTime-Meteo Parallel Corpus of Naturally Occurring Forecast Texts and Weather Data(2).pdf}
}

@inproceedings{srivastava2017JointConceptLearning,
  title = {Joint {{Concept Learning}} and {{Semantic Parsing}} from {{Natural Language Explanations}}},
  booktitle = {{{EMNLP}}},
  author = {Srivastava, S. and Labutov, S. and Mitchell, T. M.},
  date = {2017},
  pages = {1527--1536},
  url = {http://aclweb.org/anthology/D17-1161},
  abstract = {Natural language constitutes a predomi-nant medium for much of human learn-ing and pedagogy. We consider the prob-lem of concept learning from natural lan-guage explanations, and a small number of labeled examples of the concept. For example, in learning the concept of a phish-ing email, one might say 'this is a phishing email because it asks for your bank account number'. Solving this problem involves both learning to interpret open-ended nat-ural language statements, as well as learn-ing the concept itself. We present a joint model for (1) language interpretation (se-mantic parsing) and (2) concept learning (classification) that does not require label-ing statements with logical forms. Instead, the model prefers discriminative interpre-tations of statements in context of observ-able features of the data as a weak signal for parsing. On a dataset of email-related concepts, this approach yields across-the-board improvements in classification per-formance, with a 30\% relative improve-ment in F1 score over competitive classifi-cation methods in the low data regime.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5JJ2ZUB9/Srivastava, Labutov, Mitchell - 2017 - Joint Concept Learning and Semantic Parsing from Natural Language Explanations(2).pdf}
}

@misc{srivastava2019HighwayNetworks,
  title = {Highway {{Networks}}},
  author = {Srivastava, R. K. and Greff, K. and Schmidhuber, J.},
  date = {2019},
  eprint = {1505.00387},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1505.00387},
  abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
  archiveprefix = {arXiv},
  annotation = {938 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4Y7252PS/Srivastava, Greff, Schmidhuber - 2019 - Highway Networks(2).pdf}
}

@inproceedings{stede2000HyperonymProblemRevisited,
  title = {The Hyperonym Problem Revisited: {{Conceptual}} and Lexical Hierarchies in Language},
  booktitle = {{{INLG}}},
  author = {Stede, M.},
  date = {2000},
  pages = {93--99},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C6CFUXRH/Stede - 2000 - The hyperonym problem revisited Conceptual and lexical hierarchies in language(2).pdf}
}

@inproceedings{steinhardt2017CertifiedDefensesData,
  title = {Certified {{Defenses}} for {{Data Poisoning Attacks}}},
  booktitle = {{{NeurIPS}}},
  author = {Steinhardt, J. and Koh, P. W. and Liang, P.},
  date = {2017},
  number = {i},
  eprint = {1706.03691},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.03691},
  abstract = {Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12\% to 23\% test error by adding only 3\% poisoned data.},
  archiveprefix = {arXiv},
  annotation = {253 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UEAPUL8K/Steinhardt, Koh, Liang - 2017 - Certified Defenses for Data Poisoning Attacks(2).pdf}
}

@inproceedings{stengel-eskin2020UniversalDecompositionalSemantic,
  title = {Universal {{Decompositional Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Stengel-Eskin, Elias and White, Aaron Steven and Zhang, Sheng and Van Durme, Benjamin},
  date = {2020-05-02},
  eprint = {1910.10138},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.10138},
  urldate = {2021-02-17},
  abstract = {We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores. We also introduce a strong pipeline model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction. By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups.},
  archiveprefix = {arXiv},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FUANW7XI/Stengel-Eskin et al. - 2020 - Universal Decompositional Semantic Parsing.pdf;/home/hiaoxui/.local/share/zotero_files/storage/736XBABT/1910.html}
}

@article{stengel-eskin2021JointUniversalSyntactic,
  title = {Joint {{Universal Syntactic}} and {{Semantic Parsing}}},
  author = {Stengel-Eskin, Elias and Murray, Kenton and Zhang, Sheng and White, Aaron Steven and Van Durme, Benjamin},
  date = {2021-08-02},
  journaltitle = {TACL},
  volume = {9},
  pages = {756--773},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00396},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00396/106796/Joint-Universal-Syntactic-and-Semantic-Parsing},
  urldate = {2022-03-21},
  abstract = {While numerous attempts have been made to jointly parse syntax and semantics, high performance in one domain typically comes at the price of performance in the other. This trade-off contradicts the large body of research focusing on the rich interactions at the syntax–semantics interface. We explore multiple model architectures that allow us to exploit the rich syntactic and semantic annotations contained in the Universal Decompositional Semantics (UDS) dataset, jointly parsing Universal Dependencies and UDS to obtain state-of-the-art results in both formalisms. We analyze the behavior of a joint model of syntax and semantics, finding patterns supported by linguistic theory at the syntax–semantics interface. We then investigate to what degree joint modeling generalizes to a multilingual setting, where we find similar trends across 8 languages.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ACIUSLXC/Stengel-Eskin et al. - 2021 - Joint Universal Syntactic and Semantic Parsing.pdf}
}

@inproceedings{stent2004TrainableSentencePlanning,
  title = {Trainable {{Sentence Planning}} for {{Complex Information Presentation}} in {{Spoken Dialog Systems}}},
  booktitle = {{{ACL}}},
  author = {Stent, A. and Prasad, R. and Walker, M.},
  date = {2004},
  pages = {79-es},
  doi = {10.3115/1218955.1218966},
  abstract = {A challenging problem for spoken dialog systems is the design of utterance generation modules that are fast, flexible and general, yet produce high quality output in particular domains. A promising approach is trainable generation, which uses general-purpose linguistic knowledge automatically adapted to the application domain. This paper presents a trainable sentence planner for the MATCH dialog system. We show that trainable sentence planning can produce output comparable to that of MATCH’s template-based generator even for quite complex information presentations.},
  keywords = {unread},
  annotation = {148 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PEA3NBDC/Stent, Prasad, Walker - 2004 - Trainable Sentence Planning for Complex Information Presentation in Spoken Dialog Systems(2).pdf}
}

@misc{storks2019CommonsenseReasoningNatural,
  title = {Commonsense {{Reasoning}} for {{Natural Language Understanding}}: {{A Survey}} of {{Benchmarks}}, {{Resources}}, and {{Approaches}}},
  author = {Storks, S. and Gao, Q. and Chai, J. Y.},
  date = {2019},
  eprint = {1904.01172},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.01172},
  abstract = {Commonsense knowledge and commonsense reasoning are some of the main bottlenecks in machine intelligence. In the NLP community, many benchmark datasets and tasks have been created to address commonsense reasoning for language understanding. These tasks are designed to assess machines' ability to acquire and learn commonsense knowledge in order to reason and understand natural language text. As these tasks become instrumental and a driving force for commonsense research, this paper aims to provide an overview of existing tasks and benchmarks, knowledge resources, and learning and inference approaches toward commonsense reasoning for natural language understanding. Through this, our goal is to support a better understanding of the state of the art, its limitations, and future challenges.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {30 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VZ7W7BMG/Storks, Gao, Chai - 2019 - Commonsense Reasoning for Natural Language Understanding A Survey of Benchmarks, Resources, and Approaches(2).pdf}
}

@inproceedings{strakova2019NeuralArchitecturesNested,
  title = {Neural {{Architectures}} for {{Nested NER}} through {{Linearization}}},
  booktitle = {{{ACL}}},
  author = {Straková, Jana and Straka, Milan and Hajič, Jan},
  date = {2019-08-19},
  eprint = {1908.06926},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C4PQ933J/Straková et al. - 2019 - Neural Architectures for Nested NER through Linear.pdf}
}

@inproceedings{strapparava2005MakingComputersLaugh,
  title = {Making {{Computers Laugh}} : {{Investigations}} in {{Automatic Humor Recognition}}},
  booktitle = {{{EMNLP-HLT}}},
  author = {Strapparava, C.},
  date = {2005},
  pages = {531--538},
  issue = {October},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EJEX8E7M/Strapparava - 2005 - Making Computers Laugh Investigations in Automatic Humor Recognition(2).pdf}
}

@inproceedings{strubell2018LinguisticallyInformedSelfAttentionSemantic,
  title = {Linguistically-{{Informed Self-Attention}} for {{Semantic Role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Strubell, E. and Verga, P. and Andor, D. and Weiss, D. and McCallum, A.},
  date = {2018},
  eprint = {1804.08199v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QRX4KNN3/Strubell et al. - 2018 - Linguistically-Informed Self-Attention for Semantic Role Labeling(2).pdf}
}

@inproceedings{strubell2018SyntaxHelpsELMo,
  title = {Syntax {{Helps ELMo Understand Semantics}}: {{Is Syntax Still Relevant}} in a {{Deep Neural Architecture}} for {{SRL}}?},
  booktitle = {Workshop on the {{Relevance}} of {{Linguistic Structure}} in {{Neural Architectures}} for {{NLP}}},
  author = {Strubell, E. and Mccallum, A.},
  date = {2018},
  pages = {19--27},
  url = {https://nlp.stanford.edu/projects/},
  abstract = {Do unsupervised methods for learning rich, contextualized token representations obviate the need for explicit modeling of linguistic structure in neural network models for semantic role labeling (SRL)? We address this question by incorporating the massively successful ELMo em-beddings (Peters et al., 2018) into LISA (Strubell and McCallum, 2018), a strong, linguistically-informed neural network architecture for SRL. In experiments on the CoNLL-2005 shared task we find that though ELMo out-performs typical word embeddings, beginning to close the gap in F1 between LISA with predicted and gold syntactic parses, syntactically-informed models still out-perform syntax-free models when both use ELMo, especially on out-of-domain data. Our results suggest that linguistic structures are indeed still relevant in this golden age of deep learning for NLP.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PUSNUJVV/Strubell, Mccallum - 2018 - Syntax Helps ELMo Understand Semantics Is Syntax Still Relevant in a Deep Neural Architecture for SRL(2).pdf}
}

@article{styler2014TemporalAnnotationClinical,
  title = {Temporal {{Annotation}} in the {{Clinical Domain}}},
  author = {Styler, W. F. and Bethard, S. and Finan, S. and Palmer, M. and Pradhan, S. and de Groen, P. C and Erickson, B. and Miller, T. and Lin, C. and Savova, G. and Pustejovsky, J.},
  options = {useprefix=true},
  date = {2014},
  journaltitle = {TACL},
  volume = {2},
  number = {1},
  pages = {143--154},
  doi = {10.1162/tacl_a_00172},
  abstract = {This article discusses the requirements of a formal specification for the annotation of temporal information in clinical narratives. We discuss the implementation and extension of ISO-TimeML for annotating a corpus of clinical notes, known as the THYME corpus. To reflect the information task and the heavily inference-based reasoning demands in the domain, a new annotation guideline has been developed, “the THYME Guidelines to ISO-TimeML (THYME-TimeML)”. To clarify what relations merit annotation, we distinguish between linguistically-derived and inferentially-derived temporal orderings in the text. We also apply a top performing TempEval 2013 system against this new resource to measure the difficulty of adapting systems to the clinical domain. The corpus is available to the community and has been proposed for use in a SemEval 2015 task.},
  keywords = {unread},
  annotation = {162 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UE9RFU6K/Styler et al. - 2014 - Temporal Annotation in the Clinical Domain(2).pdf}
}

@article{subhashini2011SurveyOntologyConstruction,
  title = {A {{Survey}} on {{Ontology Construction Methodologies}}},
  author = {Subhashini, R. and Akilandeswari, J.},
  date = {2011},
  journaltitle = {International Journal of Enterprise Computing and Business Systems},
  volume = {1},
  number = {1},
  url = {http://www.ijecbs.com},
  abstract = {Ontology is defined as partial specification of conceptual vocabulary used for formulating knowledge-level theories about a domain of discourse. Ontology is applied in domains like natural disaster management system, medicine, military intelligence, cooking, enterprise, jobs, agriculture, wikipedia, automobiles and so on. This paper presents a review on various ontology construction methodologies for different domains. This paper also presents the merits and drawbacks in those methodologies.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZL3VEXEX/Subhashini, Akilandeswari - 2011 - A Survey on Ontology Construction Methodologies(2).pdf}
}

@inproceedings{subramanian2019DeepOrdinalRegression,
  title = {Deep {{Ordinal Regression}} for {{Pledge Specificity Prediction}}},
  booktitle = {{{EMNLP}}},
  author = {Subramanian, S. and Cohn, T. and Baldwin, T.},
  date = {2019},
  eprint = {1909.00187},
  eprinttype = {arxiv},
  pages = {1729--1740},
  doi = {10.18653/v1/d19-1182},
  abstract = {Many pledges are made in the course of an election campaign, forming important corpora for political analysis of campaign strategy and governmental accountability. At present, there are no publicly available annotated datasets of pledges, and most political analyses rely on manual analysis. In this paper we collate a novel dataset of manifestos from eleven Australian federal election cycles, with over 12,000 sentences annotated with specificity (e.g., rhetorical vs.\textbackslash{} detailed pledge) on a fine-grained scale. We propose deep ordinal regression approaches for specificity prediction, under both supervised and semi-supervised settings, and provide empirical results demonstrating the effectiveness of the proposed techniques over several baseline approaches. We analyze the utility of pledge specificity modeling across a spectrum of policy issues in performing ideology prediction, and further provide qualitative analysis in terms of capturing party-specific issue salience across election cycles.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QKA9Q9Z7/Subramanian, Cohn, Baldwin - 2019 - Deep Ordinal Regression for Pledge Specificity Prediction(2).pdf}
}

@inproceedings{suhr2018LearningMapContextDependent,
  title = {Learning to {{Map Context-Dependent Sentences}} to {{Executable Formal Queries}}},
  booktitle = {{{NAACL}}},
  author = {Suhr, A. and Iyer, S. and Artzi, Y.},
  date = {2018},
  pages = {2238--2249},
  doi = {10.18653/v1/n18-1203},
  abstract = {We propose a context-dependent model to map utterances within an interaction to executable formal queries. To incorporate interaction his-tory, the model maintains an interaction-level encoder that updates after each turn, and can copy sub-sequences of previously predicted queries during generation. Our approach com-bines implicit and explicit modeling of refer-ences between utterances. We evaluate our model on the ATIS flight planning interac-tions, and demonstrate the benefits of model-ing context and explicit references.},
  keywords = {unread},
  annotation = {60 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T6KEMBCT/Suhr, Iyer, Artzi - 2018 - Learning to Map Context-Dependent Sentences to Executable Formal Queries(2).pdf}
}

@inproceedings{sukhbaatar2019AdaptiveAttentionSpan,
  title = {Adaptive {{Attention Span}} in {{Transformers}}},
  booktitle = {{{ACL}}},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  date = {2019-08-08},
  eprint = {1905.07799},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.07799},
  urldate = {2021-03-28},
  abstract = {We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.},
  archiveprefix = {arXiv},
  annotation = {95 citations (Semantic Scholar/arXiv) [2021-03-28]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KH9IHCBY/Sukhbaatar et al. - 2019 - Adaptive Attention Span in Transformers.pdf;/home/hiaoxui/.local/share/zotero_files/storage/BCX78SAI/1905.html}
}

@misc{sukhbaatar2019AugmentingSelfattentionPersistent,
  title = {Augmenting {{Self-attention}} with {{Persistent Memory}}},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
  date = {2019-07-02},
  eprint = {1907.01470},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1907.01470},
  urldate = {2021-03-10},
  abstract = {Transformer networks have lead to important progress in language modeling and machine translation. These models include two consecutive modules, a feed-forward layer and a self-attention layer. The latter allows the network to capture long term dependencies and are often regarded as the key ingredient in the success of Transformers. Building upon this intuition, we propose a new model that solely consists of attention layers. More precisely, we augment the self-attention layers with persistent memory vectors that play a similar role as the feed-forward layer. Thanks to these vectors, we can remove the feed-forward layer without degrading the performance of a transformer. Our evaluation shows the benefits brought by our model on standard character and word level language modeling benchmarks.},
  archiveprefix = {arXiv},
  annotation = {28 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7XP7VBMG/Sukhbaatar et al. - 2019 - Augmenting Self-attention with Persistent Memory.pdf;/home/hiaoxui/.local/share/zotero_files/storage/Q965MJYH/1907.html}
}

@article{sukthanker2020AnaphoraCoreferenceResolution,
  title = {Anaphora and Coreference Resolution: {{A}} Review},
  shorttitle = {Anaphora and Coreference Resolution},
  author = {Sukthanker, Rhea and Poria, Soujanya and Cambria, Erik and Thirunavukarasu, Ramkumar},
  date = {2020-07},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {59},
  pages = {139--162},
  issn = {15662535},
  doi = {10.1016/j.inffus.2020.01.010},
  langid = {english},
  annotation = {37 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W4RGY4LW/Sukthanker et al. - 2020 - Anaphora and coreference resolution A review.pdf}
}

@misc{sun2017NeuralNetworkPushdown,
  title = {The {{Neural Network Pushdown Automaton}}: {{Model}}, {{Stack}} and {{Learning Simulations}}},
  shorttitle = {The {{Neural Network Pushdown Automaton}}},
  author = {Sun, G. Z. and Giles, C. L. and Chen, H. H. and Lee, Y. C.},
  date = {2017-11-15},
  eprint = {1711.05738},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.05738},
  urldate = {2020-10-09},
  abstract = {In order for neural networks to learn complex languages or grammars, they must have sufficient computational power or resources to recognize or generate such languages. Though many approaches have been discussed, one ob- vious approach to enhancing the processing power of a recurrent neural network is to couple it with an external stack memory - in effect creating a neural network pushdown automata (NNPDA). This paper discusses in detail this NNPDA - its construction, how it can be trained and how useful symbolic information can be extracted from the trained network. In order to couple the external stack to the neural network, an optimization method is developed which uses an error function that connects the learning of the state automaton of the neural network to the learning of the operation of the external stack. To minimize the error function using gradient descent learning, an analog stack is designed such that the action and storage of information in the stack are continuous. One interpretation of a continuous stack is the probabilistic storage of and action on data. After training on sample strings of an unknown source grammar, a quantization procedure extracts from the analog stack and neural network a discrete pushdown automata (PDA). Simulations show that in learning deterministic context-free grammars - the balanced parenthesis language, 1*n0*n, and the deterministic Palindrome - the extracted PDA is correct in the sense that it can correctly recognize unseen strings of arbitrary length. In addition, the extracted PDAs can be shown to be identical or equivalent to the PDAs of the source grammars which were used to generate the training strings.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {25 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3WNBCX7M/Sun et al. - 2017 - The Neural Network Pushdown Automaton Model, Stac.pdf;/home/hiaoxui/.local/share/zotero_files/storage/H5JCMBNV/1711.html}
}

@misc{sun2020GuessingWhatPlausible,
  title = {Guessing {{What}}'s {{Plausible But Remembering What}}'s {{True}}: {{Accurate Neural Reasoning}} for {{Question-Answering}}},
  author = {Sun, H. and Arnold, A. O. and Bedrax-Weiss, T. and Pereira, F. and Cohen, W. W.},
  date = {2020},
  eprint = {2004.03658},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.03658},
  abstract = {Neural approaches to natural language processing (NLP) often fail at the logical reasoning needed for deeper language understanding. In particular, neural approaches to reasoning that rely on embedded \textbackslash emph\{generalizations\} of a knowledge base (KB) implicitly model which facts that are \textbackslash emph\{plausible\}, but may not model which facts are \textbackslash emph\{true\}, according to the KB. While generalizing the facts in a KB is useful for KB completion, the inability to distinguish between plausible inferences and logically entailed conclusions can be problematic in settings like as KB question answering (KBQA). We propose here a novel KB embedding scheme that supports generalization, but also allows accurate logical reasoning with a KB. Our approach introduces two new mechanisms for KB reasoning: neural retrieval over a set of embedded triples, and "memorization" of highly specific information with a compact sketch structure. Experimentally, this leads to substantial improvements over the state-of-the-art on two KBQA benchmarks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DEWKWQ6R/Sun et al. - 2020 - Guessing What's Plausible But Remembering What's True Accurate Neural Reasoning for Question-Answering(2).pdf}
}

@inproceedings{sun2021LongRangeLanguageModels,
  title = {Do {{Long-Range Language Models Actually Use Long-Range Context}}?},
  booktitle = {{{EMNLP}}},
  author = {Sun, Simeng and Krishna, Kalpesh and Mattarella-Micke, Andrew and Iyyer, Mohit},
  date = {2021},
  pages = {16},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/INW2A7WF/Sun et al. - Do Long-Range Language Models Actually Use Long-Ra.pdf}
}

@inproceedings{sun2022ChapterBreakChallengeDataset,
  title = {{{ChapterBreak}}: {{A Challenge Dataset}} for {{Long-Range Language Models}}},
  booktitle = {{{NAACL}}},
  author = {Sun, Simeng and Thai, Katherine and Iyyer, Mohit},
  date = {2022},
  pages = {11},
  eventtitle = {{{NAACL}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y66JFH8W/Sun et al. - CHAPTERBREAK A Challenge Dataset for Long-Range L.pdf}
}

@inproceedings{sun2022RethinkingDocumentlevelNeural,
  title = {Rethinking {{Document-level Neural Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Sun, Zewei and Wang, Mingxuan and Zhou, Hao and Zhao, Chengqi and Huang, Shujian and Chen, Jiajun and Li, Lei},
  date = {2022-03-14},
  eprint = {2010.08961},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.08961},
  urldate = {2022-03-23},
  abstract = {This paper does not aim at introducing a novel model for document-level neural machine translation. Instead, we head back to the original Transformer model and hope to answer the following question: Is the capacity of current models strong enough for document-level translation? Interestingly, we observe that the original Transformer with appropriate training techniques can achieve strong results for document translation, even with a length of 2000 words. We evaluate this model and several recent approaches on nine document-level datasets and two sentence-level datasets across six languages. Experiments show that document-level Transformer models outperforms sentence-level ones and many previous methods in a comprehensive set of metrics, including BLEU, four lexical indices, three newly proposed assistant linguistic indicators, and human evaluation.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EH24DMXR/Sun et al. - 2022 - Rethinking Document-level Neural Machine Translati.pdf;/home/hiaoxui/.local/share/zotero_files/storage/5KHAZ65L/2010.html}
}

@inproceedings{surdeanu2012MultiinstanceMultilabelLearning,
  title = {Multi-Instance {{Multi-label Learning}} for {{Relation Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Surdeanu, M. and Tibshirani, J. and Nallapati, R. and Manning, C. D.},
  date = {2012},
  eprint = {91150},
  eprinttype = {pmid},
  pages = {455--465},
  doi = {10.3115/v1/D14-1200},
  url = {http://dl.acm.org/citation.cfm?id=2390948.2391003},
  abstract = {Distant supervision for relation extraction (RE) -- gathering training data by aligning a database of facts with text -- is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains.},
  archiveprefix = {arXiv},
  isbn = {978-1-937284-43-5},
  issue = {July},
  annotation = {141 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VBTH8AU2/Surdeanu et al. - 2012 - Multi-instance Multi-label Learning for Relation Extraction(2).pdf}
}

@misc{surdeanu2014OverviewEnglishSlot,
  title = {Overview of the {{English Slot Filling Track}} at the {{TAC2014 Knowledge Base Population Evaluation}}},
  author = {Surdeanu, Mihai and Ji, Heng},
  date = {2014},
  abstract = {We overview the English Slot Filling (SF) track of the TAC2014 Knowledge Base Population (KBP) evaluation. The goal of this KBP track is to promote research in the extraction of binary relations between named and numeric entities from free text. The main changes this year include: (a) the inclusion of ambiguous queries, i.e., queries that point to multiple real-life entities with the same name; (b) accepting outputs created through inference; and (c) a simplification of the task and of the input format by removing references to the knowledge base for the entities included in queries. The SF track attracted 31 registered teams, out of which 18 teams submitted at least one run. The highest score this year was 36.72 F1, with a median of 19.80 F1.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P6VNZZYN/Surdeanu and Ji - Overview of the English Slot Filling Track at the .pdf}
}

@inproceedings{suresh2021NotAllNegatives,
  title = {Not {{All Negatives}} Are {{Equal}}: {{Label-Aware Contrastive Loss}} for {{Fine-grained Text Classification}}},
  shorttitle = {Not {{All Negatives}} Are {{Equal}}},
  booktitle = {{{EMNLP}}},
  author = {Suresh, Varsha and Ong, Desmond C.},
  date = {2021-09-12},
  eprint = {2109.05427},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.05427},
  urldate = {2021-09-15},
  abstract = {Fine-grained classification involves dealing with datasets with larger number of classes with subtle differences between them. Guiding the model to focus on differentiating dimensions between these commonly confusable classes is key to improving performance on fine-grained tasks. In this work, we analyse the contrastive fine-tuning of pre-trained language models on two fine-grained text classification tasks, emotion classification and sentiment analysis. We adaptively embed class relationships into a contrastive objective function to help differently weigh the positives and negatives, and in particular, weighting closely confusable negatives more than less similar negative examples. We find that Label-aware Contrastive Loss outperforms previous contrastive methods, in the presence of larger number and/or more confusable classes, and helps models to produce output distributions that are more differentiated.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-15]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8JMNKFJA/Suresh and Ong - 2021 - Not All Negatives are Equal Label-Aware Contrasti.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7RZTEDAL/2109.html}
}

@inproceedings{sutskever2014SequenceSequenceLearning,
  title = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {{{NeurIPS}}},
  author = {Sutskever, I. and Vinyals, O. and Le, Q. V.},
  date = {2014},
  eprint = {2079951},
  eprinttype = {pmid},
  pages = {3104--3112},
  issn = {09205691},
  doi = {10.1007/s10107-014-0839-0},
  url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arXiv},
  isbn = {1409.3215},
  annotation = {397 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2HU2QGGD/Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks(2).pdf}
}

@misc{swayamdipta2017FrameSemanticParsingSoftmaxMargin,
  title = {Frame-{{Semantic Parsing}} with {{Softmax-Margin Segmental RNNs}} and a {{Syntactic Scaffold}}},
  author = {Swayamdipta, S. and Thomson, S. and Dyer, C. and Smith, N. A.},
  date = {2017},
  eprint = {1706.09528},
  eprinttype = {arxiv},
  abstract = {We present a new, efficient frame-semantic parser that labels semantic arguments to FrameNet predicates. Built using an extension to the segmental RNN that emphasizes recall, our basic system achieves competitive performance without any calls to a syntactic parser. We then introduce a method that uses phrase-syntactic annotations from the Penn Treebank during training only, through a multitask objective; no parsing is required at training or test time. This "syntactic scaffold" offers a cheaper alternative to traditional syntactic pipelining, and achieves state-of-the-art performance.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V8IMWSFH/Swayamdipta et al. - 2017 - Frame-Semantic Parsing with Softmax-Margin Segmental RNNs and a Syntactic Scaffold(2).pdf}
}

@inproceedings{swayamdipta2018SyntacticScaffoldsSemantic,
  title = {Syntactic {{Scaffolds}} for {{Semantic Structures}}},
  booktitle = {{{EMNLP}}},
  author = {Swayamdipta, S. and Thomson, S. and Lee, K. and Zettlemoyer, L. S. and Dyer, C. and Smith, N. A.},
  date = {2018},
  eprint = {1808.10485},
  eprinttype = {arxiv},
  pages = {3772--3782},
  doi = {10.18653/v1/d18-1412},
  abstract = {We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.},
  archiveprefix = {arXiv},
  annotation = {57 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7B43XIAV/Swayamdipta et al. - 2018 - Syntactic Scaffolds for Semantic Structures(2).pdf}
}

@misc{szegedy2013IntriguingPropertiesNeural,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, C. and Zaremba, W. and Sutskever, I. and Bruna, J. and Erhan, D. and Goodfellow, I. and Fergus, R.},
  date = {2013},
  eprint = {1312.6199},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1312.6199},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  archiveprefix = {arXiv},
  annotation = {6175 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LC32KVQN/Szegedy et al. - 2013 - Intriguing properties of neural networks(2).pdf}
}

@inproceedings{szegedy2016RethinkingInceptionArchitecture,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  booktitle = {{{CVPR}}},
  author = {Szegedy, C. and Vanhoucke, V. and Ioffe, S. and Shlens, J. and Wojna, Z.},
  date = {2016},
  eprint = {1512.00567},
  eprinttype = {arxiv},
  pages = {2818--2826},
  issn = {10636919},
  doi = {10.1109/CVPR.2016.308},
  abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2\% top-1 and 5:6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5\% top-5 error and 17:3\% top-1 error on the validation set and 3:6\% top-5 error on the official test set.},
  archiveprefix = {arXiv},
  isbn = {978-1-4673-8850-4},
  keywords = {unread},
  annotation = {9993 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G7ADUFIG/Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer Vision(2).pdf}
}

@inproceedings{taghipour2015SemiSupervisedWordSense,
  title = {Semi-{{Supervised Word Sense Disambiguation Using Word Embeddings}} in {{General}} and {{Specific Domains}}},
  booktitle = {{{HLT}}},
  author = {Taghipour, K.},
  date = {2015},
  pages = {314--323},
  doi = {10.3115/v1/N15-1035},
  abstract = {One of the weaknesses of current supervised word sense disambiguation (WSD) systems is that they only treat a word as a discrete en-tity. However, a continuous-space represen-tation of words (word embeddings) can pro-vide valuable information and thus improve generalization accuracy. Since word embed-dings are typically obtained from unlabeled data using unsupervised methods, this method can be seen as a semi-supervised word sense disambiguation approach. This paper investi-gates two ways of incorporating word embed-dings in a word sense disambiguation setting and evaluates these two methods on some Sen-sEval/SemEval lexical sample and all-words tasks and also a domain-specific lexical sam-ple task. The obtained results show that such representations consistently improve the ac-curacy of the selected supervised WSD sys-tem. Moreover, our experiments on a domain-specific dataset show that our supervised base-line system beats the best knowledge-based systems by a large margin.},
  annotation = {96 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HCT3L229/Taghipour - 2015 - Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains(2).pdf}
}

@inproceedings{tahaei2022KroneckerBERTSignificantCompression,
  title = {{{KroneckerBERT}}: {{Significant Compression}} of {{Pre-trained Language Models Through Kronecker Decomposition}} and {{Knowledge Distillation}}},
  booktitle = {{{NAACL}}},
  author = {Tahaei, Marzieh S and Charlaix, Ella and Nia, Vahid Partovi and Ghodsi, Ali and Rezagholizadeh, Mehdi},
  date = {2022},
  pages = {12},
  abstract = {The development of over-parameterized pretrained language models has made a significant contribution toward the success of natural language processing. While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices. We push the limits of state-of-the-art Transformer-based pre-trained language model compression using Kronecker decomposition. We present our KroneckerBERT, a compressed version of the BERTBASE model obtained by compressing the embedding layer and the linear mappings in the multi-head attention, and the feed-forward network modules in the Transformer layers. Our KroneckerBERT is trained via a very efficient two-stage knowledge distillation scheme using far fewer data samples than state-of-the-art models like MobileBERT and TinyBERT. We evaluate the performance of KroneckerBERT on well-known NLP benchmarks. We show that our KroneckerBERT with compression factors of 7.7× and 21× outperforms state-of-theart compression methods on the GLUE and SQuAD benchmarks. In particular, using only 13\% of the teacher model parameters, it retain more than 99\% of the accuracy on the majority of GLUE tasks.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BH5HQRAP/Tahaei et al. - KroneckerBERT Significant Compression of Pre-trai.pdf}
}

@inproceedings{tai2015ImprovedSemanticRepresentations,
  title = {Improved {{Semantic Representations From Tree-Structured Long Short-Term Memory Networks}}},
  booktitle = {{{ACL}}},
  author = {Tai, K. S. and Socher, R. and Manning, C. D.},
  date = {2015},
  eprint = {1503.00075},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1503.00075},
  abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {2038 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8RFYLWJ2/Tai, Socher, Manning - 2015 - Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks(2).pdf}
}

@inproceedings{taketa2004TextRankBriningOrder,
  title = {{{TextRank}}: {{Brining Order}} into {{Texts}}},
  booktitle = {{{EMNLP}}},
  author = {Taketa, F.},
  date = {2004},
  issn = {03050491},
  doi = {10.1016/0305-0491(73)90144-2},
  abstract = {1. 1. The hemoglobins found in various members of the Felidae have been separated and compared with respect to structure in relation to 2,3-diphosphoglycerate (2,3-DPG) sensitivity. 2. 2. Multiple hemoglobin components are found in the blood of all Felidae, and they are characterized by the presence of one of two types of β-chains that are common to the members of the family. 3. 3. The two types, A-β and B-β, are distinguished from one another by differences in positions of the βT-1 and βT-XIV peptides in fingerprints of tryptic digests. 4. 4. Components that contain the A-β type are invariably 2,3-DPG-sensitive whereas those that contain the B-β type are insensitive. 5. 5. The A-β and B-β chains are apparently products of nonallelic genes but are found in widely variable proportions in mixtures of hemoglobins in the blood of different members of the Felidae. © 1973.},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ND8VCP8T/Taketa - 2004 - TextRank Brining Order into Texts(2).pdf}
}

@misc{tallec2017UnbiasingTruncatedBackpropagation,
  title = {Unbiasing {{Truncated Backpropagation Through Time}}},
  author = {Tallec, C. and Ollivier, Y.},
  date = {2017},
  eprint = {1705.08209},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.08209},
  abstract = {Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step. However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce Anticipated Reweighted Truncated Backpropagation (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling, ARTBP slightly outperforms truncated BPTT.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {32 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NTFB7NBN/Tallec, Ollivier - 2017 - Unbiasing Truncated Backpropagation Through Time(2).pdf}
}

@inproceedings{talmor2018WebKnowledgebaseAnswering,
  title = {The {{Web}} as a {{Knowledge-base}} for {{Answering Complex Questions}}},
  booktitle = {{{NAACL}}},
  author = {Talmor, A. and Berant, J.},
  date = {2018},
  eprint = {1803.06643},
  eprinttype = {arxiv},
  pages = {1--10},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CHDQW99W/Talmor, Berant - 2018 - The Web as a Knowledge-base for Answering Complex Questions(2).pdf}
}

@inproceedings{talmor2019CommonsenseQAQuestionAnswering,
  title = {{{CommonsenseQA}}: {{A Question Answering Challenge Targeting Commonsense Knowledge}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Talmor, A. and Herzig, J. and Lourie, N. and Berant, J.},
  date = {2019},
  eprint = {1811.00937},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1811.00937},
  abstract = {When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56\% accuracy, well below human performance, which is 89\%.},
  archiveprefix = {arXiv},
  annotation = {174 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FP5V3T9K/Talmor et al. - 2019 - CommonsenseQA A Question Answering Challenge Targeting Commonsense Knowledge(2).pdf}
}

@misc{talmor2019OLMpicsWhatLanguage,
  title = {{{oLMpics}} -- {{On}} What {{Language Model Pre-training Captures}}},
  author = {Talmor, A. and Elazar, Y. and Goldberg, Y. and Berant, J.},
  date = {2019},
  eprint = {1912.13283},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1912.13283},
  abstract = {Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.},
  archiveprefix = {arXiv},
  annotation = {66 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UU47V2DQ/Talmor et al. - 2019 - oLMpics -- On what Language Model Pre-training Captures(2).pdf}
}

@inproceedings{tamborrino2020PretrainingAlmostAll,
  title = {Pre-Training {{Is}} ({{Almost}}) {{All You Need}}: {{An Application}} to {{Commonsense Reasoning}}},
  booktitle = {{{ACL}}},
  author = {Tamborrino, A. and Pellicano, N. and Pannier, B. and Voitot, P. and Naudin, L.},
  date = {2020},
  eprint = {2004.14074},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.14074},
  abstract = {Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets. By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80\% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g \$\textbackslash times 10\$ standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.},
  archiveprefix = {arXiv},
  annotation = {11 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HEK2DRAQ/Tamborrino et al. - 2020 - Pre-training Is (Almost) All You Need An Application to Commonsense Reasoning(2).pdf}
}

@inproceedings{tan2018DeepSemanticRole,
  title = {Deep {{Semantic Role Labeling}} with {{Self-Attention}}},
  booktitle = {{{AAAI}}},
  author = {Tan, Zhixing and Wang, Mingxuan and Xie, Jun and Chen, Yidong and Shi, Xiaodong},
  date = {2018},
  pages = {8},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YXEU5HXB/Tan et al. - Deep Semantic Role Labeling with Self-Attention.pdf}
}

@inproceedings{tan2022DocumentLevelRelationExtraction,
  title = {Document-{{Level Relation Extraction}} with {{Adaptive Focal Loss}} and {{Knowledge Distillation}}},
  booktitle = {{{ACL}}},
  author = {Tan, Qingyu and He, Ruidan and Bing, Lidong and Ng, Hwee Tou},
  date = {2022-03-21},
  eprint = {2203.10900},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.10900},
  urldate = {2022-03-23},
  abstract = {Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign\_F1 score on the DocRED leaderboard. Our code and data will be released at https://github.com/tonytan48/KD-DocRE.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SYSBSWBJ/Tan et al. - 2022 - Document-Level Relation Extraction with Adaptive F.pdf;/home/hiaoxui/.local/share/zotero_files/storage/58LYEGPJ/2203.html}
}

@inproceedings{tang2014LearningSentimentSpecificWord,
  title = {Learning {{Sentiment-Specific Word Embedding}} for {{Twitter Sentiment Classification}}},
  booktitle = {{{ACL}}},
  author = {Tang, D. and Wei, F. and Yang, N. and Zhou, M. and Liu, T. and Qin, B.},
  date = {2014},
  volume = {2},
  eprint = {18487783},
  eprinttype = {pmid},
  pages = {1--54},
  issn = {03029743},
  doi = {10.1561/2200000006},
  archiveprefix = {arXiv},
  isbn = {978-1-937284-72-5},
  keywords = {unread},
  annotation = {6482 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XRR5DNS7/Tang et al. - 2014 - Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification(2).pdf}
}

@inproceedings{tang2015DocumentModelingGated,
  title = {Document {{Modeling}} with {{Gated Recurrent Neural Network}} for {{Sentiment Classification}}},
  booktitle = {{{EMNLP}}},
  author = {Tang, D. and Qin, B. and Liu, T.},
  date = {2015},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QXNZ9VWJ/Tang, Qin, Liu - 2015 - Document Modeling with Gated Recurrent Neural Network for Sentiment Classification(2).pdf}
}

@inproceedings{tang2015LINELargescaleInformation,
  title = {{{LINE}}: {{Large-scale Information Network Embedding}}},
  booktitle = {{{WWW}}},
  author = {Tang, J. and Qu, M. and Wang, M. and Zhang, M. and Yan, J. and Mei, Q.},
  date = {2015},
  eprint = {1503.03578},
  eprinttype = {arxiv},
  doi = {10.1145/2736277.2741093},
  url = {http://arxiv.org/abs/1503.03578%0Ahttp://dx.doi.org/10.1145/2736277.2741093},
  abstract = {This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the "LINE," which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.},
  archiveprefix = {arXiv},
  isbn = {978-1-4503-3469-3},
  annotation = {2615 citations (Semantic Scholar/DOI) [2021-03-26] 2615 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XFSEFNUG/Tang et al. - 2015 - LINE Large-scale Information Network Embedding(2).pdf}
}

@inproceedings{tanzer2022MemorisationGeneralisationPretrained,
  title = {Memorisation versus {{Generalisation}} in {{Pre-trained Language Models}}},
  booktitle = {{{ACL}}},
  author = {Tänzer, Michael and Ruder, Sebastian and Rei, Marek},
  date = {2022-03-14},
  eprint = {2105.00828},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.00828},
  urldate = {2022-03-23},
  abstract = {State-of-the-art pre-trained language models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal results even on extremely noisy datasets. However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition. To mitigate such limitations, we propose an extension based on prototypical networks that improves performance in low-resource named entity recognition tasks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C6NBJIH2/Tänzer et al. - 2022 - Memorisation versus Generalisation in Pre-trained .pdf;/home/hiaoxui/.local/share/zotero_files/storage/KK3PZFQA/2105.html}
}

@misc{tay2020EfficientTransformersSurvey,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  shorttitle = {Efficient {{Transformers}}},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  date = {2020-09-16},
  eprint = {2009.06732},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.06732},
  urldate = {2021-02-09},
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  archiveprefix = {arXiv},
  annotation = {35 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6DYVIM72/Tay et al. - 2020 - Efficient Transformers A Survey.pdf;/home/hiaoxui/.local/share/zotero_files/storage/YGGZZD62/2009.html}
}

@misc{tay2020LongRangeArena,
  title = {Long {{Range Arena}}: {{A Benchmark}} for {{Efficient Transformers}}},
  shorttitle = {Long {{Range Arena}}},
  author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  date = {2020-11-08},
  eprint = {2011.04006},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2011.04006},
  urldate = {2022-02-07},
  abstract = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from \$1K\$ to \$16K\$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CQRDUXXK/Tay et al. - 2020 - Long Range Arena A Benchmark for Efficient Transf.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6VRE2465/2011.html}
}

@inproceedings{tay2020SparseSinkhornAttention,
  title = {Sparse {{Sinkhorn Attention}}},
  booktitle = {{{ICML}}},
  author = {Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
  date = {2020-02-25},
  eprint = {2002.11296},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.11296},
  urldate = {2021-06-29},
  abstract = {We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.},
  archiveprefix = {arXiv},
  annotation = {36 citations (Semantic Scholar/arXiv) [2021-06-29]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YNLLTZIN/Tay et al. - 2020 - Sparse Sinkhorn Attention.pdf;/home/hiaoxui/.local/share/zotero_files/storage/GZZHYP9B/2002.html}
}

@misc{tay2022TransformerMemoryDifferentiable,
  title = {Transformer {{Memory}} as a {{Differentiable Search Index}}},
  author = {Tay, Yi and Tran, Vinh Q. and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and Schuster, Tal and Cohen, William W. and Metzler, Donald},
  date = {2022-02-16},
  eprint = {2202.06991},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.06991},
  urldate = {2022-04-27},
  abstract = {In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6QZH29BE/Tay et al. - 2022 - Transformer Memory as a Differentiable Search Inde.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7AFPW7AI/2202.html}
}

@incollection{taylor2003PennTreebankOverview,
  title = {The {{Penn Treebank}}: {{An Overview}}},
  shorttitle = {The {{Penn Treebank}}},
  booktitle = {Treebanks},
  author = {Taylor, Ann and Marcus, Mitchell and Santorini, Beatrice},
  editor = {Abeillé, Anne},
  date = {2003},
  series = {Text, {{Speech}} and {{Language Technology}}},
  volume = {20},
  pages = {5--22},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-010-0201-1_1},
  url = {http://link.springer.com/10.1007/978-94-010-0201-1_1},
  urldate = {2021-02-16},
  editorb = {Ide, Nancy and Véronis, Jean},
  editorbtype = {redactor},
  isbn = {978-1-4020-1335-5 978-94-010-0201-1},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ILIMHIJE/Taylor et al. - 2003 - The Penn Treebank An Overview.pdf}
}

@inproceedings{teh2006HierarchicalBayesianLanguage,
  title = {A Hierarchical {{Bayesian}} Language Model Based on {{Pitman-Yor}} Processes},
  booktitle = {{{ACL}}},
  author = {Teh, Y. W.},
  date = {2006},
  pages = {985--992},
  doi = {10.3115/1220175.1220299},
  abstract = {We propose a new hierarchical Bayesian n-gram model of natural languages. Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages. We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models. Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney. © 2006 Association for Computational Linguistics.},
  isbn = {1-932432-65-5},
  annotation = {524 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KKLQSXNL/Teh - 2006 - A hierarchical Bayesian language model based on Pitman-Yor processes(2).pdf}
}

@inproceedings{teichert2017SemanticProtoRoleLabeling,
  title = {Semantic {{Proto-Role Labeling}}},
  booktitle = {{{AAAI}}},
  author = {Teichert, A. and Poliak, A. and Van Durme, B. and Gormley, M. R.},
  date = {2017},
  pages = {4459--4465},
  abstract = {The semantic function tags of Bonial, Stowe, and Palmer (2013) and the ordinal, multi-property annotations of Reisinger et al. (2015) draw inspiration from Dowty's seman-tic proto-role theory. We approach proto-role labeling as a multi-label classification problem and establish strong results for the task by adapting a successful model of traditional se-mantic role labeling. We achieve a proto-role micro-averaged F1 of 81.7 using gold syntax and explore joint and condi-tional models of proto-roles and categorical roles. In compar-ing the effect of Bonial, Stowe, and Palmer's tags to Prop-Bank ArgN-style role labels, we are surprised that neither an-notations greatly improve proto-role prediction; however, we observe that ArgN models benefit much from observed syntax and from observed or modeled proto-roles while our models of the semantic function tags do not.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MWRXZW46/Teichert et al. - 2017 - Semantic Proto-Role Labeling(2).pdf}
}

@inproceedings{tellex2011UnderstandingNaturalLanguage,
  title = {Understanding {{Natural Language Commands}} for {{Robotic Navigation}} and {{Mobile Manipulation}}.},
  booktitle = {{{AAAI}}},
  author = {Tellex, S. and Kollar, T. and Dickerson, S.},
  date = {2011},
  pages = {1507--1514},
  url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewFile/3623/4113},
  abstract = {This paper describes a new model for understanding natural language commands given to autonomous systems that perform navigation and mobile manipulation in semi-structured environments. Previous approaches have used models with fixed structure to infer the likelihood of a sequence of actions given the environment and the command. In contrast, our framework, called Generalized Grounding Graphs (G3), dynamically instantiates a probabilistic graphical model for a particular natural language command according to the command’s hierarchical and compositional semantic structure. Our system performs inference in the model to successfully find and execute plans corresponding to natural language commands such as “Put the tire pallet on the truck.” The model is trained using a corpus of commands collected using crowdsourcing. We pair each command with robot actions and use the corpus to learn the parameters of the model. We evaluate the robot’s performance by inferring plans from natural language commands, executing each plan in a realistic robot simulator, and asking users to evaluate the system’s performance. We demonstrate that our system can successfully follow many natural language commands from the corpus.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NQSQ3RIX/Tellex, Kollar, Dickerson - 2011 - Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation(2).pdf}
}

@inproceedings{tenney2019WhatYouLearn,
  title = {What {{Do You Learn}} from {{Context}}? {{Probing}} for {{Sentence Structure}} in {{Contextualized Word Representations}}},
  booktitle = {{{ICLR}}},
  author = {Tenney, I. and Xia, P. and Chen, B. and Wang, A. and Poliak, A. and McCoy, R. T. and Kim, N. and Van Durme, B. and Bowman, S. R. and Das, D. and Pavlick, E.},
  date = {2019},
  pages = {1--17},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9E66WV76/Tenney et al. - 2019 - What Do You Learn from Context Probing for Sentence Structure in Contextualized Word Representations(2).pdf}
}

@inproceedings{thakur2021BEIRHeterogenousBenchmark,
  title = {{{BEIR}}: {{A Heterogenous Benchmark}} for {{Zero-shot Evaluation}} of {{Information Retrieval Models}}},
  shorttitle = {{{BEIR}}},
  booktitle = {{{NAACL}}},
  author = {Thakur, Nandan and Reimers, Nils and Rücklé, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  date = {2021-10-20},
  eprint = {2104.08663},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.08663},
  urldate = {2022-09-30},
  abstract = {Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TKJXWLAK/Thakur et al. - 2021 - BEIR A Heterogenous Benchmark for Zero-shot Evalu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/Q5PTE2MU/2104.html}
}

@misc{thakur2022DomainAdaptationMemoryEfficient,
  title = {Domain {{Adaptation}} for {{Memory-Efficient Dense Retrieval}}},
  author = {Thakur, Nandan and Reimers, Nils and Lin, Jimmy},
  date = {2022-05-23},
  number = {arXiv:2205.11498},
  eprint = {2205.11498},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.11498},
  urldate = {2022-05-29},
  abstract = {Dense retrievers encode documents into fixed dimensional embeddings. However, storing all the document embeddings within an index produces bulky indexes which are expensive to serve. Recently, BPR (Yamada et al., 2021) and JPQ (Zhan et al., 2021a) have been proposed which train the model to produce binary document vectors, which reduce the index 32x and more. The authors showed these binary embedding models significantly outperform more traditional index compression techniques like Product Quantization (PQ). Previous work evaluated these approaches just in-domain, i.e. the methods were evaluated on tasks for which training data is available. In practice, retrieval models are often used in an out-of-domain setting, where they have been trained on a publicly available dataset, like MS MARCO, but are then used for some custom dataset for which no training data is available. In this work, we show that binary embedding models like BPR and JPQ can perform significantly worse than baselines once there is a domain-shift involved. We propose a modification to the training procedure of BPR and JPQ and combine it with a corpus specific generative procedure which allow the adaptation of BPR and JPQ to any corpus without requiring labeled training data. Our domain-adapted strategy known as GPL is model agnostic, achieves an improvement by up-to 19.3 and 11.6 points in nDCG@10 across the BEIR benchmark in comparison to BPR and JPQ while maintaining its 32x memory efficiency. JPQ+GPL even outperforms our upper baseline: uncompressed TAS-B model on average by 2.0 points.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FX53NHYG/Thakur et al. - 2022 - Domain Adaptation for Memory-Efficient Dense Retri.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KKTHGHTW/2205.html}
}

@article{theune2001DataSpeechGeneral,
  title = {From {{Data}} to {{Speech}}: {{A General Approach}}},
  author = {Theune, M. and Klabbers, E. and Odijk, J. and De Pijper, J. R. and Krahmer, E.},
  date = {2001},
  journaltitle = {Natural Language Engineering},
  volume = {7},
  number = {1},
  pages = {47--86},
  issn = {1351-3249, 1351-3249},
  url = {http://findit.lib.cuhk.edu.hk/852cuhk/?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Allbashell&atitle=From+Data+to+Speech%3A+A+General+Approach&title=Natural+Language+Engineering&issn=13513249&date=2001-03-},
  abstract = {We present a data-to-speech system called D2S, which can be used for the creation of data-to-speech systems in different languages \& domains. The most important characteristic of a data-to-speech system is that it combines language \& speech generation: language generation is used to produce a natural language text expressing the system's input data, \& speech generation is used to make this text audible. In D2S, this combination is exploited by using linguistic information available in the language generation module for the computation of prosody. This allows us to achieve a better prosodic output quality than can be achieved in a plain text-to-speech system. For language generation in D2S, the use of syntactically enriched templates is guided by knowledge of the discourse context, while for speech generation pre-recorded phrases are combined in a prosodically sophisticated manner. This combination of techniques makes it possible to create linguistically sound but efficient systems with a high quality language \& speech output. 1 Table, 16 Figures, 69 References. Adapted from the source document},
  isbn = {1469-8110},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LG2BIEZR/Theune et al. - 2001 - From Data to Speech A General Approach(2).pdf}
}

@inproceedings{thirukovalluru2021ScalingDocumentCoreference,
  title = {Scaling {{Within Document Coreference}} to {{Long Texts}}},
  booktitle = {{{ACL}}},
  author = {Thirukovalluru, Raghuveer and Monath, Nicholas and Shridhar, Kumar and Zaheer, Manzil and Sachan, Mrinmaya and McCallum, Andrew},
  date = {2021},
  pages = {3921--3931},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.343},
  url = {https://aclanthology.org/2021.findings-acl.343},
  urldate = {2021-09-07},
  abstract = {State of the art end-to-end coreference resolution models use expensive span representations and antecedent prediction mechanisms. These approaches are expensive both in terms of their memory requirements as well as compute time, and are particularly ill-suited for long documents. In this paper, we propose an approximation to end-to-end models which scales gracefully to documents of any length. Replacing span representations with token representations, we reduce the time/memory complexity via token windows and nearest neighbor sparsification methods for more efficient antecedent prediction. We show our approach’s resulting reduction of training and inference time compared to state-of-the-art methods with only a minimal loss in accuracy.},
  eventtitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3BE9RFAX/Thirukovalluru et al. - 2021 - Scaling Within Document Coreference to Long Texts.pdf}
}

@inproceedings{thompson2019OvercomingCatastrophicForgetting,
  title = {Overcoming {{Catastrophic Forgetting During Domain Adaptation}} of {{Neural Machine Translation}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Thompson, Brian and Gwinnup, Jeremy and Khayrallah, Huda and Duh, Kevin and Koehn, Philipp},
  date = {2019},
  pages = {2062--2068},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1209},
  url = {http://aclweb.org/anthology/N19-1209},
  urldate = {2021-08-18},
  eventtitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  langid = {english},
  annotation = {57 citations (Semantic Scholar/DOI) [2021-08-18]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J8ZHLAA8/Thompson et al. - 2019 - Overcoming Catastrophic Forgetting During Domain A.pdf}
}

@inproceedings{thorne2021DatabaseReasoningText,
  title = {Database {{Reasoning Over Text}}},
  booktitle = {{{ACL}}},
  author = {Thorne, James and Yazdani, Majid and Saeidi, Marzieh and Silvestri, Fabrizio and Riedel, Sebastian and Halevy, Alon},
  date = {2021-06-02},
  eprint = {2106.01074},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.01074},
  urldate = {2022-02-09},
  abstract = {Neural models have shown impressive performance gains in answering queries from natural language text. However, existing works are unable to support database queries, such as "List/Count all female athletes who were born in 20th century", which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation. We show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data, numerical operations, and queries that aggregate facts. We propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale. We evaluate the architecture using WikiNLDB, a novel dataset for exploring such queries. Our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded. In direct comparison on small databases, our approach increases overall answer accuracy from 85\% to 90\%. On larger databases, our approach retains its accuracy whereas transformer baselines could not encode the context.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LP3GV4VI/Thorne et al. - 2021 - Database Reasoning Over Text.pdf;/home/hiaoxui/.local/share/zotero_files/storage/SEBVUC59/2106.html}
}

@misc{tomasello2022STOPDatasetSpoken,
  title = {{{STOP}}: {{A}} Dataset for {{Spoken Task Oriented Semantic Parsing}}},
  shorttitle = {{{STOP}}},
  author = {Tomasello, Paden and Shrivastava, Akshat and Lazar, Daniel and Hsu, Po-Chun and Le, Duc and Sagar, Adithya and Elkahky, Ali and Copet, Jade and Hsu, Wei-Ning and Mordechay, Yossef and Algayres, Robin and Nguyen, Tu Ahn and Dupoux, Emmanuel and Zettlemoyer, Luke and Mohamed, Abdelrahman},
  date = {2022-07-22},
  number = {arXiv:2207.10643},
  eprint = {2207.10643},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.10643},
  urldate = {2022-08-12},
  abstract = {End-to-end spoken language understanding (SLU) predicts intent directly from audio using a single model. It promises to improve the performance of assistant systems by leveraging acoustic information lost in the intermediate textual representation and preventing cascading errors from Automatic Speech Recognition (ASR). Further, having one unified model has efficiency advantages when deploying assistant systems on-device. However, the limited number of public audio datasets with semantic parse labels hinders the research progress in this area. In this paper, we release the Spoken Task-Oriented semantic Parsing (STOP) dataset, the largest and most complex SLU dataset to be publicly available. Additionally, we define low-resource splits to establish a benchmark for improving SLU when limited labeled data is available. Furthermore, in addition to the human-recorded audio, we are releasing a TTS-generated version to benchmark the performance for low-resource domain adaptation of end-to-end SLU systems. Initial experimentation show end-to-end SLU models performing slightly worse than their cascaded counterparts, which we hope encourages future work in this direction.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IAQ795FA/Tomasello et al. - 2022 - STOP A dataset for Spoken Task Oriented Semantic .pdf;/home/hiaoxui/.local/share/zotero_files/storage/ATC7WKW9/2207.html}
}

@inproceedings{toshniwal2020LearningIgnoreLong,
  title = {Learning to {{Ignore}}: {{Long Document Coreference}} with {{Bounded Memory Neural Networks}}},
  shorttitle = {Learning to {{Ignore}}},
  booktitle = {{{EMNLP}}},
  author = {Toshniwal, Shubham and Wiseman, Sam and Ettinger, Allyson and Livescu, Karen and Gimpel, Kevin},
  date = {2020-11-16},
  eprint = {2010.02807},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.02807},
  urldate = {2021-03-28},
  abstract = {Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-28]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LMBP7IC9/Toshniwal et al. - 2020 - Learning to Ignore Long Document Coreference with.pdf;/home/hiaoxui/.local/share/zotero_files/storage/SU2S269X/2010.html}
}

@inproceedings{toshniwal2020PeTraSparselySupervised,
  title = {{{PeTra}}: {{A Sparsely Supervised Memory Model}} for {{People Tracking}}},
  shorttitle = {{{PeTra}}},
  booktitle = {{{ACL}}},
  author = {Toshniwal, Shubham and Ettinger, Allyson and Gimpel, Kevin and Livescu, Karen},
  date = {2020-05-06},
  eprint = {2005.02990},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.02990},
  urldate = {2021-03-28},
  abstract = {We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots. PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture. We empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance. To measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach. PeTra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-28]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IQ95KKPC/Toshniwal et al. - 2020 - PeTra A Sparsely Supervised Memory Model for Peop.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QBIDKIB5/2005.html}
}

@inproceedings{tran2016UnsupervisedNeuralHidden,
  title = {Unsupervised {{Neural Hidden Markov Models}}},
  booktitle = {{{EMNLP}}},
  author = {Tran, K. and Bisk, Y. and Vaswani, A. and Marcu, D. and Knight, K.},
  date = {2016},
  eprint = {1609.09007},
  eprinttype = {arxiv},
  pages = {63--71},
  doi = {10.18653/v1/W16-5907},
  url = {http://arxiv.org/abs/1609.09007},
  abstract = {In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.},
  archiveprefix = {arXiv},
  annotation = {34 citations (Semantic Scholar/DOI) [2021-03-26] 34 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TTRMJYYD/Tran et al. - 2016 - Unsupervised Neural Hidden Markov Models(2).pdf}
}

@inproceedings{tran2017NamedEntityRecognition,
  title = {Named {{Entity Recognition}} with Stack Residual {{LSTM}} and Trainable Bias Decoding},
  booktitle = {{{IJCNLP}}},
  author = {Tran, Q. and MacKinlay, A. and Yepes, A.},
  date = {2017-07-11},
  eprint = {1706.07598},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.07598},
  urldate = {2020-07-23},
  abstract = {Recurrent Neural Network models are the state-of-the-art for Named Entity Recognition (NER). We present two innovations to improve the performance of these models. The first innovation is the introduction of residual connections between the Stacked Recurrent Neural Network model to address the degradation problem of deep neural networks. The second innovation is a bias decoding mechanism that allows the trained system to adapt to non-differentiable and externally computed objectives, such as the entity-based F-measure. Our work improves the state-of-the-art results for both Spanish and English languages on the standard train/development/test split of the CoNLL 2003 Shared Task NER dataset.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {35 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XBBJI4Y5/Tran et al. - 2017 - Named Entity Recognition with stack residual LSTM .pdf}
}

@inproceedings{transformers2017,
  title = {Attention {{Is All You Need}}},
  booktitle = {{{NeurIPS}}},
  author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. N. and Kaiser, L. and Polosukhin, I.},
  date = {2017},
  eprint = {1000303116},
  eprinttype = {pmid},
  issn = {0140-525X},
  doi = {10.1017/S0140525X16001837},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  annotation = {1102 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L6N3VRGP/Vaswani et al. - 2017 - Attention Is All You Need(2).pdf}
}

@misc{trask2018NeuralArithmeticLogic,
  title = {Neural {{Arithmetic Logic Units}}},
  author = {Trask, A. and Hill, F. and Reed, S. and Rae, J. and Dyer, C. and Blunsom, P.},
  date = {2018},
  eprint = {1808.00508v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P3HIEFWY/Trask et al. - 2018 - Neural Arithmetic Logic Units(2).pdf}
}

@inproceedings{trivedi2017KnowevolveDeepTemporal,
  title = {Know-Evolve: {{Deep}} Temporal Reasoning for Dynamic Knowledge Graphs},
  booktitle = {{{ICML}}},
  author = {Trivedi, R. and Dai, H. and Wang, Y. and Song, L.},
  date = {2017},
  volume = {7},
  eprint = {1705.05742v3},
  eprinttype = {arxiv},
  pages = {5313--5327},
  abstract = {The availability of large scale event data with time stamps has given rise to dynamically evolving knowledge graphs that contain temporal information for each edge. Reasoning over time in such dynamic knowledge graphs is not yet well understood. To this end, we present Know-Evolve, a novel deep evolutionary knowledge network that learns non-linearly evolving entity representations over time. The occurrence of a fact (edge) is modeled as a multivariate point process whose intensity function is modulated by the score for that fact computed based on the learned entity embeddings. We demonstrate significantly improved performance over various relational learning approaches on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to prior reasoning approaches in multi-relational setting.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-5514-4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HHDEKV5W/Trivedi et al. - 2017 - Know-evolve Deep temporal reasoning for dynamic knowledge graphs(2).pdf}
}

@inproceedings{trivedi2019DyREPLearningRepresentations,
  title = {{{DyREP}}: {{Learning Representations}} over {{Dynamic Graphs}}},
  booktitle = {{{ICLR}}},
  author = {Trivedi, R. and Farajtabar, M. and Biswal, P. and Zha, H.},
  date = {2019},
  eprint = {1812.03928},
  eprinttype = {arxiv},
  abstract = {Representations of sets are challenging to learn because operations on sets should be permutation-invariant. To this end, we propose a Permutation-Optimisation module that learns how to permute a set end-to-end. The permuted set can be further processed to learn a permutation-invariant representation of that set, avoiding a bottleneck in traditional set models. We demonstrate our model's ability to learn permutations and set representations with either explicit or implicit supervision on four datasets, on which we achieve state-of-the-art results: number sorting, image mosaics, classification from image mosaics, and visual question answering.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U8QYRYIB/Trivedi et al. - 2019 - DyREP Learning Representations over Dynamic Graphs(2).pdf}
}

@inproceedings{tromble2008LatticeMinimumBayesRisk,
  title = {Lattice {{Minimum Bayes-Risk Decoding}} for {{Statistical Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Tromble, R. W. and Kumar, S. and Och, F. and Macherey, W.},
  date = {2008},
  volume = {17},
  number = {1},
  pages = {64--69},
  doi = {10.3115/1613715.1613792},
  abstract = {I address the commentators' calls for clarification of theoretical terms, discussion of similarities to other proposals, and extension of the ideas. In doing so, I keep the focus on the purpose of memory: enabling the organism to make sense of its environment so that it can take action appropriate to constraints resulting from the physical, personal, social, and cultural situations.},
  isbn = {0-00-140110-6},
  keywords = {unread},
  annotation = {135 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FUG9NKWM/Tromble et al. - 2008 - Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation(2).pdf}
}

@inproceedings{tsai2019TransformerDissectionUnified,
  title = {Transformer {{Dissection}}: {{A Unified Understanding}} of {{Transformer}}'s {{Attention}} via the {{Lens}} of {{Kernel}}},
  shorttitle = {Transformer {{Dissection}}},
  booktitle = {{{EMNLP}}},
  author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  date = {2019-11-11},
  eprint = {1908.11775},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1908.11775},
  urldate = {2022-01-23},
  abstract = {Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer's attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer's attention. As an example, we propose a new variant of Transformer's attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T68ZUPTY/Tsai et al. - 2019 - Transformer Dissection A Unified Understanding of.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ZD43KXA8/1908.html}
}

@inproceedings{tsatsaronis2007WordSenseDisambiguation,
  title = {Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri},
  booktitle = {{{IJCAI}}},
  author = {Tsatsaronis, G. and Vazirgiannis, M. and Androutsopoulos, I.},
  date = {2007},
  eprint = {18353985},
  eprinttype = {pmid},
  pages = {1725--1730},
  issn = {10450823},
  doi = {10.1145/1459352.1459355},
  abstract = {Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data and/or do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networksâ€™ links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.},
  archiveprefix = {arXiv},
  isbn = {0360-0300},
  annotation = {1726 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P3CF78VG/Tsatsaronis, Vazirgiannis, Androutsopoulos - 2007 - Word sense disambiguation with spreading activation networks generated from thesa(2).pdf}
}

@inproceedings{tsdae2021,
  title = {{{TSDAE}}: {{Using Transformer-based Sequential Denoising Auto-Encoder}} for {{Unsupervised Sentence Embedding Learning}}},
  shorttitle = {{{TSDAE}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, Kexin and Reimers, Nils and Gurevych, Iryna},
  date = {2021-09-10},
  eprint = {2104.06979},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2104.06979},
  urldate = {2022-09-30},
  abstract = {Learning sentence embeddings often requires a large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1\% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong domain adaptation and pre-training method for sentence embeddings, significantly outperforming other approaches like Masked Language Model. A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TSDAE and other recent approaches on four different datasets from heterogeneous domains.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZIHWQK94/Wang et al. - 2021 - TSDAE Using Transformer-based Sequential Denoisin.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6I9JGRMD/2104.html}
}

@inproceedings{tsuboi2008TrainingConditionalRandoma,
  title = {Training Conditional Random Fields Using Incomplete Annotations},
  booktitle = {{{COLING}}},
  author = {Tsuboi, Yuta and Kashima, Hisashi and Oda, Hiroki and Mori, Shinsuke and Matsumoto, Yuji},
  date = {2008},
  volume = {1},
  pages = {897--904},
  publisher = {{Association for Computational Linguistics}},
  location = {{Manchester, United Kingdom}},
  doi = {10.3115/1599081.1599194},
  url = {http://portal.acm.org/citation.cfm?doid=1599081.1599194},
  urldate = {2021-06-22},
  abstract = {We address corpus building situations, where complete annotations to the whole corpus is time consuming and unrealistic. Thus, annotation is done only on crucial part of sentences, or contains unresolved label ambiguities. We propose a parameter estimation method for Conditional Random Fields (CRFs), which enables us to use such incomplete annotations. We show promising results of our method as applied to two types of NLP tasks: a domain adaptation task of a Japanese word segmentation using partial annotations, and a partof-speech tagging task using ambiguous tags in the Penn treebank corpus.},
  isbn = {978-1-905593-44-6},
  langid = {english},
  annotation = {79 citations (Semantic Scholar/DOI) [2021-06-22]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HVBJWAID/Tsuboi et al. - 2008 - Training conditional random fields using incomplet.pdf}
}

@inproceedings{tsukagoshi2022ComparisonCombinationSentence,
  title = {Comparison and {{Combination}} of {{Sentence Embeddings Derived}} from {{Different Supervision Signals}}},
  booktitle = {*{{SEM}}},
  author = {Tsukagoshi, Hayato and Sasano, Ryohei and Takeda, Koichi},
  date = {2022-06-10},
  eprint = {2202.02990},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.02990},
  urldate = {2022-10-03},
  abstract = {There have been many successful applications of sentence embedding methods. However, it has not been well understood what properties are captured in the resulting sentence embeddings depending on the supervision signals. In this paper, we focus on two types of sentence embedding methods with similar architectures and tasks: one fine-tunes pre-trained language models on the natural language inference task, and the other fine-tunes pre-trained language models on word prediction task from its definition sentence, and investigate their properties. Specifically, we compare their performances on semantic textual similarity (STS) tasks using STS datasets partitioned from two perspectives: 1) sentence source and 2) superficial similarity of the sentence pairs, and compare their performances on the downstream and probing tasks. Furthermore, we attempt to combine the two methods and demonstrate that combining the two methods yields substantially better performance than the respective methods on unsupervised STS tasks and downstream tasks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZNJ58UH4/Tsukagoshi et al. - 2022 - Comparison and Combination of Sentence Embeddings .pdf;/home/hiaoxui/.local/share/zotero_files/storage/6ST6C8JU/2202.html}
}

@inproceedings{tu2017CANEContextAwareNetwork,
  title = {{{CANE}}: {{Context-Aware Network Embedding}} for {{Relation Modeling}}},
  booktitle = {{{ACL}}},
  author = {Tu, C. and Liu, H. and Liu, Z. and Sun, M.},
  date = {2017},
  pages = {1722--1731},
  doi = {10.18653/v1/p17-1158},
  abstract = {OBJECTIVE To evaluate the effect of an intensivist-model of critical care delivery on the risk of death following injury. SUMMARY BACKGROUND DATA An intensivist-model of ICU care is associated with improved outcomes and less resource utilization in mixed medical and surgical ICUs. The process of trauma center verification assures a relatively high standard of care and quality assurance; thus, it is unclear what the effect of a specific model of ICU care delivery might have on trauma-related mortality. METHODS Using data from a large multicenter (68 centers) prospective cohort study, we evaluated the relationship between the model of ICU care (open vs. intensivist-model) and in-hospital mortality following severe injury. An intensivist-model was defined as an ICU where critically ill trauma patients were either on a distinct ICU service (led by an intensivist) or were comanaged with an intensivist (a physician board-certified in critical care). RESULTS After adjusting for differences in baseline characteristics, the relative risk of death in intensivist-model ICUs was 0.78 (0.58-1.04) compared with an open ICU model. The effect was greatest in the elderly [RR, 0.55 (0.39-0.77)], in units led by surgical intensivists [RR, 0.67 (0.50-0.90)], and in designated trauma centers 0.64 (0.46-0.88). CONCLUSIONS Care in an intensivist-model ICU is associated with a large reduction in in-hospital mortality following trauma, particularly in elderly patients who might have limited physiologic reserve and extensive comorbidity. That the effect is greatest in trauma centers and in units led by surgical intensivists suggests the importance of content expertise in the care of the critically injured. Injured patients are best cared for using an intensivist-model of dedicated critical care delivery, a criterion that should be considered in the verification of trauma centers.},
  keywords = {unread},
  annotation = {172 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EFQZD22J/Tu et al. - 2017 - CANE Context-Aware Network Embedding for Relation Modeling(2).pdf}
}

@inproceedings{urbizu2020SequenceSequenceCoreference,
  title = {Sequence to {{Sequence Coreference Resolution}}},
  booktitle = {{{COLING}}},
  author = {Urbizu, Gorka and Soraluze, Ander and Arregi, Olatz},
  date = {2020},
  pages = {8},
  abstract = {Until recently, coreference resolution has been a critical task on the pipeline of any NLP task involving deep language understanding, such as machine translation, chatbots, summarization or sentiment analysis. However, nowadays, those end tasks are learned end-to-end by deep neural networks without adding any explicit knowledge about coreference. Thus, coreference resolution is used less in the training of other NLP tasks or trending pretrained language models. In this paper we present a new approach to face coreference resolution as a sequence to sequence task based on the Transformer architecture. This approach is simple and universal, compatible with any language or dataset (regardless of singletons) and easier to integrate with current language models architectures. We test it on the ARRAU corpus, where we get 65.6 F1 CoNLL. We see this approach not as a final goal, but a means to pretrain sequence to sequence language models (T5) on coreference resolution.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZTDUZJ44/Urbizu et al. - Sequence to Sequence Coreference Resolution.pdf}
}

@inproceedings{uryupina2016ARRAULinguisticallyMotivatedAnnotation,
  title = {{{ARRAU}}: {{Linguistically-Motivated Annotation}} of {{Anaphoric Descriptions}}},
  booktitle = {{{LREC}}},
  author = {Uryupina, Olga and Artstein, Ron and Bristot, Antonella and Cavicchio, Federica and Rodriguez, Kepa J and Poesio, Massimo},
  date = {2016},
  pages = {5},
  abstract = {This paper presents a second release of the ARRAU dataset: a multi-domain corpus with thorough linguistically motivated annotation of anaphora and related phenomena. Building upon the first release almost a decade ago, a considerable effort had been invested in improving the data both quantitatively and qualitatively. Thus, we have doubled the corpus size, expanded the selection of covered phenomena to include referentiality and genericity and designed and implemented a methodology for enforcing the consistency of the manual annotation. We believe that the new release of ARRAU provides a valuable material for ongoing research in complex cases of coreference as well as for a variety of related tasks. The corpus is publicly available through LDC.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LKKQ2MYF/Uryupina et al. - ARRAU Linguistically-Motivated Annotation of Anap.pdf}
}

@inproceedings{valera2017AutomaticDiscoveryStatistical,
  title = {Automatic {{Discovery}} of the {{Statistical Types}} of {{Variables}} in a {{Dataset}}},
  booktitle = {{{ICML}}},
  author = {Valera, I. and Ghahramani, Z.},
  date = {2017},
  eprint = {1602.05003},
  eprinttype = {arxiv},
  pages = {4--5},
  url = {http://arxiv.org/abs/1602.05003},
  abstract = {Circular variables arise in a multitude of data-modelling contexts ranging from robotics to the social sciences, but they have been largely overlooked by the machine learning community. This paper partially redresses this imbalance by extending some standard probabilistic modelling tools to the circular domain. First we introduce a new multivariate distribution over circular variables, called the multivariate Generalised von Mises (mGvM) distribution. This distribution can be constructed by restricting and renormalising a general multivariate Gaussian distribution to the unit hyper-torus. Previously proposed multivariate circular distributions are shown to be special cases of this construction. Second, we introduce a new probabilistic model for circular regression, that is inspired by Gaussian Processes, and a method for probabilistic principal component analysis with circular hidden variables. These models can leverage standard modelling tools (e.g. covariance functions and methods for automatic relevance determination). Third, we show that the posterior distribution in these models is a mGvM distribution which enables development of an efficient variational free-energy scheme for performing approximate inference and approximate maximum-likelihood learning.},
  archiveprefix = {arXiv},
  annotation = {8 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KNAHMLMX/Valera, Ghahramani - 2017 - Automatic Discovery of the Statistical Types of Variables in a Dataset(2).pdf}
}

@inproceedings{valiant1984TheoryLearnable,
  title = {A Theory of the Learnable},
  booktitle = {Annual {{ACM Symposium}} on {{Theory}} of {{Computing}}},
  author = {Valiant, L. G.},
  date = {1984},
  volume = {27},
  number = {11},
  pages = {436--445},
  issn = {07378017},
  doi = {10.1145/800057.808710},
  abstract = {Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.},
  isbn = {0-89791-133-4},
  annotation = {3668 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6BGFBN3J/Valiant - 1984 - A theory of the learnable(2).pdf}
}

@inproceedings{valmadre2017EndtoendRepresentationLearning,
  title = {End-to-End Representation Learning for {{Correlation Filter}} Based Tracking},
  booktitle = {{{CVPR}}},
  author = {Valmadre, J. and Bertinetto, L. and Henriques, J. F. and Vedaldi, A. and Torr, P. H. S.},
  date = {2017},
  eprint = {1704.06036},
  eprinttype = {arxiv},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.531},
  url = {http://arxiv.org/abs/1704.06036},
  abstract = {The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.},
  archiveprefix = {arXiv},
  isbn = {978-1-5386-0457-1},
  annotation = {812 citations (Semantic Scholar/DOI) [2021-03-26] 812 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2LDWFITW/Valmadre et al. - 2017 - End-to-end representation learning for Correlation Filter based tracking(2).pdf}
}

@inproceedings{vanaken2017AutomaticDatabaseManagement,
  title = {Automatic Database Management System Tuning through Large-Scale Machine Learning},
  booktitle = {Special {{Interest Group}} on {{Management}} of {{Data}}},
  author = {Van Aken, D. and Pavlo, A. and Gordon, G. J.},
  date = {2017},
  volume = {Part F1277},
  pages = {1009--1024},
  issn = {07308078},
  doi = {10.1145/3035918.3064029},
  abstract = {© 2017 Copyright held by the owner/author(s). Database management system (DBMS) configuration tuning is an essential aspect of any data-intensive application effort. But this is historically a difficult task because DBMSs have hundreds of configuration "knobs" that control everything in the system, such as the amount of memory to use for caches and how often data is written to storage. The problem with these knobs is that they are not standardized (i.e., two DBMSs use a different name for the same knob), not independent (i.e., changing one knob can impact others), and not universal (i.e., what works for one application may be sub-optimal for another). Worse, information about the effects of the knobs typically comes only from (expensive) experience. To overcome these challenges, we present an automated approach that leverages past experience and collects new information to tune DBMS configurations: we use a combination of supervised and un-supervised machine learning methods to (1) select the most impactful knobs, (2) map unseen database workloads to previous workloads from which we can transfer experience, and (3) recommend knob settings. We implemented our techniques in a new tool called OtterTune and tested it on three DBMSs. Our evaluation shows that OtterTune recommends configurations that are as good as or better than ones generated by existing tools or a human expert.},
  isbn = {978-1-4503-4197-4},
  annotation = {220 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/47XNEUUC/Van Aken, Pavlo, Gordon - 2017 - Automatic database management system tuning through large-scale machine learning(2).pdf}
}

@article{vanderheijden2013LearningBayesianNetworks,
  title = {Learning {{Bayesian}} Networks for Clinical Time Series Analysis},
  author = {van der Heijden, M. and Velikova, M. and Lucas, P. J. F.},
  options = {useprefix=true},
  date = {2013},
  journaltitle = {Journal of Biomedical Infomatics},
  volume = {48},
  pages = {94--105},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4ND7GBC6/van der Heijden, Velikova, Lucas - 2013 - Learning Bayesian networks for clinical time series analysis(2).pdf}
}

@inproceedings{vandurme2009DerivingGeneralizedKnowledge,
  title = {Deriving Generalized Knowledge from Corpora Using {{WordNet}} Abstraction},
  booktitle = {{{EACL}}},
  author = {Van Durme, Benjamin and Michalak, Phillip and Schubert, Lenhart K.},
  date = {2009},
  pages = {808--816},
  publisher = {{Association for Computational Linguistics}},
  location = {{Athens, Greece}},
  doi = {10.3115/1609067.1609157},
  url = {http://portal.acm.org/citation.cfm?doid=1609067.1609157},
  urldate = {2021-04-01},
  eventtitle = {The 12th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {unread},
  annotation = {34 citations (Semantic Scholar/DOI) [2021-04-01]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UPN39XGE/Van Durme et al. - 2009 - Deriving generalized knowledge from corpora using .pdf}
}

@thesis{vandurme2009ExtractingImplicitKnowledge,
  title = {Extracting {{Implicit Knowledge}} from {{Text}}},
  author = {Van Durme, B.},
  date = {2009},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UCZXQBKI/Van Durme - 2009 - Extracting Implicit Knowledge from Text(2).pdf}
}

@inproceedings{vandurme2009StreamingPointwiseMutual,
  title = {Streaming Pointwise Mutual Information},
  booktitle = {{{NeurIPS}}},
  author = {Van Durme, B. and Lall, A.},
  date = {2009},
  pages = {1892--1900},
  abstract = {Recent work has led to the ability to perform space efficient, approximate counting over large vocabularies in a streaming context. Motivated by the existence of data structures of this type, we explore the computation of associativity scores, other- wise known as pointwise mutual information (PMI), in a streaming context. We give theoretical bounds showing the impracticality of perfect online PMI compu- tation, and detail an algorithm with high expected accuracy. Experiments on news articles show our approach gives high accuracy on real world data.},
  isbn = {978-1-61567-911-9},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7EEPV6MZ/Van Durme, Lall - 2009 - Streaming pointwise mutual information(2).pdf}
}

@inproceedings{vandurme2013PPDBParaphraseDatabase,
  title = {{{PPDB}}: {{The Paraphrase Database}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Van Durme, B. and Callison-burch, C.},
  date = {2013},
  pages = {758--764},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/729V6QIS/Van Durme, Callison-burch - 2013 - PPDB The Paraphrase Database(2).pdf}
}

@article{vanhoeve2006RevisitingSequenceConstraint,
  title = {Revisiting the Sequence Constraint},
  author = {Van Hoeve, W. J. and Pesant, G. and Rousseau, L. M. and Sabharwal, A.},
  date = {2006},
  journaltitle = {Lecture Notes in Computer Science},
  volume = {4204},
  pages = {620--634},
  issn = {16113349},
  doi = {10.1007/11889205_44},
  abstract = {Many combinatorial problems, such as car sequencing and fostering, feature sequence constraints, restricting the number of occurrences of certain values in every subsequence of a given width. To date, none of the filtering algorithms proposed guaranteed domain consistency. In this paper, we present three filtering algorithms for the sequence constraint, with complementary strengths. One borrows ideas from dynamic programming; another reformulates it as a regular constraint; the last is customized. The last two algorithms establish domain consistency. Our customized algorithm does so in polynomial time, and can even be applied to a generalized sequence constraint for subsequences of variable widths. Experimental results show the practical usefulness of each. © Springer-Verlag Berlin Heidelberg 2006.},
  isbn = {3540462678},
  annotation = {66 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IZ4UURIS/Van Hoeve et al. - 2006 - Revisiting the sequence constraint(2).pdf}
}

@inproceedings{vannguyen2021CrossTaskInstanceRepresentation,
  title = {Cross-{{Task Instance Representation Interactions}} and {{Label Dependencies}} for {{Joint Information Extraction}} with {{Graph Convolutional Networks}}},
  booktitle = {{{NAACL}}},
  author = {Van Nguyen, Minh and Lai, Viet Dac and Nguyen, Thien Huu},
  date = {2021-03-26},
  eprint = {2103.09330},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.09330},
  urldate = {2021-04-05},
  abstract = {Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single model (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four tasks that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a dependency graph for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new regularization mechanism is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve representation learning. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-04-05]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LWFVTMYS/Nguyen et al. - 2021 - Cross-Task Instance Representation Interactions an.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ZXENNMJ4/Van Nguyen et al. - 2021 - Cross-Task Instance Representation Interactions an.pdf;/home/hiaoxui/.local/share/zotero_files/storage/EL585EVE/2103.html}
}

@inproceedings{vashishtha2019FineGrainedTemporalRelation,
  title = {Fine-{{Grained Temporal Relation Extraction}}},
  booktitle = {{{ACL}}},
  author = {Vashishtha, S. and Van Durme, B. and White, A. S.},
  date = {2019},
  eprint = {1902.01390v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9AQNHYWF/Vashishtha, Van Durme, White - 2019 - Fine-Grained Temporal Relation Extraction(2).pdf}
}

@inproceedings{vaswani2013DecodingLargeScaleNeural,
  title = {Decoding with {{Large-Scale Neural Language Models Improves Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Vaswani, A. and Zhao, Y. and Fossum, V. and Chiang, D.},
  date = {2013},
  abstract = {We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K9ZU49VW/Vaswani et al. - 2013 - Decoding with Large-Scale Neural Language Models Improves Translation(2).pdf}
}

@inproceedings{vaswani2016SupertaggingLSTMs,
  title = {Supertagging {{With LSTMs}}},
  booktitle = {{{NAACL}}},
  author = {Vaswani, A. and Bisk, Y. and Sagae, K. and Musa, R.},
  date = {2016},
  pages = {232--237},
  url = {http://www.aclweb.org/anthology/N16-1027},
  abstract = {In this paper we present new state-of-the-art performance on CCG supertagging and pars-ing. Our model outperforms existing ap-proaches by an absolute gain of 1.5\%. We an-alyze the performance of several neural mod-els and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.},
  isbn = {978-1-941643-91-4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4P32JZBG/Vaswani et al. - 2016 - Supertagging With LSTMs(2).pdf}
}

@inproceedings{veyseh2021ModelingDocumentLevelContext,
  title = {Modeling {{Document-Level Context}} for {{Event Detection}} via {{Important Context Selection}}},
  booktitle = {{{EMNLP}}},
  author = {Veyseh, Amir Pouran Ben and Nguyen, Minh Van and Trung, Nghia Ngo and Min, Bonan and Nguyen, Thien Huu},
  date = {2021},
  pages = {11},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HLCYEZAV/Veyseh et al. - Modeling Document-Level Context for Event Detectio.pdf}
}

@inproceedings{vieira2017DynaSelfoptimizingDeclarative,
  title = {Dyna: {{Toward}} a Self-Optimizing Declarative Language for Machine Learning Applications},
  booktitle = {{{ACM SIGPLAN International Workshop}} on {{Machine Learning}} and {{Programming Languages}}},
  author = {Vieira, T. and Francis-Landau, M. and Filardo, N. W. and Khorasani, F. and Eisner, J. M.},
  date = {2017},
  pages = {8--17},
  doi = {10.1145/3088525.3088562},
  abstract = {Declarative programming is a paradigm that allows programmers to specify what they want to compute, leaving how to compute it to a solver. Our declarative programming language, Dyna, is designed to compactly specify computations like those that are frequently encountered in machine learning. As a declarative language, Dyna's solver has a large space of (correct) strategies available to it. We describe a reinforcement learning framework for adaptively choosing among these strategies to maximize efficiency for a given workload. Adaptivity in execution is especially important for software that will run under a variety of workloads, where no fixed policy works well. We hope that reinforcement learning will identify good policies reasonably quickly - offloading the burden of writing efficient code from human programmers.},
  isbn = {978-1-4503-5071-6},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DA9IRFGS/Vieira et al. - 2017 - Dyna Toward a self-optimizing declarative language for machine learning applications(2).pdf}
}

@article{vieira2017LearningPruneExploring,
  title = {Learning to {{Prune}}: {{Exploring}} the {{Frontier}} of {{Fast}} and {{Accurate Parsing}}},
  author = {Vieira, T. and Eisner, J. M.},
  date = {2017},
  journaltitle = {TACL},
  volume = {5},
  number = {2011},
  pages = {263--278},
  issn = {2307-387X},
  abstract = {Pruning hypotheses during dynamic program- ming is commonly used to speed up inference in settings such as parsing. Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy. This poses a difficult machine learning problem, which we tackle with the LOLS algorithm. LOLS training must continually compute the ef- fects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CF3QLBHR/Vieira, Eisner - 2017 - Learning to Prune Exploring the Frontier of Fast and Accurate Parsing(2).pdf}
}

@inproceedings{vilain1995ModeltheoreticCoreferenceScoring,
  title = {A Model-Theoretic Coreference Scoring Scheme},
  booktitle = {Conference on {{Message Understanding}}},
  author = {Vilain, Marc and Burger, John and Aberdeen, John and Connolly, Dennis and Hirschman, Lynette},
  date = {1995},
  pages = {45},
  publisher = {{Association for Computational Linguistics}},
  location = {{Columbia, Maryland}},
  doi = {10.3115/1072399.1072405},
  url = {http://portal.acm.org/citation.cfm?doid=1072399.1072405},
  urldate = {2021-12-09},
  eventtitle = {The 6th Conference},
  isbn = {978-1-55860-402-5},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HDY8U273/Vilain et al. - 1995 - A model-theoretic coreference scoring scheme.pdf}
}

@inproceedings{vogel1996HMMbasedWordAlignment,
  title = {{{HMM-based Word Alignment}} in {{Statistical Machine Translation}}},
  booktitle = {{{COLING}}},
  author = {Vogel, S. and Ney, H. and Tillmann, C.},
  date = {1996},
  volume = {96pp},
  pages = {836--841},
  doi = {10.3115/993268.993313},
  url = {http://portal.acm.org/citation.cfm?doid=993268.993313},
  abstract = {In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.},
  keywords = {unread},
  annotation = {910 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q88WGUFF/Vogel, Ney, Tillmann - 1996 - HMM-based Word Alignment in Statistical Machine Translation(2).pdf}
}

@inproceedings{vogel2010LearningFollowNavigational,
  title = {Learning to {{Follow Navigational Directions}}},
  booktitle = {{{ACL}}},
  author = {Vogel, A. and Jurafsky, D.},
  date = {2010},
  pages = {806--814},
  abstract = {We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.},
  isbn = {978-1-61738-808-8},
  issue = {July},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3BLYWLNT/Vogel, Jurafsky - 2010 - Learning to Follow Navigational Directions(2).pdf}
}

@inproceedings{volpi2018GeneralizingUnseenDomains,
  title = {Generalizing to Unseen Domains via Adversarial Data Augmentation},
  booktitle = {{{NeurIPS}}},
  author = {Volpi, R. and Duchi, J. and Namkoong, H. and Murino, V. and Sener, O. and Savarese, S.},
  date = {2018},
  eprint = {1805.12018},
  eprinttype = {arxiv},
  pages = {5334--5344},
  issn = {10495258},
  abstract = {We are concerned with learning models that generalize well to different unseen domains. We consider a worst-case formulation over data distributions that are near the source domain in the feature space. Only using training data from a single source distribution, we propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is "hard" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). On digit recognition and semantic segmentation tasks, our method learns models improve performance across a range of a priori unknown target domains.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X2CXAYKB/Volpi et al. - 2018 - Generalizing to unseen domains via adversarial data augmentation(2).pdf}
}

@inproceedings{vulic2019WeReallyNeed,
  title = {Do {{We Really Need Fully Unsupervised Cross-Lingual Embeddings}}?},
  booktitle = {{{EMNLP}}},
  author = {Vulić, I. and Glavaš, G. and Reichart, R. and Korhonen, A.},
  date = {2019},
  eprint = {1909.01638},
  eprinttype = {arxiv},
  pages = {4406--4417},
  doi = {10.18653/v1/d19-1449},
  abstract = {Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly focused on fully unsupervised approaches that project monolingual embeddings into a shared cross-lingual space without any cross-lingual signal. The lack of any supervision makes such approaches conceptually attractive. Yet, their only core difference from (weakly) supervised projection-based CLWE methods is in the way they obtain a seed dictionary used to initialize an iterative self-learning procedure. The fully unsupervised methods have arguably become more robust, and their primary use case is CLWE induction for pairs of resource-poor and distant languages. In this paper, we question the ability of even the most robust unsupervised CLWE approaches to induce meaningful CLWEs in these more challenging settings. A series of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {31 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7MC99SBG/Vulić et al. - 2019 - Do We Really Need Fully Unsupervised Cross-Lingual Embeddings(2).pdf}
}

@inproceedings{vyas2021LinkingEntitiesUnseen,
  title = {Linking {{Entities}} to {{Unseen Knowledge Bases}} with {{Arbitrary Schemas}}},
  booktitle = {{{NAACL}}},
  author = {Vyas, Yogarshi and Ballesteros, Miguel},
  date = {2021},
  pages = {834--844},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.65},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.65},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B4HSNTQH/Vyas and Ballesteros - 2021 - Linking Entities to Unseen Knowledge Bases with Ar.pdf}
}

@inproceedings{wadden2019EntityRelationEvent,
  title = {Entity, {{Relation}}, and {{Event Extraction}} with {{Contextualized Span Representations}}},
  booktitle = {{{EMNLP}}},
  author = {Wadden, David and Wennberg, Ulme and Luan, Yi and Hajishirzi, Hannaneh},
  date = {2019-09-09},
  eprint = {1909.03546},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/58SHQC2Q/Wadden et al. - 2019 - Entity, Relation, and Event Extraction with Contex.pdf;/home/hiaoxui/.local/share/zotero_files/storage/Q8GBAIHJ/Wadden et al. - 2019 - Entity, Relation, and Event Extraction with Contex.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KIHJEE4G/1909.html}
}

@inproceedings{wallace2019NLPModelsKnow,
  title = {Do {{NLP Models Know Numbers}}? {{Probing Numeracy}} in {{Embeddings}}},
  booktitle = {{{EMNLP}}},
  author = {Wallace, E. and Wang, Y. and Li, S. and Singh, S. and Gardner, M.},
  date = {2019},
  eprint = {1909.07940},
  eprinttype = {arxiv},
  pages = {5310--5318},
  doi = {10.18653/v1/d19-1534},
  abstract = {The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens---they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise---ELMo captures numeracy the best for all pre-trained methods---but BERT, which uses sub-word units, is less exact.},
  archiveprefix = {arXiv},
  annotation = {52 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N5ZFL2BS/Wallace et al. - 2019 - Do NLP Models Know Numbers Probing Numeracy in Embeddings(2).pdf}
}

@inproceedings{wallace2019UniversalAdversarialTriggers,
  title = {Universal {{Adversarial Triggers}} for {{Attacking}} and {{Analyzing NLP}}},
  booktitle = {{{EMNLP}}},
  author = {Wallace, E. and Feng, S. and Kandpal, N. and Gardner, M. and Singh, S.},
  date = {2019},
  eprint = {1908.07125},
  eprinttype = {arxiv},
  pages = {2153--2162},
  url = {http://arxiv.org/abs/1908.07125},
  abstract = {Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94\% to 0.55\%, 72\% of "why" questions in SQuAD to be answered "to kill american people", and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.},
  archiveprefix = {arXiv},
  issue = {Section 5},
  keywords = {unread},
  annotation = {123 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CDR7XJET/Wallace et al. - 2019 - Universal Adversarial Triggers for Attacking and Analyzing NLP(2).pdf;/home/hiaoxui/.local/share/zotero_files/storage/Y4A33VDZ/Wallace et al. - 2019 - Universal Adversarial Triggers for Attacking and A.pdf}
}

@inproceedings{walzer2007TemporalConstraintsRulebased,
  title = {Temporal Constraints for Rule-Based Event Processing},
  booktitle = {{{PIKM}}},
  author = {Walzer, K. and Schill, A. and Löser, A.},
  date = {2007},
  pages = {93--99},
  doi = {10.1145/1316874.1316890},
  abstract = {Complex event processing (CEP) is an important technology for event-driven systems with a broad application space ranging from supply chain management for RFID, systems monitoring, and stock market analysis to news services. The purpose of CEP is the identification of patterns of events with logical, temporal or causal relationships out of single occurring events. The Rete algorithm is commonly used in rule-based systems to trigger certain actions if a corresponding rule holds. It allows for a high number of rules and is therefore ideally suited for event processing systems. However, traditional Rete networks are limited to operations such as unification and the extraction of predicates from a knowledge base. There is no support for temporal operators. We propose an extension of the Rete algorithm for support of temporal operators. Thereby, we are using interval time semantics. We present the issues created by this extension as well as our pursued methodology to address them. A description language is used to specify the patterns of interest. This specification is also called subscription to a complex event. On the occurrence of the specified event pattern, an action such as the creation of a new event or the execution of certain function is performed. © 2007 ACM.},
  isbn = {978-1-59593-832-9},
  annotation = {11 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P34GJMZT/Walzer, Schill, Löser - 2007 - Temporal constraints for rule-based event processing(2).pdf}
}

@inproceedings{wang2013FastDropoutTraining,
  title = {Fast Dropout Training},
  booktitle = {{{ICML}}},
  author = {Wang, S. and Manning, C. D.},
  date = {2013},
  volume = {28},
  pages = {118--126},
  url = {http://machinelearning.wustl.edu/mlpapers/papers/wang13a},
  abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZIGE8WCM/Wang, Manning - 2013 - Fast dropout training(2).pdf}
}

@inproceedings{wang2013FeatureNoisingLoglinear,
  title = {Feature Noising for Log-Linear Structured Prediction},
  booktitle = {{{EMNLP}}},
  author = {Wang, S. I. and Wang, M. and Wager, S. and Liang, P. and Manning, C. D.},
  date = {2013},
  pages = {1170--1179},
  abstract = {© 2013 Association for Computational Linguistics. NLP models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting. A recently re-popularized form of regularization is to generate fake training data by repeatedly adding noise to real data. We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data. We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs. We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently. The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transductive extension. Applied to text classification and NER, our method provides a {$>$}1\% absolute performance gain over use of standard L2 regularization.},
  isbn = {978-1-937284-97-8},
  issue = {October},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2RBX5QLI/Wang et al. - 2013 - Feature noising for log-linear structured prediction(2).pdf}
}

@inproceedings{wang2014KnowledgeGraphText,
  title = {Knowledge {{Graph}} and {{Text Jointly Embedding}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, Z. and Zhang, J. and Feng, J. and Chen, Z.},
  date = {2014},
  doi = {10.3115/v1/d14-1167},
  abstract = {We examine the embedding approach to reason new relational facts from a large- scale knowledge graph and a text corpus. We propose a novel method of jointly em- bedding entities and words into the same continuous vector space. The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus. Entity names and Wikipedia an- chors are utilized to align the embeddings of entities and words in the same space. Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text. Particularly, jointly embedding enables the prediction of facts containing entities out of the knowledge graph, which cannot be han- dled by previous embedding methods. At the same time, concerning the quality of the word embeddings, experiments on the analogical reasoning task showthat jointly embedding is comparable to or slightly better than word2vec (Skip-Gram).},
  annotation = {285 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZVN7X6AF/Wang et al. - 2014 - Knowledge Graph and Text Jointly Embedding(2).pdf}
}

@inproceedings{wang2015BuildingSemanticParser,
  title = {Building a {{Semantic Parser Overnight}}},
  booktitle = {{{ACL}}},
  author = {Wang, Y. and Berant, J. and Liang, P.},
  date = {2015},
  eprint = {1684229},
  eprinttype = {pmid},
  pages = {1332--1342},
  doi = {10.3115/v1/P15-1129},
  url = {http://aclweb.org/anthology/P15-1129},
  abstract = {How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We fur- ther study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.},
  isbn = {978-1-941643-72-3},
  keywords = {unread},
  annotation = {212 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8WXNT8Y4/Wang, Berant, Liang - 2015 - Building a Semantic Parser Overnight(2).pdf}
}

@article{wang2016GalacticDependenciesTreebanks,
  title = {The {{Galactic Dependencies Treebanks}}: {{Getting More Data}} by {{Synthesizing New Languages}}},
  author = {Wang, D. and Eisner, J. M.},
  date = {2016},
  journaltitle = {TACL},
  volume = {4},
  eprint = {1710.03838},
  eprinttype = {arxiv},
  pages = {491--505},
  issn = {2307-387X},
  url = {http://arxiv.org/abs/1710.03838},
  abstract = {We release Galactic Dependencies 1.0---a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format. This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages. Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages. We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages. As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a "nearby" source language. We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.},
  archiveprefix = {arXiv},
  annotation = {34 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FZHATEKS/Wang, Eisner - 2016 - The Galactic Dependencies Treebanks Getting More Data by Synthesizing New Languages(2).pdf}
}

@article{wang2017FineGrainedPredictionSyntactic,
  title = {Fine-{{Grained Prediction}} of {{Syntactic Typology}}: {{Discovering Latent Structure}} with {{Supervised Learning}}},
  author = {Wang, D. and Eisner, J. M.},
  date = {2017},
  journaltitle = {TACL},
  volume = {5},
  pages = {147--161},
  doi = {10.1162/tacl_a_00052},
  abstract = {We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction. While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data. The supervised learner must identify surface features of a language’s POS sequence (hand-engineered or neural features) that correlate with the language’s deeper structure (latent trees). In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar induction baseline by a large margin.},
  annotation = {14 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ETWBJLJ6/Wang, Eisner - 2017 - Fine-Grained Prediction of Syntactic Typology Discovering Latent Structure with Supervised Learning(2).pdf}
}

@article{wang2018DeepVisualDomain,
  title = {Deep {{Visual Domain Adaptation}}: {{A Survey}}},
  author = {Wang, M. and Deng, W.},
  date = {2018},
  journaltitle = {Neurocomputing},
  eprint = {1802.03601v4},
  eprinttype = {arxiv},
  pages = {1--20},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9GKZEW9U/Wang, Deng - 2018 - Deep Visual Domain Adaptation A Survey(2).pdf}
}

@inproceedings{wang2018SyntheticDataMade,
  title = {Synthetic {{Data Made}} to {{Order}}: {{The Case}} of {{Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, D. and Eisner, J. M.},
  date = {2018},
  doi = {10.1109/IEEESTD.1998.87897},
  abstract = {To approximately parse an unfamiliar language , it helps to have a treebank of a similar language. But what if the closest available treebank still has the wrong word order? We show how to (stochastically) permute the constituents of an existing dependency tree-bank so that its surface part-of-speech statistics approximately match those of the target language. The parameters of the permutation model can be evaluated for quality by dynamic programming and tuned by gradient descent (up to a local optimum). This optimization procedure yields trees for a new artificial language that resembles the target language. We show that delexicalized parsers for the target language can be successfully trained using such "made to order" artificial languages.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WQQISFPY/Wang, Eisner - 2018 - Synthetic Data Made to Order The Case of Parsing(2).pdf}
}

@inproceedings{wang2019DeepFactorsForecasting,
  title = {Deep {{Factors}} for {{Forecasting}}},
  booktitle = {{{ICML}}},
  author = {Wang, Y. and Smola, A. and Maddix, D. C. and Gasthaus, J. and Foster, D. and Januschowski, T.},
  date = {2019},
  eprint = {1905.12417},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.12417},
  abstract = {Producing probabilistic forecasts for large collections of similar and/or dependent time series is a practically relevant and challenging task. Classical time series models fail to capture complex patterns in the data, and multivariate techniques struggle to scale to large problem sizes. Their reliance on strong structural assumptions makes them data-efficient, and allows them to provide uncertainty estimates. The converse is true for models based on deep neural networks, which can learn complex patterns and dependencies given enough data. In this paper, we propose a hybrid model that incorporates the benefits of both approaches. Our new method is data-driven and scalable via a latent, global, deep component. It also handles uncertainty through a local classical model. We provide both theoretical and empirical evidence for the soundness of our approach through a necessary and sufficient decomposition of exchangeable time series into a global and a local part. Our experiments demonstrate the advantages of our model both in term of data efficiency, accuracy and computational complexity.},
  archiveprefix = {arXiv},
  annotation = {35 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JKZCY53Q/Wang et al. - 2019 - Deep Factors for Forecasting(2).pdf}
}

@inproceedings{wang2019GLUEMultiTaskBenchmark,
  title = {{{GLUE}}: {{A Multi-Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  booktitle = {{{ICLR}}},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  date = {2019},
  eprint = {1804.07461},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.07461},
  urldate = {2020-11-19},
  abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
  archiveprefix = {arXiv},
  annotation = {1222 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JDECJCNA/Wang et al. - 2019 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf;/home/hiaoxui/.local/share/zotero_files/storage/TJ3DNYCH/1804.html}
}

@inproceedings{wang2019HowBestUse,
  title = {How to Best Use {{Syntax}} in {{Semantic Role Labelling}}},
  booktitle = {{{ACL}}},
  author = {Wang, Yufei and Johnson, Mark and Wan, Stephen and Sun, Yifang and Wang, Wei},
  date = {2019-06-01},
  eprint = {1906.00266},
  eprinttype = {arxiv},
  abstract = {There are many different ways in which external information might be used in an NLP task. This paper investigates how external syntactic information can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL'05 and CoNLL'12 benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FJBSH4T4/Wang et al. - 2019 - How to best use Syntax in Semantic Role Labelling.pdf}
}

@inproceedings{wang2019LearningSemanticParsers,
  title = {Learning {{Semantic Parsers}} from {{Denotations}} with {{Latent Structured Alignments}} and {{Abstract Programs}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, B. and Titov, I. and Lapata, M.},
  date = {2019},
  eprint = {1909.04165},
  eprinttype = {arxiv},
  pages = {3765--3776},
  url = {http://arxiv.org/abs/1909.04165},
  abstract = {Semantic parsing aims to map natural language utterances onto machine interpretable meaning representations, aka programs whose execution against a real-world environment produces a denotation. Weakly-supervised semantic parsers are trained on utterance-denotation pairs treating programs as latent. The task is challenging due to the large search space and spuriousness of programs which may execute to the correct answer but do not generalize to unseen examples. Our goal is to instill an inductive bias in the parser to help it distinguish between spurious and correct programs. We capitalize on the intuition that correct programs would likely respect certain structural constraints were they to be aligned to the question (e.g., program fragments are unlikely to align to overlapping text spans) and propose to model alignments as structured latent variables. In order to make the latent-alignment framework tractable, we decompose the parsing task into (1) predicting a partial "abstract program" and (2) refining it while modeling structured alignments with differential dynamic programming. We obtain state-of-the-art performance on the WIKITABLEQUESTIONS and WIKISQL datasets. When compared to a standard attention baseline, we observe that the proposed structured-alignment mechanism is highly beneficial.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/45DG9YL3/Wang, Titov, Lapata - 2019 - Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs(2).pdf}
}

@inproceedings{wang2019NeuralGaussianCopula,
  title = {Neural {{Gaussian Copula}} for {{Variational Autoencoder}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, P. Z. and Wang, W. Y.},
  date = {2019},
  eprint = {1909.03569},
  eprinttype = {arxiv},
  pages = {4332--4342},
  doi = {10.18653/v1/d19-1442},
  abstract = {Variational language models seek to estimate the posterior of latent variables with an approximated variational posterior. The model often assumes the variational posterior to be factorized even when the true posterior is not. The learned variational posterior under this assumption does not capture the dependency relationships over latent variables. We argue that this would cause a typical training problem called posterior collapse observed in all other variational language models. We propose Gaussian Copula Variational Autoencoder (VAE) to avert this problem. Copula is widely used to model correlation and dependencies of high-dimensional random variables, and therefore it is helpful to maintain the dependency relationships that are lost in VAE. The empirical results show that by modeling the correlation of latent variables explicitly using a neural parametric copula, we can avert this training difficulty while getting competitive results among all other VAE approaches.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ECCYF6HL/Wang, Wang - 2019 - Neural Gaussian Copula for Variational Autoencoder(2).pdf}
}

@inproceedings{wang2019SATNetBridgingDeep,
  title = {{{SATNet}}: {{Bridging}} Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver},
  booktitle = {{{ICML}}},
  author = {Wang, P. and Donti, P. L. and Wilder, B. and Kolter, Z.},
  date = {2019},
  eprint = {1905.12149},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.12149},
  abstract = {Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a "visual Sudok" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {49 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H8PWCGNI/Wang et al. - 2019 - SATNet Bridging deep learning and logical reasoning using a differentiable satisfiability solver(2).pdf}
}

@inproceedings{wang2019SuperGLUEStickierBenchmark,
  title = {{{SuperGLUE}}: {{A Stickier Benchmark}} for {{General-Purpose Language Understanding Systems}}},
  booktitle = {{{NeurIPS}}},
  author = {Wang, A. and Pruksachatkun, Y. and Nangia, N. and Singh, A. and Michael, J. and Hill, F. and Levy, O. and Bowman, S. R.},
  date = {2019},
  volume = {2019},
  eprint = {1905.00537},
  eprinttype = {arxiv},
  pages = {1--30},
  url = {http://arxiv.org/abs/1905.00537},
  abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
  archiveprefix = {arXiv},
  issue = {July},
  keywords = {unread},
  annotation = {295 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q7DI8QYV/Wang et al. - 2019 - SuperGLUE A Stickier Benchmark for General-Purpose Language Understanding Systems(2).pdf}
}

@inproceedings{wang2019TCVAETransformerBasedConditioned,
  title = {T-{{CVAE}}: {{Transformer-Based Conditioned Variational Autoencoder}} for {{Story Completion}}},
  shorttitle = {T-{{CVAE}}},
  booktitle = {{{IJCAI}}},
  author = {Wang, Tianming and Wan, Xiaojun},
  date = {2019-08},
  pages = {5233--5239},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Macao, China}},
  doi = {10.24963/ijcai.2019/727},
  url = {https://www.ijcai.org/proceedings/2019/727},
  urldate = {2022-04-28},
  abstract = {Story completion is a very challenging task of generating the missing plot for an incomplete story, which requires not only understanding but also inference of the given contextual clues. In this paper, we present a novel conditional variational autoencoder based on Transformer for missing plot generation. Our model uses shared attention layers for encoder and decoder, which make the most of the contextual clues, and a latent variable for learning the distribution of coherent story plots. Through drawing samples from the learned distribution, diverse reasonable plots can be generated. Both automatic and manual evaluations show that our model generates better story plots than stateof-the-art models in terms of readability, diversity and coherence.},
  eventtitle = {Twenty-{{Eighth International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-19}}\vphantom\{\}},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RZWUM2UY/Wang and Wan - 2019 - T-CVAE Transformer-Based Conditioned Variational .pdf}
}

@misc{wang2020LanguageModelsAre,
  title = {Language {{Models}} Are {{Open Knowledge Graphs}}},
  author = {Wang, C. and Liu, X. and Song, D.},
  date = {2020},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3V6GAZE8/Language Models are Open Knowledge Graphs.pdf}
}

@misc{wang2020LinformerSelfAttentionLinear,
  title = {Linformer: {{Self-Attention}} with {{Linear Complexity}}},
  shorttitle = {Linformer},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  date = {2020-06-14},
  eprint = {2006.04768},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.04768},
  urldate = {2021-03-28},
  abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n\^2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n\^2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the \textbackslash textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  archiveprefix = {arXiv},
  annotation = {51 citations (Semantic Scholar/arXiv) [2021-03-27]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IBTND84I/Wang et al. - 2020 - Linformer Self-Attention with Linear Complexity.pdf;/home/hiaoxui/.local/share/zotero_files/storage/BUUYMGYM/2006.html}
}

@inproceedings{wang2020PyramidLayeredModel,
  title = {Pyramid: {{A Layered Model}} for {{Nested Named Entity Recognition}}},
  shorttitle = {Pyramid},
  booktitle = {{{ACL}}},
  author = {Wang, Jue and Shou, Lidan and Chen, Ke and Chen, Gang},
  date = {2020},
  pages = {5918--5928},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.525},
  url = {https://www.aclweb.org/anthology/2020.acl-main.525},
  urldate = {2021-02-16},
  abstract = {This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER). In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape. Each time an embedding passes through a layer of the pyramid, its length is reduced by one. Its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention. We also design an inverse pyramid to allow bidirectional interaction between layers. The proposed method achieves state-of-the-art F1 scores in nested NER on ACE-2004, ACE2005, GENIA, and NNE, which are 80.27, 79.42, 77.78, and 93.70 with conventional embeddings, and 87.74, 86.34, 79.31, and 94.68 with pre-trained contextualized embeddings. In addition, our model can be used for the more general task of Overlapping Named Entity Recognition. A preliminary experiment confirms the effectiveness of our method in overlapping NER.},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IRTM8THD/Wang et al. - 2020 - Pyramid A Layered Model for Nested Named Entity R.pdf}
}

@inproceedings{wang2020RATSQLRelationAwareSchema,
  title = {{{RAT-SQL}}: {{Relation-Aware Schema Encoding}} and {{Linking}} for {{Text-to-SQL Parsers}}},
  booktitle = {{{ACL}}},
  author = {Wang, B. and Shin, R. and Liu, X. and Polozov, O. and Richardson, M.},
  date = {2020},
  eprint = {1911.04942},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.04942},
  abstract = {When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 53.7\%, compared to 47.4\% for the state-of-the-art model unaugmented with BERT embeddings. In addition, we observe qualitative improvements in the model's understanding of schema linking and alignment.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {47 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LYE5DBNY/Wang et al. - 2019 - RAT-SQL Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers(2).pdf}
}

@inproceedings{wang2020TPLinkerSinglestageJoint,
  title = {{{TPLinker}}: {{Single-stage Joint Extraction}} of {{Entities}} and {{Relations Through Token Pair Linking}}},
  shorttitle = {{{TPLinker}}},
  booktitle = {{{COLING}}},
  author = {Wang, Yucheng and Yu, Bowen and Zhang, Yueyang and Liu, Tingwen and Zhu, Hongsong and Sun, Limin},
  date = {2020-10-26},
  eprint = {2010.13415},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.13415},
  urldate = {2021-06-10},
  abstract = {Extracting entities and relations from unstructured text has attracted increasing attention in recent years but remains challenging, due to the intrinsic difficulty in identifying overlapping relations with shared entities. Prior works show that joint learning can result in a noticeable performance gain. However, they usually involve sequential interrelated steps and suffer from the problem of exposure bias. At training time, they predict with the ground truth conditions while at inference it has to make extraction from scratch. This discrepancy leads to error accumulation. To mitigate the issue, we propose in this paper a one-stage joint extraction model, namely, TPLinker, which is capable of discovering overlapping relations sharing one or both entities while immune from the exposure bias. TPLinker formulates joint extraction as a token pair linking problem and introduces a novel handshaking tagging scheme that aligns the boundary tokens of entity pairs under each relation type. Experiment results show that TPLinker performs significantly better on overlapping and multiple relation extraction, and achieves state-of-the-art performance on two public datasets.},
  archiveprefix = {arXiv},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-06-10]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RLBAB64I/Wang et al. - 2020 - TPLinker Single-stage Joint Extraction of Entitie.pdf;/home/hiaoxui/.local/share/zotero_files/storage/MIYP4L8T/2010.html}
}

@inproceedings{wang2020TwoAreBetter,
  title = {Two Are {{Better}} than {{One}}: {{Joint Entity}} and {{Relation Extraction}} with {{Table-Sequence Encoders}}},
  shorttitle = {Two Are {{Better}} than {{One}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, Jue and Lu, Wei},
  date = {2020},
  pages = {1706--1721},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.133},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.133},
  urldate = {2021-03-16},
  eventtitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  langid = {english},
  annotation = {2 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XEK6C59C/Wang and Lu - 2020 - Two are Better than One Joint Entity and Relation.pdf}
}

@inproceedings{wang2020UnderstandingContrastiveRepresentation,
  title = {Understanding {{Contrastive Representation Learning}} through {{Alignment}} and {{Uniformity}} on the {{Hypersphere}}},
  booktitle = {{{ICML}}},
  author = {Wang, Tongzhou and Isola, Phillip},
  date = {2020},
  pages = {11},
  abstract = {Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning. Project Page: ssnl.github.io/hypersphere. Code: github.com/SsnL/align uniform. Alignment: Similar samples have similar features.},
  langid = {english},
  keywords = {constrastive},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4G82VLYU/Wang and Isola - Understanding Contrastive Representation Learning .pdf}
}

@inproceedings{wang2021CLEVEContrastivePretraining,
  title = {{{CLEVE}}: {{Contrastive Pre-training}} for {{Event Extraction}}},
  shorttitle = {{{CLEVE}}},
  booktitle = {{{ACL}}},
  author = {Wang, Ziqi and Wang, Xiaozhi and Han, Xu and Lin, Yankai and Hou, Lei and Liu, Zhiyuan and Li, Peng and Li, Juanzi and Zhou, Jie},
  date = {2021-05-30},
  eprint = {2105.14485},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.14485},
  urldate = {2021-06-08},
  abstract = {Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised "liberal" EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-06-08]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7B37PT68/Wang et al. - 2021 - CLEVE Contrastive Pre-training for Event Extracti.pdf;/home/hiaoxui/.local/share/zotero_files/storage/WL9WJ38T/2105.html}
}

@inproceedings{wang2021MetaLearningDomainGeneralization,
  title = {Meta-{{Learning}} for {{Domain Generalization}} in {{Semantic Parsing}}},
  booktitle = {{{NAACL}}},
  author = {Wang, Bailin and Lapata, Mirella and Titov, Ivan},
  date = {2021},
  pages = {366--379},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.33},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.33},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FEG39CEN/Wang et al. - 2021 - Meta-Learning for Domain Generalization in Semanti.pdf}
}

@inproceedings{wang2021MetaSelftrainingFewshot,
  title = {Meta {{Self-training}} for {{Few-shot Neural Sequence Labeling}}},
  booktitle = {{{KDD}}},
  author = {Wang, Yaqing and Mukherjee, Subhabrata and Chu, Haoda and Tu, Yuancheng and Wu, Ming and Gao, Jing and Awadallah, Ahmed Hassan},
  date = {2021},
  pages = {12},
  abstract = {Neural sequence labeling is widely adopted for many Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER) and slot tagging for dialog systems and semantic parsing. Recent advances with large-scale pre-trained language models have shown remarkable success in these tasks when fine-tuned on large amounts of task-specific labeled data. However, obtaining such large-scale labeled training data is not only costly, but also may not be feasible in many sensitive user applications due to data access and privacy constraints. This is exacerbated for sequence labeling tasks requiring such annotations at token-level. In this work, we develop techniques to address the label scarcity challenge for neural sequence labeling models. Specifically, we propose a meta self-training framework which leverages very few manually annotated labels for training neural sequence models. While self-training serves as an effective mechanism to learn from large amounts of unlabeled data via iterative knowledge exchange – meta-learning helps in adaptive sample re-weighting to mitigate error propagation from noisy pseudo-labels. Extensive experiments on six benchmark datasets including two for massive multilingual NER and four slot tagging datasets for task-oriented dialog systems demonstrate the effectiveness of our method. With only 10 labeled examples for each class in each task, the proposed method achieves 10\% improvement over state-of-the-art methods demonstrating its effectiveness for limited training labels regime.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H7RQQE52/Wang et al. - Meta Self-training for Few-shot Neural Sequence La.pdf}
}

@inproceedings{wang2021UniREUnifiedLabel,
  title = {{{UniRE}}: {{A Unified Label Space}} for {{Entity Relation Extraction}}},
  shorttitle = {{{UniRE}}},
  booktitle = {{{ACL}}},
  author = {Wang, Yijun and Sun, Changzhi and Wu, Yuanbin and Zhou, Hao and Li, Lei and Yan, Junchi},
  date = {2021-07-09},
  eprint = {2107.04292},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.04292},
  urldate = {2021-07-13},
  abstract = {Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks' label spaces. The input of our model is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell's label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-13]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MQH76TLY/Wang et al. - 2021 - UniRE A Unified Label Space for Entity Relation E.pdf;/home/hiaoxui/.local/share/zotero_files/storage/RPRL4TGW/2107.html}
}

@inproceedings{wang2021ZeroShotInformationExtraction,
  title = {Zero-{{Shot Information Extraction}} as a {{Unified Text-to-Triple Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, Chenguang and Liu, Xiao and Chen, Zui and Hong, Haoyun and Tang, Jie and Song, Dawn},
  date = {2021},
  pages = {14},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M6RGQ7JH/Wang et al. - Zero-Shot Information Extraction as a Unified Text.pdf}
}

@inproceedings{wang2022ExpandingPretrainedModels,
  title = {Expanding {{Pretrained Models}} to {{Thousands More Languages}} via {{Lexicon-based Adaptation}}},
  booktitle = {{{ACL}}},
  author = {Wang, Xinyi and Ruder, Sebastian and Neubig, Graham},
  date = {2022-03-17},
  eprint = {2203.09435},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.09435},
  urldate = {2022-03-23},
  abstract = {The performance of multilingual pretrained models is highly dependent on the availability of monolingual or parallel text present in a target language. Thus, the majority of the world's languages cannot benefit from recent progress in NLP as they have no or limited textual data. To expand possibilities of using NLP technology in these under-represented languages, we systematically study strategies that relax the reliance on conventional language resources through the use of bilingual lexicons, an alternative resource with much better language coverage. We analyze different strategies to synthesize textual or labeled data using lexicons, and how this data can be combined with monolingual or parallel text when available. For 19 under-represented languages across 3 tasks, our methods lead to consistent improvements of up to 5 and 15 points with and without extra monolingual text respectively. Overall, our study highlights how NLP methods can be adapted to thousands more languages that are under-served by current technology},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FNHTSS73/Wang et al. - 2022 - Expanding Pretrained Models to Thousands More Lang.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ESF3ESZY/2203.html}
}

@inproceedings{wang2022GPLGenerativePseudo,
  title = {{{GPL}}: {{Generative Pseudo Labeling}} for {{Unsupervised Domain Adaptation}} of {{Dense Retrieval}}},
  shorttitle = {{{GPL}}},
  booktitle = {{{NAACL}}},
  author = {Wang, Kexin and Thakur, Nandan and Reimers, Nils and Gurevych, Iryna},
  date = {2022-04-25},
  eprint = {2112.07577},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.07577},
  urldate = {2022-09-30},
  abstract = {Dense retrieval approaches can overcome the lexical gap and lead to significantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets. In this paper, we propose the novel unsupervised domain adaptation method Generative Pseudo Labeling (GPL), which combines a query generator with pseudo labeling from a cross-encoder. On six representative domain-specialized datasets, we find the proposed GPL can outperform an out-of-the-box state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL requires less (unlabeled) data from the target domain and is more robust in its training than previous methods. We further investigate the role of six recent pre-training methods in the scenario of domain adaptation for retrieval tasks, where only three could yield improved results. The best approach, TSDAE (Wang et al., 2021) can be combined with GPL, yielding another average improvement of 1.4 points nDCG@10 across the six tasks. The code and the models are available at https://github.com/UKPLab/gpl.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9L32INTG/Wang et al. - 2022 - GPL Generative Pseudo Labeling for Unsupervised D.pdf;/home/hiaoxui/.local/share/zotero_files/storage/HZ6ME3ZK/2112.html}
}

@thesis{wanzare2019ScriptAcquisitionCrowdsourcing,
  title = {Script {{Acquisition}} : {{A Crowdsourcing}} and {{Text}} Mining Approach},
  author = {Wanzare, L D. A.},
  date = {2019},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DX4PRPRM/Wanzare - 2019 - Script Acquisition A Crowdsourcing and Text mining approach(2).pdf}
}

@inproceedings{warstadt2019InvestigatingBERTKnowledge,
  title = {Investigating {{BERT}}'s {{Knowledge}} of {{Language}}: {{Five Analysis Method}} with {{NPIs}}},
  booktitle = {{{EMNLP}}},
  author = {Warstadt, A. and Cao, Y. and Grosu, I. and Peng, W. and Blix, H. and Nie, Y. and Alsop, A. and Bordia, S. and Liu, H. and Parrish, A. and Wang, S. and Phang, J. and Mohananey, A. and Htut, P. M. and Jeretiˇ, P. and Bowman, S. R.},
  date = {2019},
  eprint = {1909.02597v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/448GGR5G/Warstadt et al. - 2019 - Investigating BERT's Knowledge of Language Five Analysis Method with NPIs(2).pdf}
}

@article{webberb.2003AnaphoraDiscourseStructure,
  title = {Anaphora and Discourse Structure},
  author = {{Webber B.} and {Stone M.} and Joshi, A. and {Knott A}},
  date = {2003},
  journaltitle = {Computational Linguistics},
  volume = {29},
  pages = {545--587},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EKX2UKKM/Webber B. et al. - 2003 - Anaphora and discourse structure(2).pdf}
}

@inproceedings{weber2018FineLineLinguistic,
  title = {The {{Fine Line}} between {{Linguistic Generalization}} and {{Failure}} in {{Seq2Seq-Attention Models}}},
  booktitle = {Workshop on {{Generalization}} in the {{Age}} of {{Deep Learning}}},
  author = {Weber, N. and Shekhar, L. and Balasubramanian, N.},
  date = {2018},
  pages = {24--27},
  doi = {10.18653/v1/w18-1004},
  abstract = {Seq2Seq based neural architectures have become the go-to architecture to apply to sequence to sequence language tasks. Despite their excellent performance on these tasks, recent work has noted that these models usually do not fully capture the linguistic structure required to generalize beyond the dense sections of the data distribution \textbackslash cite\{ettinger2017towards\}, and as such, are likely to fail on samples from the tail end of the distribution (such as inputs that are noisy \textbackslash citep\{belkinovnmtbreak\} or of different lengths \textbackslash citep\{bentivoglinmtlength\}). In this paper, we look at a model's ability to generalize on a simple symbol rewriting task with a clearly defined structure. We find that the model's ability to generalize this structure beyond the training distribution depends greatly on the chosen random seed, even when performance on the standard test set remains the same. This suggests that a model's ability to capture generalizable structure is highly sensitive. Moreover, this sensitivity may not be apparent when evaluating it on standard test sets.},
  keywords = {unread},
  annotation = {22 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FI7TDL7V/Weber, Shekhar, Balasubramanian - 2018 - The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models(2).pdf}
}

@inproceedings{weber2018HierarchicalQuantizedRepresentations,
  title = {Hierarchical {{Quantized Representations}} for {{Script Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Weber, N. and Shekhar, L. and Balasubramanian, N. and Chambers, N.},
  date = {2018},
  pages = {3783--3792},
  doi = {10.18653/v1/d18-1413},
  abstract = {Scripts define knowledge about how everyday scenarios (such as going to a restaurant) are expected to unfold. One of the challenges to learning scripts is the hierarchical nature of the knowledge. For example, a suspect arrested might plead innocent or guilty, and a very different track of events is then expected to happen. To capture this type of information, we propose an autoencoder model with a latent space defined by a hierarchy of categorical variables. We utilize a recently proposed vector quantization based approach, which allows continuous embeddings to be associated with each latent variable value. This permits the decoder to softly decide what portions of the latent hierarchy to condition on by attending over the value embeddings for a given setting. Our model effectively encodes and generates scripts, outperforming a recent language modeling-based method on several standard tasks, and allowing the autoencoder model to achieve substantially lower perplexity scores compared to the previous language modeling-based method.},
  annotation = {12 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X8ACWF6Z/Weber et al. - 2018 - Hierarchical Quantized Representations for Script Generation(2).pdf}
}

@inproceedings{webson2022PromptBasedModelsReally,
  title = {Do {{Prompt-Based Models Really Understand}} the {{Meaning}} of Their {{Prompts}}?},
  booktitle = {{{NAACL}}},
  author = {Webson, Albert and Pavlick, Ellie},
  date = {2022-04-21},
  eprint = {2109.01247},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.01247},
  urldate = {2022-07-15},
  abstract = {Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompt templates manually written for natural language inference (NLI). We find that models learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively "good" prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2022). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models' impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/98VL9AP7/Webson and Pavlick - 2022 - Do Prompt-Based Models Really Understand the Meani.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ZGJRZ28I/2109.html}
}

@inproceedings{webster2014LimitedMemoryIncremental,
  title = {Limited Memory Incremental Coreference Resolution},
  booktitle = {{{COLING}}},
  author = {Webster, Kellie and Curran, James R},
  date = {2014},
  pages = {11},
  abstract = {We propose an algorithm for coreference resolution based on analogy with shift-reduce parsing. By reconceptualising the task in this way, we unite ranking- and cluster-based approaches to coreference resolution, which have until now been largely orthogonal. Additionally, our framework naturally lends itself to rich discourse modelling, which we use to define a series of psycholinguistically motivated features. We achieve CoNLL scores of 63.33 and 62.91 on the CoNLL-2012 DEV and TEST splits of the OntoNotes 5 corpus, beating the publicly available state of the art systems. These results are also competitive with the best reported research systems despite our system having low memory requirements and a simpler model.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N6F98VN3/Webster and Curran - Limited memory incremental coreference resolution.pdf}
}

@inproceedings{webster2016UsingMentionAccessibility,
  title = {Using Mention Accessibility to Improve Coreference Resolution},
  booktitle = {{{ACL}}},
  author = {Webster, Kellie and Nothman, Joel},
  date = {2016},
  pages = {432--437},
  publisher = {{Association for Computational Linguistics}},
  location = {{Berlin, Germany}},
  doi = {10.18653/v1/P16-2070},
  url = {http://aclweb.org/anthology/P16-2070},
  urldate = {2021-03-28},
  abstract = {Modern coreference resolution systems require linguistic and general knowledge typically sourced from costly, manually curated resources. Despite their intuitive appeal, results have been mixed. In this work, we instead implement fine-grained surface-level features motivated by cognitive theory. Our novel fine-grained feature specialisation approach significantly improves the performance of a strong baseline, achieving state-of-the-art results of 65.29 and 61.13\% on CoNLL-2012 using gold and automatic preprocessing, with system extracted mentions.},
  eventtitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  langid = {english},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-03-28]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z9KG3ZRM/Webster and Nothman - 2016 - Using mention accessibility to improve coreference.pdf}
}

@inproceedings{wei2018MoreAdaptiveAlgorithms,
  title = {More {{Adaptive Algorithms}} for {{Adversarial Bandits}}},
  booktitle = {{{COLT}}},
  author = {Wei, C. and Luo, H.},
  date = {2018},
  volume = {75},
  eprint = {1801.03265v3},
  eprinttype = {arxiv},
  pages = {1--29},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4VY5JUHZ/Wei, Luo - 2018 - More Adaptive Algorithms for Adversarial Bandits(2).pdf}
}

@inproceedings{wei2021MaskedConditionalRandom,
  title = {Masked {{Conditional Random Fields}} for {{Sequence Labeling}}},
  booktitle = {{{NAACL}}},
  author = {Wei, Tianwen and Qi, Jianwei and He, Shenghuan and Sun, Songtao},
  date = {2021},
  pages = {12},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GZPGH6ID/Wei et al. - Masked Conditional Random Fields for Sequence Labe.pdf}
}

@inproceedings{wei2021TriggerNotSufficient,
  title = {Trigger Is {{Not Sufficient}}: {{Exploiting Frame-aware Knowledge}} for {{Implicit Event Argument Extraction}}},
  shorttitle = {Trigger Is {{Not Sufficient}}},
  booktitle = {{{ACL}}},
  author = {Wei, Kaiwen and Sun, Xian and Zhang, Zequn and Zhang, Jingyuan and Zhi, Guo and Jin, Li},
  date = {2021},
  pages = {4672--4682},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-long.360},
  url = {https://aclanthology.org/2021.acl-long.360},
  urldate = {2021-09-07},
  abstract = {Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-ofthe-art performance on the RAMS dataset.},
  eventtitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7I2Q3PDA/Wei et al. - 2021 - Trigger is Not Sufficient Exploiting Frame-aware .pdf}
}

@misc{weir2020ProbingNeuralLanguage,
  title = {Probing {{Neural Language Models}} for {{Human Tacit Assumptions}}},
  author = {Weir, N. and Poliak, A. and Van Durme, B.},
  date = {2020},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P46HAFQI/Weir, Poliak, Van Durme - 2020 - Probing Neural Language Models for Human Tacit Assumptions(2).pdf}
}

@misc{weischedel2010OntoNotesRelease,
  title = {{{OntoNotes Release}} 5.0},
  author = {Weischedel, R. and Pradhan, S. and Ramshaw, L. and Kaufman, J. and Franchini, M. and El-Bachouti, M. and Xue, N. and Palmer, M. and Marcus, M. and Taylor, A. and Greenberg, C. and Hovy, E. and Belvin, R. and Houston, A.},
  date = {2010},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MK8UQMVR/OntoNotes-Release-5.0.pdf}
}

@incollection{weischedel2011OntoNotesLargeTraining,
  title = {{{OntoNotes}}: {{A}} Large Training Corpus for Enhanced Processing},
  booktitle = {Handbook of {{Natural Language Processing}} and {{Machine Translation}}. {{Springer}}},
  author = {Weischedel, R. and Hovy, E. and Marcus, M. and Palmer, M. and Belvin, R. and Pradhan, S. and Ramshaw, L. and Xue, N.},
  date = {2011},
  publisher = {{Springer}},
  abstract = {1 This paper describes a large multilingual richly annotated corpus which is being made available to the community. There is an emphasis on quality and consistency with interannotator agreement rates targeted at 90\%. The data covers multiple genres in English, Chinese, and Arabic, including a significant amount of parallel data. The annotation, intended to capture a skeletal representation of literal meaning, includes parse trees, predicate argument structures , word senses localized in an ontology, co-reference, and name types. The resource is delivered as an integrated database, supporting combined queries that access multiple annotation layers. Annual incremental releases are distributed via LDC.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NWGFWGLR/Weischedel et al. - 2011 - OntoNotes A large training corpus for enhanced processing(2).pdf}
}

@misc{weiss2019ClinicalRiskWavelet,
  title = {Clinical {{Risk}}: {{Wavelet Reconstruction Networks}} for {{Marked Point Processes}}},
  author = {Weiss, J. C.},
  date = {2019},
  keywords = {review},
  annotation = {ICLR 2019 review (secondary for Hongyuan)},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V2JWQW3P/Weiss - 2019 - Clinical Risk Wavelet Reconstruction Networks for Marked Point Processes(2).pdf}
}

@inproceedings{welleck2019NeuralTextGeneration,
  title = {Neural {{Text Generation}} with {{Unlikelihood Training}}},
  booktitle = {{{ICLR}}},
  author = {Welleck, S. and Kulikov, I. and Roller, S. and Dinan, E. and Cho, K. and Weston, J.},
  date = {2019},
  eprint = {1908.04319v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NELCDAD3/Welleck et al. - 2019 - Neural Text Generation with Unlikelihood Training(2).pdf}
}

@inproceedings{welleck2019NonMonotonicSequentialText,
  title = {Non-{{Monotonic Sequential Text Generation}}},
  booktitle = {{{ICML}}},
  author = {Welleck, Sean and Brantley, Kianté and Daumé III, Hal and Cho, Kyunghyun},
  date = {2019-10-23},
  eprint = {1902.02192},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1902.02192},
  urldate = {2020-08-06},
  abstract = {Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy’s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {61 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6C96MJBJ/Welleck et al. - 2019 - Non-Monotonic Sequential Text Generation.pdf}
}

@inproceedings{welling2011BayesianLearningStochastic,
  title = {Bayesian {{Learning}} via {{Stochastic Gradient Langevin Dynamics}}},
  booktitle = {{{ICML}}},
  author = {Welling, M. and Teh, Y. W.},
  date = {2011},
  eprint = {1203.5753v5},
  eprinttype = {arxiv},
  pages = {681--688},
  issn = {10495258},
  doi = {10.1515/jip-2012-0071},
  abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an in-built protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  archiveprefix = {arXiv},
  isbn = {978-1-4503-0619-5},
  keywords = {unread},
  annotation = {30 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YA4YJLB2/Welling, Teh - 2011 - Bayesian Learning via Stochastic Gradient Langevin Dynamics(2).pdf}
}

@inproceedings{wen2015SemanticallyConditionedLSTMbased,
  title = {Semantically {{Conditioned LSTM-based Natural Language Generation}} for {{Spoken Dialogue Systems}}},
  booktitle = {{{EMNLP}}},
  author = {Wen, T. and Gasic, M. and Mrksic, N. and Su, P. and Vandyke, D. and Young, S.},
  date = {2015},
  eprint = {1508.01745},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1508.01745},
  abstract = {Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.},
  archiveprefix = {arXiv},
  annotation = {623 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2U9HSC84/Wen et al. - 2015 - Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems(2).pdf}
}

@inproceedings{wen2017MultiHorizonQuantileRecurrent,
  title = {A {{Multi-Horizon Quantile Recurrent Forecaster}}},
  booktitle = {{{NeurIPS}}},
  author = {Wen, R. and Torkkola, K. and Narayanaswamy, B. and Madeka, D.},
  date = {2017},
  eprint = {1711.11053},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.11053},
  abstract = {We propose a framework for general probabilistic multi-step time series regression. Specifically, we exploit the expressiveness and temporal nature of Sequence-to-Sequence Neural Networks (e.g. recurrent and convolutional structures), the nonparametric nature of Quantile Regression and the efficiency of Direct Multi-Horizon Forecasting. A new training scheme, *forking-sequences*, is designed for sequential nets to boost stability and performance. We show that the approach accommodates both temporal and static covariates, learning across multiple related series, shifting seasonality, future planned event spikes and cold-starts in real life large-scale forecasting. The performance of the framework is demonstrated in an application to predict the future demand of items sold on Amazon.com, and in a public probabilistic forecasting competition to predict electricity price and load.},
  archiveprefix = {arXiv},
  annotation = {80 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GYS6CGTT/Wen et al. - 2017 - A Multi-Horizon Quantile Recurrent Forecaster(2).pdf}
}

@inproceedings{wen2021EventTimeExtraction,
  title = {Event {{Time Extraction}} and {{Propagation}} via {{Graph Attention Networks}}},
  booktitle = {{{NAACL}}},
  author = {Wen, Haoyang and Qu, Yanru and Ji, Heng and Ning, Qiang and Han, Jiawei and Sil, Avi and Tong, Hanghang and Roth, Dan},
  date = {2021},
  pages = {62--73},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.6},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.6},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U824TGVB/Wen et al. - 2021 - Event Time Extraction and Propagation via Graph At.pdf}
}

@misc{west2021SymbolicKnowledgeDistillation,
  title = {Symbolic {{Knowledge Distillation}}: From {{General Language Models}} to {{Commonsense Models}}},
  shorttitle = {Symbolic {{Knowledge Distillation}}},
  author = {West, Peter and Bhagavatula, Chandra and Hessel, Jack and Hwang, Jena D. and Jiang, Liwei and Bras, Ronan Le and Lu, Ximing and Welleck, Sean and Choi, Yejin},
  date = {2021-10-14},
  eprint = {2110.07178},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.07178},
  urldate = {2021-10-29},
  abstract = {The common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z8S22AS4/West et al. - 2021 - Symbolic Knowledge Distillation from General Lang.pdf;/home/hiaoxui/.local/share/zotero_files/storage/LMG2G5VK/2110.html}
}

@article{weston2013ConnectingLanguageKnowledge,
  title = {Connecting {{Language}} and {{Knowledge Bases}} with {{Embedding Models}} for {{Relation Extraction}}},
  author = {Weston, J. and Bordes, A. and Yakhnenko, O. and Usunier, N.},
  date = {2013},
  journaltitle = {Computational Linguistics},
  eprint = {1307.7973},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1307.7973},
  abstract = {This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on two scoring functions that operate by learning low-dimensional embeddings of words and of entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over existing methods that rely on text features alone.},
  archiveprefix = {arXiv},
  isbn = {9781937284978},
  annotation = {185 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/URVK2GN7/Weston et al. - 2013 - Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction(2).pdf}
}

@inproceedings{weston2015MemoryNetworks,
  title = {Memory Networks},
  booktitle = {{{ICLR}}},
  author = {Weston, J. and Chopra, S. and Bordes, A.},
  date = {2015},
  eprint = {9377276},
  eprinttype = {pmid},
  pages = {1--15},
  issn = {1098-7576},
  doi = {v0},
  abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term mem- ory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chainingmultiple supporting sentences to an- swer questions that require understanding the intension of verbs.},
  archiveprefix = {arXiv},
  isbn = {978-1-4244-6917-8},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3EE782JP/Weston, Chopra, Bordes - 2015 - Memory networks(2).pdf}
}

@misc{white2016ComputationalLinkingTheory,
  title = {Computational {{Linking Theory}}},
  author = {White, A. S. and Reisinger, D. and Rudinger, R. and Rawlins, K. and Van Durme, B.},
  date = {2016},
  number = {1},
  eprint = {1610.02544},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1610.02544},
  abstract = {A linking theory explains how verbs' semantic arguments are mapped to their syntactic arguments---the inverse of the Semantic Role Labeling task from the shallow semantic parsing literature. In this paper, we develop the Computational Linking Theory framework as a method for implementing and testing linking theories proposed in the theoretical literature. We deploy this framework to assess two cross-cutting types of linking theory: local v. global models and categorical v. featural models. To further investigate the behavior of these models, we develop a measurement model in the spirit of previous work in semantic role induction: the Semantic Proto-Role Linking Model. We use this model, which implements a generalization of Dowty's seminal Proto-Role Theory, to induce semantic proto-roles, which we compare to those Dowty proposes.},
  archiveprefix = {arXiv},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VD6ZKBXT/White et al. - 2016 - Computational Linking Theory(2).pdf}
}

@inproceedings{white2016UniversalDecompositionalSemantics,
  title = {Universal {{Decompositional Semantics}} on {{Universal Dependencies}}},
  booktitle = {{{EMNLP}}},
  author = {White, A. S. and Reisinger, D. and Sakaguchi, K. and Vieira, T. and Zhang, S. and Rudinger, R. and Rawlins, K. and Van Durme, B.},
  date = {2016},
  pages = {1713--1723},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CVIJ4ZZ5/White et al. - 2016 - Universal Decompositional Semantics on Universal Dependencies(2).pdf}
}

@inproceedings{white2017InferenceEverythingRecasting,
  title = {Inference Is Everything: {{Recasting}} Semantic Resources into a Unified Evaluation Framework},
  booktitle = {{{IJCNLP}}},
  author = {White, A. S. and Rastogi, P. and Duh, K. and Van Durme, B.},
  date = {2017},
  pages = {996--1005},
  abstract = {We propose to unify a variety of existing se-mantic classification tasks, such as seman-tic role labeling, anaphora resolution, and paraphrase detection, under the heading of Recognizing Textual Entailment (RTE). We present a general strategy to automat-ically generate one or more sentential hy-potheses based on an input sentence and pre-existing manual semantic annotations. The resulting suite of datasets enables us to probe a statistical RTE model's perfor-mance on different aspects of semantics. We demonstrate the value of this approach by investigating the behavior of a popular neural network RTE model.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SGJX3JB2/White et al. - 2017 - Inference is everything Recasting semantic resources into a unified evaluation framework(2).pdf}
}

@inproceedings{white2017SemanticProtoroleLinking,
  title = {The Semantic Proto-Role Linking Model},
  booktitle = {{{EACL}}},
  author = {White, A. S. and Rawlins, K. and Van Durme, B.},
  date = {2017},
  volume = {2},
  pages = {92--98},
  abstract = {We propose the semantic proto-role linking model, which jointly induces both predicate-specific semantic roles and predicate-general semantic proto-roles based on semantic proto-role property likelihood judgments. We use this model to empirically evaluate Dowty\{'\}s thematic proto-role linking theory.},
  isbn = {978-1-5108-3860-4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K3LJ239M/White, Rawlins, Van Durme - 2017 - The semantic proto-role linking model(2).pdf}
}

@inproceedings{white2018LexicosyntacticInferenceNeural,
  title = {Lexicosyntactic {{Inference}} in {{Neural Models}}},
  booktitle = {{{EMNLP}}},
  author = {White, A. S. and Rudinger, R. and Rawlins, K. and Van Durme, B.},
  date = {2018},
  number = {i},
  eprint = {1808.06232},
  eprinttype = {arxiv},
  pages = {4717--4724},
  url = {http://arxiv.org/abs/1808.06232},
  abstract = {We investigate neural models' ability to capture lexicosyntactic inferences: inferences triggered by the interaction of lexical and syntactic information. We take the task of event factuality prediction as a case study and build a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts. We use this dataset, which we make publicly available, to probe the behavior of current state-of-the-art neural systems, showing that these systems make certain systematic errors that are clearly visible through the lens of factuality prediction.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {14 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IFI4L5QN/White et al. - 2018 - Lexicosyntactic Inference in Neural Models(2).pdf}
}

@inproceedings{white2021ExaminingInductiveBias,
  title = {Examining the {{Inductive Bias}} of {{Neural Language Models}} with {{Artificial Languages}}},
  booktitle = {{{ACL}}},
  author = {White, Jennifer C. and Cotterell, Ryan},
  date = {2021-06-02},
  eprint = {2106.01044},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.01044},
  urldate = {2021-06-04},
  abstract = {Since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages. Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup. Languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders. We propose a novel method for investigating the inductive biases of language models using artificial languages. These languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated, such as word order. We then use them to train and test language models. This constitutes a fully controlled causal framework, and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models. Using this method, we find that commonly used neural architectures exhibit different inductive biases: LSTMs display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others. Further, we find that neither the inductive bias of the LSTM nor that of the transformer appears to reflect any tendencies that we see in attested natural languages.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-06-04]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZDJCZBDL/White and Cotterell - 2021 - Examining the Inductive Bias of Neural Language Mo.pdf;/home/hiaoxui/.local/share/zotero_files/storage/WEIVNA8S/2106.html}
}

@inproceedings{wiegreffe2019AttentionNotExplanation,
  title = {Attention Is Not {{Explanation}}},
  booktitle = {{{NAACL}}},
  author = {Wiegreffe, S. and Pinter, Y.},
  date = {2019},
  eprint = {1908.04626},
  eprinttype = {arxiv},
  pages = {3543--3556},
  url = {http://arxiv.org/abs/1908.04626},
  abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
  archiveprefix = {arXiv},
  annotation = {149 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9PTLEFH5/Wiegreffe, Pinter - 2019 - Attention is not Explanation(2).pdf}
}

@inproceedings{wigren2019ParameterEliminationParticle,
  title = {Parameter Elimination in Particle {{Gibbs}} Sampling},
  booktitle = {{{NeurIPS}}},
  author = {Wigren, A. and Risuleo, R. S. and Murray, L. and Lindsten, F.},
  date = {2019},
  eprint = {1910.14145v1},
  eprinttype = {arxiv},
  abstract = {Bayesian inference in state-space models is challenging due to high-dimensional state trajectories. A viable approach is particle Markov chain Monte Carlo, combining MCMC and sequential Monte Carlo to form "exact approximations" to otherwise intractable MCMC methods. The performance of the approximation is limited to that of the exact method. We focus on particle Gibbs and particle Gibbs with ancestor sampling, improving their performance beyond that of the underlying Gibbs sampler (which they approximate) by marginalizing out one or more parameters. This is possible when the parameter prior is conjugate to the complete data likelihood. Marginalization yields a non-Markovian model for inference, but we show that, in contrast to the general case, this method still scales linearly in time. While marginalization can be cumbersome to implement, recent advances in probabilistic programming have enabled its automation. We demonstrate how the marginalized methods are viable as efficient inference backends in probabilistic programming, and demonstrate with examples in ecology and epidemiology.},
  archiveprefix = {arXiv},
  issue = {NeurIPS},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U8JPVJY3/Wigren et al. - 2019 - Parameter elimination in particle Gibbs sampling(2).pdf}
}

@article{williams1999LatentTreeLearning,
  title = {Do Latent Tree Learning Models Identify Meaningful Structure in Sentences ?},
  author = {Williams, A. and Drozdov, A. and Bowman, S. R.},
  date = {1999},
  journaltitle = {TACL},
  eprint = {1709.01121v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S5QTVNUB/Williams, Drozdov, Bowman - 1999 - Do latent tree learning models identify meaningful structure in sentences(2).pdf}
}

@inproceedings{wilson2005RecognizingContextualPolarity,
  title = {Recognizing {{Contextual Polarity}} in {{Phrase-Level Sentiment Analysis}}},
  booktitle = {{{EMNLP-HLT}}},
  author = {Wilson, T. and Wiebe, J and Hoffmann, P.},
  date = {2005},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2BDRSNQL/Wilson, Wiebe, Hoffmann - 2005 - Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis(2).pdf}
}

@misc{winata2019LowRankMatrixFactorization,
  title = {Low-{{Rank Matrix Factorization}} of {{LSTM}} as {{Effective Model Compression}}},
  author = {Winata, G. I. and Madotto, A. and Shin, J. and Barezi, E. J.},
  date = {2019},
  keywords = {review},
  annotation = {ICLR 2019 review (secondary for Hongyuan)},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MZ7X8BVF/Winata et al. - 2019 - Low-Rank Matrix Factorization of LSTM as Effective Model Compression(2).pdf}
}

@inproceedings{winn2018LighterCanStill,
  title = {‘ {{Lighter}} ’ {{Can Still Be Dark}} : {{Modeling Comparative Color Descriptions}}},
  booktitle = {{{ACL}}},
  author = {Winn, O. and Muresan, S.},
  date = {2018},
  pages = {1--6},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3FDSK3PR/Winn, Muresan - 2018 - ‘ Lighter ’ Can Still Be Dark Modeling Comparative Color Descriptions(2).pdf}
}

@inproceedings{wiseman2017ChallengesDatatoDocumentGeneration,
  title = {Challenges in {{Data-to-Document Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Wiseman, S. and Shieber, S. M. and Rush, A. M.},
  date = {2017},
  eprint = {1707.08052},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.08052},
  abstract = {Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.},
  archiveprefix = {arXiv},
  annotation = {233 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SKAICTCQ/Wiseman, Shieber, Rush - 2017 - Challenges in Data-to-Document Generation(2).pdf}
}

@inproceedings{wiseman2018LearningNeuralTemplates,
  title = {Learning {{Neural Templates}} for {{Text Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Wiseman, S. and Shieber, S. M. and Rush, A. M.},
  date = {2018},
  eprint = {1808.10122},
  eprinttype = {arxiv},
  pages = {1--11},
  doi = {arXiv:1808.10122v1},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4C6WIFJM/Wiseman, Shieber, Rush - 2018 - Learning Neural Templates for Text Generation(2).pdf}
}

@misc{wolf2019TransferTransfoTransferLearning,
  title = {{{TransferTransfo}}: {{A Transfer Learning Approach}} for {{Neural Network Based Conversational Agents}}},
  shorttitle = {{{TransferTransfo}}},
  author = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
  date = {2019-02-04},
  eprint = {1901.08149},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.08149},
  urldate = {2022-04-28},
  abstract = {We introduce a new approach to generative data-driven dialogue systems (e.g. chatbots) called TransferTransfo which is a combination of a Transfer learning based training scheme and a high-capacity Transformer model. Fine-tuning is performed by using a multi-task objective which combines several unsupervised prediction tasks. The resulting fine-tuned model shows strong improvements over the current state-of-the-art end-to-end conversational models like memory augmented seq2seq and information-retrieval models. On the privately held PERSONA-CHAT dataset of the Conversational Intelligence Challenge 2, this approach obtains a new state-of-the-art, with respective perplexity, Hits@1 and F1 metrics of 16.28 (45 \% absolute improvement), 80.7 (46 \% absolute improvement) and 19.5 (20 \% absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WNGBFTDJ/Wolf et al. - 2019 - TransferTransfo A Transfer Learning Approach for .pdf;/home/hiaoxui/.local/share/zotero_files/storage/6YLC6WT8/1901.html}
}

@inproceedings{wolf2020TransformersStateoftheArtNatural,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  booktitle = {{{EMNLP}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  options = {useprefix=true},
  date = {2020},
  pages = {38--45},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  urldate = {2022-02-14},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered stateof-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/ huggingface/transformers.},
  eventtitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ED2C4J8V/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf}
}

@inproceedings{wong2006LearningSemanticParsing,
  title = {Learning for {{Semantic Parsing}} with {{Statistical Machine Translation}}},
  booktitle = {{{HLT}}},
  author = {Wong, Y. W. and Mooney, R. J.},
  date = {2006},
  pages = {439--446},
  doi = {10.3115/1220835.1220891},
  url = {http://www.aclweb.org/anthology/N/N06/N06-1056},
  abstract = {We present a novel statistical approach to semantic parsing, WASP, for construct-ing a complete, formal meaning represen-tation of a sentence. A semantic parser is learned given a set of sentences anno-tated with their correct meaning represen-tations. The main innovation of WASP is its use of state-of-the-art statistical ma-chine translation techniques. A word alignment model is used for lexical acqui-sition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods re-quiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.},
  issue = {June},
  annotation = {272 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X9K4XB43/Wong, Mooney - 2006 - Learning for Semantic Parsing with Statistical Machine Translation(2).pdf}
}

@inproceedings{wong2007GenerationInvertingSemantic,
  title = {Generation by {{Inverting}} a {{Semantic Parser That Uses Statistical Machine Translation}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Wong, Y. W. and Mooney, R. J.},
  date = {2007},
  pages = {172--179},
  abstract = {This paper explores the use of statistical machine translation (SMT) methods for tactical natural language generation. We present results on using phrase-based SMT for learning to map meaning representations to natural language. Improved results are obtained by inverting a semantic parser that uses SMT methods to map sentences into meaning representations. Finally, we show that hybridizing these two approaches results in still more accurate generation systems. Automatic and human evaluation of generated sentences are presented across two domains and four languages. © 2007 Association for Computational Linguistics.},
  issue = {April},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SDASNKT9/Wong, Mooney - 2007 - Generation by Inverting a Semantic Parser That Uses Statistical Machine Translation(2).pdf}
}

@inproceedings{wong2008LearningSynchronousGrammars,
  title = {Learning {{Synchronous Grammars}} for {{Semantic Parsing}} with {{Lambda Calculus}}},
  booktitle = {{{ICTAI}}},
  author = {Wong, Y. W. and Mooney, R. J.},
  date = {2008},
  volume = {2},
  pages = {135--142},
  issn = {10823409},
  doi = {10.1109/ICTAI.2008.96},
  abstract = {We formulate semantic parsing as a parsing problem on a synchronous context free grammar (SCFG) which is automatically built on the corpus of natural language sentences and the representation of semantic outputs. We then present an online learning fr...},
  isbn = {978-0-7695-3440-4},
  issue = {June},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SB8CT6MD/Wong, Mooney - 2008 - Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus(2).pdf}
}

@inproceedings{wu2017DynamicWindowNeural,
  title = {A {{Dynamic Window Neural Network}} for {{CCG Supertagging}}},
  booktitle = {{{AAAI}}},
  author = {Wu, H. and Zhang, J. and Zong, C.},
  date = {2017},
  eprint = {1610.02749},
  eprinttype = {arxiv},
  pages = {3337--3343},
  abstract = {Combinatory Category Grammar (CCG) supertag-ging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. How-ever, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window ap-proach, which can be treated as an attention mech-anism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words di-rectly, which is superior to the regular dropout on word embeddings. We use this approach to demon-strate the state-of-the-art CCG supertagging perfor-mance on the standard test set.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QEWFTGX5/Wu, Zhang, Zong - 2017 - A Dynamic Window Neural Network for CCG Supertagging(2).pdf}
}

@inproceedings{wu2021DATransformerDistanceawareTransformer,
  title = {{{DA-Transformer}}: {{Distance-aware Transformer}}},
  booktitle = {{{NAACL}}},
  author = {Wu, Chuhan and Wu, Fangzhao and Huang, Yongfeng},
  date = {2021},
  pages = {10},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AZRGAZWX/Wu et al. - DA-Transformer Distance-aware Transformer.pdf}
}

@misc{wu2021FastformerAdditiveAttention,
  title = {Fastformer: {{Additive Attention Can Be All You Need}}},
  shorttitle = {Fastformer},
  author = {Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng and Xie, Xing},
  date = {2021-09-02},
  eprint = {2108.09084},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.09084},
  urldate = {2021-09-05},
  abstract = {Transformer is a powerful model for text understanding. However, it is inefficient due to its quadratic complexity to input sequence length. Although there are many methods on Transformer acceleration, they are still either inefficient on long sequences or not effective enough. In this paper, we propose Fastformer, which is an efficient Transformer model based on additive attention. In Fastformer, instead of modeling the pair-wise interactions between tokens, we first use additive attention mechanism to model global contexts, and then further transform each token representation based on its interaction with global context representations. In this way, Fastformer can achieve effective context modeling with linear complexity. Extensive experiments on five datasets show that Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-05]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MJ7GHSPT/Wu et al. - 2021 - Fastformer Additive Attention Can Be All You Need.pdf;/home/hiaoxui/.local/share/zotero_files/storage/B78IJI73/2108.html}
}

@inproceedings{wu2021HiTransformerHierarchicalInteractive,
  title = {Hi-{{Transformer}}: {{Hierarchical Interactive Transformer}} for {{Efficient}} and {{Effective Long Document Modeling}}},
  shorttitle = {Hi-{{Transformer}}},
  booktitle = {{{ACL}}},
  author = {Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng},
  date = {2021-07-14},
  eprint = {2106.01040},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.01040},
  urldate = {2021-07-23},
  abstract = {Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically, we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/55HA3TAS/Wu et al. - 2021 - Hi-Transformer Hierarchical Interactive Transform.pdf;/home/hiaoxui/.local/share/zotero_files/storage/3DAKTNZD/2106.html}
}

@misc{wu2021RecursivelySummarizingBooks,
  title = {Recursively {{Summarizing Books}} with {{Human Feedback}}},
  author = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M. and Stiennon, Nissan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
  date = {2021-09-22},
  eprint = {2109.10862},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.10862},
  urldate = {2021-09-24},
  abstract = {A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases (\$\textbackslash sim5\textbackslash\%\$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/US2IIXPD/Wu et al. - 2021 - Recursively Summarizing Books with Human Feedback.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7FGD2XLD/2109.html}
}

@inproceedings{wu2022MemorizingTransformers,
  title = {Memorizing {{Transformers}}},
  booktitle = {{{ICLR}}},
  author = {Wu, Y. and Rabe, M. N. and Hutchins, D. and Szegedy, C.},
  date = {2022},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NLSAERSW/memorizing_transformers.pdf}
}

@inproceedings{wu2022NoisyTuneLittleNoise,
  title = {{{NoisyTune}}: {{A Little Noise Can Help You Finetune Pretrained Language Models Better}}},
  shorttitle = {{{NoisyTune}}},
  booktitle = {{{ACL}}},
  author = {Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng and Xie, Xing},
  date = {2022-02-24},
  eprint = {2202.12024},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.12024},
  urldate = {2022-03-23},
  abstract = {Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks. However, PLMs may have risks in overfitting pretraining signals, and there are some gaps between downstream tasks and the pretraining tasks. It can be difficult for vanilla finetuning methods to overcome the barrier between pretraining and downstream tasks, which leads to suboptimal performance. In this paper, we propose a very simple yet effective method named NoisyTune which can help better finetune PLMs in downstream tasks by adding some noise to the parameters of PLMs before finetuning. More specifically, we propose a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices, which can consider the varied characteristics of different types of parameters in PLMs. Extensive experiments on the GLUE English benchmark and the XTREME multilingual benchmark show that NoisyTune can consistently improve the performance of different PLMs in many downstream tasks.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L3T5KSE7/Wu et al. - 2022 - NoisyTune A Little Noise Can Help You Finetune Pr.pdf;/home/hiaoxui/.local/share/zotero_files/storage/9SAH99Q5/2202.html}
}

@inproceedings{xia2019SyntaxAwareNeuralSemantic,
  title = {Syntax-{{Aware Neural Semantic Role Labeling}}},
  booktitle = {{{AAAI}}},
  author = {Xia, Qingrong and Li, Zhenghua and Zhang, Min and Zhang, Meishan and Fu, Guohong and Wang, Rui and Si, Luo},
  date = {2019},
  pages = {9},
  abstract = {Semantic role labeling (SRL), also known as shallow semantic parsing, is an important yet challenging task in NLP. Motivated by the close correlation between syntactic and semantic structures, traditional discrete-feature-based SRL approaches make heavy use of syntactic features. In contrast, deep-neural-network-based approaches usually encode the input sentence as a word sequence without considering the syntactic structures. In this work, we investigate several previous approaches for encoding syntactic trees, and make a thorough study on whether extra syntax-aware representations are beneficial for neural SRL models. Experiments on the benchmark CoNLL-2005 dataset show that syntax-aware SRL approaches can effectively improve performance over a strong baseline with external word representations from ELMo. With the extra syntax-aware representations, our approaches achieve new state-of-the-art 85.6 F1 (single model) and 86.6 F1 (ensemble) on the test data, outperforming the corresponding strong baselines with ELMo by 0.8 and 1.0, respectively. Detailed error analysis are conducted to gain more insights on the investigated approaches.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9YB2PSGP/Xia et al. - Syntax-Aware Neural Semantic Role Labeling.pdf}
}

@misc{xia2020CGBERTConditionalText,
  title = {{{CG-BERT}}: {{Conditional Text Generation}} with {{BERT}} for {{Generalized Few-shot Intent Detection}}},
  author = {Xia, C. and Zhang, C. and Nguyen, H. and Zhang, J. and Yu, P.},
  date = {2020},
  eprint = {2004.01881},
  eprinttype = {arxiv},
  abstract = {In this paper, we formulate a more realistic and difficult problem setup for the intent detection task in natural language understanding, namely Generalized Few-Shot Intent Detection (GFSID). GFSID aims to discriminate a joint label space consisting of both existing intents which have enough labeled data and novel intents which only have a few examples for each class. To approach this problem, we propose a novel model, Conditional Text Generation with BERT (CG-BERT). CG-BERT effectively leverages a large pre-trained language model to generate text conditioned on the intent label. By modeling the utterance distribution with variational inference, CG-BERT can generate diverse utterances for the novel intents even with only a few utterances available. Experimental results show that CG-BERT achieves state-of-the-art performance on the GFSID task with 1-shot and 5-shot settings on two real-world datasets.},
  archiveprefix = {arXiv},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9BAT4Z38/Xia et al. - 2020 - CG-BERT Conditional Text Generation with BERT for Generalized Few-shot Intent Detection(2).pdf}
}

@inproceedings{xia2020IncrementalNeuralCoreference,
  title = {Incremental {{Neural Coreference Resolution}} in {{Constant Memory}}},
  booktitle = {{{EMNLP}}},
  author = {Xia, Patrick and Sedoc, João and Van Durme, Benjamin},
  date = {2020},
  pages = {8617--8624},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.695},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.695},
  urldate = {2021-02-09},
  abstract = {We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our endto-end algorithm proposes and scores each mention span against explicit entity representations created from the earlier document context (if any). These spans are then used to update the entity’s representations before being forgotten; we only retain a fixed set of salient entities throughout the document. In this work, we successfully convert a highperforming model (Joshi et al., 2020), asymptotically reducing its memory usage to constant space with only a 0.3\% relative loss in F1 on OntoNotes 5.0.},
  eventtitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2JHB7IW6/Xia et al. - 2020 - Incremental Neural Coreference Resolution in Const.pdf}
}

@misc{xia2020RecentIdeasContextualized,
  title = {Recent {{Ideas}} in {{Contextualized Encoders}} and {{Representations}}},
  author = {Xia, P. and Van Durme, B.},
  date = {2020},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZMVPKNGC/Xia, Van Durme - 2020 - Recent Ideas in Contextualized Encoders and Representations(2).pdf}
}

@inproceedings{xia2021LOMELargeOntology,
  title = {{{LOME}}: {{Large Ontology Multilingual Extraction}}},
  booktitle = {{{EACL}}},
  author = {Xia, Patrick and Qin, G. and Vashishtha, Siddharth and Chen, Yunmo and Chen, Tongfei and May, Chandler and Harman, Craig and Rawlins, Kyle and White, Aaron Steven and Van Durme, Benjamin},
  date = {2021},
  pages = {9},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DPVPH6ZT/Xia et al. - LOME Large Ontology Multilingual Extraction.pdf}
}

@inproceedings{xia2021MetaXLMetaRepresentation,
  title = {{{MetaXL}}: {{Meta Representation Transformation}} for {{Low-resource Cross-lingual Learning}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Xia, Mengzhou and Zheng, Guoqing and Mukherjee, Subhabrata and Shokouhi, Milad and Neubig, Graham and Awadallah, Ahmed Hassan},
  date = {2021},
  pages = {13},
  abstract = {The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for lowresource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an under-studied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages (Singh et al., 2019), bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages – without access to large-scale monolingual corpora or large amounts of labeled data – for tasks like crosslingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U792MA4Y/Xia et al. - MetaXL Meta Representation Transformation for Low.pdf}
}

@inproceedings{xiao2017LearningConditionalGenerative,
  title = {Learning {{Conditional Generative Models}} for {{Temporal Point Processes}}},
  booktitle = {{{AAAI}}},
  author = {Xiao, S. and Xu, H. and Yan, J. and Farajtabar, M. and Yang, X. and Song, L. and Zha, H.},
  date = {2017},
  pages = {6302--6309},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/69ZUT8LW/Xiao et al. - 2017 - Learning Conditional Generative Models for Temporal Point Processes(2).pdf}
}

@inproceedings{xiao2017WassersteinLearningDeep,
  title = {Wasserstein {{Learning}} of {{Deep Generative Point Process Models}}},
  booktitle = {{{NeurIPS}}},
  author = {Xiao, S. and Farajtabar, M. and Ye, X. and Yan, J. and Song, L. and Zha, H.},
  date = {2017},
  eprint = {1705.08051},
  eprinttype = {arxiv},
  issn = {10495258},
  url = {http://arxiv.org/abs/1705.08051},
  abstract = {Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.},
  archiveprefix = {arXiv},
  annotation = {76 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7DJVCKW4/Xiao et al. - 2017 - Wasserstein Learning of Deep Generative Point Process Models(2).pdf}
}

@inproceedings{xie2013SemanticFramesPredict,
  title = {Semantic {{Frames}} to {{Predict Stock Price Movement}}},
  booktitle = {{{ACL}}},
  author = {Xie, B. and Passonneau, R. J. and Wu, L.},
  date = {2013},
  pages = {873--883},
  abstract = {Semantic frames are a rich linguistic re-source. There has been much work on semantic frame parsers, but less that applies them to general NLP problems. We address a task to predict change in stock price from financial news. Seman-tic frames help to generalize from spe-cific sentences to scenarios, and to de-tect the (positive or negative) roles of spe-cific companies. We introduce a novel tree representation, and use it to train predic-tive models with tree kernels using sup-port vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that fea-tures derived from semantic frame pars-ing have significantly better performance across years on the polarity task.},
  isbn = {978-1-937284-50-3},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U8U4FVY7/Xie, Passonneau, Wu - 2013 - Semantic Frames to Predict Stock Price Movement(2).pdf}
}

@inproceedings{xie2017DiverseNeuralNetwork,
  title = {Diverse Neural Network Learns True Target Functions},
  booktitle = {{{AISTATS}}},
  author = {Xie, B. and Liang, Y. and Song, L.},
  date = {2017},
  eprint = {1611.03131},
  eprinttype = {arxiv},
  abstract = {Neural networks are a powerful class of functions that can be trained with simple gradient descent to achieve state-of-the-art performance on a variety of applications. Despite their practical success, there is a paucity of results that provide theoretical guarantees on why they are so effective. Lying in the center of the problem is the difficulty of analyzing the non-convex loss function with potentially numerous local minima and saddle points. Can neural networks corresponding to the stationary points of the loss function learn the true target function? If yes, what are the key factors contributing to such nice optimization properties? In this paper, we answer these questions by analyzing one-hidden-layer neural networks with ReLU activation, and show that despite the non-convexity, neural networks with diverse units have no spurious local minima. We bypass the non-convexity issue by directly analyzing the first order optimality condition, and show that the loss can be made arbitrarily small if the minimum singular value of the “extended feature matrix” is large enough. We make novel use of techniques from kernel methods and geometric discrepancy, and identify a new relation linking the smallest singular value to the spectrum of a kernel function associated with the activation function and to the diversity of the units. Our results also suggest a novel regularization function to promote unit diversity for potentially better generalization.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KD5F4WF2/Xie, Liang, Song - 2017 - Diverse neural network learns true target functions(2).pdf}
}

@inproceedings{xie2020DifferentiableTopkOperator,
  title = {Differentiable {{Top-k Operator}} with {{Optimal Transport}}},
  booktitle = {{{NeurIPS}}},
  author = {Xie, Yujia and Dai, Hanjun and Chen, Minshuo and Dai, Bo and Zhao, Tuo and Zha, Hongyuan and Wei, Wei and Pfister, Tomas},
  date = {2020-02-18},
  eprint = {2002.06504},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.06504},
  urldate = {2021-02-24},
  abstract = {The top-k operation, i.e., finding the k largest or smallest elements from a collection of scores, is an important model component, which is widely used in information retrieval, machine learning, and data mining. However, if the top-k operation is implemented in an algorithmic way, e.g., using bubble algorithm, the resulting model cannot be trained in an end-to-end way using prevalent gradient descent algorithms. This is because these implementations typically involve swapping indices, whose gradient cannot be computed. Moreover, the corresponding mapping from the input scores to the indicator vector of whether this element belongs to the top-k set is essentially discontinuous. To address the issue, we propose a smoothed approximation, namely the SOFT (Scalable Optimal transport-based diFferenTiable) top-k operator. Specifically, our SOFT top-k operator approximates the output of the top-k operation as the solution of an Entropic Optimal Transport (EOT) problem. The gradient of the SOFT operator can then be efficiently approximated based on the optimality conditions of EOT problem. We apply the proposed operator to the k-nearest neighbors and beam search algorithms, and demonstrate improved performance.},
  archiveprefix = {arXiv},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C4EJYD9R/Xie et al. - 2020 - Differentiable Top-k Operator with Optimal Transpo.pdf;/home/hiaoxui/.local/share/zotero_files/storage/9QP3I95X/2002.html}
}

@inproceedings{xie2022ExplanationIncontextLearning,
  title = {An {{Explanation}} of {{In-context Learning}} as {{Implicit Bayesian Inference}}},
  booktitle = {{{ICLR}}},
  author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  date = {2022-05-04},
  eprint = {2111.02080},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.02080},
  urldate = {2022-05-15},
  abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XDQ85PZY/Xie et al. - 2022 - An Explanation of In-context Learning as Implicit .pdf;/home/hiaoxui/.local/share/zotero_files/storage/CTPEVJQX/2111.html}
}

@inproceedings{xiong2017DynamicCoattentionNetworks,
  title = {Dynamic {{Coattention Networks For Question Answering}}},
  booktitle = {{{ICLR}}},
  author = {Xiong, C. and Zhong, V. and Socher, R.},
  date = {2017},
  eprint = {1611.01604},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.01604},
  abstract = {Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0\% F1 to 75.9\%, while a DCN ensemble obtains 80.4\% F1.},
  archiveprefix = {arXiv},
  annotation = {487 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IF3TGKAC/Xiong, Zhong, Socher - 2017 - Dynamic Coattention Networks For Question Answering(2).pdf}
}

@misc{xiong2020ApproximateNearestNeighbor,
  title = {Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval},
  author = {Xiong, Lee and Xiong, Chenyan and Li, Ye and Tang, Kwok-Fung and Liu, Jialin and Bennett, Paul and Ahmed, Junaid and Overwijk, Arnold},
  date = {2020},
  abstract = {Conducting text retrieval in a dense representation space has many intriguing advantages. Yet the end-to-end learned dense retrieval (DR) often underperforms word-based sparse retrieval. In this paper, we first theoretically show the learning bottleneck of dense retrieval is due to the domination of uninformative negatives sampled locally in batch, which yield diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. We then propose Approximate nearest neighbor Negative Contrastive Learning (ANCE), a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index. Our experiments demonstrate the effectiveness of ANCE on web search, question answering, and in a commercial search environment, showing ANCE dot-product retrieval nearly matches the accuracy of BERT-based cascade IR pipeline, while being 100x more efficient.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5PNKUZH9/Xiong et al. - APPROXIMATE NEAREST NEIGHBOR NEGATIVE CON- TRASTIV.pdf}
}

@inproceedings{xiong2020LayerNormalizationTransformer,
  title = {On {{Layer Normalization}} in the {{Transformer Architecture}}},
  booktitle = {{{ICML}}},
  author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tie-Yan},
  date = {2020},
  pages = {17},
  abstract = {The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyperparameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P5JL6GXW/Xiong et al. - On Layer Normalization in the Transformer Architec.pdf}
}

@inproceedings{xu2018SphericalLatentSpaces,
  title = {Spherical {{Latent Spaces}} for {{Stable Variational Autoencoders}}},
  booktitle = {{{EMNLP}}},
  author = {Xu, J. and Durrett, G.},
  date = {2018},
  eprint = {1808.10805},
  eprinttype = {arxiv},
  pages = {4503--4513},
  doi = {10.18653/v1/d18-1480},
  abstract = {A hallmark of variational autoencoders (VAEs) for text processing is their combination of powerful encoder-decoder models, such as LSTMs, with simple latent distributions, typically multivariate Gaussians. These models pose a difficult optimization problem: there is an especially bad local optimum where the variational posterior always equals the prior and the model does not use the latent variable at all, a kind of "collapse" which is encouraged by the KL divergence term of the objective. In this work, we experiment with another choice of latent distribution, namely the von Mises-Fisher (vMF) distribution, which places mass on the surface of the unit hypersphere. With this choice of prior and posterior, the KL divergence term now only depends on the variance of the vMF distribution, giving us the ability to treat it as a fixed hyperparameter. We show that doing so not only averts the KL collapse, but consistently gives better likelihoods than Gaussians across a range of modeling conditions, including recurrent language modeling and bag-of-words document modeling. An analysis of the properties of our vMF representations shows that they learn richer and more nuanced structures in their latent representations than their Gaussian counterparts.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {95 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MMQMJXQC/Xu, Durrett - 2019 - Spherical Latent Spaces for Stable Variational Autoencoders(2).pdf}
}

@inproceedings{xu2018StockMovementPrediction,
  title = {Stock {{Movement Prediction}} from {{Tweets}} and {{Historical Prices}}},
  booktitle = {{{ACL}}},
  author = {Xu, Y. and Cohen, S. B.},
  date = {2018},
  abstract = {Stock movement prediction is a challeng-ing problem: the market is highly stochas-tic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly ex-ploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with tempo-ral auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement predic-tion dataset which we collected. 1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/II9K8V24/Xu, Cohen - 2018 - Stock Movement Prediction from Tweets and Historical Prices(2).pdf}
}

@inproceedings{xu2019HowPowerfulAre,
  title = {How Powerful Are Graph Neural Networks?},
  booktitle = {{{ICLR}}},
  author = {Xu, K. and Jegelka, S. and Hu, W. and Leskovec, J.},
  date = {2019},
  eprint = {1810.00826},
  eprinttype = {arxiv},
  pages = {1--17},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H55SVB4V/Xu et al. - 2019 - How powerful are graph neural networks(2).pdf}
}

@inproceedings{xu2020DiscourseAwareNeuralExtractive,
  title = {Discourse-{{Aware Neural Extractive Text Summarization}}},
  booktitle = {{{ACL}}},
  author = {Xu, Jiacheng and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  date = {2020-04-24},
  eprint = {1910.14142},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.14142},
  urldate = {2022-03-07},
  abstract = {Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MXVYT98X/Xu et al. - 2020 - Discourse-Aware Neural Extractive Text Summarizati.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6J9J27AQ/1910.html}
}

@misc{xu2021FewCLUEChineseFewshot,
  title = {{{FewCLUE}}: {{A Chinese Few-shot Learning Evaluation Benchmark}}},
  shorttitle = {{{FewCLUE}}},
  author = {Xu, Liang and Lu, Xiaojing and Yuan, Chenyang and Zhang, Xuanwei and Yuan, Hu and Xu, Huilin and Wei, Guoao and Pan, Xiang and Hu, Hai},
  date = {2021-07-15},
  eprint = {2107.07498},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.07498},
  urldate = {2021-07-23},
  abstract = {Pretrained Language Models (PLMs) have achieved tremendous success in natural language understanding tasks. While different learning schemes -- fine-tuning, zero-shot and few-shot learning -- have been widely explored and compared for languages such as English, there is comparatively little work in Chinese to fairly and comprehensively evaluate and compare these methods. This work first introduces Chinese Few-shot Learning Evaluation Benchmark (FewCLUE), the first comprehensive small sample evaluation benchmark in Chinese. It includes nine tasks, ranging from single-sentence and sentence-pair classification tasks to machine reading comprehension tasks. Given the high variance of the few-shot learning performance, we provide multiple training/validation sets to facilitate a more accurate and stable evaluation of few-shot modeling. An unlabeled training set with up to 20,000 additional samples per task is provided, allowing researchers to explore better ways of using unlabeled samples. Next, we implement a set of state-of-the-art (SOTA) few-shot learning methods (including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their performance with fine-tuning and zero-shot learning schemes on the newly constructed FewCLUE benchmark.Our results show that: 1) all five few-shot learning methods exhibit better performance than fine-tuning or zero-shot learning; 2) among the five methods, PET is the best performing few-shot method; 3) few-shot learning performance is highly dependent on the specific task. Our benchmark and code are available at https://github.com/CLUEbenchmark/FewCLUE},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W6DVG389/Xu et al. - 2021 - FewCLUE A Chinese Few-shot Learning Evaluation Be.pdf;/home/hiaoxui/.local/share/zotero_files/storage/D8R8ADDX/2107.html}
}

@inproceedings{xu2021GradualFineTuningLowResource,
  title = {Gradual {{Fine-Tuning}} for {{Low-Resource Domain Adaptation}}},
  booktitle = {{{EACL}}},
  author = {Xu, Haoran and Ebner, Seth and Yarmohammadi, Mahsa and White, Aaron Steven and Van Durme, Benjamin and Murray, Kenton},
  date = {2021-09-01},
  eprint = {2103.02205},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.02205},
  urldate = {2021-09-07},
  abstract = {Fine-tuning is known to improve NLP models by adapting an initial model trained on more plentiful but less domain-salient examples to data in a target domain. Such domain adaptation is typically done using one stage of fine-tuning. We demonstrate that gradually fine-tuning in a multi-stage process can yield substantial further gains and can be applied without modifying the model or learning objective.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HIBDCULG/Xu et al. - 2021 - Gradual Fine-Tuning for Low-Resource Domain Adapta.pdf;/home/hiaoxui/.local/share/zotero_files/storage/LSTR2ZXD/2103.html}
}

@inproceedings{xu2021SoftLayerSelection,
  title = {Soft {{Layer Selection}} with {{Meta-Learning}} for {{Zero-Shot Cross-Lingual Transfer}}},
  booktitle = {{{ACL}}},
  author = {Xu, Weijia and Haider, Batool and Krone, Jason and Mansour, Saab},
  date = {2021-07-20},
  eprint = {2107.09840},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.09840},
  urldate = {2021-07-23},
  abstract = {Multilingual pre-trained contextual embedding models (Devlin et al., 2019) have achieved impressive performance on zero-shot cross-lingual transfer tasks. Finding the most effective fine-tuning strategy to fine-tune these models on high-resource languages so that it transfers well to the zero-shot languages is a non-trivial task. In this paper, we propose a novel meta-optimizer to soft-select which layers of the pre-trained model to freeze during fine-tuning. We train the meta-optimizer by simulating the zero-shot transfer scenario. Results on cross-lingual natural language inference show that our approach improves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al., 2020).},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JXTZZX27/Xu et al. - 2021 - Soft Layer Selection with Meta-Learning for Zero-S.pdf;/home/hiaoxui/.local/share/zotero_files/storage/59FZIBLJ/2107.html}
}

@inproceedings{xu2022HowWeAnswer,
  title = {How {{Do We Answer Complex Questions}}: {{Discourse Structure}} of {{Long-form Answers}}},
  shorttitle = {How {{Do We Answer Complex Questions}}},
  booktitle = {{{ACL}}},
  author = {Xu, Fangyuan and Li, Junyi Jessy and Choi, Eunsol},
  date = {2022-03-21},
  eprint = {2203.11048},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.11048},
  urldate = {2022-03-23},
  abstract = {Long-form answers, consisting of multiple sentences, can provide nuanced and comprehensive answers to a broader set of questions. To better understand this complex and understudied task, we study the functional structure of long-form answers collected from three datasets, ELI5, WebGPT and Natural Questions. Our main goal is to understand how humans organize information to craft complex answers. We develop an ontology of six sentence-level functional roles for long-form answers, and annotate 3.9k sentences in 640 answer paragraphs. Different answer collection methods manifest in different discourse structures. We further analyze model-generated answers -- finding that annotators agree less with each other when annotating model-generated answers compared to annotating human-written answers. Our annotated data enables training a strong classifier that can be used for automatic analysis. We hope our work can inspire future research on discourse-level modeling and evaluation of long-form QA systems.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ERGQ4YFK/Xu et al. - 2022 - How Do We Answer Complex Questions Discourse Stru.pdf;/home/hiaoxui/.local/share/zotero_files/storage/24S9SVG7/2203.html}
}

@inproceedings{xu2022LaPraDoRUnsupervisedPretrained,
  title = {{{LaPraDoR}}: {{Unsupervised Pretrained Dense Retriever}} for {{Zero-Shot Text Retrieval}}},
  shorttitle = {{{LaPraDoR}}},
  booktitle = {{{ACL}}},
  author = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  date = {2022-03-11},
  eprint = {2203.06169},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.06169},
  urldate = {2022-03-23},
  abstract = {In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training. Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism. ICoL not only enlarges the number of negative instances but also keeps representations of cached examples in the same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching. We evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18 datasets of 9 zero-shot text retrieval tasks. Experimental results show that LaPraDoR achieves state-of-the-art performance compared with supervised dense retrieval models, and further analysis reveals the effectiveness of our training strategy and objectives. Compared to re-ranking, our lexicon-enhanced approach can be run in milliseconds (22.5x faster) while achieving superior performance.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3SSECLXM/Xu et al. - 2022 - LaPraDoR Unsupervised Pretrained Dense Retriever .pdf;/home/hiaoxui/.local/share/zotero_files/storage/ZLWK8L55/2203.html}
}

@article{xue2005PennChineseTreeBank,
  title = {The {{Penn Chinese TreeBank}}: {{Phrase}} Structure Annotation of a Large Corpus},
  author = {Xue, N. and Xia, F. and Chiou, F. D. and Palmer, M.},
  date = {2005},
  journaltitle = {Natural Language Engineering},
  volume = {11},
  number = {2},
  pages = {207--238},
  issn = {13513249},
  doi = {10.1017/S135132490400364X},
  abstract = {With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with different segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are difficult. As a first step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The first two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking efforts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality.},
  isbn = {1469-8110},
  annotation = {263 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E7CFTHG4/Xue et al. - 2005 - The Penn Chinese TreeBank Phrase structure annotation of a large corpus(2).pdf}
}

@inproceedings{xue2007TappingImplicitInformation,
  title = {Tapping the Implicit Information for the {{PS}} to {{DS}} Conversion of the {{Chinese Treebank}}},
  booktitle = {Treebanks and {{Linguistic Theories}}},
  author = {Xue, N.},
  date = {2007},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QA8KA6SU/Xue - 2007 - Tapping the implicit information for the PS to DS conversion of the Chinese Treebank(2).pdf}
}

@misc{xue2022CorefDREDocumentlevelRelation,
  title = {{{CorefDRE}}: {{Document-level Relation Extraction}} with Coreference Resolution},
  shorttitle = {{{CorefDRE}}},
  author = {Xue, Zhongxuan and Li, Rongzhen and Dai, Qizhu and Jiang, Zhong},
  date = {2022-02-22},
  eprint = {2202.10744},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.10744},
  urldate = {2022-02-25},
  abstract = {Document-level relation extraction is to extract relation facts from a document consisting of multiple sentences, in which pronoun crossed sentences are a ubiquitous phenomenon against a single sentence. However, most of the previous works focus more on mentions coreference resolution except for pronouns, and rarely pay attention to mention-pronoun coreference and capturing the relations. To represent multi-sentence features by pronouns, we imitate the reading process of humans by leveraging coreference information when dynamically constructing a heterogeneous graph to enhance semantic information. Since the pronoun is notoriously ambiguous in the graph, a mention-pronoun coreference resolution is introduced to calculate the affinity between pronouns and corresponding mentions, and the noise suppression mechanism is proposed to reduce the noise caused by pronouns. Experiments on the public dataset, DocRED, DialogRE and MPDD, show that Coref-aware Doc-level Relation Extraction based on Graph Inference Network outperforms the state-of-the-art.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VGCN4M33/Xue et al. - 2022 - CorefDRE Document-level Relation Extraction with .pdf;/home/hiaoxui/.local/share/zotero_files/storage/A4PGJ57Z/2202.html}
}

@inproceedings{yadav2018SurveyRecentAdvances,
  title = {A {{Survey}} on {{Recent Advances}} in {{Named Entity Recognition}} from {{Deep Learning}} Models},
  booktitle = {{{COLING}}},
  author = {Yadav, Vikas and Bethard, Steven},
  date = {2018},
  eprint = {1910.11470},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3PLZZW67/Yadav and Bethard - 2019 - A Survey on Recent Advances in Named Entity Recogn.pdf}
}

@inproceedings{yang2015HumorRecognitionHumor,
  title = {Humor {{Recognition}} and {{Humor Anchor Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Yang, D. and Lavie, A. and Dyer, C. and Hovy, E.},
  date = {2015},
  pages = {2367--2376},
  abstract = {Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge. In this work, we first identify several semantic structures behind humor and design sets of features for each structure, and next employ a com- putational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations.},
  isbn = {978-1-941643-32-7},
  issue = {September},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R6EB32AM/Yang et al. - 2015 - Humor Recognition and Humor Anchor Extraction(2).pdf}
}

@inproceedings{yang2015NetworkRepresentationLearning,
  title = {Network Representation Learning with Rich Text Information},
  booktitle = {{{IJCAI}}},
  author = {Yang, C. and Liu, Z. and Zhao, D. and Sun, M. and Chang, E. Y.},
  date = {2015},
  pages = {2111--2117},
  issn = {10450823},
  abstract = {Representation learning has shown its effectiveness in many tasks such as image classification and text mining. Network representation learning aims at learning distributed vector representation for each vertex in a network, which is also increasingly recognized as an important aspect for network analysis. Most network representation learning methods investigate network structures for learning. In reality, network vertices contain rich information (such as text), which cannot be well applied with algorithmic frameworks of typical representation learning methods. By proving that DeepWalk, a state-of-the-art network representation method, is actually equivalent to matrix factorization (MF), we propose text-associated DeepWalk (TADW). TADW incorporates text features of vertices into network representation learning under the framework of matrix factorization. We evaluate our method and various baseline methods by applying them to the task of multi-class classification of vertices. The experimental results show that, our method outperforms other baselines on all three datasets, especially when networks are noisy and training ratio is small. The source code of this paper can be obtained from https://github.com/ albertyang33/TADW},
  isbn = {978-1-57735-738-4},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B8ZYRQJJ/Yang et al. - 2015 - Network representation learning with rich text information(2).pdf}
}

@inproceedings{yang2015WikiQAChallengeDataset,
  title = {{{WikiQA}} : {{A Challenge Dataset}} for {{Open-Domain Question Answering}}},
  booktitle = {{{EMNLP}}},
  author = {Yang, Y. and Yih, W. and Meek, C.},
  date = {2015},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C6S57CIM/Yang, Yih, Meek - 2015 - WikiQA A Challenge Dataset for Open-Domain Question Answering(2).pdf}
}

@inproceedings{yang2017JointSequentialRelational,
  title = {A Joint Sequential and Relational Model for Frame-Semantic Parsing},
  booktitle = {{{EMNLP}}},
  author = {Yang, B. and Mitchell, T. M.},
  date = {2017},
  pages = {1247--1256},
  abstract = {We introduce a new method for frame-semantic parsing that significantly improves the prior state of the art. Our model leverages the advantages of a deep bidirectional LSTM network which predicts semantic role labels word by word and a relational network which predicts semantic roles for individual text expressions in relation to a predicate. The two networks are integrated into a single model via knowledge distillation, and a unified graphical model is employed to jointly decode frames and semantic roles during inference. Experiments on the standard FrameNet data show that our model significantly outperforms existing neural and non-neural approaches, achieving a 5.7 F1 gain over the current state of the art, for full frame structure extraction.},
  isbn = {978-1-945626-83-8},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VK88TAHC/Yang, Mitchell - 2017 - A joint sequential and relational model for frame-semantic parsing(2).pdf}
}

@inproceedings{yang2017LieAccessNeuralTuring,
  title = {Lie-{{Access Neural Turing Machines}}},
  booktitle = {{{ICLR}}},
  author = {Yang, G. and Rush, A. M.},
  date = {2017},
  eprint = {1611.02854},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.02854},
  abstract = {External neural memory structures have recently become a popular tool for algorithmic deep learning (Graves et al. 2014, Weston et al. 2014). These models generally utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the storage necessary for computational tasks. In this work, we argue that these neural memory systems lack specific structure important for relative indexing, and propose an alternative model, Lie-access memory, that is explicitly designed for the neural setting. In this paradigm, memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and memory access is performed by linear smoothing in key space. We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintaining differentiability. To experiment with this approach, we implement a simplified Lie-access neural Turing machine (LANTM) with different Lie groups. We find that this approach is able to perform well on a range of algorithmic tasks.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TSMLHHF6/Yang, Rush - 2017 - Lie-Access Neural Turing Machines(2).pdf}
}

@inproceedings{yang2018BreakingSoftmaxBottleneck,
  title = {Breaking the {{Softmax Bottleneck}}: {{A High-Rank RNN Language Model}}},
  booktitle = {{{ICLR}}},
  author = {Yang, Z. and Dai, Z and Salakhutdinov, R. and Cohen, W. W.},
  date = {2018},
  pages = {1--13},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CVUIWM3J/Yang et al. - 2018 - Breaking the Softmax Bottleneck A High-Rank RNN Language Model(2).pdf}
}

@inproceedings{yang2018HotpotQADatasetDiverse,
  title = {{{HotpotQA}}: {{A Dataset}} for {{Diverse}}, {{Explainable Multi-hop Question Answering}}},
  shorttitle = {{{HotpotQA}}},
  booktitle = {{{EMNLP}}},
  author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D.},
  date = {2018},
  pages = {2369--2380},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1259},
  url = {http://aclweb.org/anthology/D18-1259},
  urldate = {2022-01-18},
  abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HOTPOTQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. We show that HOTPOTQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
  eventtitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X8TFTA5U/Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi.pdf}
}

@inproceedings{yang2019DeepReinforcedSequencetoSet,
  title = {A {{Deep Reinforced Sequence-to-Set Model}} for {{Multi-Label Classification}}},
  booktitle = {{{ACL}}},
  author = {Yang, P. and Luo, F. and Ma, S. and Lin, J. and Sun, X.},
  date = {2019},
  eprint = {1809.03118},
  eprinttype = {arxiv},
  pages = {5252--5258},
  doi = {10.18653/v1/p19-1518},
  abstract = {Multi-label text classification (MLTC) aims to assign multiple labels to each sample in the dataset. The labels usually have internal correlations. However, traditional methods tend to ignore the correlations between labels. In order to capture the correlations between labels, the sequence-to-sequence (Seq2Seq) model views the MLTC task as a sequence generation problem, which achieves excellent performance on this task. However, the Seq2Seq model is not suitable for the MLTC task in essence. The reason is that it requires humans to predefine the order of the output labels, while some of the output labels in the MLTC task are essentially an unordered set rather than an ordered sequence. This conflicts with the strict requirement of the Seq2Seq model for the label order. In this paper, we propose a novel sequence-to-set framework utilizing deep reinforcement learning, which not only captures the correlations between labels, but also reduces the dependence on the label order. Extensive experimental results show that our proposed method outperforms the competitive baselines by a large margin.},
  archiveprefix = {arXiv},
  annotation = {9 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HMRJVXVU/Yang et al. - 2019 - A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification(2).pdf}
}

@inproceedings{yang2019ReadAttendComment,
  title = {Read, {{Attend}} and {{Comment}}: {{A Deep Architecture}} for {{Automatic News Comment Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Yang, Z. and Xu, C. and Wu, W. and Li, Z.},
  date = {2019},
  eprint = {1909.11974},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.11974},
  abstract = {Automatic news comment generation is beneficial for real applications but has not attracted enough attention from the research community. In this paper, we propose a "read-attend-comment" procedure for news comment generation and formalize the procedure with a reading network and a generation network. The reading network comprehends a news article and distills some important points from it, then the generation network creates a comment by attending to the extracted discrete points and the news title. We optimize the model in an end-to-end manner by maximizing a variational lower bound of the true objective using the back-propagation algorithm. Experimental results on two public datasets indicate that our model can significantly outperform existing methods in terms of both automatic evaluation and human judgment.},
  archiveprefix = {arXiv},
  annotation = {6 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D2RY27EI/Yang et al. - 2019 - Read, Attend and Comment A Deep Architecture for Automatic News Comment Generation(2).pdf}
}

@inproceedings{yang2019ReducingWordOmission,
  title = {Reducing {{Word Omission Errors}} in {{Neural Machine Translation}}: {{A Contrastive Learning Approach}}},
  shorttitle = {Reducing {{Word Omission Errors}} in {{Neural Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Yang, Zonghan and Cheng, Yong and Liu, Yang and Sun, Maosong},
  date = {2019},
  pages = {6191--6196},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1623},
  url = {https://www.aclweb.org/anthology/P19-1623},
  urldate = {2022-04-28},
  eventtitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7DJGX89E/Yang et al. - 2019 - Reducing Word Omission Errors in Neural Machine Tr.pdf}
}

@inproceedings{yang2019XLNetGeneralizedAutoregressive,
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  booktitle = {{{NeurIPS}}},
  author = {Yang, Z. and Dai, Z. and Yang, Y. and Carbonell, J. and Salakhutdinov, R. and Le, Q. V.},
  date = {2019},
  eprint = {1906.08237v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QPQ72KBV/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for Language Understanding(2).pdf}
}

@inproceedings{yang2021CHoRaLCollectingHumor,
  title = {{{CHoRaL}}: {{Collecting Humor Reaction Labels}} from {{Millions}} of {{Social Media Users}}},
  booktitle = {{{EMNLP}}},
  author = {Yang, Zixiaofan and Hooshmand, Shayan and Hirschberg, Julia},
  date = {2021},
  pages = {7},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KFNDAIQD/Yang et al. - CHoRaL Collecting Humor Reaction Labels from Mill.pdf}
}

@inproceedings{yao2013AnswerExtractionSequence,
  title = {Answer Extraction as Sequence Tagging with Tree Edit Distance},
  booktitle = {{{NAACL-HLT}}},
  author = {Yao, X. and Van Durme, B. and Callison-Burch, C. and Clark, P.},
  date = {2013},
  pages = {858--867},
  abstract = {Our goal is to extract answers from pre- retrieved sentences for Question Answering (QA). We construct a linear-chain Conditional Random Field based on pairs of questions and their possible answer sentences, learning the association between questions and answer types. This casts answer extraction as an an- swer sequence tagging problem for the first time, where knowledge of shared structure be- tween question and source sentence is incor- porated through features based on Tree Edit Distance (TED). Our model is free of man- ually created question and answer templates, fast to run (processing 200 QA pairs per sec- ond excluding parsing time), and yields an F1 of 63.3\% on a new public dataset based on prior TREC QA evaluations. The developed system is open-source, and includes an imple- mentation of the TED model that is state of the art in the task of ranking QA pairs.},
  isbn = {978-1-937284-47-3},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9NATBXAY/Yao et al. - Answer Extraction as Sequence Tagging with Tree Ed.pdf;/home/hiaoxui/.local/share/zotero_files/storage/A5IPMA76/Yao et al. - 2013 - Answer extraction as sequence tagging with tree edit distance(2).pdf}
}

@inproceedings{yao2014InformationExtractionStructured,
  title = {Information {{Extraction}} over {{Structured Data}}: {{Question Answering}} with {{Freebase}}},
  booktitle = {{{ACL}}},
  author = {Yao, X. and Van Durme, B.},
  date = {2014},
  pages = {956--966},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JVKJ4942/Yao, Van Durme - 2014 - Information Extraction over Structured Data Question Answering with Freebase(2).pdf}
}

@inproceedings{yao2015CompressiveDocumentSummarization,
  title = {Compressive Document Summarization via Sparse Optimization},
  booktitle = {{{IJCAI}}},
  author = {Yao, J. and Wan, X. and Xiao, J.},
  date = {2015},
  pages = {1376--1382},
  issn = {10450823},
  abstract = {In this paper, we formulate a sparse optimization framework for extractive document summarization. The proposed framework has a decomposable con-vex objective function. We derive an efficient ADMM algorithm to solve it. To encourage di-versity in the summaries, we explicitly introduce an additional sentence dissimilarity term in the op-timization framework. We achieve significant im-provement over previous related work under sim-ilar data reconstruction framework. We then gen-eralize our formulation to the case of compressive summarization and derive a block coordinate de-scent algorithm to optimize the objective function. Performance on DUC 2006 and DUC 2007 datasets shows that our compressive summarization results are competitive against the state-of-the-art results while maintaining reasonable readability.},
  isbn = {978-1-57735-738-4},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AFSEAEAT/Yao, Wan, Xiao - 2015 - Compressive document summarization via sparse optimization(2).pdf}
}

@inproceedings{yao2017GreedyFlippingConstrained,
  title = {Greedy {{Flipping}} for {{Constrained Word Deletion}}},
  booktitle = {{{AAAI}}},
  author = {Yao, J. and Wan, X.},
  date = {2017},
  pages = {3518--3524},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MFRS2M3P/Yao, Wan - 2017 - Greedy Flipping for Constrained Word Deletion(2).pdf}
}

@inproceedings{yao2019DocREDLargeScaleDocumentLevel,
  title = {{{DocRED}}: {{A Large-Scale Document-Level Relation Extraction Dataset}}},
  shorttitle = {{{DocRED}}},
  booktitle = {{{ACL}}},
  author = {Yao, Yuan and Ye, Deming and Li, Peng and Han, Xu and Lin, Yankai and Liu, Zhenghao and Liu, Zhiyuan and Huang, Lixin and Zhou, Jie and Sun, Maosong},
  date = {2019},
  pages = {764--777},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1074},
  url = {https://www.aclweb.org/anthology/P19-1074},
  urldate = {2021-12-30},
  abstract = {Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest humanannotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of documentlevel RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https: //github.com/thunlp/DocRED.},
  eventtitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AC2YMBGJ/Yao et al. - 2019 - DocRED A Large-Scale Document-Level Relation Extr.pdf}
}

@inproceedings{yao2022MetaLearningFewerTasks,
  title = {Meta-{{Learning}} with {{Fewer Tasks}} through {{Task Interpolation}}},
  booktitle = {{{ICLR}}},
  author = {Yao, H. and Zhang, L. and Finn, C.},
  date = {2022},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HDSTLUAB/meta_learning_with_fewer_tasks.pdf}
}

@inproceedings{yao2022NLPScratchLargeScale,
  title = {{{NLP From Scratch Without Large-Scale Pretraining}}: {{A Simple}} and {{Efficient Framework}}},
  booktitle = {{{ICML}}},
  author = {Yao, Xingcheng and Zheng, Yanan and Yang, Xiaocong and Yang, Zhilin},
  date = {2022},
  pages = {14},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4YGYG9X8/Yao et al. - NLP From Scratch Without Large-Scale Pretraining .pdf}
}

@inproceedings{yarmohammadi2021EverythingAllIt,
  title = {Everything {{Is All It Takes}}: {{A Multipronged Strategy}} for {{Zero-Shot Cross-Lingual Information Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Yarmohammadi, M. and Wu, S. and Marone, M. and Xu, H. and Ebner, S. and Qin, G. and Chen, Y. and Guo, J. and Harman, C. and Murray, K. and White, A. S. and Dredze, M. and Van Durme, B.},
  date = {2021},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6QFZ9JSB/BETTER_IE.pdf}
}

@inproceedings{yarowsky1995UnsupervisedWordSense,
  title = {Unsupervised Word Sense Disambiguation Rivaling Supervised Methods},
  booktitle = {{{ACL}}},
  author = {Yarowsky, D.},
  date = {1995},
  eprint = {15003161},
  eprinttype = {pmid},
  pages = {189--196},
  issn = {0736587X},
  doi = {10.3115/981658.981684},
  url = {http://portal.acm.org/citation.cfm?doid=981658.981684},
  abstract = {This paper presents an unsupervised learn- ing algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints - that words tend to have one sense per discourse and one sense per collocation - exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96\%.},
  archiveprefix = {arXiv},
  isbn = {0736-587X},
  annotation = {2304 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M4BHV663/Yarowsky - 1995 - Unsupervised word sense disambiguation rivaling supervised methods(2).pdf}
}

@inproceedings{yasunaga2022LinkBERTPretrainingLanguage,
  title = {{{LinkBERT}}: {{Pretraining Language Models}} with {{Document Links}}},
  shorttitle = {{{LinkBERT}}},
  booktitle = {{{ACL}}},
  author = {Yasunaga, Michihiro and Leskovec, Jure and Liang, Percy},
  date = {2022-03-29},
  eprint = {2203.15827},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.15827},
  urldate = {2022-04-08},
  abstract = {Language model (LM) pretraining can learn various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5\% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7\% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data at https://github.com/michiyasunaga/LinkBERT.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FTXR2BGJ/Yasunaga et al. - 2022 - LinkBERT Pretraining Language Models with Documen.pdf;/home/hiaoxui/.local/share/zotero_files/storage/GJLUWA6T/2203.html}
}

@misc{ye2019BPTransformerModellingLongRange,
  title = {{{BP-Transformer}}: {{Modelling Long-Range Context}} via {{Binary Partitioning}}},
  shorttitle = {{{BP-Transformer}}},
  author = {Ye, Zihao and Guo, Qipeng and Gan, Quan and Qiu, Xipeng and Zhang, Zheng},
  date = {2019-11-10},
  eprint = {1911.04070},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.04070},
  urldate = {2021-03-28},
  abstract = {The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of self-attention limit its application on long text. In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields \$O(k\textbackslash cdot n\textbackslash log (n/k))\$ connections where \$k\$ is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch.},
  archiveprefix = {arXiv},
  annotation = {15 citations (Semantic Scholar/arXiv) [2021-03-28]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/588JPSV6/Ye et al. - 2019 - BP-Transformer Modelling Long-Range Context via B.pdf;/home/hiaoxui/.local/share/zotero_files/storage/YVTDFQGC/1911.html}
}

@misc{ye2021CrossFitFewshotLearning,
  title = {{{CrossFit}}: {{A Few-shot Learning Challenge}} for {{Cross-task Generalization}} in {{NLP}}},
  shorttitle = {{{CrossFit}}},
  author = {Ye, Qinyuan and Lin, Bill Yuchen and Ren, Xiang},
  date = {2021-04-18},
  eprint = {2104.08835},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.08835},
  urldate = {2021-06-22},
  abstract = {Humans can learn a new language task more efficiently than machines, conceivably by leveraging their prior experience and knowledge in learning other tasks. In this paper, we explore whether such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a task setup for studying cross-task few-shot learning ability, which standardizes seen/unseen task splits, data access during different learning stages, and the evaluation protocols. In addition, we present NLP Few-shot Gym, a repository of 160 few-shot NLP tasks, covering diverse task categories and applications, and converted to a unified text-to-text format. Our empirical analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. Additionally, the advantage lasts into medium-resource scenarios when thousands of training examples are available. We also observe that selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-06-22]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5WU6VUF9/Ye et al. - 2021 - CrossFit A Few-shot Learning Challenge for Cross-.pdf;/home/hiaoxui/.local/share/zotero_files/storage/5YFGCD9S/2104.html}
}

@inproceedings{ye2021LearningGenerateTaskSpecific,
  title = {Learning to {{Generate Task-Specific Adapters}} from {{Task Description}}},
  booktitle = {{{ACL}}},
  author = {Ye, Qinyuan and Ren, Xiang},
  date = {2021-06-15},
  eprint = {2101.00420},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.00420},
  urldate = {2021-06-16},
  abstract = {Pre-trained text-to-text transformers such as BART have achieved impressive performance across a range of NLP tasks. Recent study further shows that they can learn to generalize to novel tasks, by including task descriptions as part of the source sequence and training the model with (source, target) examples. At test time, these fine-tuned models can make inferences on new tasks using the new task descriptions as part of the input. However, this approach has potential limitations, as the model learns to solve individual (source, target) examples (i.e., at the instance level), instead of learning to solve tasks by taking all examples within a task as a whole (i.e., at the task level). To this end, we introduce Hypter, a framework that improves text-to-text transformer's generalization ability to unseen tasks by training a hypernetwork to generate task-specific, light-weight adapters from task descriptions. Experiments on ZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves upon fine-tuning baselines. Notably, when using BART-Large as the main network, Hypter brings 11.3\% comparative improvement on ZEST dataset.},
  archiveprefix = {arXiv},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-06-15]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W955KT6T/Ye and Ren - 2021 - Learning to Generate Task-Specific Adapters from T.pdf;/home/hiaoxui/.local/share/zotero_files/storage/SSWVU8J9/2101.html}
}

@inproceedings{ye2021One2SetGeneratingDiverse,
  title = {{{One2Set}}: {{Generating Diverse Keyphrases}} as a {{Set}}},
  shorttitle = {{{One2Set}}},
  booktitle = {{{ACL}}},
  author = {Ye, Jiacheng and Gui, Tao and Luo, Yichao and Xu, Yige and Zhang, Qi},
  date = {2021-05-24},
  eprint = {2105.11134},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.11134},
  urldate = {2022-07-21},
  abstract = {Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predefined order as a target sequence during training. However, the keyphrases are inherently an unordered set rather than an ordered sequence. Imposing a predefined order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases. In this work, we propose a new training paradigm One2Set without predefining an order to concatenate the keyphrases. To fit this paradigm, we propose a novel model that utilizes a fixed set of learned control codes as conditions to generate a set of keyphrases in parallel. To solve the problem that there is no correspondence between each prediction and target during training, we propose a \$K\$-step target assignment mechanism via bipartite matching, which greatly increases the diversity and reduces the duplication ratio of generated keyphrases. The experimental results on multiple benchmarks demonstrate that our approach significantly outperforms the state-of-the-art methods.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BI4FYCBD/Ye et al. - 2021 - One2Set Generating Diverse Keyphrases as a Set.pdf;/home/hiaoxui/.local/share/zotero_files/storage/MA2K6T9U/2105.html}
}

@inproceedings{yeh2019UnsupervisedSpeechRecognition,
  title = {Unsupervised {{Speech Recognition}} via {{Segmentation Empirical Output Distribution Matching}}},
  booktitle = {{{ICLR}}},
  author = {Yeh, C. and Chen, J. and Yu, C. and Yu, D.},
  date = {2019},
  pages = {1--14},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JHE3JFIX/Yeh et al. - 2019 - Unsupervised Speech Recognition via Segmentation Empirical Output Distribution Matching(2).pdf}
}

@inproceedings{yi2017DualGANUnsupervisedDual,
  title = {{{DualGAN}}: {{Unsupervised Dual Learning}} for {{Image-to-Image Translation}}},
  shorttitle = {{{DualGAN}}},
  booktitle = {{{ICCV}}},
  author = {Yi, Zili and Zhang, Hao and Tan, Ping and Gong, Minglun},
  date = {2017},
  eprint = {1704.02510},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1704.02510},
  urldate = {2022-04-23},
  abstract = {Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/37HWQJ5F/Yi et al. - 2018 - DualGAN Unsupervised Dual Learning for Image-to-I.pdf;/home/hiaoxui/.local/share/zotero_files/storage/47RNEQ7L/1704.html}
}

@inproceedings{yi2018NeuralSymbolicVQADisentangling,
  title = {Neural-{{Symbolic VQA}}: {{Disentangling Reasoning}} from {{Vision}} and {{Language Understanding}}},
  booktitle = {{{NeurIPS}}},
  author = {Yi, K. and Wu, J. and Gan, C. and Torralba, A. and Kohli, P. and Tenenbaum, J. B.},
  date = {2018},
  eprint = {19723035},
  eprinttype = {pmid},
  issn = {1749-6632},
  doi = {10.1111/j.1749-6632.2009.04729.x},
  url = {http://arxiv.org/abs/1810.02338},
  abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8\% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
  archiveprefix = {arXiv},
  isbn = {978-1-57331-737-5},
  annotation = {7 citations (Semantic Scholar/DOI) [2021-03-26] 157 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DUND783R/Yi et al. - 2018 - Neural-Symbolic VQA Disentangling Reasoning from Vision and Language Understanding(2).pdf}
}

@inproceedings{yih2015SemanticParsingStaged,
  title = {Semantic {{Parsing}} via {{Staged Query Graph Generation}}: {{Question Answering}} with {{Knowledge Base}}},
  booktitle = {{{ACL}}},
  author = {Yih, W. and Chang, M. and He, X. and Gao, J.},
  date = {2015},
  pages = {1321--1331},
  doi = {10.3115/v1/P15-1128},
  url = {http://aclweb.org/anthology/P15-1128},
  abstract = {We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowl-edge base and can be directly mapped to a logical form. Semantic parsing is re-duced to query graph generation, formu-lated as a staged search problem. Unlike traditional approaches, our method lever-ages the knowledge base in an early stage to prune the search space and thus simpli-fies the semantic matching problem. By applying an advanced entity linking sys-tem and a deep convolutional neural net-work model that matches questions and predicate sequences, our system outper-forms previous methods substantially, and achieves an F 1 measure of 52.5\% on the WEBQUESTIONS dataset.},
  isbn = {978-1-941643-72-3},
  keywords = {unread},
  annotation = {438 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5E79VCFU/Yih et al. - 2015 - Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base(2).pdf}
}

@inproceedings{yin2017SyntacticNeuralModel,
  title = {A {{Syntactic Neural Model}} for {{General-Purpose Code Generation}}},
  booktitle = {{{ACL}}},
  author = {Yin, P. and Neubig, G.},
  date = {2017},
  eprint = {1704.01696},
  eprinttype = {arxiv},
  pages = {440--450},
  doi = {10.18653/v1/P17-1041},
  url = {http://arxiv.org/abs/1704.01696},
  abstract = {We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-75-3},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YXS7PPRK/Yin, Neubig - 2017 - A Syntactic Neural Model for General-Purpose Code Generation(2).pdf}
}

@inproceedings{yin2021DocNLILargescaleDataset,
  title = {{{DocNLI}}: {{A Large-scale Dataset}} for {{Document-level Natural Language Inference}}},
  shorttitle = {{{DocNLI}}},
  booktitle = {{{ACL}}},
  author = {Yin, Wenpeng and Radev, Dragomir and Xiong, Caiming},
  date = {2021-06-17},
  eprint = {2106.09449},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.09449},
  urldate = {2021-06-22},
  abstract = {Natural language inference (NLI) is formulated as a unified framework for solving various NLP problems such as relation extraction, question answering, summarization, etc. It has been studied intensively in the past few years thanks to the availability of large-scale labeled datasets. However, most existing studies focus on merely sentence-level inference, which limits the scope of NLI's application in downstream NLP problems. This work presents DocNLI -- a newly-constructed large-scale dataset for document-level NLI. DocNLI is transformed from a broad range of NLP problems and covers multiple genres of text. The premises always stay in the document granularity, whereas the hypotheses vary in length from single sentences to passages with hundreds of words. Additionally, DocNLI has pretty limited artifacts which unfortunately widely exist in some popular sentence-level NLI datasets. Our experiments demonstrate that, even without fine-tuning, a model pretrained on DocNLI shows promising performance on popular sentence-level benchmarks, and generalizes well to out-of-domain NLP tasks that rely on inference at document granularity. Task-specific fine-tuning can bring further improvements. Data, code, and pretrained models can be found at https://github.com/salesforce/DocNLI.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-06-22]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IKE4LCMS/Yin et al. - 2021 - DocNLI A Large-scale Dataset for Document-level N.pdf;/home/hiaoxui/.local/share/zotero_files/storage/MM7V28AR/2106.html}
}

@inproceedings{yogatama2017LearningComposeWords,
  title = {Learning to {{Compose Words}} into {{Sentences}} with {{Reinforcement Learning}}},
  booktitle = {{{ICLR}}},
  author = {Yogatama, D. and Blunsom, P. and Dyer, C. and Grefenstette, E. and Ling, W.},
  date = {2017},
  eprint = {1611.09100v1},
  eprinttype = {arxiv},
  pages = {1--10},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JP85QGMR/Yogatama et al. - 2017 - Learning to Compose Words into Sentences with Reinforcement Learning(2).pdf}
}

@misc{yoran2021TurningTablesGenerating,
  title = {Turning {{Tables}}: {{Generating Examples}} from {{Semi-structured Tables}} for {{Endowing Language Models}} with {{Reasoning Skills}}},
  shorttitle = {Turning {{Tables}}},
  author = {Yoran, Ori and Talmor, Alon and Berant, Jonathan},
  date = {2021-07-15},
  eprint = {2107.07261},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.07261},
  urldate = {2021-07-23},
  abstract = {Models pre-trained with a language modeling objective possess ample world knowledge and language skills, but are known to struggle in tasks that require reasoning. In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we propose sampling strategies that focus training on reasoning skills the model is currently lacking. We evaluate our approach on three reading comprehension datasets that are focused on reasoning, and show that our model, PReasM, substantially outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling examples based on current model errors leads to faster training and higher overall performance.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-23]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3S7WHSP5/Yoran et al. - 2021 - Turning Tables Generating Examples from Semi-stru.pdf;/home/hiaoxui/.local/share/zotero_files/storage/T4T4ZGZQ/2107.html}
}

@misc{young2021NeurosymbolicDeepGenerative,
  title = {Neurosymbolic {{Deep Generative Models}} for {{Sequence Data}} with {{Relational Constraints}}},
  author = {Young, Halley and Du, Maxwell and Bastani, Osbert},
  date = {2021},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JHGWNT6V/neurosymbolic_deep_generative_.pdf}
}

@inproceedings{yu2004IntegrationGroundingLanguage,
  title = {On the {{Integration}} of {{Grounding Language}} and {{Lear}} Ning {{Objects}}},
  booktitle = {{{AAAI}}},
  author = {Yu, C. and Ballard, D.},
  date = {2004},
  pages = {488--493},
  doi = {10.1080/10656210509484979},
  isbn = {0-262-51183-5},
  keywords = {unread},
  annotation = {28 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UHCPH4IJ/Yu, Ballard - 2004 - On the Integration of Grounding Language and Lear ning Objects(2).pdf}
}

@inproceedings{yu2013GroundedLanguageLearning,
  title = {Grounded {{Language Learning}} from {{Video Described}} with {{Sentences}}},
  booktitle = {{{ACL}}},
  author = {Yu, H. and Siskind, J. M.},
  date = {2013},
  eprint = {1306.5263},
  eprinttype = {arxiv},
  pages = {53--63},
  url = {http://www.aclweb.org/anthology/P13-1006},
  abstract = {We present a method that learns repre- sentations for word meanings from short video clips paired with sentences. Un- like prior work on learning language from symbolic input, our input consists of video of people interacting with multiple com- plex objects in outdoor environments. Un- like prior computer-vision approaches that learn from videos with verb labels or im- ages with noun labels, our labels are sen- tences containing nouns, verbs, preposi- tions, adjectives, and adverbs. The cor- respondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts si- multaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video.},
  archiveprefix = {arXiv},
  isbn = {978-1-937284-50-3},
  issue = {August},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IJ33Z74H/Yu, Siskind - 2013 - Grounded Language Learning from Video Described with Sentences(2).pdf}
}

@misc{yu2018LongtermForecastingUsing,
  title = {Long-Term {{Forecasting}} Using {{Tensor-Train RNNs}}},
  author = {Yu, R. and Zheng, S. and Anandkumar, A. and Yue, Y.},
  date = {2018},
  eprint = {1711.00073v2},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YQMWVYPE/Yu et al. - 2018 - Long-term Forecasting using Tensor-Train RNNs(2).pdf}
}

@article{yu2020BetterDocumentLevelMachine,
  title = {Better {{Document-Level Machine Translation}} with {{Bayes}}’ {{Rule}}},
  author = {Yu, Lei and Sartran, Laurent and Stokowiec, Wojciech and Ling, Wang and Kong, Lingpeng and Blunsom, Phil and Dyer, Chris},
  date = {2020-12},
  journaltitle = {TACL},
  shortjournal = {TACL},
  volume = {8},
  pages = {346--360},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00319},
  url = {https://direct.mit.edu/tacl/article/96457},
  urldate = {2021-05-27},
  abstract = {We show that Bayes’ rule provides an effective mechanism for creating document translation models that can be learned from only parallel sentences and monolingual documents—a compelling benefit as parallel documents are not always available. In our formulation, the posterior probability of a candidate translation is the product of the unconditional (prior) probability of the candidate output document and the “reverse translation probability” of translating the candidate output back into the source language. Our proposed model uses a powerful autoregressive language model as the prior on target language documents, but it assumes that each sentence is translated independently from the target to the source language. Crucially, at test time, when a source document is observed, the document language model prior induces dependencies between the translations of the source sentences in the posterior. The model’s independence assumption not only enables efficient use of available data, but it additionally admits a practical left-to-right beam-search algorithm for carrying out inference. Experiments show that our model benefits from using cross-sentence context in the language model, and it outperforms existing document translation approaches.},
  langid = {english},
  annotation = {14 citations (Semantic Scholar/DOI) [2021-05-27]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HYRGLDTR/Yu et al. - 2020 - Better Document-Level Machine Translation with Bay.pdf}
}

@inproceedings{yu2021CrosslingualLanguageModel,
  title = {Cross-Lingual {{Language Model Pretraining}} for {{Retrieval}}},
  booktitle = {{{WWW}}},
  author = {Yu, Puxuan and Fei, Hongliang and Li, Ping},
  date = {2021-04-19},
  pages = {1029--1039},
  publisher = {{ACM}},
  location = {{Ljubljana Slovenia}},
  doi = {10.1145/3442381.3449830},
  url = {https://dl.acm.org/doi/10.1145/3442381.3449830},
  urldate = {2022-03-15},
  eventtitle = {{{WWW}} '21: {{The Web Conference}} 2021},
  isbn = {978-1-4503-8312-7},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/22M7A9PA/Yu et al. - 2021 - Cross-lingual Language Model Pretraining for Retri.pdf}
}

@inproceedings{yu2021FineTuningPretrainedLanguagea,
  title = {Fine-{{Tuning Pre-trained Language Model}} with {{Weak Supervision}}: {{A Contrastive-Regularized Self-Training Approach}}},
  shorttitle = {Fine-{{Tuning Pre-trained Language Model}} with {{Weak Supervision}}},
  booktitle = {{{NAACL}}},
  author = {Yu, Yue and Zuo, Simiao and Jiang, Haoming and Ren, Wendi and Zhao, Tuo and Zhang, Chao},
  date = {2021},
  eprint = {2010.07835},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.07835},
  urldate = {2021-02-24},
  abstract = {Fine-tuned pre-trained language models (LMs) achieve enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the fine-tuning stage. We study the problem of fine-tuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision. To address this problem, we develop a contrastive self-training framework, COSINE, to enable fine-tuning LMs with weak supervision. Underpinned by contrastive regularization and confidence-based reweighting, this contrastive self-training framework can gradually improve model fitting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classification tasks show that our model outperforms the strongest baseline by large margins on 7 benchmarks in 6 tasks, and achieves competitive performance with fully-supervised fine-tuning methods.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BT58H6RY/Yu et al. - 2021 - Fine-Tuning Pre-trained Language Model with Weak S.pdf;/home/hiaoxui/.local/share/zotero_files/storage/N8UE5L7M/Yu et al. - 2020 - Fine-Tuning Pre-trained Language Model with Weak S.pdf;/home/hiaoxui/.local/share/zotero_files/storage/PRWPZX7H/2010.html}
}

@inproceedings{yuan2018MultilingualAnchoringInteractive,
  title = {Multilingual {{Anchoring}}: {{Interactive Topic Modeling}} and {{Alignment Across Languages}}},
  booktitle = {{{NeurIPS}}},
  author = {Yuan, M. and Van Durme, B. and Boyd-Graber, J.},
  date = {2018},
  pages = {8667--8677},
  abstract = {Multilingual topic models can reveal patterns in cross-lingual document collections. However, existing models lack speed and interactivity, which prevents adoption in everyday corpora exploration or quick moving situations (e.g., natural disasters, political instability). First, we propose a multilingual anchoring algorithm that builds an anchor-based topic model for documents in different languages. Then, we incorporate interactivity to develop MTAnchor (Multilingual Topic Anchors), a system that allows users to refine the topic model. We test our algorithms on labeled English, Chinese, and Sinhalese documents. Within minutes, our methods can produce interpretable topics that are useful for specific classification tasks.},
  issue = {NeurIPS},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9IX6R2FD/Yuan, Van Durme, Boyd-Graber - 2018 - Multilingual Anchoring Interactive Topic Modeling and Alignment Across Languages(2).pdf}
}

@misc{yuan2019InteractiveRefinementCrossLingual,
  title = {Interactive {{Refinement}} of {{Cross-Lingual Word Embeddings}}},
  author = {Yuan, M. and Zhang, M. and Van Durme, B. and Findlater, L. and Boyd-Graber, J.},
  date = {2019},
  eprint = {1911.03070},
  eprinttype = {arxiv},
  abstract = {Cross-lingual word embeddings transfer knowledge between languages: models trained for a high-resource language can be used in a low-resource language. These embeddings are usually trained on general-purpose corpora but used for a domain-specific task. We introduce CLIME, an interactive system that allows a user to quickly adapt cross-lingual word embeddings for a given classification problem. First, words in the vocabulary are ranked by their salience to the downstream task. Then, salient keywords are displayed on an interface. Users mark the similarity between each keyword and its nearest neighbors in the embedding space. Finally, CLIME updates the embeddings using the annotations. We experiment clime on a cross-lingual text classification benchmark for four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings. CLIME also improves test accuracy faster than an active learning baseline, and a simple combination of CLIME with active learning has the highest test accuracy.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IRPZRN5F/Yuan et al. - 2019 - Interactive Refinement of Cross-Lingual Word Embeddings(2).pdf}
}

@inproceedings{yuan2020OneSizeDoes,
  title = {One {{Size Does Not Fit All}}: {{Generating}} and {{Evaluating Variable Number}} of {{Keyphrases}}},
  shorttitle = {One {{Size Does Not Fit All}}},
  booktitle = {{{ACL}}},
  author = {Yuan, Xingdi and Wang, Tong and Meng, Rui and Thaker, Khushboo and Brusilovsky, Peter and He, Daqing and Trischler, Adam},
  date = {2020},
  eprint = {1810.05241},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1810.05241},
  urldate = {2022-07-21},
  abstract = {Different texts shall by nature correspond to different number of keyphrases. This desideratum is largely missing from existing neural keyphrase generation models. In this study, we address this problem from both modeling and evaluation perspectives. We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences. Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states. In contrast to previous approaches, our model is capable of generating diverse keyphrases and controlling number of outputs. We further propose two evaluation metrics tailored towards the variable-number generation. We also introduce a new dataset StackEx that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. With both previous and new evaluation metrics, our model outperforms strong baselines on all datasets.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YWAR3KML/Yuan et al. - 2020 - One Size Does Not Fit All Generating and Evaluati.pdf;/home/hiaoxui/.local/share/zotero_files/storage/PXA8SMT8/1810.html}
}

@inproceedings{yuan2021UnsupervisedRelationExtraction,
  title = {Unsupervised {{Relation Extraction}}: {{A Variational Autoencoder Approach}}},
  booktitle = {{{EMNLP}}},
  author = {Yuan, Chenhan and Eldardiry, Hoda},
  date = {2021},
  pages = {10},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9LY5NJGS/Yuan and Eldardiry - Unsupervised Relation Extraction A Variational Au.pdf}
}

@inproceedings{yue2022CMOREPretrainingAnswer,
  title = {C-{{MORE}}: {{Pretraining}} to {{Answer Open-Domain Questions}} by {{Consulting Millions}} of {{References}}},
  shorttitle = {C-{{MORE}}},
  booktitle = {{{ACL}}},
  author = {Yue, Xiang and Pan, Xiaoman and Yao, Wenlin and Yu, Dian and Yu, Dong and Chen, Jianshu},
  date = {2022-03-22},
  eprint = {2203.08928},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.08928},
  urldate = {2022-04-12},
  abstract = {We consider the problem of pretraining a two-stage open-domain question answering (QA) system (retriever + reader) with strong transfer capabilities. The key challenge is how to construct a large amount of high-quality question-answer-context triplets without task-specific annotations. Specifically, the triplets should align well with downstream tasks by: (i) covering a wide range of domains (for open-domain applications), (ii) linking a question to its semantically relevant context with supporting evidence (for training the retriever), and (iii) identifying the correct answer in the context (for training the reader). Previous pretraining approaches generally fall short of one or more of these requirements. In this work, we automatically construct a large-scale corpus that meets all three criteria by consulting millions of references cited within Wikipedia. The well-aligned pretraining signals benefit both the retriever and the reader significantly. Our pretrained retriever leads to 2\%-10\% absolute gains in top-20 accuracy. And with our pretrained reader, the entire system improves by up to 4\% in exact match.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KRCZ8JV6/Yue et al. - 2022 - C-MORE Pretraining to Answer Open-Domain Question.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KXXE2KKU/2203.html}
}

@inproceedings{zaheer2020BigBirdTransformers,
  title = {Big {{Bird}}: {{Transformers}} for {{Longer Sequences}}},
  shorttitle = {Big {{Bird}}},
  booktitle = {{{NeurIPS}}},
  author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  date = {2020},
  eprint = {2007.14062},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2007.14062},
  urldate = {2022-01-16},
  abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9RD4UYIZ/Zaheer et al. - 2021 - Big Bird Transformers for Longer Sequences.pdf;/home/hiaoxui/.local/share/zotero_files/storage/NV3I3SYR/2007.html}
}

@inproceedings{zarriess2016GeneratingColourTerms,
  title = {Towards {{Generating Colour Terms}} for {{Referents}} in {{Photographs}} : {{Prefer}} the {{Expected}} or the {{Unexpected}} ?},
  booktitle = {{{INLG}}},
  author = {Zarrieß, S. and Schlangen, D.},
  date = {2016},
  pages = {246--255},
  abstract = {Colour terms have been a prime phenomenon for studying language grounding, though pre-vious work focussed mostly on descriptions of simple objects or colour swatches. This paper investigates whether colour terms can be learned from more realistic and potentially noisy visual inputs, using a corpus of referring expressions to objects represented as regions in real-world images. We obtain promising re-sults from combining a classifier that grounds colour terms in visual input with a recalibra-tion model that adjusts probability distribu-tions over colour terms according to contex-tual and object-specific preferences.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KEQ4AWKG/Zarrieß, Schlangen - 2016 - Towards Generating Colour Terms for Referents in Photographs Prefer the Expected or the Unexpected(2).pdf}
}

@inproceedings{zelle1996LearningParseDatabase,
  title = {Learning to {{Parse Database}} Queries Using Inductive Logic Proramming},
  booktitle = {{{AAAI}}},
  author = {Zelle, J. M. and Mooney, R. J.},
  date = {1996},
  pages = {1050--1055},
  abstract = {This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a pre-existing, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AWVMCXVK/Zelle, Mooney - 1996 - Learning to Parse Database queries using inductive logic proramming(2).pdf}
}

@inproceedings{zeng2021YouOnlySample,
  title = {You {{Only Sample}} ({{Almost}}) {{Once}}: {{Linear Cost Self-Attention Via Bernoulli Sampling}}},
  shorttitle = {You {{Only Sample}} ({{Almost}}) {{Once}}},
  booktitle = {{{ICML}}},
  author = {Zeng, Zhanpeng and Xiong, Yunyang and Ravi, Sathya N. and Acharya, Shailesh and Fung, Glenn and Singh, Vikas},
  date = {2021-11-18},
  eprint = {2111.09714},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.09714},
  urldate = {2021-11-23},
  abstract = {Transformer-based models are widely used in natural language processing (NLP). Central to the transformer model is the self-attention mechanism, which captures the interactions of token pairs in the input sequences and depends quadratically on the sequence length. Training such models on longer sequences is expensive. In this paper, we show that a Bernoulli sampling attention mechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic complexity of such models to linear. We bypass the quadratic cost by considering self-attention as a sum of individual tokens associated with Bernoulli random variables that can, in principle, be sampled at once by a single hash (although in practice, this number may be a small constant). This leads to an efficient sampling scheme to estimate self-attention which relies on specific modifications of LSH (to enable deployment on GPU architectures). We evaluate our algorithm on the GLUE benchmark with standard 512 sequence length where we see favorable performance relative to a standard pretrained Transformer. On the Long Range Arena (LRA) benchmark, for evaluating performance on long sequences, our method achieves results consistent with softmax self-attention but with sizable speed-ups and memory savings and often outperforms other efficient self-attention methods. Our code is available at https://github.com/mlpen/YOSO},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8IAAZBUD/Zeng et al. - 2021 - You Only Sample (Almost) Once Linear Cost Self-At.pdf;/home/hiaoxui/.local/share/zotero_files/storage/V8Q8Q39U/2111.html}
}

@inproceedings{zeng2022TaskguidedDisentangledTuning,
  title = {Task-Guided {{Disentangled Tuning}} for {{Pretrained Language Models}}},
  booktitle = {{{ACL}}},
  author = {Zeng, Jiali and Jiang, Yufan and Wu, Shuangzhi and Yin, Yongjing and Li, Mu},
  date = {2022-03-21},
  eprint = {2203.11431},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.11431},
  urldate = {2022-03-23},
  abstract = {Pretrained language models (PLMs) trained on large-scale unlabeled corpus are typically fine-tuned on task-specific downstream datasets, which have produced state-of-the-art results on various NLP tasks. However, the data discrepancy issue in domain and scale makes fine-tuning fail to efficiently capture task-specific patterns, especially in the low data regime. To address this issue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which enhances the generalization of representations by disentangling task-relevant signals from the entangled representations. For a given task, we introduce a learnable confidence model to detect indicative guidance from context, and further propose a disentangled regularization to mitigate the over-reliance problem. Experimental results on GLUE and CLUE benchmarks show that TDT gives consistently better results than fine-tuning with different PLMs, and extensive analysis demonstrates the effectiveness and robustness of our method. Code is available at https://github.com/lemon0830/TDT.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NN8IIPES/Zeng et al. - 2022 - Task-guided Disentangled Tuning for Pretrained Lan.pdf;/home/hiaoxui/.local/share/zotero_files/storage/HVEZ2L4Y/2203.html}
}

@inproceedings{zettlemoyer2007OnlineLearningRelaxed,
  title = {Online Learning of Relaxed {{CCG}} Grammars for Parsing to Logical Form},
  booktitle = {{{EMNLP}}},
  author = {Zettlemoyer, L. S. and Collins, M.},
  date = {2007},
  pages = {678--687},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.71.3926},
  abstract = {We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammarfor example allowing flexible word order, or insertion of lexical items with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86 \% F-measure in recovering fully correct semantic analyses and 95.9\% F-measure by a partial-match criterion, a more than 5 \% improvement over the 90.3\% partial-match figure reported by He and Young (2006).},
  issue = {June},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VV2MPWNM/Zettlemoyer, Collins - 2007 - Online learning of relaxed CCG grammars for parsing to logical form(2).pdf}
}

@inproceedings{zettlemoyer2009LearningContextdependentMappings,
  title = {Learning Context-Dependent Mappings from Sentences to Logical Form},
  booktitle = {{{ACL}}},
  author = {Zettlemoyer, L. S. and Collins, M.},
  date = {2009},
  volume = {2},
  pages = {976},
  doi = {10.3115/1690219.1690283},
  url = {http://portal.acm.org/citation.cfm?doid=1690219.1690283},
  abstract = {We consider the problem of learning context-dependent mappings from sentences to logical form. The training examples are sequences of sentences annotated with lambda-calculus meaning representations. We develop an algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7\% accuracy.},
  isbn = {978-1-932432-46-6},
  issue = {August},
  keywords = {unread},
  annotation = {184 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LLYAQQ9R/Zettlemoyer, Collins - 2009 - Learning context-dependent mappings from sentences to logical form(2).pdf}
}

@inproceedings{zettlemoyer2012LearningMapSentences,
  title = {Learning to {{Map Sentences}} to {{Logical Form}}: {{Structured Classification}} with {{Probabilistic Categorial Grammars}}},
  booktitle = {{{UAI}}},
  author = {Zettlemoyer, L. S. and Collins, M.},
  date = {2012},
  eprint = {1207.1420},
  eprinttype = {arxiv},
  doi = {10.1093/acprof:oso/9780199654680.003.0006},
  url = {http://arxiv.org/abs/1207.1420},
  abstract = {This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.},
  archiveprefix = {arXiv},
  isbn = {0-9749039-1-4},
  keywords = {unread},
  annotation = {772 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q2KLDQ7V/Zettlemoyer, Collins - 2012 - Learning to Map Sentences to Logical Form Structured Classification with Probabilistic Categorial Gramm(2).pdf}
}

@misc{zettlemoyer2012LearningMapSentencesa,
  title = {Learning to {{Map Sentences}} to {{Logical Form}}: {{Structured Classiﬁcation}} with {{Probabilistic Categorial Grammars}}},
  author = {Zettlemoyer, Luke S and Collins, Michael},
  date = {2012},
  abstract = {This paper addresses the problem of mapping natural language sentences to lambda–calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RB96YR89/Zettlemoyer and Collins - Learning to Map Sentences to Logical Form Structu.pdf}
}

@inproceedings{zhang2017ConvexifiedConvolutionalNeural,
  title = {Convexified Convolutional Neural Networks},
  booktitle = {{{ICML}}},
  author = {Zhang, Y. and Liang, P. and Wainwright, M. J.},
  date = {2017},
  eprint = {1609.01000},
  eprinttype = {arxiv},
  abstract = {We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented in terms of a low-rank matrix, and the rank constraint can be relaxed so as to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, we find that CCNNs achieve competitive or better performance than CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-5514-4},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZSN72ZAE/Zhang, Liang, Wainwright - 2017 - Convexified convolutional neural networks(2).pdf}
}

@inproceedings{zhang2017HittingTimeAnalysis,
  title = {A {{Hitting Time Analysis}} of {{Stochastic Gradient Langevin Dynamics}}},
  booktitle = {{{COLT}}},
  author = {Zhang, Y. and Liang, P. and Charikar, M.},
  date = {2017},
  eprint = {1702.05575},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1702.05575},
  abstract = {We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for non-convex optimization. The algorithm performs stochastic gradient descent, where in each step it injects appropriately scaled Gaussian noise to the update. We analyze the algorithm's hitting time to an arbitrary subset of the parameter space. Two results follow from our general theory: First, we prove that for empirical risk minimization, if the empirical risk is point-wise close to the (smooth) population risk, then the algorithm achieves an approximate local minimum of the population risk in polynomial time, escaping suboptimal local minima that only exist in the empirical risk. Second, we show that SGLD improves on one of the best known learnability results for learning linear classifiers under the zero-one loss.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {138 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YNTLSGT2/Zhang, Liang, Charikar - 2017 - A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics(2).pdf}
}

@inproceedings{zhang2017InterpretableConvolutionalNeural,
  title = {Interpretable {{Convolutional Neural Networks}}},
  booktitle = {{{CVPR}}},
  author = {Zhang, Q. and Wu, Y. N. and Zhu, S.},
  date = {2017},
  eprint = {1710.00935},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.00935},
  abstract = {This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of CNNs with different structures. The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision. Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.},
  archiveprefix = {arXiv},
  annotation = {325 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NCYGGDL7/Zhang, Wu, Zhu - 2017 - Interpretable Convolutional Neural Networks(2).pdf}
}

@inproceedings{zhang2017LearnabilityFullyconnectedNeural,
  title = {On the Learnability of Fully-Connected Neural Networks},
  booktitle = {{{AISTATS}}},
  author = {Zhang, Y. and Lee, J. D. and Wainwright, M. J. and Jordan, M. I.},
  date = {2017},
  abstract = {Despite the empirical success of deep neural networks, there is limited theoretical understanding of the learnability of these models with respect to polynomial-time algorithms. In this paper, we characterize the learnability of fully-connected neural networks via both positive and negative results. We focus on ℓ1-regularized networks, where the ℓ1-norm of the incoming weights of every neuron is assumed to be bounded by a constant B {$>$} 0. Our first result shows that such networks are properly learnable in poly(n, d, exp(1/ϵ2)) time, where n and d are the sample size and the input dimension, and ϵ {$>$} 0 is the gap to optimality. The bound is achieved by repeatedly sampling over a low-dimensional manifold so as to ensure approximate optimality, but avoids the exp(d) cost of exhaustively searching over the parameter space. We also establish a hardness result showing that the exponential dependence on 1/ϵ is unavoidable unless RP = NP. Our second result shows that the exponential dependence on 1/ϵ can be avoided by exploiting the underlying structure of the data distribution. In particular, if the positive and negative examples can be separated with margin γ {$>$} 0 by an unknown neural network, then the network can be learned in poly(n, d, 1/ϵ) time. The bound is achieved by an ensemble method which uses the first algorithm as a weak learner. We further show that the separability assumption can be weakened to tolerate noisy labels. Finally, we show that the exponential dependence on 1/γ is unimprovable under a certain cryptographic assumption.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7LJYSBCL/Zhang et al. - 2017 - On the learnability of fully-connected neural networks(2).pdf}
}

@inproceedings{zhang2017MacroGrammarsHolistic,
  title = {Macro {{Grammars}} and {{Holistic Triggering}} for {{Efficient Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, Y. and Pasupat, P. and Liang, P.},
  date = {2017},
  eprint = {1707.07806},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.07806},
  abstract = {To learn a semantic parser from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing model to improve the state-of-the-art accuracy from 38.7\% to 42.7\%, and then use macro grammars and holistic triggering to achieve an 11x speedup and an accuracy of 43.7\%.},
  archiveprefix = {arXiv},
  annotation = {29 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GGB5AQDI/Zhang, Pasupat, Liang - 2017 - Macro Grammars and Holistic Triggering for Efficient Semantic Parsing(2).pdf}
}

@article{zhang2017OrdinalCommonsenseInference,
  title = {Ordinal {{Common-sense Inference}}},
  author = {Zhang, S. and Rudinger, R. and Duh, K. and Van Durme, B.},
  date = {2017},
  journaltitle = {TACL},
  volume = {5},
  pages = {379--395},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SH5QPJZD/Zhang et al. - 2017 - Ordinal Common-sense Inference(2).pdf}
}

@inproceedings{zhang2017SemiSupervisedStructuredPrediction,
  title = {Semi-{{Supervised Structured Prediction}} with {{Neural CRF Autoencoder}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, X. and Jiang, Y. and Peng, H. and Tu, K. and Goldwasser, D.},
  date = {2017},
  pages = {1702--1712},
  abstract = {In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning of se-quential structured prediction problems. Our NCRF-AE consists of two parts: an encoder which is a CRF model enhanced by deep neural networks, and a decoder which is a generative model trying to re-construct the input. Our model has a uni-fied structure with different loss functions for labeled and unlabeled data with shared parameters. We developed a variation of the EM algorithm for optimizing both the encoder and the decoder simultaneously by decoupling their parameters. Our ex-perimental results over the Part-of-Speech (POS) tagging task on eight different lan-guages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised sce-narios.},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HMYG5DV5/Zhang et al. - 2017 - Semi-Supervised Structured Prediction with Neural CRF Autoencoder(2).pdf}
}

@inproceedings{zhang2017UnderstandingDeepLearning,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  booktitle = {{{ICLR}}},
  author = {Zhang, C. and Bengio, S. and Hardt, M. and Recht, B. and Vinyals, O.},
  date = {2017},
  eprint = {1611.03530},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.03530},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  archiveprefix = {arXiv},
  keywords = {unread},
  annotation = {2466 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JZFMBRHK/Zhang et al. - 2017 - Understanding deep learning requires rethinking generalization(2).pdf}
}

@inproceedings{zhang2018CrosslingualDecompositionalSemantic,
  title = {Cross-Lingual {{Decompositional Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, S. and Ma, X. and Rudinger, R. and Duh, K. and Van Durme, B.},
  date = {2018},
  pages = {1664--1675},
  doi = {10.18653/v1/d18-1194},
  abstract = {We introduce the task of cross-lingual decompositional semantic parsing: mapping content provided in a source language into a decompositional semantic analysis based on a target language. We present: (1) a form of decompositional semantic analysis designed to allow systems to target varying levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score.},
  keywords = {unread},
  annotation = {16 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9X37VHBP/Zhang et al. - 2018 - Cross-lingual Decompositional Semantic Parsing(2).pdf}
}

@inproceedings{zhang2018CrosslingualSemanticParsing,
  title = {Cross-Lingual {{Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, S. and Duh, K. and Van Durme, B.},
  date = {2018},
  eprint = {1804.08037},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.08037},
  abstract = {We introduce the task of cross-lingual semantic parsing: mapping content provided in a source language into a meaning representation based on a target language. We present: (1) a meaning representation designed to allow systems to target varying levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference meaning representations, (3) an end-to-end model with a novel copy mechanism that supports intrasentential coreference, and (4) an evaluation dataset where experiments show our model outperforms strong baselines by at least 1.18 F1 score.},
  archiveprefix = {arXiv},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZTJCBQMK/Zhang, Duh, Van Durme - 2018 - Cross-lingual Semantic Parsing(2).pdf}
}

@inproceedings{zhang2018PersonalizingDialogueAgents,
  title = {Personalizing {{Dialogue Agents}}: {{I}} Have a Dog, Do You Have Pets Too?},
  shorttitle = {Personalizing {{Dialogue Agents}}},
  booktitle = {{{ACL}}},
  author = {Zhang, Saizheng and Dinan, Emily and Urbanek, Jack and Szlam, Arthur and Kiela, Douwe and Weston, Jason},
  date = {2018-09-25},
  eprint = {1801.07243},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1801.07243},
  urldate = {2021-09-24},
  abstract = {Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A5SC7GVS/Zhang et al. - 2018 - Personalizing Dialogue Agents I have a dog, do yo.pdf;/home/hiaoxui/.local/share/zotero_files/storage/J63ALRSI/1801.html}
}

@inproceedings{zhang2019AMRParsingSequencetoGraph,
  title = {{{AMR Parsing}} as {{Sequence-to-Graph Transduction}}},
  booktitle = {{{ACL}}},
  author = {Zhang, S. and Ma, X. and Duh, K. and Van Durme, B.},
  date = {2019},
  volume = {1},
  pages = {80--94},
  doi = {10.18653/v1/p19-1009},
  annotation = {53 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GBHMWWBP/Zhang et al. - 2019 - AMR Parsing as Sequence-to-Graph Transduction(2).pdf}
}

@inproceedings{zhang2019BroadCoverageSemanticParsing,
  title = {Broad-{{Coverage Semantic Parsing}} as {{Transduction}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, S. and Ma, X. and Duh, K. and Van Durme, B.},
  date = {2019},
  eprint = {1909.02607},
  eprinttype = {arxiv},
  abstract = {We unify different broad-coverage semantic parsing tasks under a transduction paradigm, and propose an attention-based neural framework that incrementally builds a meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the transducer can be effectively trained without relying on a pre-trained aligner. Experiments conducted on three separate broad-coverage semantic parsing tasks -- AMR, SDP and UCCA -- demonstrate that our attention-based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FIEC9233/Zhang et al. - 2019 - Broad-Coverage Semantic Parsing as Transduction(2).pdf}
}

@inproceedings{zhang2019HIBERTDocumentLevel,
  title = {{{HIBERT}}: {{Document Level Pre-training}} of {{Hierarchical Bidirectional Transformers}} for {{Document Summarization}}},
  shorttitle = {{{HIBERT}}},
  booktitle = {{{ACL}}},
  author = {Zhang, Xingxing and Wei, Furu and Zhou, Ming},
  date = {2019-05-16},
  eprint = {1905.06566},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.06566},
  urldate = {2021-03-28},
  abstract = {Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these \textbackslash emph\{inaccurate\} labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders \textbackslash cite\{devlin:2018:arxiv\}, we propose \{\textbackslash sc Hibert\} (as shorthand for \{\textbackslash bf HI\}erachical \{\textbackslash bf B\}idirectional \{\textbackslash bf E\}ncoder \{\textbackslash bf R\}epresentations from \{\textbackslash bf T\}ransformers) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained \{\textbackslash sc Hibert\} to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets.},
  archiveprefix = {arXiv},
  annotation = {98 citations (Semantic Scholar/arXiv) [2021-03-27]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJXHGKB6/Zhang et al. - 2019 - HIBERT Document Level Pre-training of Hierarchica.pdf;/home/hiaoxui/.local/share/zotero_files/storage/HLKX42B5/1905.html}
}

@inproceedings{zhang2019SyntaxEnhancedSelfAttentionBasedSemantic,
  title = {Syntax-{{Enhanced Self-Attention-Based Semantic Role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, Y. and Wang, R. and Si, L.},
  date = {2019},
  eprint = {1910.11204},
  eprinttype = {arxiv},
  pages = {616--626},
  abstract = {As a fundamental NLP task, semantic role labeling (SRL) aims to discover the semantic roles for each predicate within one sentence. This paper investigates how to incorporate syntactic knowledge into the SRL task effectively. We present different approaches of encoding the syntactic information derived from dependency trees of different quality and representations; we propose a syntax-enhanced self-attention model and compare it with other two strong baseline methods; and we conduct experiments with newly published deep contextualized word representations as well. The experiment results demonstrate that with proper incorporation of the high quality syntactic information, our model achieves a new state-of-the-art performance for the Chinese SRL task on the CoNLL-2009 dataset.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E9ACKSBI/Zhang, Wang, Si - 2019 - Syntax-Enhanced Self-Attention-Based Semantic Role Labeling(2).pdf}
}

@misc{zhang2020GlobalAttentionName,
  title = {Global {{Attention}} for {{Name Tagging}}},
  author = {Zhang, Boliang and Whitehead, Spencer and Huang, Lifu and Ji, Heng},
  date = {2020-10-19},
  eprint = {2010.09270},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.09270},
  urldate = {2020-10-21},
  abstract = {Many name tagging approaches use local contextual information with much success, but fail when the local context is ambiguous or limited. We present a new framework to improve name tagging by utilizing local, documentlevel, and corpus-level contextual information. We retrieve document-level context from other sentences within the same document and corpus-level context from sentences in other topically related documents. We propose a model that learns to incorporate documentlevel and corpus-level contextual information alongside local contextual information via global attentions, which dynamically weight their respective contextual information, and gating mechanisms, which determine the influence of this information. Extensive experiments on benchmark datasets show the effectiveness of our approach, which achieves state-of-the-art results for Dutch, German, and Spanish on the CoNLL-2002 and CoNLL2003 datasets.1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  annotation = {11 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2RJY57EG/Zhang et al. - 2020 - Global Attention for Name Tagging.pdf}
}

@inproceedings{zhang2020TwoStepApproachImplicit,
  title = {A {{Two-Step Approach}} for {{Implicit Event Argument Detection}}},
  booktitle = {{{ACL}}},
  author = {Zhang, Zhisong and Kong, Xiang and Liu, Zhengzhong and Ma, Xuezhe and Hovy, Eduard},
  date = {2020},
  pages = {7479--7485},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.667},
  url = {https://www.aclweb.org/anthology/2020.acl-main.667},
  urldate = {2021-03-29},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {unread},
  annotation = {0 citations (Semantic Scholar/DOI) [2021-03-29]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FR3KZFXZ/Zhang et al. - 2020 - A Two-Step Approach for Implicit Event Argument De.pdf;/home/hiaoxui/.local/share/zotero_files/storage/GYDIS8UT/Zhang et al. - 2020 - A Two-Step Approach for Implicit Event Argument De.pdf}
}

@inproceedings{zhang2021AbstractMeaningRepresentation,
  title = {Abstract {{Meaning Representation Guided Graph Encoding}} and {{Decoding}} for {{Joint Information Extraction}}},
  booktitle = {{{NAACL}}},
  author = {Zhang, Zixuan and Ji, Heng},
  date = {2021},
  pages = {39--49},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.4},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.4},
  urldate = {2021-06-25},
  abstract = {The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator to let the candidate entity and event trigger nodes collect neighborhood information from AMR graph for passing message among related knowledge elements; 2) an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR. Experiments on multiple datasets have shown that the AMR graph encoder and decoder have provided significant gains and our approach has achieved new state-of-the-art performance on all IE subtasks 1.},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L374PK2N/Zhang and Ji - 2021 - Abstract Meaning Representation Guided Graph Encod.pdf}
}

@inproceedings{zhang2021CertifiedRobustnessProgrammable,
  title = {Certified {{Robustness}} to {{Programmable Transformations}} in {{LSTMs}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, Yuhao and Albarghouthi, Aws and D'Antoni, Loris},
  date = {2021},
  pages = {16},
  abstract = {Deep neural networks for natural language processing are fragile in the face of adversarial examples—small input perturbations, like synonym substitution or word duplication, which cause a neural network to change its prediction. We present an approach to certifying the robustness of to the movie ...},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PFBR9NNA/Zhang et al. - Certified Robustness to Programmable Transformatio.pdf}
}

@inproceedings{zhang2021ContextTrackingNetwork,
  title = {Context {{Tracking Network}}: {{Graph-based Context Modeling}} for {{Implicit Discourse Relation Recognition}}},
  shorttitle = {Context {{Tracking Network}}},
  booktitle = {{{NAACL}}},
  author = {Zhang, Yingxue and Meng, Fandong and Li, Peng and Jian, Ping and Zhou, Jie},
  date = {2021},
  pages = {1592--1599},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.126},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.126},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S9QKIUGB/Zhang et al. - 2021 - Context Tracking Network Graph-based Context Mode.pdf}
}

@inproceedings{zhang2021DisentanglingRepresentationsText,
  title = {Disentangling {{Representations}} of {{Text}} by {{Masking Transformers}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, Xiongyi},
  date = {2021},
  pages = {14},
  abstract = {Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and syntax from semantics. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as — and often better than —previously proposed methods based on variational autoencoders and adversarial training.},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YU98WR3I/Zhang - Disentangling Representations of Text by Masking T.pdf}
}

@inproceedings{zhang2021ExploratoryStudyLong,
  title = {An {{Exploratory Study}} on {{Long Dialogue Summarization}}: {{What Works}} and {{What}}'s {{Next}}},
  shorttitle = {An {{Exploratory Study}} on {{Long Dialogue Summarization}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, Yusen and Ni, Ansong and Yu, Tao and Zhang, Rui and Zhu, Chenguang and Deb, Budhaditya and Celikyilmaz, Asli and Awadallah, Ahmed Hassan and Radev, Dragomir},
  date = {2021-09-09},
  eprint = {2109.04609},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.04609},
  urldate = {2021-12-07},
  abstract = {Dialogue summarization helps readers capture salient information from long conversations in meetings, interviews, and TV series. However, real-world dialogues pose a great challenge to current summarization models, as the dialogue length typically exceeds the input limits imposed by recent transformer-based pre-trained models, and the interactive nature of dialogues makes relevant information more context-dependent and sparsely distributed than news articles. In this work, we perform a comprehensive study on long dialogue summarization by investigating three strategies to deal with the lengthy input problem and locate relevant information: (1) extended transformer models such as Longformer, (2) retrieve-then-summarize pipeline models with several dialogue utterance retrieval methods, and (3) hierarchical dialogue encoding models such as HMNet. Our experimental results on three long dialogue datasets (QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline models yield the best performance. We also demonstrate that the summary quality can be further improved with a stronger retrieval model and pretraining on proper external summarization datasets.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W4778579/Zhang et al. - 2021 - An Exploratory Study on Long Dialogue Summarizatio.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7YKK3B8B/2109.html}
}

@inproceedings{zhang2021FewShotIntentDetection,
  title = {Few-{{Shot Intent Detection}} via {{Contrastive Pre-Training}} and {{Fine-Tuning}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, Jianguo and Bui, Trung and Yoon, Seunghyun and Chen, Xiang and Liu, Zhiwei and Xia, Congying and Tran, Quan Hung and Chang, Walter and Yu, Philip},
  date = {2021-09-13},
  eprint = {2109.06349},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.06349},
  urldate = {2021-09-17},
  abstract = {In this work, we focus on a more challenging few-shot intent detection scenario where many intents are fine-grained and semantically similar. We present a simple yet effective few-shot intent detection schema via contrastive pre-training and fine-tuning. Specifically, we first conduct self-supervised contrastive pre-training on collected intent datasets, which implicitly learns to discriminate semantically similar utterances without using any labels. We then perform few-shot intent detection together with supervised contrastive learning, which explicitly pulls utterances from the same intent closer and pushes utterances across different intents farther. Experimental results show that our proposed method achieves state-of-the-art performance on three challenging intent detection datasets under 5-shot and 10-shot settings.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-17]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CVQPVGTX/Zhang et al. - 2021 - Few-Shot Intent Detection via Contrastive Pre-Trai.pdf;/home/hiaoxui/.local/share/zotero_files/storage/88UWTH5F/2109.html}
}

@inproceedings{zhang2021KnowledgeRouterLearning,
  title = {Knowledge {{Router}}: {{Learning Disentangled Representations}} for {{Knowledge Graphs}}},
  shorttitle = {Knowledge {{Router}}},
  booktitle = {{{NAACL}}},
  author = {Zhang, Shuai and Rao, Xi and Tay, Yi and Zhang, Ce},
  date = {2021},
  pages = {1--10},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.1},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.1},
  urldate = {2021-06-25},
  eventtitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LJ8GZVPA/Zhang et al. - 2021 - Knowledge Router Learning Disentangled Representa.pdf}
}

@inproceedings{zhang2021MetaLabelCorrection,
  title = {Meta {{Label Correction}} for {{Noisy Label Learning}}},
  booktitle = {{{AAAI}}},
  author = {Zhang, G. and Awadallah, H. A. and Dumais, S.},
  date = {2021},
  pages = {9},
  abstract = {Leveraging weak or noisy supervision for building effective machine learning models has long been an important research problem. Its importance has further increased recently due to the growing need for large-scale datasets to train deep learning models. Weak or noisy supervision could originate from multiple sources including non-expert annotators or automatic labeling based on heuristics or user interaction signals. There is an extensive amount of previous work focusing on leveraging noisy labels. Most notably, recent work has shown impressive gains by using a meta-learned instance re-weighting approach where a meta-learning framework is used to assign instance weights to noisy labels. In this paper, we extend this approach via posing the problem as a label correction problem within a meta-learning framework. We view the label correction procedure as a meta-process and propose a new meta-learning based framework termed MLC (Meta Label Correction) for learning with noisy labels. Specifically, a label correction network is adopted as a meta-model to produce corrected labels for noisy labels while the main model is trained to leverage the corrected labels. Both models are jointly trained by solving a bi-level optimization problem. We run extensive experiments with different label noise levels and types on both image recognition and text classification tasks. We compare the re-weighing and correction approaches showing that the correction framing addresses some of the limitations of re-weighting. We also show that the proposed MLC approach outperforms previous methods in both image and language tasks.},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J4HLRI3X/PRELIMINARY VERSION DO NOT CITE.pdf}
}

@inproceedings{zhang2021ModularSelfSupervisionDocumentLevel,
  title = {Modular {{Self-Supervision}} for {{Document-Level Relation Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, Sheng and Wong, Cliff and Usuyama, Naoto and Jain, Sarthak and Naumann, Tristan and Poon, Hoifung},
  date = {2021-09-11},
  eprint = {2109.05362},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.05362},
  urldate = {2021-09-15},
  abstract = {Extracting relations across large text spans has been relatively underexplored in NLP, but it is particularly important for high-value domains such as biomedicine, where obtaining high recall of the latest findings is crucial for practical applications. Compared to conventional information extraction confined to short text spans, document-level relation extraction faces additional challenges in both inference and learning. Given longer text spans, state-of-the-art neural architectures are less effective and task-specific self-supervision such as distant supervision becomes very noisy. In this paper, we propose decomposing document-level relation extraction into relation detection and argument resolution, taking inspiration from Davidsonian semantics. This enables us to incorporate explicit discourse modeling and leverage modular self-supervision for each sub-problem, which is less noise-prone and can be further refined end-to-end via variational EM. We conduct a thorough evaluation in biomedical machine reading for precision oncology, where cross-paragraph relation mentions are prevalent. Our method outperforms prior state of the art, such as multi-scale learning and graph neural networks, by over 20 absolute F1 points. The gain is particularly pronounced among the most challenging relation instances whose arguments never co-occur in a paragraph.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-15]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AR9CDT9Z/Zhang et al. - 2021 - Modular Self-Supervision for Document-Level Relati.pdf;/home/hiaoxui/.local/share/zotero_files/storage/3B6KZYBJ/2109.html}
}

@inproceedings{zhang2021SituatedQAIncorporatingExtraLinguistic,
  title = {{{SituatedQA}}: {{Incorporating Extra-Linguistic Contexts}} into {{QA}}},
  shorttitle = {{{SituatedQA}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, Michael J. Q. and Choi, Eunsol},
  date = {2021-09-13},
  eprint = {2109.06157},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.06157},
  urldate = {2021-11-25},
  abstract = {Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SITUATEDQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SITUATEDQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g., roughly 16.5\% of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at https: //situatedqa.github.io/.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FKI4TN9N/Zhang and Choi - 2021 - SituatedQA Incorporating Extra-Linguistic Context.pdf}
}

@misc{zhang2021SummMultiStageSummarization,
  title = {Summ\^{{N}}: {{A Multi-Stage Summarization Framework}} for {{Long Input Dialogues}} and {{Documents}}},
  shorttitle = {Summ\^{{N}}},
  author = {Zhang, Yusen and Ni, Ansong and Mao, Ziming and Wu, Chen Henry and Zhu, Chenguang and Deb, Budhaditya and Awadallah, Ahmed H. and Radev, Dragomir and Zhang, Rui},
  date = {2021-10-16},
  eprint = {2110.10150},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.10150},
  urldate = {2021-10-30},
  abstract = {Text summarization is an essential task to help readers capture salient information from documents, news, interviews, and meetings. However, most state-of-the-art pretrained language models are unable to efficiently process long text commonly seen in the summarization problem domain. In this paper, we propose Summ\^N, a simple, flexible, and effective multi-stage framework for input texts that are longer than the maximum context lengths of typical pretrained LMs. Summ\^N first generates the coarse summary in multiple stages and then produces the final fine-grained summary based on them. The framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM context size fixed. Moreover, it can deal with both documents and dialogues and can be used on top of any underlying backbone abstractive summarization model. Our experiments demonstrate that Summ\^N significantly outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a newly proposed long document summarization dataset GovReport. Our data and code are available at https://github.com/chatc/Summ-N.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A87R479J/Zhang et al. - 2021 - Summ^N A Multi-Stage Summarization Framework for .pdf;/home/hiaoxui/.local/share/zotero_files/storage/CW78EK6U/2110.html}
}

@inproceedings{zhang2021VideoaidedUnsupervisedGrammar,
  title = {Video-Aided {{Unsupervised Grammar Induction}}},
  booktitle = {{{NAACL-HLT}}},
  author = {Zhang, Songyang and Song, Linfeng and Jin, Lifeng and Xu, Kun and Yu, Dong and Luo, Jiebo},
  date = {2021-05-03},
  eprint = {2104.04369},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.04369},
  urldate = {2021-06-03},
  abstract = {We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on learning syntactic grammars from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we explore rich features (e.g. action, object, scene, audio, face, OCR and speech) from videos, taking the recent Compound PCFG model as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-06-03]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5YK4GD2R/Zhang et al. - 2021 - Video-aided Unsupervised Grammar Induction.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7A8UCFZA/2104.html}
}

@inproceedings{zhang2022HowRobustifyBlackBox,
  title = {How to {{Robustify Black-Box ML Models}}? {{A Zeroth-Order Optimization Perspective}}},
  booktitle = {{{ICLR}}},
  author = {Zhang, Y. and Yao, Y. and Jia, J. and Yi, J. and Hong, M. and Chang, S. and Liu, S.},
  date = {2022},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B98FDGFV/how_to_robustify_black_box_ml_.pdf}
}

@misc{zhang2022UnveilingTransformersLEGO,
  title = {Unveiling {{Transformers}} with {{LEGO}}: A Synthetic Reasoning Task},
  shorttitle = {Unveiling {{Transformers}} with {{LEGO}}},
  author = {Zhang, Yi and Backurs, Arturs and Bubeck, Sébastien and Eldan, Ronen and Gunasekar, Suriya and Wagner, Tal},
  date = {2022-06-09},
  number = {arXiv:2206.04301},
  eprint = {2206.04301},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.04301},
  urldate = {2022-06-22},
  abstract = {We propose a synthetic task, LEGO (Learning Equality and Group Operations), that encapsulates the problem of following a chain of reasoning, and we study how the transformer architecture learns this task. We pay special attention to data effects such as pretraining (on seemingly unrelated NLP tasks) and dataset composition (e.g., differing chain length at training and test time), as well as architectural variants such as weight-tied layers or adding convolutional components. We study how the trained models eventually succeed at the task, and in particular, we are able to understand (to some extent) some of the attention heads as well as how the information flows in the network. Based on these observations we propose a hypothesis that here pretraining helps merely due to being a smart initialization rather than some deep knowledge stored in the network. We also observe that in some data regime the trained transformer finds "shortcut" solutions to follow the chain of reasoning, which impedes the model's ability to generalize to simple variants of the main task, and moreover we find that one can prevent such shortcut with appropriate architecture modification or careful data preparation. Motivated by our findings, we begin to explore the task of learning to execute C programs, where a convolutional modification to transformers, namely adding convolutional structures in the key/query/value maps, shows an encouraging edge.},
  archiveprefix = {arXiv},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X4HUESXE/unveiling_transformers_with_le.pdf}
}

@inproceedings{zhao2014TypeDrivenIncrementalSemantic,
  title = {Type-{{Driven Incremental Semantic Parsing}} with {{Polymorphism}}},
  booktitle = {{{NAACL}}},
  author = {Zhao, K. and Huang, L.},
  date = {2014},
  volume = {0041},
  eprint = {1411.5379},
  eprinttype = {arxiv},
  pages = {1416--1421},
  url = {http://arxiv.org/abs/1411.5379},
  abstract = {Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GeoQuery, Jobs and Atis domains.},
  archiveprefix = {arXiv},
  isbn = {978-1-941643-49-5},
  keywords = {unread},
  annotation = {44 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S7KYPE36/Zhao, Huang - 2014 - Type-Driven Incremental Semantic Parsing with Polymorphism(2).pdf}
}

@inproceedings{zhao2017MenAlsoShopping,
  title = {Men {{Also Like Shopping}}: {{Reducing Gender Bias Amplification}} Using {{Corpus-level Constraints}}},
  booktitle = {{{EMNLP}}},
  author = {Zhao, J. and Wang, T. and Yatskar, M. and Ordonez, V. and Chang, K.},
  date = {2017},
  eprint = {1707.09457},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.09457},
  abstract = {Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68\% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5\% and 40.5\% for multilabel classification and visual semantic role labeling, respectively.},
  archiveprefix = {arXiv},
  annotation = {308 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HXYGM5DU/Zhao et al. - 2017 - Men Also Like Shopping Reducing Gender Bias Amplification using Corpus-level Constraints(2).pdf}
}

@inproceedings{zhao2020ComplexFactoidQuestion,
  title = {Complex {{Factoid Question Answering}} with a {{Free-Text Knowledge Graph}}},
  booktitle = {{{WWW}}},
  author = {Zhao, C. and Xiong, C. and Qian, X. and Boyd-Graber, J.},
  date = {2020},
  pages = {1205--1216},
  isbn = {978-1-4503-7023-3},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6YQC7EAW/Zhao et al. - 2020 - Complex Factoid Question Answering with a Free-Text Knowledge Graph(2).pdf}
}

@misc{zhao2021CalibrateUseImproving,
  title = {Calibrate {{Before Use}}: {{Improving Few-Shot Performance}} of {{Language Models}}},
  shorttitle = {Calibrate {{Before Use}}},
  author = {Zhao, Tony Z. and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  date = {2021-02-18},
  eprint = {2102.09690},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.09690},
  urldate = {2021-03-03},
  abstract = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0\% absolute) and reduces variance across different choices of the prompt.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MVYUZM92/Zhao et al. - 2021 - Calibrate Before Use Improving Few-Shot Performan.pdf;/home/hiaoxui/.local/share/zotero_files/storage/4WP5KJGV/2102.html}
}

@misc{zhao2022DecoupledKnowledgeDistillation,
  title = {Decoupled {{Knowledge Distillation}}},
  author = {Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
  date = {2022-07-12},
  number = {arXiv:2203.08679},
  eprint = {2203.08679},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.08679},
  urldate = {2022-10-22},
  abstract = {State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we reformulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the "difficulty" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megvii-research/mdistiller.},
  archiveprefix = {arXiv},
  keywords = {distillation},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UN3EYA79/Zhao et al. - 2022 - Decoupled Knowledge Distillation.pdf;/home/hiaoxui/.local/share/zotero_files/storage/XPLIS9K2/2203.html}
}

@inproceedings{zheng2013DeepLearningChinese,
  title = {Deep {{Learning}} for {{Chinese Word Segmentation}} and {{POS Tagging}}.},
  booktitle = {{{EMNLP}}},
  author = {Zheng, X. and Chen, H. and Xu, T.},
  date = {2013},
  eprint = {18244602},
  eprinttype = {pmid},
  issn = {15324435},
  doi = {10.1162/153244303322533223},
  abstract = {This study explores the feasibility of perform-ing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representa-tion of Chinese characters, and use these im-proved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-the-art performance with minimal computational cost. We also describe a perceptron-style al-gorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.},
  archiveprefix = {arXiv},
  isbn = {978-1-937284-97-8},
  annotation = {4776 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/APJXJ3YD/Zheng, Chen, Xu - 2013 - Deep Learning for Chinese Word Segmentation and POS Tagging(2).pdf}
}

@misc{zheng2022MultilingualCoreferenceResolution,
  title = {Multilingual {{Coreference Resolution}} in {{Multiparty Dialogue}}},
  author = {Zheng, Boyuan and Xia, Patrick and Yarmohammadi, Mahsa and Van Durme, Benjamin},
  date = {2022-08-02},
  number = {arXiv:2208.01307},
  eprint = {2208.01307},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.01307},
  urldate = {2022-08-04},
  abstract = {Existing multiparty dialogue datasets for coreference resolution are nascent, and many challenges are still unaddressed. We create a large-scale dataset, Multilingual Multiparty Coref (MMC), for this task based on TV transcripts. Due to the availability of gold-quality subtitles in multiple languages, we propose reusing the annotations to create silver coreference data in other languages (Chinese and Farsi) via annotation projection. On the gold (English) data, off-the-shelf models perform relatively poorly on MMC, suggesting that MMC has broader coverage of multiparty coreference than prior datasets. On the silver data, we find success both using it for data augmentation and training from scratch, which effectively simulates the zero-shot cross-lingual setting.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BFNT445U/Zheng et al. - 2022 - Multilingual Coreference Resolution in Multiparty .pdf;/home/hiaoxui/.local/share/zotero_files/storage/EIGRTI55/2208.html}
}

@misc{zhong2017Seq2SQLGeneratingStructured,
  title = {{{Seq2SQL}}: {{Generating Structured Queries}} from {{Natural Language}} Using {{Reinforcement Learning}}},
  author = {Zhong, V. and Xiong, C. and Socher, R.},
  date = {2017},
  eprint = {1709.00103},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1709.00103},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
  archiveprefix = {arXiv},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZLXJ8XVK/Zhong, Xiong, Socher - 2017 - Seq2SQL Generating Structured Queries from Natural Language using Reinforcement Learning(2).pdf}
}

@inproceedings{zhong2018LegalJudgmentPrediction,
  title = {Legal {{Judgment Prediction}} via {{Topological Learning}}},
  booktitle = {{{EMNLP}}},
  author = {Zhong, Haoxi and Guo, Zhipeng and Tu, Cunchao and Xiao, Chaojun and Liu, Zhiyuan and Sun, Maosong},
  date = {2018},
  pages = {3540--3549},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1390},
  url = {http://aclweb.org/anthology/D18-1390},
  urldate = {2020-07-24},
  abstract = {Legal Judgment Prediction (LJP) aims to predict the judgment result based on the facts of a case and becomes a promising application of artificial intelligence techniques in the legal field. In real-world scenarios, legal judgment usually consists of multiple subtasks, such as the decisions of applicable law articles, charges, fines, and the term of penalty. Moreover, there exist topological dependencies among these subtasks. While most existing works only focus on a specific subtask of judgment prediction and ignore the dependencies among subtasks, we formalize the dependencies among subtasks as a Directed Acyclic Graph (DAG) and propose a topological multi-task learning framework, TOPJUDGE, which incorporates multiple subtasks and DAG dependencies into judgment prediction. We conduct experiments on several realworld large-scale datasets of criminal cases in the civil law system. Experimental results show that our model achieves consistent and significant improvements over baselines on all judgment prediction tasks. The source code can be obtained from https://github. com/thunlp/TopJudge.},
  eventtitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  langid = {english},
  keywords = {unread},
  annotation = {70 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D6VPDP3I/Zhong et al. - 2018 - Legal Judgment Prediction via Topological Learning.pdf}
}

@inproceedings{zhong2020GroundedAdaptationZeroshot,
  title = {Grounded {{Adaptation}} for {{Zero-shot Executable Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Zhong, Victor and Lewis, Mike and Wang, Sida I. and Zettlemoyer, Luke},
  date = {2020-09-16},
  eprint = {2009.07396},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.07396},
  urldate = {2020-09-27},
  abstract = {We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycleconsistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose inputoutput consistency are verified. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms dataaugmentation in the training environment, performance increases with the amount of GAZPsynthesized data, and cycle-consistency is central to successful adaptation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {unread},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HMMG6QC5/Zhong et al. - 2020 - Grounded Adaptation for Zero-shot Executable Seman.pdf}
}

@inproceedings{zhong2021FrustratinglyEasyApproach,
  title = {A {{Frustratingly Easy Approach}} for {{Entity}} and {{Relation Extraction}}},
  booktitle = {{{NAACL}}},
  author = {Zhong, Zexuan and Chen, Danqi},
  date = {2021},
  pages = {50--61},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.5},
  url = {https://www.aclweb.org/anthology/2021.naacl-main.5},
  urldate = {2021-06-25},
  eventtitle = {{{NAACL}}},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6MUHMXFH/Zhong and Chen - 2021 - A Frustratingly Easy Approach for Entity and Relat.pdf}
}

@misc{zhong2022TrainingLanguageModels,
  title = {Training {{Language Models}} with {{Memory Augmentation}}},
  author = {Zhong, Zexuan and Lei, Tao and Chen, Danqi},
  date = {2022-05-25},
  number = {arXiv:2205.12674},
  eprint = {2205.12674},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.12674},
  urldate = {2022-05-31},
  abstract = {Recent work has improved language models remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce memories at testing time, or represent them using a separately trained encoder -- resulting in sub-optimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training language models with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories -- local, long-term, and external memory -- at testing time. We evaluate our approach on multiple language modeling and machine translation benchmarks. We find that simply replacing the vanilla language modeling objective by ours greatly reduces the perplexity, without modifying the model architecture or incorporating extra context (e.g., 18.70 \$\textbackslash to\$ 17.76 on WikiText-103). We further augment language models with long-range contexts and external knowledge and demonstrate significant gains over previous memory-augmented approaches.},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YX3Y9F66/Zhong et al. - 2022 - Training Language Models with Memory Augmentation.pdf;/home/hiaoxui/.local/share/zotero_files/storage/3849AQ8H/2205.html}
}

@inproceedings{zhou2013LearningTriggeringKernels,
  title = {Learning {{Triggering Kernels}} for {{Multi-dimensional Hawkes Processes}}},
  booktitle = {{{ICML}}},
  author = {Zhou, K. and Zha, H. and Song, L.},
  date = {2013},
  volume = {28},
  pages = {1301--1309},
  url = {http://jmlr.org/proceedings/papers/v28/zhou13.html},
  abstract = {Abstract How does the activity of one person affect that of another person? Does the strength of influence remain periodic or decay exponentially over time? In this paper, we study these critical questions in social network analysis quantitatively under the framework of multi- ...\textbackslash n},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YNE99U42/Zhou, Zha, Song - 2013 - Learning Triggering Kernels for Multi-dimensional Hawkes Processes(2).pdf}
}

@inproceedings{zhou2015EndtoendLearningSemantic,
  title = {End-to-End Learning of Semantic Role Labeling Using Recurrent Neural Networks},
  booktitle = {{{ACL}}},
  author = {Zhou, J. and Xu, W.},
  date = {2015},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {1127--1137},
  issn = {0147-006X},
  doi = {10.3115/v1/P15-1109},
  url = {http://www.aclweb.org/anthology/P15-1109},
  abstract = {Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems. To this date, most of the successful SRL systems were built on top of some form of parsing results (Koomen et al., 2005; Palmer et al., 2010; Pradhan et al., 2013), where pre-defined feature templates over the syntactic structure are used. The attempts of building an end-to-end SRL learning system without using parsing were less successful (Collobert et al., 2011). In this work, we propose to use deep bi-directional recurrent network as an end-to-end system for SRL. We take only original text information as input feature, without using any syntactic knowledge. The proposed algorithm for semantic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F1 score of 81.07. This result outperforms the previous state-of-the-art system from the combination of different parsing trees or models. We also obtained the same conclusion with F1 = 81.27 on CoNLL- 2012 shared task. As a result of simplicity, our model is also computationally efficient that the parsing speed is 6.7k tokens per second. Our analysis shows that our model is better at handling longer sentences than traditional models. And the latent variables of our model implicitly capture the syntactic structure of a sentence.},
  archiveprefix = {arXiv},
  isbn = {978-1-941643-72-3},
  annotation = {273 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3P6WMZLM/Zhou, Xu - 2015 - End-to-end learning of semantic role labeling using recurrent neural networks(2).pdf}
}

@inproceedings{zhou2017MultispaceVariationalEncoderDecoders,
  title = {Multi-Space {{Variational Encoder-Decoders}} for {{Semi-supervised Labeled Sequence Transduction}}},
  booktitle = {{{ACL}}},
  author = {Zhou, C. and Neubig, G.},
  date = {2017},
  eprint = {1704.01691},
  eprinttype = {arxiv},
  doi = {10.18653/v1/P17-1029},
  url = {http://arxiv.org/abs/1704.01691},
  abstract = {Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.},
  archiveprefix = {arXiv},
  isbn = {978-1-945626-75-3},
  keywords = {unread},
  annotation = {49 citations (Semantic Scholar/DOI) [2021-03-26] 49 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NYVC59PZ/Zhou, Neubig - 2017 - Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction(2).pdf}
}

@inproceedings{zhou2019GoingVacationTakes,
  title = {"{{Going}} on a Vacation" Takes Longer than "{{Going}} for a Walk": {{A Study}} of {{Temporal Commonsense Understanding}}},
  booktitle = {{{EMNLP}}},
  author = {Zhou, B. and Khashabi, D. and Ning, Q. and Roth, D.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3ETBWGEK/Zhou et al. - 2019 - Going on a vacation takes longer than Going for a walk A Study of Temporal Commonsense Understanding(2).pdf}
}

@article{zhou2020GraphNeuralNetworks,
  title = {Graph Neural Networks: {{A}} Review of Methods and Applications},
  shorttitle = {Graph Neural Networks},
  author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  date = {2020},
  journaltitle = {AI Open},
  shortjournal = {AI Open},
  volume = {1},
  pages = {57--81},
  issn = {26666510},
  doi = {10.1016/j.aiopen.2021.01.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651021000012},
  urldate = {2022-07-01},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JX64L7H4/Zhou et al. - 2020 - Graph neural networks A review of methods and app.pdf}
}

@misc{zhou2020LIMITBERTLinguisticsInformed,
  title = {{{LIMIT-BERT}} : {{Linguistics Informed Multi-Task BERT}}},
  author = {Zhou, J. and Zhang, Z. and Zhao, H.},
  date = {2020},
  keywords = {review},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2ZGHGZPB/Zhou, Zhang, Zhao - 2020 - LIMIT-BERT Linguistics Informed Multi-Task BERT(2).pdf}
}

@inproceedings{zhou2020ParsingAllSyntax,
  title = {Parsing {{All}}: {{Syntax}} and {{Semantics}}, {{Dependencies}} and {{Spans}}},
  shorttitle = {Parsing {{All}}},
  booktitle = {{{EMNLP}}},
  author = {Zhou, Junru and Li, Zuchao and Zhao, Hai},
  date = {2020},
  eprint = {1908.11522},
  eprinttype = {arxiv},
  abstract = {Both syntactic and semantic structures are key linguistic contextual clues, in which parsing the latter has been well shown beneficial from parsing the former. However, few works ever made an attempt to let semantic parsing help syntactic parsing. As linguistic representation formalisms, both syntax and semantics may be represented in either span (constituent/phrase) or dependency, on both of which joint learning was also seldom explored. In this paper, we propose a novel joint model of syntactic and semantic parsing on both span and dependency representations, which incorporates syntactic information effectively in the encoder of neural network and benefits from two representation formalisms in a uniform way. The experiments show that semantics and syntax can benefit each other by optimizing joint objectives. Our single model achieves new state-of-the-art or competitive results on both span and dependency semantic parsing on Propbank benchmarks and both dependency and constituent syntactic parsing on Penn Treebank.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3ZV6637X/Zhou et al. - 2020 - Parsing All Syntax and Semantics, Dependencies an.pdf}
}

@inproceedings{zhou2021ContrastiveOutofDistributionDetection,
  title = {Contrastive {{Out-of-Distribution Detection}} for {{Pretrained Transformers}}},
  booktitle = {{{EMNLP}}},
  author = {Zhou, Wenxuan and Liu, Fangyu and Chen, Muhao},
  date = {2021},
  pages = {12},
  langid = {english},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6RS5PMSQ/Zhou et al. - Contrastive Out-of-Distribution Detection for Pret.pdf}
}

@inproceedings{zhou2022CloserLookHow,
  title = {A {{Closer Look}} at {{How Fine-tuning Changes BERT}}},
  booktitle = {{{ACL}}},
  author = {Zhou, Yichu and Srikumar, Vivek},
  date = {2022-03-15},
  eprint = {2106.14282},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.14282},
  urldate = {2022-03-23},
  abstract = {Given the prevalence of pre-trained contextualized representations in today's NLP, there have been many efforts to understand what information they contain, and why they seem to be universally successful. The most common approach to use these representations involves fine-tuning them for an end task. Yet, how fine-tuning changes the underlying embedding space is less studied. In this work, we study the English BERT family and use two probing techniques to analyze how fine-tuning changes the space. We hypothesize that fine-tuning affects classification performance by increasing the distances between examples associated with different labels. We confirm this hypothesis with carefully designed experiments on five different NLP tasks. Via these experiments, we also discover an exception to the prevailing wisdom that "fine-tuning always improves performance". Finally, by comparing the representations before and after fine-tuning, we discover that fine-tuning does not introduce arbitrary changes to representations; instead, it adjusts the representations to downstream tasks while largely preserving the original spatial structure of the data points.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W82WAT49/Zhou and Srikumar - 2022 - A Closer Look at How Fine-tuning Changes BERT.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6KLSKFYK/2106.html}
}

@inproceedings{zhu2021FindingsConversationDisentanglement,
  title = {Findings on {{Conversation Disentanglement}}},
  booktitle = {{{ALTA}}},
  author = {Zhu, Rongxin and Lau, Jey Han and Qi, Jianzhong},
  date = {2021-12-10},
  eprint = {2112.05346},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.05346},
  urldate = {2021-12-15},
  abstract = {Conversation disentanglement, the task to identify separate threads in conversations, is an important pre-processing step in multi-party conversational NLP applications such as conversational question answering and conversation summarization. Framing it as a utterance-to-utterance classification problem -- i.e. given an utterance of interest (UOI), find which past utterance it replies to -- we explore a number of transformer-based models and found that BERT in combination with handcrafted features remains a strong baseline. We then build a multi-task learning model that jointly learns utterance-to-utterance and utterance-to-thread classification. Observing that the ground truth label (past utterance) is in the top candidates when our model makes an error, we experiment with using bipartite graphs as a post-processing step to learn how to best match a set of UOIs to past utterances. Experiments on the Ubuntu IRC dataset show that this approach has the potential to outperform the conventional greedy approach of simply selecting the highest probability candidate for each UOI independently, indicating a promising future research direction.},
  archiveprefix = {arXiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XLIZNHPH/Zhu et al. - 2021 - Findings on Conversation Disentanglement.pdf;/home/hiaoxui/.local/share/zotero_files/storage/77RADC3E/2112.html}
}

@inproceedings{zhu2021HTransformer1DFastOneDimensional,
  title = {H-{{Transformer-1D}}: {{Fast One-Dimensional Hierarchical Attention}} for {{Sequences}}},
  shorttitle = {H-{{Transformer-1D}}},
  booktitle = {{{ACL}}},
  author = {Zhu, Zhenhai and Soricut, Radu},
  date = {2021-07-25},
  eprint = {2107.11906},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.11906},
  urldate = {2021-07-27},
  abstract = {We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-07-27]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L48TN6PZ/Zhu and Soricut - 2021 - H-Transformer-1D Fast One-Dimensional Hierarchica.pdf;/home/hiaoxui/.local/share/zotero_files/storage/FX7MCGLM/2107.html}
}

@inproceedings{zhu2021TWAGTopicGuidedWikipedia,
  title = {{{TWAG}}: {{A Topic-Guided Wikipedia Abstract Generator}}},
  shorttitle = {{{TWAG}}},
  booktitle = {{{ACL}}},
  author = {Zhu, Fangwei and Tu, Shangqing and Shi, Jiaxin and Li, Juanzi and Hou, Lei and Cui, Tong},
  date = {2021-06-29},
  eprint = {2106.15135},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.15135},
  urldate = {2021-09-07},
  abstract = {Wikipedia abstract generation aims to distill a Wikipedia abstract from web sources and has met significant success by adopting multi-document summarization techniques. However, previous works generally view the abstract as plain text, ignoring the fact that it is a description of a certain entity and can be decomposed into different topics. In this paper, we propose a two-stage model TWAG that guides the abstract generation with topical information. First, we detect the topic of each input paragraph with a classifier trained on existing Wikipedia articles to divide input documents into different topics. Then, we predict the topic distribution of each abstract sentence, and decode the sentence from topic-aware representations with a Pointer-Generator network. We evaluate our model on the WikiCatSum dataset, and the results show that \textbackslash modelnames outperforms various existing baselines and is capable of generating comprehensive abstracts. Our code and dataset can be accessed at \textbackslash url\{https://github.com/THU-KEG/TWAG\}},
  archiveprefix = {arXiv},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-09-07]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KXBYL9YW/Zhu et al. - 2021 - TWAG A Topic-Guided Wikipedia Abstract Generator.pdf;/home/hiaoxui/.local/share/zotero_files/storage/NXR7AJ6E/2106.html}
}

@inproceedings{zhuang2018NoiseContrastiveEstimation,
  title = {Noise {{Contrastive Estimation}} and {{Negative Sampling}} for {{Conditional Models}}: {{Consistency}} and {{Statistical Efficiency}}},
  booktitle = {{{EMNLP}}},
  author = {Zhuang, M. and Collins, M.},
  date = {2018},
  eprint = {1809.01812v1},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {unread},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MV8444NA/Zhuang, Collins - 2018 - Noise Contrastive Estimation and Negative Sampling for Conditional Models Consistency and Statistical Effici(2).pdf}
}

@inproceedings{zou2013BilingualWordEmbeddings,
  title = {Bilingual {{Word Embeddings}} for {{Phrase-Based Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Zou, W. Y. and Socher, R. and Cer, D. and Manning, C. D.},
  date = {2013},
  url = {http://www.aclweb.org/anthology/D13-1141},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AW27YM53/Zou et al. - 2013 - Bilingual Word Embeddings for Phrase-Based Machine Translation(2).pdf}
}

