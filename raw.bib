@inproceedings{Abend2013,
abstract = {Syntactic structures, by their nature, re-flect first and foremost the formal con-structions used for expressing meanings. This renders them sensitive to formal vari-ation both within and across languages, and limits their value to semantic ap-plications. We present UCCA, a novel multi-layered framework for semantic rep-resentation that aims to accommodate the semantic distinctions expressed through linguistic utterances. We demonstrate UCCA's portability across domains and languages, and its relative insensitivity to meaning-preserving syntactic variation. We also show that UCCA can be ef-fectively and quickly learned by annota-tors with no linguistic background, and describe the compilation of a UCCA-annotated corpus.},
author = {Abend, O. and Rappoport, A.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Abend, Rappoport/Abend, Rappoport - Universal Conceptual Cognitive Annotation ( UCCA ).pdf:pdf},
isbn = {978-1-937284-50-3},
number = {Section 2},
pages = {228--238},
title = {{Universal Conceptual Cognitive Annotation ( UCCA )}},
url = {http://www.aclweb.org/anthology/P13-1023},
year = {2013}
}
@inproceedings{Al-Shedivat2018,
author = {Al-Shedivat, M. and Bansal, T. and Burda, Y. and Sutskever, I. and Mordatch, I. and Abbeel, P.},
booktitle = {ICLR},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Reddi, Kale, Kumar/Reddi, Kale, Kumar - On the convergence of adam and beyond.pdf:pdf},
number = {March},
pages = {1--21},
title = {{Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments}},
year = {2018}
}
@inproceedings{Ammar2014,
abstract = {We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observable data using a feature-rich conditional random field. Then a reconstruction of the input is (re)generated, conditional on the latent structure, using models for which maximum likelihood estimation has a closed-form. Our autoencoder formulation enables efficient learning without making unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. We show competitive results with instantiations of the model for two canonical NLP tasks: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.},
archivePrefix = {arXiv},
arxivId = {1411.1147},
author = {Ammar, W. and Dyer, C. and Smith, N. A.},
booktitle = {NIPS},
eprint = {1411.1147},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ammar, Dyer, Smith/Ammar, Dyer, Smith - Conditional Random Field Autoencoders for Unsupervised Structured Prediction.pdf:pdf},
issn = {10495258},
title = {{Conditional Random Field Autoencoders for Unsupervised Structured Prediction}},
url = {http://arxiv.org/abs/1411.1147},
year = {2014}
}
@inproceedings{Andreas2016,
abstract = {We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.},
archivePrefix = {arXiv},
arxivId = {1601.01705},
author = {Andreas, J. and Rohrbach, M. and Darrell, T. and Klein, D.},
booktitle = {NAACL},
doi = {10.18653/v1/N16-1181},
eprint = {1601.01705},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Andreas et al/Andreas et al. - Learning to Compose Neural Networks for Question Answering.pdf:pdf},
isbn = {9781941643914},
title = {{Learning to Compose Neural Networks for Question Answering}},
url = {http://arxiv.org/abs/1601.01705},
year = {2016}
}
@inproceedings{Angeli2010,
abstract = {We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework. In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domainsRobocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-of-the-art domain-specific systems both in terms of BLEU scores and human evaluation.},
author = {Angeli, G. and Liang, P. and Klein, D.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Angeli, Liang, Klein/Angeli, Liang, Klein - A Simple Domain-Independent Probabilistic Approach to Generation.pdf:pdf},
isbn = {1932432868},
number = {October},
pages = {502--512},
title = {{A Simple Domain-Independent Probabilistic Approach to Generation}},
url = {http://www.aclweb.org/anthology/D10-1049},
year = {2010}
}
@inproceedings{Arnaud2017,
author = {Arnaud, A. S. and Beck, D.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Arnaud, Beck/Arnaud, Beck - Identifying Cognate Sets Across Dictionaries of Related Languages.pdf:pdf},
pages = {2519--2528},
title = {{Identifying Cognate Sets Across Dictionaries of Related Languages}},
year = {2017}
}
@inproceedings{Artzi2015,
abstract = {We propose a grammar induction tech-nique for AMR semantic parsing. While previous grammar induction techniques were designed to re-learn a new parser for each target application, the recently anno-tated AMR Bank provides a unique op-portunity to induce a single model for un-derstanding broad-coverage newswire text and support a wide range of applications. We present a new model that combines CCG parsing to recover compositional aspects of meaning and a factor graph to model non-compositional phenomena, such as anaphoric dependencies. Our ap-proach achieves 66.2 Smatch F1 score on the AMR bank, significantly outperform-ing the previous state of the art.},
author = {Artzi, Y. and Lee, K. and Zettlemoyer, L. S.},
booktitle = {EMNLP},
doi = {10.18653/v1/D15-1198},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Artzi, Lee, Zettlemoyer/Artzi, Lee, Zettlemoyer - Broad-coverage CCG Semantic Parsing with AMR.pdf:pdf},
isbn = {9781941643327},
number = {September},
pages = {1699--1710},
title = {{Broad-coverage CCG Semantic Parsing with AMR}},
url = {http://aclweb.org/anthology/D15-1198},
year = {2015}
}
@article{Artzi2013,
abstract = {The context in which language is used pro- vides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCGsemantic parsing approach that learns a joint model of mean- ing and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to di- rectly influence learning. It also enables algo- rithms that learn while executing instructions, for example by trying to replicate human ac- tions. Experiments on a benchmark naviga- tional dataset demonstrate strong performance under differing forms of supervision, includ- ing correctly executing 60{\%} more instruction sets relative to the previous state of the art.},
author = {Artzi, Y. and Zettlemoyer, L. S.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Artzi, Zettlemoyer/Artzi, Zettlemoyer - Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions.pdf:pdf},
issn = {2307-387X},
journal = {TACL},
pages = {49--62},
title = {{Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions}},
volume = {1},
year = {2013}
}
@article{Arulampalam2002,
abstract = {Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or "particle") representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Arulampalam, M. S. and Maskell, S. and Gordon, N. and Clapp, T.},
doi = {10.1109/9780470544198.ch73},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Arulampalam et al/Arulampalam et al. - A tutorial on particle filters for online nonlinearnongaussian bayesian tracking.pdf:pdf},
isbn = {9780470544198},
issn = {1053587X},
journal = {TSP},
keywords = {Approximation algorithms,Approximation methods,Bayesian methods,Filtering algorithms,Particle filters},
number = {2},
pages = {723--737},
pmid = {978374},
title = {{A tutorial on particle filters for online nonlinear/nongaussian bayesian tracking}},
volume = {50},
year = {2002}
}
@inproceedings{Bahdanau2015,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, D. and Cho, K. and Bengio, Y.},
booktitle = {ICLR},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Bahdanau, Cho, Bengio/Bahdanau, Cho, Bengio - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
pages = {1--15},
pmid = {14527267},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2015}
}
@inproceedings{Ballard2004,
author = {Ballard, D. and Yu, C.},
booktitle = {AAAI},
doi = {10.1080/10656210509484979},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ballard, Yu/Ballard, Yu - On the Integration of Grounding Language and Lear ning Objects.pdf:pdf},
isbn = {0262511835},
issn = {10656219},
keywords = {Copyright {\textcopyright} 2004 American Association for Artifici},
pages = {488--493},
title = {{On the Integration of Grounding Language and Lear ning Objects}},
url = {http://www.indiana.edu/{~}dll/papers/yu{\_}aaai04.pdf},
year = {2004}
}
@inproceedings{Banarescu2013,
abstract = {We describe Abstract Meaning Represen- tation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sen- tences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural lan- guage understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.},
author = {Banarescu, L. and Bonial, C. and Cai, S. and Georgescu, M. and Griffitt, K. and Hermjakob, U. and Knight, K. and Koehn, P. and Palmer, M. and Schneider, N.},
booktitle = {Linguistic Annotation Workshop {\&} Interoperability with Discourse},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Banarescu et al/Banarescu et al. - Abstract meaning representation for sembanking.pdf:pdf},
isbn = {1111111111},
pages = {178--186},
title = {{Abstract meaning representation for sembanking}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Abstract+Meaning+Representation+for+Sembanking{\#}0},
year = {2013}
}
@inproceedings{Barzilay2005,
abstract = {A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efficient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods.},
author = {Barzilay, R. and Lapata, M.},
booktitle = {EMNLP},
doi = {http://dx.doi.org/10.3115/1220575.1220617},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Barzilay, Lapata/Barzilay, Lapata - Collective content selection for concept-to-text generation.pdf:pdf},
number = {October},
pages = {331--338},
title = {{Collective content selection for concept-to-text generation}},
url = {http://dl.acm.org/citation.cfm?id=1220617},
year = {2005}
}
@article{Barzilay2008,
abstract = {This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.},
author = {Barzilay, R. and Lapata, M.},
doi = {10.1162/coli.2008.34.1.1},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Barzilay, Lapata/Barzilay, Lapata - Modeling Local Coherence An Entity-Based Approach.pdf:pdf},
isbn = {1932432515},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {1--34},
title = {{Modeling Local Coherence: An Entity-Based Approach}},
url = {http://www.mitpressjournals.org/doi/10.1162/coli.2008.34.1.1},
volume = {34},
year = {2008}
}
@article{Belz2008,
abstract = {Two important recent trends in natural language generation are (i) probabilistic techniques and (ii) comprehensive approaches that move away from traditional strictly modular and sequential models. This paper reports experiments in which pCRU a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space was used to semi-automatically create five different versions of a weather forecast generator. The generators were evaluated in terms of output quality, development time and computational efficiency against (i) human forecasters, (ii) a traditional handcrafted pipelined NLG system and (iii) a HALOGEN-style statistical generator. The most striking result is that despite acquiring all decision-making abilities automatically, the best pCRU generators produce outputs of high enough quality to be scored more highly by human judges than forecasts written by experts.},
author = {Belz, A.},
doi = {10.1017/S1351324907004664},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Belz/Belz - Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.pdf:pdf},
isbn = {1469-8110},
issn = {1351-3249},
journal = {Natural Language Engineering},
number = {04},
pages = {1--26},
title = {{Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models}},
url = {http://www.journals.cambridge.org/abstract{\_}S1351324907004664},
volume = {14},
year = {2008}
}
@inproceedings{Bengio2007,
abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Y. and Lamblin, P. and Popovici, D. and Larochelle, H.},
booktitle = {NIPS},
doi = {citeulike-article-id:4640046},
eprint = {0500581},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Bengio et al/Bengio et al. - Greedy Layer-Wise Training of Deep Networks.pdf:pdf},
isbn = {0262195682},
issn = {01628828},
number = {1},
pages = {153},
pmid = {19018704},
primaryClass = {submit},
title = {{Greedy Layer-Wise Training of Deep Networks}},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
volume = {19},
year = {2007}
}
@inproceedings{Berant2013,
abstract = {In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai andYates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.},
author = {Berant, J. and Chou, A. and Frostig, R. and Liang, P.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Berant et al/Berant et al. - Semantic Parsing on Freebase from Question-Answer Pairs.pdf:pdf},
isbn = {9781937284978},
number = {October},
pages = {1533--1544},
pmid = {2216100},
title = {{Semantic Parsing on Freebase from Question-Answer Pairs}},
url = {https://www.aclweb.org/anthology/D/D13/D13-1160.pdf{\%}5Cnhttp://www.samstyle.tk/index.pl/00/http/nlp.stanford.edu/pubs/semparseEMNLP13.pdf},
year = {2013}
}
@article{Berant2015,
abstract = {Semantic parsers conventionally construct logical forms bottom-up in a fixed order, re- sulting in the generation of many extraneous partial logical forms. In this paper, we com- bine ideas from imitation learning and agenda- based parsing to train a semantic parser that searches partial logical forms in a more strate- gic order. Empirically, our parser reduces the number of constructed partial logical forms by an order of magnitude, and obtains a 6x-9x speedup over fixed-order parsing, while main- taining comparable accuracy.},
author = {Berant, J. and Liang, P.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Berant, Liang/Berant, Liang - Imitation Learning of Agenda-based Semantic Parsers.pdf:pdf},
journal = {TACL},
pages = {545--558},
title = {{Imitation Learning of Agenda-based Semantic Parsers}},
volume = {3},
year = {2015}
}
@inproceedings{Berant2014,
abstract = {A central challenge in semantic parsing is handling the myriadways in which knowl- edge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this pa- per, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically gener- ate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best para- phrases the input, and output the corre- sponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves state- of-the-art accuracies on two recently re- leased question-answering datasets. 1},
author = {Berant, J. and Liang, P.},
booktitle = {ACL},
doi = {10.3115/v1/P14-1133},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Berant, Liang/Berant, Liang - Semantic Parsing via Paraphrasing.pdf:pdf},
isbn = {9781937284725},
issn = {00219258},
pages = {1415--1425},
pmid = {1903399},
title = {{Semantic Parsing via Paraphrasing}},
url = {http://aclweb.org/anthology/P14-1133},
year = {2014}
}
@inproceedings{Berg-kirkpatrick2010,
abstract = {We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.},
author = {Berg-kirkpatrick, T. and Bouchard-C{\^{o}}t{\'{e}}, A. and DeNero, J. and Klein, D.},
booktitle = {NAACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Berg-kirkpatrick et al/Berg-kirkpatrick et al. - Painless unsupervised learning with features.pdf:pdf},
isbn = {1-932432-65-5},
keywords = {Best,EM},
mendeley-tags = {Best,EM},
number = {June},
pages = {582----590},
title = {{Painless unsupervised learning with features}},
url = {http://portal.acm.org/citation.cfm?id=1858082},
year = {2010}
}
@inproceedings{Bertero2016,
abstract = {We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the au- dience would react. We model the setup- punchline relation of conversational humor with a Long Short-Term Memory, with utter- ance encodings obtained from a Convolutional Neural Network. Out neural network frame- work is able to improve the F-score of8{\%}over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to un- derstand jokes.},
author = {Bertero, D. and Fung, P.},
booktitle = {NAACL-HLT},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Bertero, Fung/Bertero, Fung - A Long Short-Term Memory Framework for Predicting Humor in Dialogues.pdf:pdf},
isbn = {9781941643914},
pages = {130--135},
title = {{A Long Short-Term Memory Framework for Predicting Humor in Dialogues}},
year = {2016}
}
@article{Boito2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.05631v2},
author = {Boito, M. Z. and Alexandre, B. and Villavicencio, A. and Besacier, L. and Dev, F.},
eprint = {arXiv:1709.05631v2},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Boito et al/Boito et al. - Unwritten Languages Demand Attention Too! Word Discovery with Encoder-Decoder Models.pdf:pdf},
journal = {ASRU},
title = {{Unwritten Languages Demand Attention Too! Word Discovery with Encoder-Decoder Models}},
year = {2017}
}
@inproceedings{Bollacker2008,
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
author = {Bollacker, K. and Evans, C. and Paritosh, P. and Sturge, T. and Taylor, J.},
booktitle = {International Conference on Management of Data (SIGMOD)},
doi = {10.1145/1376616.1376746},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Bollacker et al/Bollacker et al. - Freebase a collaboratively created graph database for structuring human knowledge.pdf:pdf},
isbn = {9781605581026},
issn = {07308078},
pages = {1247--1250},
pmid = {3105260},
title = {{Freebase: a collaboratively created graph database for structuring human knowledge}},
url = {http://doi.acm.org/10.1145/1376616.1376746},
year = {2008}
}
@inproceedings{Bordes2010,
abstract = {This paper studies the problem of learning from ambiguous supervision, focusing on the task of learning semantic correspondences. A learning problem is said to be ambiguously supervised when, for a given training input, a set of output candidates is provided with no prior of which one is correct. We propose to tackle this problem by solving a related unambiguous task with a label ranking ap- proach and show how and why this performs well on the original task, via the method of task-transfer. We apply it to learning to match natural language sentences to a struc- tured representation of their meaning and empirically demonstrate that this competes with the state-of-the-art on two benchmarks.},
author = {Bordes, A. and Usunier, N. and Weston, J. and Kennedy, P.},
booktitle = {ICML},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Bordes et al/Bordes et al. - Label Ranking under Ambiguous Supervision for Learning Semantic Correspondences.pdf:pdf},
isbn = {9781605589077},
number = {Icml},
pages = {103--110},
title = {{Label Ranking under Ambiguous Supervision for Learning Semantic Correspondences}},
url = {http://www.icml2010.org/papers/331.pdf},
year = {2010}
}
@unpublished{Borman2009,
abstract = {This tutorial discusses the ExpectationMaximization (EM) algorithm of Demp- ster, Laird and Rubin 1. The approach taken follows that of an unpublished note by Stuart Russel, but fleshes out some of the gory details. In order to ensure that the presentation is reasonably self-contained, some of the results on which the derivation of the algorithm is based are presented prior to the main results. The EM algorithm has become a popular tool in statistical estimation problems involving incomplete data, or in problems which can be posed in a sim- ilar form, such as mixture estimation 3, 4. The EM algorithm has also been used in various motion estimation frameworks 5 and variants of it have been used in multiframe superresolution restoration methods which combine motion estimation along the lines of 2.},
author = {Borman, S.},
doi = {10.1097/RLU.0b013e3181b06c41\r00003072-200909000-00002},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Borman/Borman - The Expectation Maximization Algorithm A short tutorial.pdf:pdf},
isbn = {0387952845},
issn = {15360229},
number = {x},
pages = {1--9},
pmid = {19692813},
title = {{The Expectation Maximization Algorithm A short tutorial}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.150.8193{\&}rep=rep1{\&}type=pdf},
volume = {25},
year = {2009}
}
@inproceedings{Bowman2015,
abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
archivePrefix = {arXiv},
arxivId = {1511.06349},
author = {Bowman, S. R. and Vilnis, L. and Vinyals, O. and Dai, A. M. and Jozefowicz, R. and Bengio, S.},
booktitle = {CoNLL},
doi = {10.18653/v1/K16-1002},
eprint = {1511.06349},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Bowman et al/Bowman et al. - Generating Sentences from a Continuous Space.pdf:pdf},
pages = {10--21},
title = {{Generating Sentences from a Continuous Space}},
url = {http://arxiv.org/abs/1511.06349},
year = {2015}
}
@inproceedings{Boyd1998,
abstract = {| A system is described that integrates knowledge-based signal processing and natural lan-guage processing to automatically generate descrip-tions of time-series data. These descriptions are based on short and long-term trends in the data which are detected using wavelet analysis. The basic architec-ture of the system is presented and some experimental results are shown for weather data.},
author = {Boyd, S.},
booktitle = {ICIPS},
doi = {10.1.1.57.3705},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Boyd/Boyd - TREND A System for Generating Intelligent Descriptions of Time-Series Data.pdf:pdf},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {1--5},
title = {{TREND: A System for Generating Intelligent Descriptions of Time-Series Data}},
year = {1998}
}
@inproceedings{Branavan2009,
abstract = {In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.},
author = {Branavan, S. R. K. and Chen, H. and Zettlemoyer, L. S. and Barzilay, R.},
booktitle = {ACL-IJCNLP},
doi = {10.3115/1687878.1687892},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Branavan et al/Branavan et al. - Reinforcement learning for mapping instructions to actions.pdf:pdf},
isbn = {9781932432459},
issn = {1742206X},
number = {August},
pages = {82},
pmid = {18493666},
title = {{Reinforcement learning for mapping instructions to actions}},
url = {http://portal.acm.org/citation.cfm?doid=1687878.1687892},
volume = {1},
year = {2009}
}
@inproceedings{Brill2002,
abstract = {We describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy. The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers. Because a wrong an-swer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer.},
author = {Brill, E. and Dumais, S. and Banko, M.},
booktitle = {EMNLP},
doi = {10.3115/1118693.1118726},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Brill, Dumais, Banko/Brill, Dumais, Banko - An analysis of the AskMSR question-answering system.pdf:pdf},
number = {July},
pages = {257--264},
title = {{An analysis of the AskMSR question-answering system}},
url = {http://portal.acm.org/citation.cfm?doid=1118693.1118726},
volume = {10},
year = {2002}
}
@article{Brown1993,
abstract = {We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.},
author = {Brown, P. F. and Pietra, S. A. D. and Pietra, V. J. D. and Mercer, R. L.},
doi = {10.1080/08839514.2011.559906},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Brown et al/Brown et al. - The mathematics of statistical machine translation Parameter estimation.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
number = {2},
pages = {263--311},
pmid = {3046723},
title = {{The mathematics of statistical machine translation: Parameter estimation}},
url = {http://www.aclweb.org/anthology/J93-2003},
volume = {19},
year = {1993}
}
@inproceedings{Bulat2017,
author = {Bulat, L. and Clark, S. and Shutova, E.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Bulat, Clark, Shutova/Bulat, Clark, Shutova - Speaking , Seeing , Understanding Correlating semantic models with conceptual representation in the brain.pdf:pdf},
number = {2},
pages = {1081--1091},
title = {{Speaking , Seeing , Understanding : Correlating semantic models with conceptual representation in the brain}},
year = {2017}
}
@inproceedings{Buys2017,
abstract = {Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focused almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69{\%} Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.},
archivePrefix = {arXiv},
arxivId = {1704.07092},
author = {Buys, J. and Blunsom, P.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1112},
eprint = {1704.07092},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Buys, Blunsom/Buys, Blunsom - Robust Incremental Neural Semantic Graph Parsing.pdf:pdf},
isbn = {9781945626753},
pages = {1215--1226},
title = {{Robust Incremental Neural Semantic Graph Parsing}},
url = {http://arxiv.org/abs/1704.07092},
year = {2017}
}
@inproceedings{Cai2017,
author = {Cai, J. and Jiang, Y. and Tu, K.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Cai, Jiang, Tu/Cai, Jiang, Tu - CRF Autoencoder for Unsupervised Dependency Parsing.pdf:pdf},
pages = {1638--1643},
title = {{CRF Autoencoder for Unsupervised Dependency Parsing}},
year = {2017}
}
@inproceedings{Cai2013,
author = {Cai, Q. and Yates, A.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Cai, Yates/Cai, Yates - Large-scale Semantic Parsing via Schema Matching and Lexicon Extension.pdf:pdf},
isbn = {9781937284503},
keywords = {semantic parsing},
mendeley-tags = {semantic parsing},
pages = {423--433},
title = {{Large-scale Semantic Parsing via Schema Matching and Lexicon Extension}},
url = {http://www.aclweb.org/anthology/P13-1042{\%}5Cnhttp://www.cis.temple.edu/{~}yates/papers/textual-schema-matching.pdf},
year = {2013}
}
@inproceedings{Chaganty2016,
abstract = {How much is 131 million US dollars? To help readers put such numbers in con-text, we propose a new task of automati-cally generating short descriptions known as perspectives, e.g. " {\$}131 million is about the cost to employ everyone in Texas over a lunch period " . First, we collect a dataset of numeric mentions in news arti-cles, where each mention is labeled with a set of rated perspectives. We then pro-pose a system to generate these descrip-tions consisting of two steps: formula con-struction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on fa-miliarity, numeric proximity and seman-tic compatibility. In generation, we con-vert a formula into natural language us-ing a sequence-to-sequence recurrent neu-ral network. Our system obtains a 15.2{\%} F 1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.},
archivePrefix = {arXiv},
arxivId = {1609.00070},
author = {Chaganty, A. and Liang, P.},
booktitle = {ACL},
doi = {10.18653/v1/P16-1055},
eprint = {1609.00070},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Chaganty, Liang/Chaganty, Liang - How Much is 131 Million Dollars Putting Numbers in Perspective with Compositional Descriptions.pdf:pdf},
isbn = {9781510827585},
pages = {578--587},
title = {{How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions}},
url = {http://aclweb.org/anthology/P16-1055},
year = {2016}
}
@unpublished{Chen2012,
author = {Chen, D. L.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Chen/Chen - Learning language from ambiguous perceptual context.pdf:pdf},
pages = {197},
title = {{Learning language from ambiguous perceptual context}},
year = {2012}
}
@article{Chen2010,
abstract = {We present a novel framework for learning to interpret and generate language using only per-ceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descrip-tive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.},
author = {Chen, D. L. and Kim, J. and Mooney, R. J.},
doi = {10.1613/jair.2962},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Chen, Kim, Mooney/Chen, Kim, Mooney - Training a multilingual sportscaster Using perceptual context to learn language.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {JAIR},
pages = {397--435},
title = {{Training a multilingual sportscaster: Using perceptual context to learn language}},
volume = {37},
year = {2010}
}
@inproceedings{Chen2011,
abstract = {The ability to understand natural-language instructions is crit- ical to building intelligent agents that interact with humans. We present a systemthat learns to transformnatural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environ- ments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation in- structions for these environments is used for training and test- ing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the sys- tem is able to automatically learn to correctly interpret a rea- sonable fraction of the complex instructions in this corpus. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chen, D. L. and Mooney, R. J.},
booktitle = {AAAI},
doi = {10.1.1.221.8069/???},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Chen, Mooney/Chen, Mooney - Learning to Interpret Natural Language Navigation Instructions from Observations.pdf:pdf},
isbn = {9781577355083},
issn = {1938-7228},
keywords = {aa dd sgsafg},
number = {August},
pages = {859--865},
pmid = {23459267},
title = {{Learning to Interpret Natural Language Navigation Instructions from Observations}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/download/3701/3974},
year = {2011}
}
@inproceedings{Chen2008,
abstract = {We present a novel commentator system that learns language from sportscasts$\backslash$nof simulated soccer games. The system learns to parse and generate$\backslash$ncommentaries without any engineered knowledge about the English language.$\backslash$nTraining is done using only ambiguous supervision in the form of$\backslash$ntextual human commentaries and simulation states of the soccer games.$\backslash$nThe system simultaneously tries to establish correspondences between$\backslash$nthe commentaries and the simulation states as well as build a translation$\backslash$nmodel. We also present a novel algorithm, Iterative Generation Strategy$\backslash$nLearning (IGSL), for deciding which events to comment on. Human evaluations$\backslash$nof the generated commentaries indicate they are of reasonable quality$\backslash$ncompared to human commentaries.},
author = {Chen, D. L. and Mooney, R. J.},
booktitle = {ICML},
doi = {http://doi.acm.org/10.1145/1390156.1390173},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Chen, Mooney/Chen, Mooney - Learning to sportscast a test of grounded language acquisition.pdf:pdf},
isbn = {978-1-60558-205-4},
number = {July},
pages = {128--135},
title = {{Learning to sportscast: a test of grounded language acquisition}},
year = {2008}
}
@inproceedings{Chen2017,
abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution {\$}p(z){\$} and decoding distribution {\$}p(x|z){\$}, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
archivePrefix = {arXiv},
arxivId = {1611.02731},
author = {Chen, X. and Kingma, D. P. and Salimans, T. and Duan, Y. and Dhariwal, P. and Schulman, J. and Sutskever, I. and Abbeel, P.},
booktitle = {ICLR},
eprint = {1611.02731},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Chen et al/Chen et al. - Variational Lossy Autoencoder.pdf:pdf},
pages = {1--17},
title = {{Variational Lossy Autoencoder}},
url = {http://arxiv.org/abs/1611.02731},
year = {2017}
}
@inproceedings{Cho2014a,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, K. and cvan Merrienboer, B. and Gulcehre, C. and Bahdanau, D. and Bougares, F. and Schwenk, H. and Bengio, Y.},
booktitle = {EMNLP},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Cho et al/Cho et al. - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@inproceedings{Cho2014,
abstract = {Neural machine translation is a relatively new approach to statistical machine trans-lation based purely on neural networks. The neural machine translation models of-ten consist of an encoder and a decoder. The encoder extracts a fixed-length repre-sentation from a variable-length input sen-tence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the proper-ties of the neural machine translation us-ing two models; RNN Encoder–Decoder and a newly proposed gated recursive con-volutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance de-grades rapidly as the length of the sentence and the number of unknown words in-crease. Furthermore, we find that the pro-posed gated recursive convolutional net-work learns a grammatical structure of a sentence automatically.},
archivePrefix = {arXiv},
arxivId = {1409.1259},
author = {Cho, K. and van Merrienboer, B. and Bahdanau, D. and Bengio, Y.},
booktitle = {Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
doi = {10.3115/v1/W14-4012},
eprint = {1409.1259},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Cho et al/Cho et al. - On the Properties of Neural Machine Translation Encoder – Decoder Approaches.pdf:pdf},
isbn = {9781937284961},
pages = {103--111},
title = {{On the Properties of Neural Machine Translation : Encoder – Decoder Approaches}},
year = {2014}
}
@inproceedings{Choi2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.02786v4},
author = {Choi, J. and Yoo, K. M. and Lee, S.},
booktitle = {AAAI},
eprint = {arXiv:1707.02786v4},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Choi, Yoo, Lee/Choi, Yoo, Lee - Learning to Compose Task-Specific Tree Structures.pdf:pdf},
title = {{Learning to Compose Task-Specific Tree Structures}},
year = {2018}
}
@inproceedings{Chung2015,
abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
archivePrefix = {arXiv},
arxivId = {1506.02216},
author = {Chung, J. and Kastner, K. and Dinh, L. and Goel, K. and Courville, A. and Bengio, Y.},
booktitle = {NIPS},
eprint = {1506.02216},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Chung et al/Chung et al. - A Recurrent Latent Variable Model for Sequential Data.pdf:pdf},
issn = {10495258},
pages = {8},
title = {{A Recurrent Latent Variable Model for Sequential Data}},
url = {http://arxiv.org/abs/1506.02216},
year = {2015}
}
@inproceedings{Clarke2010,
abstract = {Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers},
annote = {Treat semantic parsing as a binary classification problem.},
author = {Clarke, J. and Goldwasser, D. and Chang, M. and Roth, D.},
booktitle = {CoNLL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Clarke et al/Clarke et al. - Driving Semantic Parsing from the World's Response.pdf:pdf},
isbn = {9781932432831},
keywords = {semantic parsing,weakly supervised},
mendeley-tags = {semantic parsing,weakly supervised},
number = {July},
pages = {18--27},
title = {{Driving Semantic Parsing from the World's Response}},
year = {2010}
}
@inproceedings{Cohen2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1801.10130v1},
author = {Cohen, T. S. and Geiger, M. and K{\"{o}}hler, J. and Welling, M.},
booktitle = {ICLR},
eprint = {arXiv:1801.10130v1},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Cohen et al/Cohen et al. - Spherical CNNs.pdf:pdf},
number = {3},
pages = {1--15},
title = {{Spherical CNNs}},
year = {2018}
}
@phdthesis{Collins1999,
author = {Collins, M.},
booktitle = {UPenn},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Collins/Collins - Head-Driven Statistical Models for Natural Language Parsing.pdf:pdf},
title = {{Head-Driven Statistical Models for Natural Language Parsing}},
year = {1999}
}
@article{Copestake2005,
abstract = {Minimal recursion semantics (MRS) is a framework for computational semantics that is suitable for parsing and generation and that can be implemented in typed feature structure formalisms. We discuss why, in general, a semantic representation with minimal structure is desirable and illustrate how a descriptively adequate representation with a nonrecursive structure may be achieved. MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition. We have integrated MRS with a broad-coverage HPSG grammar.},
author = {Copestake, A. and Flickinger, D. and Pollard, C. and Sag, I. A.},
doi = {10.1007/s11168-006-6327-9},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Copestake et al/Copestake et al. - Minimal recursion semantics An introduction.pdf:pdf},
issn = {15707075},
journal = {Research on Language and Computation},
keywords = {Computational semantics,Flat semantics,Grammar implementation,Semantic composition},
number = {4},
pages = {281--332},
title = {{Minimal recursion semantics: An introduction}},
volume = {3},
year = {2005}
}
@inproceedings{Cotterell2017,
abstract = {Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.},
archivePrefix = {arXiv},
arxivId = {1705.01684},
author = {Cotterell, R. and Eisner, J.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1109},
eprint = {1705.01684},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Cotterell, Eisner/Cotterell, Eisner - Probabilistic Typology Deep Generative Models of Vowel Inventories.pdf:pdf},
isbn = {9781945626753},
keywords = {best},
mendeley-tags = {best},
title = {{Probabilistic Typology: Deep Generative Models of Vowel Inventories}},
url = {http://arxiv.org/abs/1705.01684},
year = {2017}
}
@inproceedings{Dai2015,
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
archivePrefix = {arXiv},
arxivId = {1511.01432},
author = {Dai, A. M. and Le, Q. V.},
booktitle = {NIPS},
eprint = {1511.01432},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Dai, Le/Dai, Le - Semi-supervised Sequence Learning.pdf:pdf},
issn = {10495258},
pages = {1--10},
pmid = {414454},
title = {{Semi-supervised Sequence Learning}},
url = {http://arxiv.org/abs/1511.01432},
year = {2015}
}
@inproceedings{Dai2017,
abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation. The code would be released.},
archivePrefix = {arXiv},
arxivId = {1703.06211},
author = {Dai, J. and Qi, H. and Xiong, Y. and Li, Y. and Zhang, G. and Hu, H. and Wei, Y.},
booktitle = {ICCV},
doi = {10.1051/0004-6361/201527329},
eprint = {1703.06211},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Dai et al/Dai et al. - Deformable Convolutional Networks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pmid = {23459267},
title = {{Deformable Convolutional Networks}},
url = {http://arxiv.org/abs/1703.06211},
year = {2017}
}
@inproceedings{DeNero2008,
abstract = {We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previ- ous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrase- based translation systems.},
author = {DeNero, J. and Bouchard-cˆ, A. and Bouchard-Cote, A. and Klein, D.},
booktitle = {EMNLP},
doi = {10.3115/1613715.1613758},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/DeNero et al/DeNero et al. - Sampling alignment structure under a Bayesian translation model.pdf:pdf},
number = {October},
pages = {314--323},
title = {{Sampling alignment structure under a Bayesian translation model}},
url = {http://dl.acm.org/citation.cfm?id=1613758},
year = {2008}
}
@unpublished{Doersch2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1606.05908v2},
author = {Doersch, C.},
eprint = {arXiv:1606.05908v2},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Doersch/Doersch - Tutorial on Variational Autoencoders.pdf:pdf},
keywords = {neural networks,prediction,structured,unsupervised learning,variational autoencoders},
pages = {1--23},
title = {{Tutorial on Variational Autoencoders}},
year = {2016}
}
@inproceedings{Dong2016,
abstract = {Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.},
archivePrefix = {arXiv},
arxivId = {1601.01280},
author = {Dong, L. and Lapata, M.},
booktitle = {ACL},
doi = {10.18653/v1/P16-1004},
eprint = {1601.01280},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Dong, Lapata/Dong, Lapata - Language to Logical Form with Neural Attention.pdf:pdf},
isbn = {9781510827585},
pages = {33--43},
title = {{Language to Logical Form with Neural Attention}},
url = {http://arxiv.org/abs/1601.01280},
year = {2016}
}
@inproceedings{Dong2015,
abstract = {Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct exten- sive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.},
author = {Dong, L. and Wei, F. and Zhou, M. and Xu, K.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Dong et al/Dong et al. - Question Answering over Freebase with Multi-Column Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643723},
keywords = {convolutional,deep learning,question answering},
pages = {260--269},
pmid = {1683577},
title = {{Question Answering over Freebase with Multi-Column Convolutional Neural Networks}},
year = {2015}
}
@incollection{Doucet2011,
abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented},
author = {Doucet, A. and Johansen, A. M.},
booktitle = {Handbook of Nonlinear Filtering},
doi = {10.1.1.157.772},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Doucet, Johansen/Doucet, Johansen - A tutorial on particle filtering and smoothing fifteen years later.pdf:pdf},
isbn = {978-0199532902},
issn = {01677152},
number = {December},
pages = {656--704},
title = {{A tutorial on particle filtering and smoothing: fifteen years later}},
url = {http://automatica.dei.unipd.it/tl{\_}files/utenti/lucaschenato/Classes/PSC10{\_}11/Tutorial{\_}PF{\_}doucet{\_}johansen.pdf},
year = {2011}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, J. and Hazan, E. and Singer, Y.},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Duchi, Hazan, Singer/Duchi, Hazan, Singer - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {JMLR},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
pmid = {2868127},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@article{Edmonds2002,
abstract = {We develop a new computational model for representing the fine-grained meanings of near-synonyms and the differences between them. We also develop a lexical-choice process that can decide which of several near-synonyms is most appropriate in a particular situation. This research has direct applications in machine translation and text generation.We first identify the problems of representing near-synonyms in a computational lexicon and show that no previous model adequately accounts for near-synonymy. We then propose a preliminary theory to account for near-synonymy, relying crucially on the notion of granularity of representation, in which the meaning of a word arises out of a context-dependent combination of a context-independent core meaning and a set of explicit differences to its near-synonyms. That is, near-synonyms cluster together.We then develop a clustered model of lexical knowledge, derived from the conventional ontological model. The model cuts off the ontology at a coarse grain, thus avoiding an awkward proliferation of language-dependent concepts in the ontology, yet maintaining the advantages of efficient computation and reasoning. The model groups near-synonyms into subconceptual clusters that are linked to the ontology. A cluster differentiates near-synonyms in terms of fine-grained aspects of denotation, implication, expressed attitude, and style. The model is general enough to account for other types of variation, for instance, in collocational behavior.An efficient, robust, and flexible fine-grained lexical-choice process is a consequence of a clustered model of lexical knowledge. To make it work, we formalize criteria for lexical choice as preferences to express certain concepts with varying indirectness, to express attitudes, and to establish certain styles. The lexical-choice process itself works on two tiers: between clusters and between near-synonyns of clusters. We describe our prototype implementation of the system, called I-Saurus.},
author = {Edmonds, P. and Hirst, G.},
doi = {10.1162/089120102760173625},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Edmonds, Hirst/Edmonds, Hirst - Near-Synonymy and Lexical Choice.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
number = {2},
pages = {105--144},
title = {{Near-Synonymy and Lexical Choice}},
url = {http://www.mitpressjournals.org/doi/10.1162/089120102760173625},
volume = {28},
year = {2002}
}
@inproceedings{Eisenstein2008,
abstract = {This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven by lexical cohesion: the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the Bayesian framework. 1},
author = {Eisenstein, J. and Barzilay, Regina},
booktitle = {EMNLP},
doi = {10.3115/1613715.1613760},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Eisenstein, Barzilay/Eisenstein, Barzilay - Bayesian unsupervised topic segmentation.pdf:pdf},
number = {October},
pages = {334},
title = {{Bayesian unsupervised topic segmentation}},
url = {http://portal.acm.org/citation.cfm?doid=1613715.1613760},
year = {2008}
}
@inproceedings{Eisner2016,
abstract = {A probabilistic or weighted grammar implies a posterior probability distribution over possi-ble parses of a given input sentence. One often needs to extract information from this distri-bution, by computing the expected counts (in the unknown parse) of various grammar rules, constituents, transitions, or states. This re-quires an algorithm such as inside-outside or forward-backward that is tailored to the gram-mar formalism. Conveniently, each such al-gorithm can be obtained by automatically dif-ferentiating an " inside " algorithm that merely computes the log-probability of the evidence (the sentence). This mechanical procedure produces correct and efficient code. As for any other instance of back-propagation, it can be carried out manually or by software. This pedagogical paper carefully spells out the con-struction and relates it to traditional and non-traditional views of these algorithms.},
author = {Eisner, J.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Eisner/Eisner - Inside-Outside and Forward-Backward Algorithms Are Just Backprop.pdf:pdf},
pages = {1--17},
title = {{Inside-Outside and Forward-Backward Algorithms Are Just Backprop}},
year = {2016}
}
@unpublished{Etesami2016,
abstract = {Learning the influence structure of multiple time series data is of great interest to many disciplines. This paper studies the problem of recovering the causal structure in network of multivariate linear Hawkes processes. In such processes, the occurrence of an event in one process affects the probability of occurrence of new events in some other processes. Thus, a natural notion of causality exists between such processes captured by the support of the excitation matrix. We show that the resulting causal influence network is equivalent to the Directed Information graph (DIG) of the processes, which encodes the causal factorization of the joint distribution of the processes. Furthermore, we present an algorithm for learning the support of excitation matrix (or equivalently the DIG). The performance of the algorithm is evaluated on synthesized multivariate Hawkes networks as well as a stock market and MemeTracker real-world dataset.},
archivePrefix = {arXiv},
arxivId = {1603.04319},
author = {Etesami, J. and Kiyavash, N. and Zhang, K. and Singhal, K.},
eprint = {1603.04319},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Etesami et al/Etesami et al. - Learning Network of Multivariate Hawkes Processes A Time Series Approach.pdf:pdf},
isbn = {9781510827806},
pages = {1--14},
title = {{Learning Network of Multivariate Hawkes Processes: A Time Series Approach}},
url = {http://arxiv.org/abs/1603.04319},
year = {2016}
}
@inproceedings{Falke2017,
abstract = {Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.},
archivePrefix = {arXiv},
arxivId = {1704.04452},
author = {Falke, T. and Gurevych, I.},
booktitle = {EMNLP},
eprint = {1704.04452},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Falke, Gurevych/Falke, Gurevych - Bringing Structure into Summaries Crowdsourcing a Benchmark Corpus of Concept Maps.pdf:pdf},
pages = {2951--2961},
title = {{Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps}},
url = {http://arxiv.org/abs/1704.04452},
year = {2017}
}
@article{Feldman2004,
abstract = {In this paper, we outline an explicitly neural theory of language (NTL) that attempts to explain how many brain functions (including emotion and social cognition) work together to understand and learn language. The focus will be on the required representations and computations, although there will be some discussion of results on specific brain structures. In this approach, one does not expect to find brain areas specialized only for language or to find language processing confined to only a few areas.},
author = {Feldman, J. and Narayanan, S.},
doi = {10.1016/S0093-934X(03)00355-9},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Feldman, Narayanan/Feldman, Narayanan - Embodied meaning in a neural theory of language.pdf:pdf},
isbn = {0093-934X, 0093-934X},
issn = {0093934X},
journal = {Brain and Language},
number = {2},
pages = {385--392},
pmid = {15068922},
title = {{Embodied meaning in a neural theory of language}},
volume = {89},
year = {2004}
}
@inproceedings{Ferreira2016,
abstract = {In this study, we introduce a nondeterministic method for referring expression generation. We describe two models that account for individual variation in the choice of referential form in automatically generated text: a Naive Bayes model and a Recurrent Neural Network. Both are evaluated using the VaREG corpus. Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those produced by a random baseline model.},
author = {Ferreira, T. C. and Krahmer, E. and Wubben, S.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ferreira, Krahmer, Wubben/Ferreira, Krahmer, Wubben - Towards more variation in text generation Developing and evaluating variation models for choice of referenti.pdf:pdf},
isbn = {9781510827585},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {568--577},
title = {{Towards more variation in text generation: Developing and evaluating variation models for choice of referential form}},
url = {http://www.aclweb.org/anthology/P16-1054},
year = {2016}
}
@inproceedings{Fitzgerald2013,
abstract = {We present a new approach to referring expression generation, casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world. Despite an extremely large space of possible expressions, we demonstrate effective learning of a globally normalized log-linear distribution. This learning is enabled by a new, multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms. We train and evaluate the approach on a new corpus of references to sets of visual objects. Experiments show the approach is able to learn accurate models, which generate over 87{\%} of the expressions people used. Additionally, on the previously studied special case of single object reference, we show a 35{\%} relative error reduction over previous state of the art.},
author = {Fitzgerald, N. and Artzi, Y. and Zettlemoyer, L. S.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Fitzgerald, Artzi, Zettlemoyer/Fitzgerald, Artzi, Zettlemoyer - Learning Distributions over Logical Forms for Referring Expression Generation.pdf:pdf},
isbn = {9781937284978},
number = {October},
pages = {1914--1925},
title = {{Learning Distributions over Logical Forms for Referring Expression Generation}},
year = {2013}
}
@inproceedings{Fleischman2002,
author = {Fleischman, M. and Hovy, E.},
booktitle = {INLG},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Fleischman, Hovy/Fleischman, Hovy - Towards emotional variation in natural language generation.pdf:pdf},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {1--8},
title = {{Towards emotional variation in natural language generation}},
year = {2002}
}
@article{Ganchev2010,
abstract = {We present Posterior Regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efficiently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior Regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efficiency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efficient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment.},
author = {Ganchev, K. and Gillenwater, J.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ganchev, Gillenwater/Ganchev, Gillenwater - Posterior Regularization for Structured Latent Variable Models.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {JMLR},
keywords = {latent variables models,natural language processing,posterior regularization framework,prior knowledge,unsupervised learning},
number = {MS-CIS-09-16},
pages = {2001--2049},
title = {{Posterior Regularization for Structured Latent Variable Models}},
url = {http://dl.acm.org/citation.cfm?id=1756006.1859918},
volume = {11},
year = {2010}
}
@inproceedings{Gao2015,
abstract = {Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.},
archivePrefix = {arXiv},
arxivId = {1511.06062},
author = {Gao, Y. and Beijbom, O. and Zhang, N. and Darrell, T.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2016.41},
eprint = {1511.06062},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Gao et al/Gao et al. - Compact Bilinear Pooling.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
number = {2},
pages = {317--326},
title = {{Compact Bilinear Pooling}},
url = {http://arxiv.org/abs/1511.06062},
year = {2015}
}
@unpublished{Gatt2017,
abstract = {This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artificial intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of Natural Language Processing, with an emphasis on different evaluation methods and the relationships between them.},
archivePrefix = {arXiv},
arxivId = {1703.09902},
author = {Gatt, A. and Krahmer, E.},
eprint = {1703.09902},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Gatt, Krahmer/Gatt, Krahmer - Survey of the State of the Art in Natural Language Generation Core tasks, applications and evaluation.pdf:pdf},
number = {c},
pages = {1--111},
title = {{Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation}},
url = {http://arxiv.org/abs/1703.09902},
year = {2017}
}
@inproceedings{Ge2005,
abstract = {We introduce a learning semantic parser, SCISSOR, thatmaps natural-language sentences to a detailed, formal, meaning-representation language. It first uses an integrated statistical parser to produce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We present experimental results demonstrating that SCISSOR produces more accurate semantic representations than several previous approaches.},
author = {Ge, R. and Mooney, R.},
booktitle = {CoNLL},
doi = {10.3115/1706543.1706546},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ge, Mooney/Ge, Mooney - A Statistical Semantic Parser that Integrates Syntax and Semantics.pdf:pdf},
number = {June},
pages = {9--16},
title = {{A Statistical Semantic Parser that Integrates Syntax and Semantics}},
year = {2005}
}
@inproceedings{Ge2006,
abstract = {Semantic parsing is the task of mapping natural language sentences to complete formal meaning representations. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. In this paper, we investigate discriminative reranking upon a baseline semantic parser, SCISSOR, where the composition of meaning representations is guided by syntax. We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics. We report experimental results on two real applications, an interpreter for coaching instructions in robotic soccer and a natural-language database interface. The results show that reranking can improve the performance on the coaching interpreter, but not on the database interface.},
author = {Ge, R. and Mooney, R. J.},
booktitle = {ACL},
doi = {10.3115/1273073.1273107},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ge, Mooney/Ge, Mooney - Discriminative reranking for semantic parsing.pdf:pdf},
number = {July},
pages = {263},
title = {{Discriminative reranking for semantic parsing}},
url = {http://portal.acm.org/citation.cfm?id=1273107},
year = {2006}
}
@article{Gildea2000,
abstract = {We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gildea, D. and Jurafsky, D.},
doi = {10.3115/1075218.1075283},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Gildea, Jurafsky/Gildea, Jurafsky - Automatic labeling of semantic roles.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1972},
pages = {512--520},
pmid = {25246403},
title = {{Automatic labeling of semantic roles}},
url = {http://portal.acm.org/citation.cfm?doid=1075218.1075283},
year = {2000}
}
@article{Giles1992,
abstract = {We show that a recurrent, second-order neural network using a real-time, forward training algorithm readily learns to infer small regular grammars from positive and negative string training samples. We present simulations that show the effect of initial conditions, training set size and order, and neural network architecture. All simulations were performed with random initial weight strengths and usually converge after approximately a hundred epochs of training. We discuss a quantization algorithm for dynamically extracting finite state automata during and after training. For a well-trained neural net, the extracted automata constitute an equivalence class of state machines that are reducible to the minimal machine of the inferred grammar. We then show through simulations that many of the neural net state machines are dynamically stable, that is, they correctly classify many long unseen strings. In addition, some of these extracted automata actually outperform the trained neural network for classification...},
author = {Giles, C. L. and Miller, C. B. and Chen, D. and Chen, H. H. and Sun, G. Z. and Lee, Y. C.},
doi = {10.1162/neco.1992.4.3.393},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Giles et al/Giles et al. - Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
number = {3},
pages = {393--405},
title = {{Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.3.393{\#}.Vr3oBfnhAuU},
volume = {4},
year = {1992}
}
@inproceedings{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, R.},
booktitle = {ICCV},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Girshick/Girshick - Fast R-CNN.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
pages = {1440--1448},
pmid = {23739795},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}
@inproceedings{Girshick2014,
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, R. and Donahue, J. and Darrell, T. and Malik, J.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Girshick et al/Girshick et al. - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
pages = {2--9},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@unpublished{Gkatzia2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1610.08375v1},
author = {Gkatzia, D.},
eprint = {arXiv:1610.08375v1},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Gkatzia/Gkatzia - Content Selection in Data-to-Text Systems A Survey.pdf:pdf},
title = {{Content Selection in Data-to-Text Systems: A Survey}},
year = {2016}
}
@inproceedings{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
booktitle = {NIPS},
doi = {10.1017/CBO9781139058452},
eprint = {arXiv:1406.2661v1},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Goodfellow et al/Goodfellow et al. - Generative Adversarial Nets.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
pages = {2672--2680},
pmid = {1000183096},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@article{Gorniak2007,
abstract = {We introduce a computational theory of situated language understanding in which the meaning of words and utterances depends on the physical environment and the goals and plans of communication partners. According to the theory, concepts that ground linguistic meaning are neither internal nor external to language users, but instead span the objective-subjective boundary. To model the possible interactions between subject and object, the theory relies on the notion of perceived affordances: structured units of interaction that can be used for prediction at multiple levels of abstraction. Language understanding is treated as a process of filtering perceived affordances. The theory accounts for many aspects of the situated nature of human language use and provides a unified solution to a number of demands on any theory of language understanding including conceptual combination, prototypicality effects, and the generative nature of lexical items. To support the theory, we describe an implemented system that understands verbal commands situated in a virtual gaming environment. The implementation uses probabilistic hierarchical plan recognition to generate perceived affordances. The system has been evaluated on its ability to correctly interpret free-form spontaneous verbal commands recorded from unrehearsed game play between human players. The system is able to "step into the shoes" of human players and correctly respond to a broad range of verbal commands in which linguistic meaning depends on social and physical context. We quantitatively compare the system's predictions in response to direct player commands with the actions taken by human players and show generalization to unseen data across a range of situations and verbal constructions.},
author = {Gorniak, P. and Roy, D.},
doi = {10.1080/15326900701221199},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Gorniak, Roy/Gorniak, Roy - Situated language understanding as filtering perceived affordances.pdf:pdf},
isbn = {0364-0213 1551-6709},
issn = {0364-0213},
journal = {Cognitive Science},
keywords = {Games,Grounding,Language Understanding,Situated Speech,Theories of Concepts},
number = {2},
pages = {197--231},
pmid = {21635295},
title = {{Situated language understanding as filtering perceived affordances.}},
volume = {31},
year = {2007}
}
@inproceedings{Graca2008,
abstract = {The expectation maximization (EM) algorithm is a widely used$\backslash$nmaximum likeli- hood estimation procedure for statistical models$\backslash$nwhen the values of some of the variables in the model are not$\backslash$nobserved. Very often, however, our aim is primar- ily to ﬁnd a$\backslash$nmodel that assigns values to the latent variables that have$\backslash$nintended meaning for our data and maximizing expected likelihood$\backslash$nonly sometimes ac- complishes this. Unfortunately, it is$\backslash$ntypically difﬁcult to add even simple a-priori information about$\backslash$nlatent variables in graphical models without making the models$\backslash$noverly complex or intractable. In this paper, we present an$\backslash$nefﬁcient, principled way to inject rich constraints on the$\backslash$nposteriors of latent variables into the EM algorithm. Our method$\backslash$ncan be used to learn tractable graphical models that sat- isfy$\backslash$nadditional, otherwise intractable constraints. Focusing on$\backslash$nclustering and the alignment problem for statistical machine$\backslash$ntranslation, we show that simple, in- tuitive posterior$\backslash$nconstraints can greatly improve the performance over standard$\backslash$nbaselines and be competitive with more complex, intractable$\backslash$nmodels.},
author = {Gra{\c{c}}a, J. V. and Ganchev, K. and Taskar, B.},
booktitle = {NIPS},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Gra{\c{c}}a, Ganchev, Taskar/Gra{\c{c}}a, Ganchev, Taskar - Expectation Maximization and Posterior Constraints.pdf:pdf},
isbn = {160560352X},
keywords = {Mendeley Import (Jun 17)},
title = {{Expectation Maximization and Posterior Constraints}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.4545{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@article{V.Graca2010,
abstract = {Word-level alignment of bilingual text is a critical resource for a growing variety of tasks. Proba- bilistic models for word alignment present a fundamental trade-off between richness of captured constraints and correlations versus efficiency and tractability of inference. In this article, we use the Posterior Regularization framework (Gra¸ ca, Ganchev, and Taskar 2007) to incorporate complex constraints into probabilistic models during learning without changing the efficiency of the underlying model. We focus on the simple and tractable hidden Markov model, and present an efficient learning algorithm for incorporating approximate bijectivity and symmetry constraints. Models estimated with these constraints produce a significant boost in performance as measured by both precision and recall of manually annotated alignments for six language pairs. We also report experiments on two different tasks where word alignments are required: phrase-based machine translation and syntax transfer, and show promising improvements over standard methods},
author = {Gra{\c{c}}a, J. V. and Ganchev, K. and Taskar, B.},
doi = {10.1162/coli_a_00007},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Gra{\c{c}}a, Ganchev, Taskar/Gra{\c{c}}a, Ganchev, Taskar - Learning Tractable Word Alignment Models with Complex Constraints.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {3},
pages = {481--504},
title = {{Learning Tractable Word Alignment Models with Complex Constraints}},
volume = {36},
year = {2010}
}
@inproceedings{Grenager2005,
abstract = {The applicability of many current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.},
author = {Grenager, T. and Klein, D. and Manning, C. D.},
booktitle = {ACL},
doi = {10.3115/1219840.1219886},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Grenager, Klein, Manning/Grenager, Klein, Manning - Unsupervised Learning of Field Segmentation Models for Information Extraction.pdf:pdf},
isbn = {1932432515},
number = {June},
pages = {371--378},
title = {{Unsupervised Learning of Field Segmentation Models for Information Extraction}},
url = {http://portal.acm.org/citation.cfm?id=1219886{\&}dl=},
year = {2005}
}
@inproceedings{Guu2017,
abstract = {Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.},
archivePrefix = {arXiv},
arxivId = {1704.07926},
author = {Guu, K. and Pasupat, P. and Liu, E. Z. and Liang, P.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1097},
eprint = {1704.07926},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Guu et al/Guu et al. - From Language to Programs Bridging Reinforcement Learning and Maximum Marginal Likelihood.pdf:pdf},
isbn = {9781945626753},
pages = {1051--1062},
title = {{From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood}},
url = {http://arxiv.org/abs/1704.07926},
year = {2017}
}
@inproceedings{Hajishirzi2014,
author = {Hajishirzi, H. and Farhadi, A.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Hajishirzi, Farhadi/Hajishirzi, Farhadi - Multi-Resolution Language Grounding with Weak Supervision.pdf:pdf},
isbn = {9781937284961},
pages = {386--396},
title = {{Multi-Resolution Language Grounding with Weak Supervision}},
year = {2014}
}
@inproceedings{Hajishirzi2012,
abstract = {This paper presents an approach for learning to translate simple narratives, i.e., texts (sequences of sentences) describing dynamic systems, into coherent sequences of events without the need for labeled training data. Our approach incorporates domain knowledge in the form of preconditions and effects of events, and we show that it outperforms state-of-the-art supervised learning systems on the task of reconstructing RoboCup soccer games from their commentaries.},
archivePrefix = {arXiv},
arxivId = {1202.3728},
author = {Hajishirzi, H. and Hockenmaier, J. and Mueller, Erik T. and Amir, E.},
booktitle = {UAI},
eprint = {1202.3728},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Hajishirzi et al/Hajishirzi et al. - Reasoning about RoboCup Soccer Narratives.pdf:pdf},
isbn = {978-0-9749039-7-2},
title = {{Reasoning about RoboCup Soccer Narratives}},
url = {http://arxiv.org/abs/1202.3728},
year = {2012}
}
@article{Hallett2007,
abstract = {This article describes a method for composing fluent and complex natural language questions, while avoiding the standard pitfalls of free text queries. The method, based on Conceptual Authoring, is targeted at question-answering systems where reliability and transparency are critical, and where users cannot be expected to undergo extensive training in question composition. This scenario is found in most corporate domains, especially in applications that are risk-averse. We present a proof-of-concept system we have developed: a question-answering interface to a large repository of medical histories in the area of cancer. We show that the method allows users to successfully and reliably compose complex queries with minimal training.},
author = {Hallett, C. and Scott, D. and Power, R.},
doi = {10.1162/coli.2007.33.1.105},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Hallett, Scott, Power/Hallett, Scott, Power - Composing Questions through Conceptual Authoring.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
number = {1},
pages = {105--133},
title = {{Composing Questions through Conceptual Authoring}},
url = {http://www.mitpressjournals.org/doi/10.1162/coli.2007.33.1.105},
volume = {33},
year = {2007}
}
@inproceedings{Havrylov2017,
abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
archivePrefix = {arXiv},
arxivId = {1705.11192},
author = {Havrylov, S. and Titov, I.},
booktitle = {NIPS},
eprint = {1705.11192},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Havrylov, Titov/Havrylov, Titov - Emergence of Language with Multi-agent Games Learning to Communicate with Sequences of Symbols.pdf:pdf},
title = {{Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols}},
url = {http://arxiv.org/abs/1705.11192},
year = {2017}
}
@inproceedings{He2017a,
abstract = {We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 23.36.},
archivePrefix = {arXiv},
arxivId = {1703.10722},
author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2016.90},
eprint = {1703.10722},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/He et al/He et al. - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {1664-1078},
pages = {770--778},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1703.10722},
year = {2017}
}
@inproceedings{He2015,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224x224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170x faster than the recent leading method R-CNN (and 24-64x faster overall), while achieving better or comparable accuracy on Pascal VOC 2007.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
booktitle = {PAMI},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {1406.4729},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/He et al/He et al. - Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition.pdf:pdf},
isbn = {9783319105772},
issn = {01628828},
keywords = {Convolutional Neural Networks,Image Classification,Object Detection,Spatial Pyramid Pooling},
number = {9},
pages = {1904--1916},
pmid = {26353135},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
volume = {37},
year = {2015}
}
@inproceedings{He2017,
abstract = {We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10{\%} relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.},
author = {He, L. and Lee, K. and Lewis, M. and Zettlemoyer, L. S.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1044},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/He et al/He et al. - Deep Semantic Role Labeling What Works and What's Next.pdf:pdf},
isbn = {9781945626753},
pages = {473--483},
title = {{Deep Semantic Role Labeling: What Works and What's Next}},
url = {http://aclweb.org/anthology/P17-1044},
year = {2017}
}
@inproceedings{Heinzerling2017,
abstract = {We introduce automatic verification as a post-processing step for entity linking (EL). The proposed method trusts EL sys-tem results collectively, by assuming en-tity mentions are mostly linked correctly, in order to create a semantic profile of the given text using geospatial and temporal information, as well as fine-grained entity types. This profile is then used to auto-matically verify each linked mention indi-vidually, i.e., to predict whether it has been linked correctly or not. Verification allows leveraging a rich set of global and pairwise features that would be prohibitively expen-sive for EL systems employing global in-ference. Evaluation shows consistent im-provements across datasets and systems. In particular, when applied to state-of-the-art systems, our method yields an abso-lute improvement in linking performance of up to 1.7 F 1 on AIDA/CoNLL'03 and up to 2.4 F 1 on the English TAC KBP 2015 TEDL dataset.},
author = {Heinzerling, B. and Strube, M. and Lin, C.},
booktitle = {EACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Heinzerling, Strube, Lin/Heinzerling, Strube, Lin - Trust, but Verify! Better Entity Linking through Automatic Verification.pdf:pdf},
isbn = {9781510838604},
pages = {828--838},
title = {{Trust, but Verify! Better Entity Linking through Automatic Verification}},
volume = {1},
year = {2017}
}
@inproceedings{Hershcovich2017,
abstract = {We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.},
archivePrefix = {arXiv},
arxivId = {1704.00552},
author = {Hershcovich, D. and Abend, O. and Rappoport, A.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1104},
eprint = {1704.00552},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Hershcovich, Abend, Rappoport/Hershcovich, Abend, Rappoport - A Transition-Based Directed Acyclic Graph Parser for UCCA.pdf:pdf},
isbn = {9781945626753},
pages = {1127--1138},
title = {{A Transition-Based Directed Acyclic Graph Parser for UCCA}},
url = {http://arxiv.org/abs/1704.00552},
year = {2017}
}
@inproceedings{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, G. E. and Vinyals, O. and Dean, J.},
booktitle = {NIPS},
doi = {10.1063/1.4931082},
eprint = {1503.02531},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Hinton, Vinyals, Dean/Hinton, Vinyals, Dean - Distilling the Knowledge in a Neural Network.pdf:pdf},
isbn = {3531207857},
issn = {0022-2488},
pages = {1--9},
pmid = {18249735},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@inproceedings{Hu2017,
abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, much existing work has shown the benefits of enhancing spatial encoding. In this work, we focus on channels and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at slight computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251{\%}, achieving a 25{\%} relative improvement over the winning entry of 2016.},
archivePrefix = {arXiv},
arxivId = {1709.01507},
author = {Hu, J. and Shen, L. and Sun, G.},
booktitle = {ILSVRC},
eprint = {1709.01507},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Hu, Shen, Sun/Hu, Shen, Sun - Squeeze-and-Excitation Networks.pdf:pdf},
title = {{Squeeze-and-Excitation Networks}},
url = {http://arxiv.org/abs/1709.01507},
year = {2017}
}
@inproceedings{Hu2016,
abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
archivePrefix = {arXiv},
arxivId = {1603.06318},
author = {Hu, Z. and Ma, X. and Liu, Z. and Hovy, E. and Xing, E. P.},
booktitle = {ACL},
doi = {10.18653/v1/P16-1228},
eprint = {1603.06318},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Hu et al/Hu et al. - Harnessing Deep Neural Networks with Logic Rules.pdf:pdf},
isbn = {9781510827585},
issn = {1541-1672},
pages = {2410--2420},
pmid = {18925972},
title = {{Harnessing Deep Neural Networks with Logic Rules}},
url = {http://arxiv.org/abs/1603.06318},
year = {2016}
}
@inproceedings{Hu2017a,
abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
archivePrefix = {arXiv},
arxivId = {1703.00955},
author = {Hu, Z. and Yang, Z. and Liang, X. and Salakhutdinov, R. and Xing, E. P.},
booktitle = {ICML},
doi = {arXiv:1},
eprint = {1703.00955},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Hu et al/Hu et al. - Toward Controlled Generation of Text.pdf:pdf},
title = {{Toward Controlled Generation of Text}},
url = {http://arxiv.org/abs/1703.00955},
year = {2017}
}
@unpublished{Huang1989,
author = {Huang, X.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Huang/Huang - Semi-Continuous Hidden Markov Models.pdf:pdf},
title = {{Semi-Continuous Hidden Markov Models}},
year = {1989}
}
@unpublished{Huang2015,
abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
archivePrefix = {arXiv},
arxivId = {1508.01991},
author = {Huang, Z. and Xu, W. and Yu, K.},
doi = {10.18653/v1/P16-1101},
eprint = {1508.01991},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Huang, Xu, Yu/Huang, Xu, Yu - Bidirectional LSTM-CRF Models for Sequence Tagging.pdf:pdf},
isbn = {9781510827585},
issn = {1098-6596},
pmid = {25246403},
title = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
url = {http://arxiv.org/abs/1508.01991},
year = {2015}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, S. and Szegedy, C.},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ioffe, Szegedy/Ioffe, Szegedy - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {JMLR},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@inproceedings{Jang2017,
abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
archivePrefix = {arXiv},
arxivId = {1611.01144},
author = {Jang, E. and Gu, S. and Poole, B.},
booktitle = {ICLR},
eprint = {1611.01144},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Jang, Gu, Poole/Jang, Gu, Poole - Categorical Reparameterization with Gumbel-Softmax.pdf:pdf},
issn = {1611.01144},
pages = {1--13},
title = {{Categorical Reparameterization with Gumbel-Softmax}},
url = {http://arxiv.org/abs/1611.01144},
year = {2017}
}
@inproceedings{Jia2017,
abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of {\$}75\backslash{\%}{\$} F1 score to {\$}36\backslash{\%}{\$}; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to {\$}7\backslash{\%}{\$}. We hope our insights will motivate the development of new models that understand language more precisely.},
archivePrefix = {arXiv},
arxivId = {1707.07328},
author = {Jia, R. and Liang, P.},
booktitle = {EMNLP},
eprint = {1707.07328},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Jia, Liang/Jia, Liang - Adversarial Examples for Evaluating Reading Comprehension Systems.pdf:pdf},
pages = {2021--2031},
title = {{Adversarial Examples for Evaluating Reading Comprehension Systems}},
url = {http://arxiv.org/abs/1707.07328},
year = {2017}
}
@inproceedings{Jia2016,
abstract = {Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.},
archivePrefix = {arXiv},
arxivId = {1606.03622},
author = {Jia, R. and Liang, P.},
booktitle = {ACL},
doi = {10.18653/v1/P16-1002},
eprint = {1606.03622},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Jia, Liang/Jia, Liang - Data Recombination for Neural Semantic Parsing.pdf:pdf},
isbn = {9781510827585},
issn = {04194217},
pages = {12--22},
pmid = {22251136},
title = {{Data Recombination for Neural Semantic Parsing}},
url = {http://arxiv.org/abs/1606.03622},
year = {2016}
}
@inproceedings{Jiang2016,
author = {Jiang, Y. and Han, W. and Tu, K.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Jiang, Han, Tu/Jiang, Han, Tu - Unsupervised Neural Dependency Parsing.pdf:pdf},
number = {61503248},
pages = {763--771},
title = {{Unsupervised Neural Dependency Parsing}},
year = {2016}
}
@unpublished{Jing2017,
abstract = {The recent work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNN) in creating artistic fantastic imagery by separating and recombing the image content and style. This process of using CNN to migrate the semantic content of one image to different styles is referred to as Neural Style Transfer. Since then, Neural Style Transfer has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention from computer vision researchers and several methods are proposed to either improve or extend the original neural algorithm proposed by Gatys et al. However, there is no comprehensive survey presenting and summarizing recent Neural Style Transfer literature. This review aims to provide an overview of the current progress towards Neural Style Transfer, as well as discussing its various applications and open problems for future research.},
archivePrefix = {arXiv},
arxivId = {1705.04058},
author = {Jing, Y. and Yang, Y. and Feng, Z. and Ye, J. and Song, M.},
eprint = {1705.04058},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Jing et al/Jing et al. - Neural Style Transfer A Review.pdf:pdf},
title = {{Neural Style Transfer: A Review}},
url = {http://arxiv.org/abs/1705.04058},
year = {2017}
}
@inproceedings{Johansson2007,
abstract = {We describe a new method to convert En- glish constituent trees using the Penn Tree- bank annotation style into dependency trees. The new format was inspired by annota- tion practices used in other dependency tree- banks with the intention to produce a better interface to further semantic processing than existing methods. In particular, we used a richer set of edge labels and introduced links to handle long-distance phenomena such as wh-movement and topicalization. The resulting trees generally have a more complex dependency structure. For exam- ple, 6{\%}of the trees contain at least one non- projective link, which is difficult for many parsing algorithms. As can be expected, the more complex structure and the enriched set of edge labels make the trees more difficult to predict, and we observed a decrease in parsing accuracy when applying two depen- dency parsers to the new corpus. However, the richer information contained in the new trees resulted in a 23{\%} error reduction in a baseline FrameNet semantic role labeler that relied on dependency arc labels only.},
author = {Johansson, R. and Nugues, P.},
booktitle = {NODALIDA},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Johansson, Nugues/Johansson, Nugues - Extended constituent-to-dependency conversion for English.pdf:pdf},
isbn = {978-9985-4-0514-7},
pages = {105--112},
title = {{Extended constituent-to-dependency conversion for English}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.5018{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@inproceedings{Johnson2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.06277v5},
author = {Johnson, M. J. and Duvenaud, D. and Wiltschko, A. B. and Datta, S. R. and Adams, R. P.},
booktitle = {NIPS},
eprint = {arXiv:1603.06277v5},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Johnson et al/Johnson et al. - Composing graphical models with neural networks for structured representations and fast inference.pdf:pdf},
title = {{Composing graphical models with neural networks for structured representations and fast inference}},
year = {2016}
}
@article{Kamper2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.08135v2},
author = {Kamper, H. and Livescu, K. and Goldwater, S.},
eprint = {arXiv:1703.08135v2},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kamper, Livescu, Goldwater/Kamper, Livescu, Goldwater - An Embedded Segmental K-Means Model for Unsupervised Segmentation and Clustering of Speech.pdf:pdf},
journal = {Computational Linguistics},
title = {{An Embedded Segmental K-Means Model for Unsupervised Segmentation and Clustering of Speech}},
year = {2017}
}
@article{Karras2012,
abstract = {A number of methods for constructing bounding volume hierarchies and point-based octrees on the GPU are based on the idea of ordering primitives along a space-filling curve. A major shortcoming with these methods is that they construct levels of the tree sequentially, which limits the amount of parallelism that they can achieve. We present a novel approach that improves scalability by constructing the entire tree in parallel. Our main contribution is an in-place algorithm for constructing binary radix trees, which we use as a building block for other types of trees.},
archivePrefix = {arXiv},
arxivId = {cs/9903011},
author = {Karras, T.},
doi = {10.2312/EGGH/HPG12/033-037},
eprint = {9903011},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Karras/Karras - Maximizing Parallelism in the Construction of BVHs , Octrees , and k-d Trees.pdf:pdf},
isbn = {978-3-905674-41-5},
issn = {00043702},
journal = {High Performance Graphics},
pages = {33--37},
primaryClass = {cs},
title = {{Maximizing Parallelism in the Construction of BVHs , Octrees , and k-d Trees}},
year = {2012}
}
@inproceedings{Kate2007,
abstract = {This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.},
author = {Kate, R. J. and Mooney, R. J.},
booktitle = {AAAI},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kate, Mooney/Kate, Mooney - Learning language semantics from ambiguous supervision.pdf:pdf},
isbn = {9781577353232},
keywords = {Natural-Language Processing,Technical Papers},
number = {July},
pages = {895--900},
title = {{Learning language semantics from ambiguous supervision}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-142.pdf},
year = {2007}
}
@inproceedings{Kate2006,
author = {Kate, R. J. and Mooney, R. J.},
booktitle = {ACL},
doi = {10.3115/1220175.1220290},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kate, Mooney/Kate, Mooney - Using string-kernels for learning semantic parsers.pdf:pdf},
isbn = {1932432655},
number = {July},
pages = {913--920},
title = {{Using string-kernels for learning semantic parsers}},
url = {http://dl.acm.org/citation.cfm?id=1220290},
year = {2006}
}
@inproceedings{Kate2005,
abstract = {This paper presents a method for inducing transformation rules that map natural-language sentences into a formal query or command language. The approach assumes a formal gram- mar for the target representation language and learns trans- formation rules that exploit the non-terminal symbols in this grammar. The learned transformation rules incrementally map a natural-language sentence or its syntactic parse tree into a parse-tree for the target formal language. Experimental results are presented for two corpora, one which maps En- glish instructions into an existing formal coaching language for simulated RoboCup soccer agents, and another which maps EnglishU.S.-geography questions into a database query language. We show that our method performs overall better and faster than previous approaches in both domains.},
author = {Kate, R. J. and Wong, Y. W. and Mooney, R. J.},
booktitle = {AAAI},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kate, Wong, Mooney/Kate, Wong, Mooney - Learning to transform natural to formal languages.pdf:pdf},
isbn = {1-57735-236-x},
number = {3},
pages = {1062--1068},
title = {{Learning to transform natural to formal languages}},
url = {http://www.aaai.org/Library/AAAI/2005/aaai05-168.php},
volume = {20},
year = {2005}
}
@article{Kennedy1999,
author = {Kennedy, C. and McNally, L.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kennedy, McNally/Kennedy, McNally - From event structure to scale structure Degree modification in deverbal adjectives.pdf:pdf},
journal = {SALT},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
number = {2},
pages = {163--180},
title = {{From event structure to scale structure: Degree modification in deverbal adjectives}},
url = {http://elanguage.net/journals/salt/article/download/9.163/1681},
year = {1999}
}
@article{Kennedy2005,
abstract = {In this article we develop a semantic typology of gradable predicates, with special emphasis on deverbal adjectives. We argue for the linguistic relevance of this typology by demonstrating that the distribution and interpretation of degree modifiers is sensitive to its twomajor classificatory parameters: (1) whether a gradable predicate is associated with what we call an open or closed scale, and (2) whether the standard of comparison for the applicability of the predicate is absolute or relative to a context. We further showthat the classification of an important subclass of adjectives within the typology is largely predictable. Specifically, the scale structure of a deverbal gradable adjective correlates either with the algebraic part structure of the event denoted by its source verb or with the part structure of the entities to which the adjective applies. These correla- tions underscore the fact that gradability is characteristic not only of adjectives but also of verbs and nouns, and that scalar properties are shared by categorially distinct but derivationally related expressions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kennedy, C. and McNally, L.},
doi = {10.1353/lan.2005.0071},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kennedy, McNally/Kennedy, McNally - Scale Structure, Degree Modification and the Semantics of Gradable Predicates.pdf:pdf},
isbn = {00978507},
issn = {1535-0665},
journal = {Language},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
number = {2},
pages = {345--381},
pmid = {25246403},
title = {{Scale Structure, Degree Modification and the Semantics of Gradable Predicates}},
url = {http://muse.jhu.edu/content/crossref/journals/language/v081/81.2kennedy.pdf},
volume = {81},
year = {2005}
}
@inproceedings{Khani2016,
abstract = {Can we train a system that, on any new input, either says "don't know" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100{\%} precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.},
archivePrefix = {arXiv},
arxivId = {1606.06368},
author = {Khani, F. and Rinard, M. and Liang, P.},
booktitle = {ACL},
eprint = {1606.06368},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Khani, Rinard, Liang/Khani, Rinard, Liang - Unanimous Prediction for 100{\%} Precision with Application to Learning Semantic Mappings.pdf:pdf},
isbn = {9781510827585},
title = {{Unanimous Prediction for 100{\%} Precision with Application to Learning Semantic Mappings}},
url = {http://arxiv.org/abs/1606.06368},
year = {2016}
}
@inproceedings{Kim2010,
author = {Kim, J. and Mooney, R. J.},
booktitle = {COLING},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kim, Mooney/Kim, Mooney - Generative alignment and semantic parsing for learning from ambiguous supervision.pdf:pdf},
number = {2008},
pages = {543--551},
title = {{Generative alignment and semantic parsing for learning from ambiguous supervision}},
url = {http://dl.acm.org/citation.cfm?id=1944628{\%}5Cnpapers3://publication/uuid/EEC4CDC8-A12E-4837-8EF4-57020ABF3026},
year = {2010}
}
@inproceedings{Kim2012,
abstract = {“Grounded” language learning employs train- ing data in the form of sentences paired with relevant but ambiguous perceptual contexts. B¨ orschinger et al. (2011) introduced an ap- proach to grounded language learning based on unsupervised PCFG induction. Their ap- proach works well when each sentence po- tentially refers to one of a small set of pos- sible meanings, such as in the sportscasting task. However, it does not scale to prob- lems with a large set of potential meanings for each sentence, such as the navigation in- struction following task studied by Chen and Mooney (2011). This paper presents an en- hancement of the PCFG approach that scales to such problems with highly-ambiguous su- pervision. Experimental results on the naviga- tion task demonstrates the effectiveness of our approach.},
author = {Kim, J. and Mooney, R. J.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kim, Mooney/Kim, Mooney - Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision.pdf:pdf},
isbn = {9781937284435},
number = {July},
pages = {433--444},
title = {{Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision}},
url = {http://dl.acm.org/citation.cfm?id=2391001{\%}5Cnhttp://www.cs.utexas.edu/users/ai-lab/pubs/kim.emnlp12.pdf},
year = {2012}
}
@inproceedings{Kimmig2012,
abstract = {Probabilistic soft logic (PSL) is a framework for collective, probabilistic reasoning in relational domains. PSL uses first order logic rules as a template language for graphical models over random variables with soft truth values from the interval [0, 1]. Inference in this setting is a continuous optimization task, which can be solved efficiently. This paper provides an overview of the PSL language and its techniques for inference and weight learning. An implementation of PSL is available at http://psl.umiacs.umd.edu/.},
author = {Kimmig, A. and Bach, S. H. and Broecheler, M. and Huang, B. and Getoor, L.},
booktitle = {NIPS},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kimmig et al/Kimmig et al. - A Short Introduction to Probabilistic Soft Logic.pdf:pdf},
number = {1},
pages = {1--4},
title = {{A Short Introduction to Probabilistic Soft Logic}},
year = {2012}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, D. P. and Ba, J. L.},
booktitle = {ICLR},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kingma, Ba/Kingma, Ba - Adam A Method for Stochastic Optimization.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
pages = {1--15},
pmid = {172668},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@inproceedings{Kingma2014,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.5298v1},
author = {Kingma, D. P. and Rezende, D. J. and Mohamed, S. and Welling, M.},
booktitle = {NIPS},
eprint = {arXiv:1406.5298v1},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kingma et al/Kingma et al. - Semi-supervised Learning with Deep Generative Models.pdf:pdf},
issn = {10495258},
pages = {1--9},
title = {{Semi-supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
year = {2014}
}
@inproceedings{Kingma2015a,
abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
archivePrefix = {arXiv},
arxivId = {1506.02557},
author = {Kingma, D. P. and Salimans, T. and Welling, M.},
booktitle = {NIPS},
doi = {10.1016/S0733-8619(03)00096-3},
eprint = {1506.02557},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kingma, Salimans, Welling/Kingma, Salimans, Welling - Variational Dropout and the Local Reparameterization Trick.pdf:pdf},
isbn = {1506.02557},
issn = {10495258},
pages = {1--14},
pmid = {15062530},
title = {{Variational Dropout and the Local Reparameterization Trick}},
url = {http://arxiv.org/abs/1506.02557},
year = {2015}
}
@inproceedings{Kingma2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6114v10},
author = {Kingma, D. P. and Welling, M.},
booktitle = {ICLR},
eprint = {arXiv:1312.6114v10},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kingma, Welling/Kingma, Welling - Auto-Encoding Variational Bayes.pdf:pdf},
pages = {1--14},
title = {{Auto-Encoding Variational Bayes}},
year = {2014}
}
@inproceedings{Klein2002,
author = {Klein, D. and Manning, C. D.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Klein, Manning/Klein, Manning - Corpus-Based Induction of Syntactic Structure Models of Dependency and Constituency.pdf:pdf},
title = {{Corpus-Based Induction of Syntactic Structure : Models of Dependency and Constituency}},
year = {2002}
}
@inproceedings{Koehn2003,
author = {Koehn, P. and Och, F. J. and Marcu, D.},
booktitle = {HLT-NAACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Koehn, Och, Marcu/Koehn, Och, Marcu - Statistical phrase-based translation.pdf:pdf},
number = {June},
pages = {48--54},
title = {{Statistical Phrase-Based Translation}},
year = {2003}
}
@inproceedings{Koehn2003a,
author = {Koehn, P. and Och, F. J. and Marcu, D.},
booktitle = {NAACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Koehn, Och, Marcu/Koehn, Och, Marcu - Statistical phrase-based translation.pdf:pdf},
pages = {48--54},
title = {{Statistical phrase-based translation}},
year = {2003}
}
@inproceedings{Koh2017,
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
archivePrefix = {arXiv},
arxivId = {1703.04730},
author = {Koh, P. W. and Liang, P.},
booktitle = {ICML},
eprint = {1703.04730},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Koh, Liang/Koh, Liang - Understanding Black-box Predictions via Influence Functions.pdf:pdf},
issn = {1938-7228},
keywords = {best},
mendeley-tags = {best},
title = {{Understanding Black-box Predictions via Influence Functions}},
url = {http://arxiv.org/abs/1703.04730},
year = {2017}
}
@article{Konstas2013,
abstract = {Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. We present a joint model that captures content selection (" what to say ") and surface realization (" how to say ") in an unsupervised domain-independent fashion. Rather than breaking up the generation process into a sequence of local decisions, we define a probabilis-tic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). We recast generation as the task of finding the best derivation tree for a set of database records and describe an algorithm for decoding in this framework that allows to intersect the grammar with additional information capturing fluency and syntactic well-formedness constraints. Experimental evaluation on several domains achieves re-sults competitive with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.},
author = {Konstas, I. and Lapata, M.},
doi = {10.1613/jair.4025},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Konstas, Lapata/Konstas, Lapata - A global model for concept-to-text generation.pdf:pdf},
issn = {10769757},
journal = {JAIR},
pages = {305--346},
title = {{A global model for concept-to-text generation}},
volume = {48},
year = {2013}
}
@inproceedings{Konstas2012,
abstract = {This paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. A key insight in our ap-proach is to reduce the tasks of content se-lection (" what to say ") and surface realization (" how to say ") into a common parsing prob-lem. We define a probabilistic context-free grammar that describes the structure of the in-put (a corpus of database records and text de-scribing some of them) and represent it com-pactly as a weighted hypergraph. The hyper-graph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. We propose a novel decoding algorithm for finding the best scoring derivation and generating in this set-ting. Experimental evaluation on the ATIS do-main shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study.},
author = {Konstas, I. and Lapata, M.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Konstas, Lapata/Konstas, Lapata - Concept-to-text Generation via Discriminative Reranking.pdf:pdf},
isbn = {9781937284244},
number = {July},
pages = {369--378},
title = {{Concept-to-text Generation via Discriminative Reranking}},
year = {2012}
}
@inproceedings{Krifka2007,
abstract = {This paper gives an explanation of the well-known phenomenon that round numbers in measure terms (like one hundred meters) are interpreted in a more approximate way than non-round numbers (like one hundred and three meters). Several possible explanations are considered: First, a preference for short expressions and approximate interpretations; second, a conditional preference for short expressions under approximate interpretations; third, an explanation in terms of strategic communication that makes use of the fact that approximate interpretations, even if not favored initially, turn out to be more likely once the probability of the reported values are factored in. These explanations are shown to be flawed, in particular because the complexity of expressions does not always matter. The theory that is put forward makes use of scales that differ insofar as they are more or less fine-grained, and proposes a principle that a number expression is interpreted on the most coarse-grained scale that it occurs on. This principle can be motivated by strategic communication that factors in the overall likelihood of the message. The emerging theory is refined in various ways. In particular, it will be shown that complexity of expressions is important after all, but mainly on the evolutionary level, where it can be shown to lead to characteristic patterns of language change. The paper ends with the discussion of some surprising facts about the influence that the number system of a language has on which numbers are actually expressed in that language.},
author = {Krifka, M.},
booktitle = {Cognitive foundations of interpretation},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Krifka/Krifka - Approximate Interpretations of Number Words A case for strategic communication.pdf:pdf},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {1--16},
title = {{Approximate Interpretations of Number Words: A case for strategic communication}},
url = {http://edoc.hu-berlin.de/oa/bookchapters/reTiXQaSYryIs/PDF/22IE37nuLs8Tg.pdf},
year = {2007}
}
@inproceedings{Krishnamurthy2013,
abstract = {This paper introduces Logical Semantics with Perception (LSP), a model for grounded lan- guage acquisition that learns to map natu- ral language statements to their referents in a physical environment. For example, given an image, LSP can map the statement “blue mug on the table” to the set of image seg- ments showing blue mugs on tables. LSP learns physical representations for both cate- gorical (“blue,” “mug”) and relational (“on”) language, and also learns to compose these representations to produce the referents of en- tire statements. We further introduce a weakly supervised training procedure that estimates LSP's parameters using annotated referents for entire statements, without annotated ref- erents for individual words or the parse struc- ture of the statement. We perform experiments on two applications: scene understanding and geographical question answering. We find that LSP outperforms existing, less expressive models that cannot represent relational lan- guage. We further find that weakly supervised training is competitive with fully supervised training while requiring significantly less an- notation effort.},
author = {Krishnamurthy, J. and Kollar, T.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Krishnamurthy, Kollar/Krishnamurthy, Kollar - Jointly Learning to Parse and Perceive Connecting Natural Language to the Physical World.pdf:pdf},
issn = {2307-387X},
pages = {193--206},
title = {{Jointly Learning to Parse and Perceive: Connecting Natural Language to the Physical World}},
url = {http://rtw.ml.cmu.edu/tacl2013{\%}7B{\_}{\%}7Dlsp/tacl2013-krishnamurthy-kollar.pdf},
volume = {1},
year = {2013}
}
@inproceedings{Krishnamurthy2012a,
abstract = {We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependency- parsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-the- art accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80{\%} precision and 56{\%} recall, despite never having seen an annotated logical form.},
author = {Krishnamurthy, J. and Mitchell, T.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Krishnamurthy, Mitchell/Krishnamurthy, Mitchell - Weakly Supervised Training of Semantic Parsers.pdf:pdf},
isbn = {9781937284435},
number = {July},
pages = {754--765},
title = {{Weakly Supervised Training of Semantic Parsers}},
year = {2012}
}
@inproceedings{Krishnamurthy2012,
abstract = {We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependency- parsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-the- art accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80{\%} precision and 56{\%} recall, despite never having seen an annotated logical form.},
author = {Krishnamurthy, J. and Mitchell, T. M.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Krishnamurthy, Mitchell/Krishnamurthy, Mitchell - Weakly Supervised Training of Semantic Parsers.pdf:pdf},
isbn = {9781937284435},
number = {July},
pages = {754--765},
title = {{Weakly Supervised Training of Semantic Parsers}},
year = {2012}
}
@inproceedings{Kuncoro2017,
author = {Kuncoro, A. and Ballesteros, M. and Kong, L. and Dyer, C. and Neubig, G. and Smith, N. A.},
booktitle = {EACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kuncoro et al/Kuncoro et al. - What Do Recurrent Neural Network Grammars Learn About Syntax.pdf:pdf},
pages = {1249--1258},
title = {{What Do Recurrent Neural Network Grammars Learn About Syntax ?}},
volume = {1},
year = {2017}
}
@inproceedings{Kuru2016,
abstract = {We describe and evaluate a character-level tagger for language-independent Named Entity Recognition (NER). Instead of words, a sentence is represented as a sequence of characters. The model consists of stacked bidirectional LSTMs which inputs characters and outputs tag probabilities for each character. These probabilities are then converted to consistent word level named entity tags using a Viterbi decoder. We are able to achieve close to state-of-the-art NER performance in seven languages with the same basic model using only labeled NER data and no hand-engineered features or other external resources like syntactic taggers or Gazetteers.},
author = {Kuru, O. and Can, O. A. and Deniz, Y.},
booktitle = {COLING},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kuru, Can, Deniz/Kuru, Can, Deniz - CharNER Character-Level Named Entity Recognition.pdf:pdf},
pages = {911--921},
title = {{CharNER : Character-Level Named Entity Recognition}},
year = {2016}
}
@inproceedings{Kushman2013,
abstract = {We consider the problem of translating natural language text queries into regular expres- sions which represent their meaning. The mis- match in the level of abstraction between the natural language representation and the regu- lar expression representation make this a novel and challenging problem. However, a given regular expression can be written in many se- mantically equivalent forms, and we exploit this flexibility to facilitate translation by find- ing a form which more directly corresponds to the natural language. We evaluate our tech- nique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a state- of-the-art semantic parsing baseline, yielding a 29{\%} absolute improvement in accuracy.},
author = {Kushman, N. and Barzilay, R.},
booktitle = {NAACL-HLT},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kushman, Barzilay/Kushman, Barzilay - Using Semantic Unification to Generate Regular Expressions from Natural Language.pdf:pdf},
isbn = {9781937284473},
number = {June},
pages = {826--836},
title = {{Using Semantic Unification to Generate Regular Expressions from Natural Language}},
url = {http://www.aclweb.org/anthology/N13-1103},
year = {2013}
}
@inproceedings{Kwiatkowski2013,
abstract = {We consider the challenge of learning seman- tic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to rep- resent in a fixed target ontology. For ex- ample, even simple phrases such as ‘daugh- ter' and ‘number of people living in' can- not be directly represented in Freebase, whose ontology instead encodes facts about gen- der, parenthood, and population. In this pa- per, we introduce a new semantic parsing ap- proach that learns to resolve such ontologi- cal mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logical- form meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art per- formance on two benchmark semantic parsing datasets, including a nine point accuracy im- provement on a recent Freebase QA corpus.},
author = {Kwiatkowski, T. and Choi, E. and Artzi, Y. and Zettlemoyer, L. S.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kwiatkowski et al/Kwiatkowski et al. - Scaling Semantic Parsers with On-the-fly Ontology Matching.pdf:pdf},
isbn = {9781937284978},
number = {October},
pages = {1545--1556},
title = {{Scaling Semantic Parsers with On-the-fly Ontology Matching}},
url = {http://www.aclweb.org/anthology/D13-1161},
year = {2013}
}
@inproceedings{Kwiatkowski2010,
abstract = {This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously es- timating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 1},
author = {Kwiatkowski, T. and Zettlemoyer, L. S. and Goldwater, S. and Steedman, M.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Kwiatkowski et al/Kwiatkowski et al. - Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification.pdf:pdf},
isbn = {1932432868},
number = {October},
pages = {1223--1233},
title = {{Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification}},
year = {2010}
}
@inproceedings{Lebret2016,
abstract = {This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.},
archivePrefix = {arXiv},
arxivId = {1603.07771},
author = {Lebret, R. and Grangier, D. and Auli, M.},
booktitle = {EMNLP},
doi = {10.18653/v1/D16-1128},
eprint = {1603.07771},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Lebret, Grangier, Auli/Lebret, Grangier, Auli - Neural Text Generation from Structured Data with Application to the Biography Domain.pdf:pdf},
title = {{Neural Text Generation from Structured Data with Application to the Biography Domain}},
url = {http://arxiv.org/abs/1603.07771},
year = {2016}
}
@inproceedings{Lee2004,
author = {Lee, Y. K. and Ng, H. T. and Chia, T. K.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Lee, Ng, Chia/Lee, Ng, Chia - Supervised word sense disambiguation with support vector machines and multiple knowledge sources.pdf:pdf},
number = {July},
pages = {137--140},
title = {{Supervised word sense disambiguation with support vector machines and multiple knowledge sources}},
year = {2004}
}
@inproceedings{Lei2015,
abstract = {The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2{\%} accuracy on the fine-grained sentiment classification task.},
archivePrefix = {arXiv},
arxivId = {1508.04112},
author = {Lei, T. and Barzilay, R. and Jaakkola, T.},
booktitle = {EMNLP},
eprint = {1508.04112},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Lei, Barzilay, Jaakkola/Lei, Barzilay, Jaakkola - Molding CNNs for text non-linear, non-consecutive convolutions.pdf:pdf},
isbn = {9781941643327},
number = {September},
pages = {1565--1575},
title = {{Molding CNNs for text: non-linear, non-consecutive convolutions}},
url = {http://arxiv.org/abs/1508.04112},
year = {2015}
}
@inproceedings{Li2016,
abstract = {We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.},
archivePrefix = {arXiv},
arxivId = {1603.06155},
author = {Li, J. and Galley, M. and Brockett, C. and Spithourakis, G. P. and Gao, J. and Dolan, B.},
booktitle = {ACL},
eprint = {1603.06155},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Li et al/Li et al. - A Persona-Based Neural Conversation Model.pdf:pdf},
isbn = {9781510827585},
title = {{A Persona-Based Neural Conversation Model}},
url = {http://arxiv.org/abs/1603.06155},
year = {2016}
}
@unpublished{Liang2013a,
abstract = {This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.},
archivePrefix = {arXiv},
arxivId = {1309.4408},
author = {Liang, P.},
doi = {10.1162/COLI},
eprint = {1309.4408},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Liang/Liang - Lambda Dependency-Based Compositional Semantics.pdf:pdf},
isbn = {9781608459858},
issn = {04194217},
pages = {1--7},
pmid = {22251136},
title = {{Lambda Dependency-Based Compositional Semantics}},
url = {http://arxiv.org/abs/1309.4408},
year = {2013}
}
@article{Liang2016,
abstract = {For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are interested in learning semantic parsers from data, which introduces a new layer of statistical and computational issues. This article lays out the components of a statistical semantic parser, highlighting the key challenges. We will see that semantic parsing is a rich fusion of the logical and the statistical world, and that this fusion will play an integral role in the future of natural language understanding systems.},
archivePrefix = {arXiv},
arxivId = {1603.06677},
author = {Liang, P.},
doi = {10.1145/2866568},
eprint = {1603.06677},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Liang/Liang - Learning Executable Semantic Parsers for Natural Language Understanding.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
title = {{Learning Executable Semantic Parsers for Natural Language Understanding}},
url = {http://arxiv.org/abs/1603.06677},
year = {2016}
}
@article{Liang2014,
abstract = {Intended for a wide circle of specialists in automated systems. Above all, however, it is intended for those who work on systems for communicating with machines.},
author = {Liang, P.},
doi = {10.1145/2659831},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Liang/Liang - Talking to computers in natural language.pdf:pdf},
issn = {15284972},
journal = {Crossroads},
number = {1},
pages = {18--21},
title = {{Talking to computers in natural language}},
url = {http://dl.acm.org/citation.cfm?doid=2677339.2659831},
volume = {21},
year = {2014}
}
@inproceedings{Liang2009,
abstract = {A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.},
author = {Liang, P. and Jordan, M. I. and Klein, D.},
booktitle = {ACL-IJCNLP},
doi = {10.3115/1687878.1687893},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Liang, Jordan, Klein/Liang, Jordan, Klein - Learning Semantic Correspondences with Less Supervision.pdf:pdf},
isbn = {978-1-932432-45-9},
number = {August},
pages = {91--99},
title = {{Learning Semantic Correspondences with Less Supervision}},
year = {2009}
}
@article{Liang2013,
abstract = {Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.},
archivePrefix = {arXiv},
arxivId = {1109.6841v1},
author = {Liang, P. and Jordan, M. and Klein, D.},
doi = {10.1162/COLI_a_00127},
eprint = {1109.6841v1},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Liang/Liang - Lambda Dependency-Based Compositional Semantics.pdf:pdf},
isbn = {9781932432879},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {2},
pages = {389--446},
pmid = {25246403},
title = {{Learning Dependency-Based Compositional Semantics}},
url = {http://www.mitpressjournals.org/doi/10.1162/COLI{\_}a{\_}00127},
volume = {39},
year = {2013}
}
@article{Liang2015,
abstract = {Computational semantics has long been considered a field divided between logical and statistical approaches, but this divide is rapidly eroding with the development of statistical models that learn compositional semantic theories from corpora and databases. This review presents a simple discriminative learning framework for defining such models and relating them to logical theories. Within this framework, we discuss the task of learning to map utterances to logical forms (semantic parsing) and the task of learning from denotations with logical forms as latent variables. We also consider models that use distributed (e.g., vector) representations rather than logical ones, showing that these can be considered part of the same overall framework for understanding meaning and structural complexity.},
author = {Liang, P. and Potts, C.},
doi = {10.1146/annurev-linguist-030514-125312},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Liang, Potts/Liang, Potts - Bringing Machine Learning and Compositional Semantics Together.pdf:pdf},
isbn = {2333-9683},
issn = {2333-9683},
journal = {Annual Reviews of Linguistics},
keywords = {compositionality,discriminative learning,distributed representations,logical forms,mantic parsing,recursive neural networks,se-},
number = {1},
pages = {355--376},
title = {{Bringing Machine Learning and Compositional Semantics Together}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-linguist-030514-125312},
volume = {1},
year = {2015}
}
@inproceedings{Liang2006,
abstract = {We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32{\%} reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29{\%} reduction in AER over symmetrized IBM model 4 predictions.},
author = {Liang, P. and Taskar, B. and Klein, D.},
booktitle = {NAACL},
doi = {10.3115/1220835.1220849},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Liang, Taskar, Klein/Liang, Taskar, Klein - Alignment by agreement.pdf:pdf},
number = {June},
pages = {104--111},
title = {{Alignment by agreement}},
url = {http://portal.acm.org/citation.cfm?doid=1220835.1220849},
year = {2006}
}
@inproceedings{Lin2015,
abstract = {Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsupervised problems has been less thoroughly explored. In this paper, we show that embeddings can likewise add value to the problem of unsupervised POS induction. In two representative models of POS induction, we replace multinomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages. We also analyze the effect of various choices while inducing word embeddings on "downstream" POS induction results.},
archivePrefix = {arXiv},
arxivId = {1503.06760},
author = {Lin, C. and Ammar, W. and Dyer, C. and Levin, L.},
booktitle = {NAACL},
eprint = {1503.06760},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Lin et al/Lin et al. - Unsupervised POS Induction with Word Embeddings.pdf:pdf},
isbn = {9781941643495},
pages = {1311--1316},
title = {{Unsupervised POS Induction with Word Embeddings}},
url = {http://arxiv.org/abs/1503.06760},
year = {2015}
}
@inproceedings{Lin2003,
abstract = {Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.},
author = {Lin, C. and Hovy, E.},
booktitle = {NAACL},
doi = {10.3115/1073445.1073465},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Lin, Hovy/Lin, Hovy - Automatic evaluation of summaries using N-gram co-occurrence statistics.pdf:pdf},
keywords = {Human Language Technology,NAACL '03},
number = {June},
pages = {71--78},
title = {{Automatic evaluation of summaries using N-gram co-occurrence statistics}},
url = {http://portal.acm.org/citation.cfm?doid=1073445.1073465},
volume = {2003},
year = {2003}
}
@inproceedings{Lin2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.07889v3},
author = {Lin, T. and RoyChowdhury, A. and Maji, S.},
booktitle = {ICCV},
eprint = {arXiv:1504.07889v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Lin, RoyChowdhury, Maji/Lin, RoyChowdhury, Maji - Bilinear CNN Models for Fine-grained Visual Recognition.pdf:pdf},
title = {{Bilinear CNN Models for Fine-grained Visual Recognition}},
year = {2017}
}
@inproceedings{Lin2017a,
abstract = {Traditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the KB may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call it as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which can be used as seed mentions to bridge other mentions and the uninformative entities. In this work, we select most linkable mentions as seed mentions and disambiguate other mentions by comparing them with the seed mentions rather than directly with the entities. Our experiments on linking mentions to seven automatically mined lists show promising results and demonstrate the effectiveness of our approach.},
author = {Lin, Y. and Lin, C. and Ji, H.},
booktitle = {ACL},
doi = {10.18653/v1/P17-2085},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Lin, Lin, Ji/Lin, Lin, Ji - List-only Entity Linking.pdf:pdf},
pages = {536--541},
title = {{List-only Entity Linking}},
url = {http://aclweb.org/anthology/P17-2085},
year = {2017}
}
@inproceedings{Ling2016,
abstract = {Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.},
archivePrefix = {arXiv},
arxivId = {1603.06744},
author = {Ling, W. and Grefenstette, E. and Hermann, K. M. and Ko{\v{c}}isk{\'{y}}, T. and Senior, A. and Wang, F. and Blunsom, P.},
booktitle = {ACL},
eprint = {1603.06744},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ling et al/Ling et al. - Latent Predictor Networks for Code Generation.pdf:pdf},
isbn = {9781510827585},
pages = {599--609},
title = {{Latent Predictor Networks for Code Generation}},
url = {http://arxiv.org/abs/1603.06744},
year = {2016}
}
@inproceedings{Liu2018,
abstract = {Reinforcement learning (RL) agents improve through trial-and-error, but when reward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying to emails, where a single mistake can ruin the entire sequence of actions. A common remedy is to "warm-start" the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level "workflows" which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., "Step 1: click on a textbox; Step 2: enter some text"). Our exploration policy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent's ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 100x.},
archivePrefix = {arXiv},
arxivId = {1802.08802},
author = {Liu, E. Z. and Guu, K. and Pasupat, P. and Shi, T. and Liang, P.},
booktitle = {ICLR},
doi = {10.1051/0004-6361/201527329},
eprint = {1802.08802},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Liu et al/Liu et al. - Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--15},
pmid = {23459267},
title = {{Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration}},
url = {http://arxiv.org/abs/1802.08802},
year = {2018}
}
@inproceedings{Liu2017,
abstract = {Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels relational facts with knowledge bases. Previous sentence level denoise models don't achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training. To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During training, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms other state-of-the-art systems. Author{\{}4{\}}{\{}Affiliation{\}}},
author = {Liu, T. and Wang, K. and Chang, B. and Sui, Z.},
booktitle = {EMNLP},
doi = {10.18653/v1/D17-1189},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Liu et al/Liu et al. - Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration.pdf:pdf},
pages = {1790--1795},
title = {{A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction}},
url = {http://aclweb.org/anthology/D17-1189},
year = {2017}
}
@inproceedings{Louizos2017,
abstract = {Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1705.08665},
author = {Louizos, C. and Ullrich, K. and Welling, M.},
booktitle = {NIPS},
eprint = {1705.08665},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Louizos, Ullrich, Welling/Louizos, Ullrich, Welling - Bayesian Compression for Deep Learning.pdf:pdf},
title = {{Bayesian Compression for Deep Learning}},
url = {http://arxiv.org/abs/1705.08665},
year = {2017}
}
@inproceedings{Lu2011,
abstract = {"This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation."},
author = {Lu, W. and Ng, H. T.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Lu, Ng/Lu, Ng - A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions.pdf:pdf},
isbn = {1937284115},
pages = {1611--1622},
title = {{A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions}},
url = {http://www.aclweb.org/anthology/D11-1149{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2145605},
year = {2011}
}
@inproceedings{Lu2008,
abstract = {In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models.},
author = {Lu, W. and Ng, H. T. and Lee, W. S. and Zettlemoyer, L. S.},
booktitle = {EMNLP},
doi = {10.3115/1613715.1613815},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Lu et al/Lu et al. - A generative model for parsing natural language to meaning representations.pdf:pdf},
number = {October},
pages = {782--791},
title = {{A generative model for parsing natural language to meaning representations}},
url = {http://dl.acm.org/citation.cfm?id=1613815},
year = {2008}
}
@inproceedings{Ma2016,
abstract = {State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55$\backslash${\%} accuracy for POS tagging and 91.21$\backslash${\%} F1 for NER.},
archivePrefix = {arXiv},
arxivId = {1603.01354},
author = {Ma, X. and Hovy, E.},
booktitle = {ACL},
doi = {10.18653/v1/P16-1101},
eprint = {1603.01354},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ma, Hovy/Ma, Hovy - End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.pdf:pdf},
isbn = {9781510827585},
issn = {1098-6596},
pmid = {25246403},
title = {{End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF}},
url = {http://arxiv.org/abs/1603.01354},
year = {2016}
}
@inproceedings{Maddison2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.00712v3},
author = {Maddison, C. J. and Mnih, A. and Teh, Y.W.},
booktitle = {ICLR},
eprint = {arXiv:1611.00712v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Maddison, Mnih, Teh/Maddison, Mnih, Teh - The Concrete Distribution A Continuous Relaxation of Discrete Random Variables.pdf:pdf},
pages = {1--20},
title = {{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
year = {2017}
}
@inproceedings{Malisiewicz2011,
abstract = {This paper proposes a conceptually simple but surpris- ingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspon- dence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar- SVMs is thus defined by a single positive instance and mil- lions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generaliza- tion. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computa- tional cost increase. But the central benefit of our approach is that it creates an explicit association between each de- tection and a single training exemplar. Because most de- tections show good alignment to their associated exemplar, it is possible to transfer any available exemplar meta-data (segmentation, geometric structure, 3D model, etc.) directly onto the detections, which can then be used as part of over- all scene understanding.},
author = {Malisiewicz, T. and Gupta, A. and Efros, A. A.},
booktitle = {ICCV},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Malisiewicz, Gupta, Efros/Malisiewicz, Gupta, Efros - Ensemble of Exemplar SVMs for Object Detection and Beyond.pdf:pdf},
isbn = {9781457711022},
pages = {89--96},
title = {{Ensemble of Exemplar SVMs for Object Detection and Beyond}},
year = {2011}
}
@unpublished{Manning2000,
author = {Manning, C.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Manning/Manning - An Introduction to Formal Computational Semantics.pdf:pdf},
pages = {1--15},
title = {{An Introduction to Formal Computational Semantics}},
year = {2000}
}
@inproceedings{Manning2014,
abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Manning, C. and Surdeanu, M. and Bauer, J. and Finkel, J. and Bethard, S. and McClosky, D.},
booktitle = {ACL},
doi = {10.3115/v1/P14-5010},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Manning et al/Manning et al. - The Stanford CoreNLP Natural Language Processing Toolkit.pdf:pdf},
isbn = {9781941643006},
issn = {1098-6596},
pages = {55--60},
pmid = {25246403},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
url = {http://aclweb.org/anthology/P14-5010},
year = {2014}
}
@inproceedings{Matuszek2012,
abstract = {As robots become more ubiquitous and ca- pable, it becomes ever more important for untrained users to easily interact with them. Recently, this has led to study of the lan- guage grounding problem, where the goal is to extract representations of the mean- ings of natural language tied to the physi- cal world. We present an approach for joint learning of language and perception models for grounded attribute induction. The per- ception model includes classifiers for phys- ical characteristics and a language model based on a probabilistic categorial grammar that enables the construction of composi- tional meaning representations. We evaluate on the task of interpreting sentences that de- scribe sets of objects in a physical workspace, and demonstrate accurate task performance and effective latent-variable concept induc- tion in physical grounded scenes. 1.},
archivePrefix = {arXiv},
arxivId = {1206.6423},
author = {Matuszek, C. and FitzGerald, N. and Zettlemoyer, L. S. and Liefeng, B. and Fox, D.},
booktitle = {ICML},
eprint = {1206.6423},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Matuszek et al/Matuszek et al. - A Joint Model of Language and Perception for Grounded Attribute Learning.pdf:pdf},
isbn = {978-1-4503-1285-1},
pages = {1671--1678},
title = {{A Joint Model of Language and Perception for Grounded Attribute Learning}},
url = {http://arxiv.org/abs/1206.6423},
year = {2012}
}
@inproceedings{Matuszek2013,
abstract = {As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor envi- ronment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our for- mal representation allows a robot to interpret route instructions online while moving through a previously unknown environment.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Matuszek, C. and Herbst, E. and Zettlemoyer, L. S. and Fox, Dieter},
booktitle = {ISER},
doi = {10.1007/978-3-319-00065-7_28},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Matuszek et al/Matuszek et al. - Learning to Parse Natural Language Commands to a Robot Control System.pdf:pdf},
isbn = {978-3-319-00064-0},
issn = {21530858},
pages = {403--415},
pmid = {19886812},
title = {{Learning to Parse Natural Language Commands to a Robot Control System}},
url = {http://link.springer.com/10.1007/978-3-319-00065-7{\_}28},
year = {2013}
}
@inproceedings{Mei2015,
abstract = {We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59{\%} relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.},
archivePrefix = {arXiv},
arxivId = {1509.00838},
author = {Mei, H. and Bansal, M. and Walter, M. R.},
booktitle = {NAACL-HLT},
doi = {10.18653/v1/N16-1086},
eprint = {1509.00838},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Mei, Bansal, Walter/Mei, Bansal, Walter - What to talk about and how Selective Generation using LSTMs with Coarse-to-Fine Alignment.pdf:pdf},
isbn = {9781941643914},
pages = {720--730},
title = {{What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment}},
url = {http://arxiv.org/abs/1509.00838},
year = {2016}
}
@inproceedings{Mei2016,
abstract = {Many events occur in the world. Some event types are stochastically excited or inhibited---in the sense of having their probabilities elevated or decreased---by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.},
archivePrefix = {arXiv},
arxivId = {1612.09328},
author = {Mei, H. and Eisner, J.},
booktitle = {NIPS},
eprint = {1612.09328},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Mei, Eisner/Mei, Eisner - The Neural Hawkes Process A Neurally Self-Modulating Multivariate Point Process.pdf:pdf},
number = {Nips},
title = {{The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process}},
url = {http://arxiv.org/abs/1612.09328},
year = {2016}
}
@inproceedings{Miao2016,
abstract = {In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.},
archivePrefix = {arXiv},
arxivId = {1609.07317},
author = {Miao, Y. and Blunsom, P.},
booktitle = {EMNLP},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1609.07317},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Miao, Blunsom/Miao, Blunsom - Language as a Latent Variable Discrete Generative Models for Sentence Compression.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
pages = {319--328},
pmid = {26353135},
title = {{Language as a Latent Variable: Discrete Generative Models for Sentence Compression}},
url = {http://arxiv.org/abs/1609.07317},
year = {2016}
}
@inproceedings{Miao2015,
abstract = {Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.},
archivePrefix = {arXiv},
arxivId = {1511.06038},
author = {Miao, Y. and Yu, L. and Blunsom, P.},
booktitle = {ICML},
eprint = {1511.06038},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Miao, Blunsom/Miao, Blunsom - Language as a Latent Variable Discrete Generative Models for Sentence Compression.pdf:pdf},
isbn = {0898716004},
number = {Mcmc},
title = {{Neural Variational Inference for Text Processing}},
url = {http://arxiv.org/abs/1511.06038},
volume = {48},
year = {2015}
}
@inproceedings{Mihalcea2007,
abstract = {This paper investigates the problem of automatic humour recognition, and provides and in-depth analysis of two of the most frequently observed features of humorous text: human-centeredness and negative polarity. Through experiments performed on two collections of humorous texts, we show that these properties of verbal humour are consistent across different data sets. Springet-Verlag Berlin Heidelberg 2001.},
author = {Mihalcea, R. and Pulman, S.},
booktitle = {CICLing},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Mihalcea, Pulman/Mihalcea, Pulman - Characterizing Humour An Exploration of Features in Humorous Texts.pdf:pdf},
isbn = {354070938X},
issn = {03029743},
pages = {337--347},
title = {{Characterizing Humour : An Exploration of Features in Humorous Texts}},
year = {2007}
}
@inproceedings{Mikolov2013a,
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, T. and Chen, K. and Corrado, G. and Dean, J.},
booktitle = {NIPS},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Mikolov et al/Mikolov et al. - Distributed Representations of Words and Phrases and Their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and Their Compositionality}},
year = {2013}
}
@unpublished{Mikolov2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, T. and Corrado, G. and Chen, K. and Dean, J.},
eprint = {arXiv:1301.3781v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Mikolov et al/Mikolov et al. - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}
@inproceedings{Miller1996,
author = {Miller, S. and Stallard, D. and Bobrow, R. and Schwartz, R.},
booktitle = {ACL},
doi = {10.3115/981863.981871},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Miller et al/Miller et al. - A Fully Statistical Approach to Natural Language Interfaces.pdf:pdf},
pages = {55--61},
title = {{A Fully Statistical Approach to Natural Language Interfaces}},
url = {http://www.aclweb.org/anthology/P96-1008},
year = {1996}
}
@inproceedings{Misra2017,
abstract = {We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent's exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.},
archivePrefix = {arXiv},
arxivId = {1704.08795},
author = {Misra, D. and Langford, J. and Artzi, Y.},
booktitle = {EMNLP},
eprint = {1704.08795},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Misra, Langford, Artzi/Misra, Langford, Artzi - Mapping Instructions and Visual Observations to Actions with Reinforcement Learning.pdf:pdf},
title = {{Mapping Instructions and Visual Observations to Actions with Reinforcement Learning}},
url = {http://arxiv.org/abs/1704.08795},
year = {2017}
}
@inproceedings{Mooney2011,
author = {Mooney, R. J. and Chen, D. L.},
booktitle = {MLSLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Mooney, Chen/Mooney, Chen - Panning for gold finding relevant semantic content for grounded language learning.pdf:pdf},
number = {June},
title = {{Panning for gold: finding relevant semantic content for grounded language learning}},
year = {2011}
}
@inproceedings{Moore2004,
abstract = {We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1. We demonstrate reduction in alignment error rate of approximately 30{\%} resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.},
author = {Moore, R.C.},
booktitle = {ACL},
doi = {10.3115/1218955.1219021},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Moore/Moore - Improving IBM word-alignment model 1.pdf:pdf},
pages = {518--es},
title = {{Improving IBM word-alignment model 1}},
url = {http://portal.acm.org/citation.cfm?doid=1218955.1219021},
year = {2004}
}
@inproceedings{Murakami2017,
abstract = {This paper presents a novel encoder-decoder model for automatically generat-ing market comments from stock prices. The model first encodes both short-and long-term series of stock prices so that it can mention short-and long-term changes in stock prices. In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic op-eration such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best model generates market comments at the fluency and the informativeness approach-ing human-generated reference texts.},
author = {Murakami, S. and Watanabe, A. and Miyazawa, A. and Goshima, K. and Yanase, T. and Takamura, H. and Miyao, Y.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1126},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Murakami et al/Murakami et al. - Learning to Generate Market Comments from Stock Prices.pdf:pdf},
isbn = {9781945626753},
pages = {1374--1384},
title = {{Learning to Generate Market Comments from Stock Prices}},
url = {https://doi.org/10.18653/v1/P17-1126},
year = {2017}
}
@inproceedings{Narisawa2013,
author = {Narisawa, K. and Watanabe, Y. and Mizuno, J. and Okazaki, N. and Inui, K.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Narisawa et al/Narisawa et al. - Is a 204 cm Man Tall or Small Acquisition of Numerical Common Sense from the Web.pdf:pdf},
isbn = {9781937284503},
pages = {382--391},
title = {{Is a 204 cm Man Tall or Small? Acquisition of Numerical Common Sense from the Web.}},
url = {http://cse.iitk.ac.in/users/cs671/2013/hw3/narisawa-watanabe-13{\_}tall-or-small-acquiring-numerical-sense-from-web.pdf},
year = {2013}
}
@article{Navigli2007,
abstract = {Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data and/or do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks{\^{a}}€™ links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.},
archivePrefix = {arXiv},
arxivId = {1508.01346},
author = {Navigli, R.},
doi = {10.1145/1459352.1459355},
eprint = {1508.01346},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Navigli/Navigli - Word Sense Disambiguation A Survey.pdf:pdf},
isbn = {0360-0300},
issn = {10450823},
journal = {ACM Computing Surveys},
number = {2},
pages = {1725--1730},
pmid = {18353985},
title = {{Word Sense Disambiguation: A Survey}},
volume = {41},
year = {2007}
}
@incollection{Neal1998,
abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Neal, R. M. and Hinton, G. E.},
booktitle = {Learning in Graphical Models},
doi = {10.1007/978-94-011-5014-9_12},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Neal, Hinton/Neal, Hinton - A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants.pdf:pdf},
isbn = {0262600323},
issn = {978-1-932432-41-1},
pages = {355--368},
pmid = {15991970},
title = {{A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants}},
url = {http://link.springer.com/10.1007/978-94-011-5014-9{\_}12},
year = {1998}
}
@article{Och2003,
abstract = {We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.},
author = {Och, F. J. and Ney, H.},
doi = {10.1162/089120103321337421},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Och, Ney/Och, Ney - A Systematic Comparison of Various Statistical Alignment Models.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {19--51},
title = {{A Systematic Comparison of Various Statistical Alignment Models}},
url = {http://www.mitpressjournals.org/doi/10.1162/089120103321337421},
volume = {29},
year = {2003}
}
@inproceedings{Papineni2002,
abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
archivePrefix = {arXiv},
arxivId = {1702.00764},
author = {Papineni, K. and Roukos, S. and Ward, T. and Zhu, W.},
booktitle = {ACL},
doi = {10.3115/1073083.1073135},
eprint = {1702.00764},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Papineni et al/Papineni et al. - BLEU a method for automatic evaluation of machine translation.pdf:pdf},
isbn = {1-55860-883-4},
issn = {00134686},
number = {July},
pages = {311--318},
title = {{BLEU: a method for automatic evaluation of machine translation}},
url = {http://dl.acm.org/citation.cfm?id=1073135},
year = {2002}
}
@inproceedings{Pasupat2015,
abstract = {Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.},
archivePrefix = {arXiv},
arxivId = {1508.00305},
author = {Pasupat, P. and Liang, P.},
booktitle = {ACL},
doi = {10.3115/v1/P15-1142},
eprint = {1508.00305},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Pasupat, Liang/Pasupat, Liang - Compositional Semantic Parsing on Semi-Structured Tables.pdf:pdf},
isbn = {9781941643723},
title = {{Compositional Semantic Parsing on Semi-Structured Tables}},
url = {http://arxiv.org/abs/1508.00305},
year = {2015}
}
@inproceedings{Pasupat2014,
abstract = {In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction sys-tems rely on seed examples or redundancy across multiple web pages. In this paper, we consider a new zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page. Our approach de-fines a log-linear model over latent extrac-tion predicates, which select lists of enti-ties from the web page. The main chal-lenge is to define features on widely vary-ing candidate entity lists. We tackle this by abstracting list elements and using aggre-gate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline.},
author = {Pasupat, P. and Liang, P.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Pasupat, Liang/Pasupat, Liang - Zero-shot Entity Extraction from Web Pages.pdf:pdf},
isbn = {9781937284725},
pages = {391--401},
title = {{Zero-shot Entity Extraction from Web Pages}},
year = {2014}
}
@inproceedings{Peng2017,
abstract = {Neural attention models have achieved great success in different NLP tasks. How- ever, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we de- scribe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural atten- tion model and our results are also compet- itive against state-of-the-art systems that do not use extra linguistic resources.},
archivePrefix = {arXiv},
arxivId = {1702.05053},
author = {Peng, X. and Wang, C. and Gildea, D. and Xue, N.},
booktitle = {EACL},
eprint = {1702.05053},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Peng et al/Peng et al. - Addressing the Data Sparsity Issue in Neural AMR Parsing.pdf:pdf},
isbn = {9781510838604},
pages = {366--375},
title = {{Addressing the Data Sparsity Issue in Neural AMR Parsing}},
url = {http://arxiv.org/abs/1702.05053},
volume = {1},
year = {2017}
}
@inproceedings{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, M. E. and Neumann, M. and Iyyer, M. and Gardner, M. and Clark, C. and Lee, K. and Zettlemoyer, L. S.},
booktitle = {NAACL},
eprint = {1802.05365},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Peters et al/Peters et al. - Deep contextualized word representations.pdf:pdf},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}
@inproceedings{Piech2015,
abstract = {Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University's CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.},
archivePrefix = {arXiv},
arxivId = {1505.05969},
author = {Piech, C. and Huang, J. and Nguyen, A. and Phulsuksombati, M. and Sahami, M. and Guibas, L.},
booktitle = {ICML},
eprint = {1505.05969},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Piech et al/Piech et al. - Learning Program Embeddings to Propagate Feedback on Student Code.pdf:pdf},
isbn = {9781510810587 (ISBN)},
title = {{Learning Program Embeddings to Propagate Feedback on Student Code}},
url = {http://arxiv.org/abs/1505.05969},
volume = {37},
year = {2015}
}
@incollection{Pierrehumberl1990,
author = {Pierrehumberl, J. and Hirschberg, J.},
booktitle = {Intentions in Communication},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Pierrehumberl, Hirschberg/Pierrehumberl, Hirschberg - The Meaning of Intonational Contours in the Interpretation of Discourse.pdf:pdf},
title = {{The Meaning of Intonational Contours in the Interpretation of Discourse}},
year = {1990}
}
@inproceedings{Pitler2008,
abstract = {Knott (1996) provides an extensive of connectives and their properties13 It is important to select a word with some syntactic mo- tivation to an argument span, but due to the lack of consistent alignment between syntax and , we must},
author = {Pitler, E. and Raghupathy, M. and Mehta, H. and Nenkova, A. and Lee, A. and Joshi, A.},
booktitle = {COLING},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Pitler et al/Pitler et al. - Easily Identifiable Discourse Relations.pdf:pdf},
isbn = {9781905593446},
number = {June},
pages = {87--90},
title = {{Easily Identifiable Discourse Relations}},
url = {http://www.aclweb.org/anthology/C08-2022},
year = {2008}
}
@inproceedings{Poon2013,
abstract = {We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM. To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84{\%}, effectively tying with the best published results by supervised approaches.},
author = {Poon, H.},
booktitle = {ACL},
doi = {10.3115/1699510.1699512},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Poon/Poon - Grounded Unsupervised Semantic Parsing.pdf:pdf},
isbn = {9781932432596},
number = {2010},
pages = {933--943},
title = {{Grounded Unsupervised Semantic Parsing}},
year = {2013}
}
@inproceedings{Poon2009,
abstract = {We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.},
author = {Poon, H. and Domingos, P.},
booktitle = {EMNLP},
doi = {10.3115/1699510.1699512},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Poon, Domingos/Poon, Domingos - Unsupervised semantic parsing.pdf:pdf},
isbn = {9781932432596},
number = {August},
pages = {1--10},
title = {{Unsupervised semantic parsing}},
url = {http://portal.acm.org/citation.cfm?doid=1699510.1699512{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1699512},
volume = {1},
year = {2009}
}
@article{Power2012,
abstract = {We describe a computational model for planning phrases like “more than a quarter” and “25.9 per cent” which describe proportions at different levels of precision. The model lays out the key choices in planning a numerical description, using formal definitions of mathematical form (e.g., the distinction between fractions and percentages) and roundness adapted from earlier studies. The task is modeled as a constraint satisfaction problem, with solutions subsequently ranked by preferences (e.g., for roundness). Detailed constraints are based on a corpus of numerical expressions collected in the NUMGEN project, and evaluated through empirical studies in which subjects were asked to produce (or complete) numerical expressions in specified contexts.},
author = {Power, R. and Williams, S.},
doi = {10.1162/COLI_a_00086},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Power, Williams/Power, Williams - Generating Numerical Approximations.pdf:pdf},
issn = {0891-2017},
journal = {Computational Linguistics},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
number = {1},
pages = {113--134},
title = {{Generating Numerical Approximations}},
url = {http://www.mitpressjournals.org/doi/10.1162/COLI{\_}a{\_}00086},
volume = {38},
year = {2012}
}
@inproceedings{Quirk2015,
abstract = {Using natural language to write programs is a touchstone problem for computational linguistics. We present an approach that learns to map natural-language descrip-tions of simple " if-then " rules to executable code. By training and testing on a large cor-pus of naturally-occurring programs (called " recipes ") and their natural language de-scriptions, we demonstrate the ability to effectively map language to code. We compare a number of semantic parsing ap-proaches on the highly noisy training data collected from ordinary users, and find that loosely synchronous systems perform best.},
author = {Quirk, C. and Mooney, R. and Galley, M.},
booktitle = {ACL-IJCNLP},
doi = {10.3115/v1/P15-1085},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Quirk, Mooney, Galley/Quirk, Mooney, Galley - Language to Code Learning Semantic Parsers for If-This-Then-That Recipes.pdf:pdf},
isbn = {9781941643723},
pages = {878--888},
title = {{Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes}},
year = {2015}
}
@unpublished{Rabanser2017,
abstract = {Tensors are multidimensional arrays of numerical values and therefore generalize matrices to multiple dimensions. While tensors first emerged in the psychometrics community in the {\$}20{\^{}}{\{}\backslashtext{\{}th{\}}{\}}{\$} century, they have since then spread to numerous other disciplines, including machine learning. Tensors and their decompositions are especially beneficial in unsupervised learning settings, but are gaining popularity in other sub-disciplines like temporal and multi-relational data analysis, too. The scope of this paper is to give a broad overview of tensors, their decompositions, and how they are used in machine learning. As part of this, we are going to introduce basic tensor concepts, discuss why tensors can be considered more rigid than matrices with respect to the uniqueness of their decomposition, explain the most important factorization algorithms and their properties, provide concrete examples of tensor decomposition applications in machine learning, conduct a case study on tensor-based estimation of mixture models, talk about the current state of research, and provide references to available software libraries.},
archivePrefix = {arXiv},
arxivId = {1711.10781},
author = {Rabanser, S. and Shchur, O. and G{\"{u}}nnemann, S.},
eprint = {1711.10781},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Rabanser, Shchur, G{\"{u}}nnemann/Rabanser, Shchur, G{\"{u}}nnemann - Introduction to Tensor Decompositions and their Applications in Machine Learning.pdf:pdf},
pages = {1--13},
title = {{Introduction to Tensor Decompositions and their Applications in Machine Learning}},
url = {http://arxiv.org/abs/1711.10781},
year = {2017}
}
@article{Rabiner1989,
abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rabiner, L. R.},
doi = {10.1109/5.18626},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Rabiner/Rabiner - A tutorial on hidden Markov models and selected applications in speech recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {IEEE},
number = {2},
pages = {257--286},
pmid = {18626},
title = {{A tutorial on hidden Markov models and selected applications in speech recognition}},
url = {http://ieeexplore.ieee.org/ielx5/5/698/00018626.pdf?tp={\&}arnumber=18626{\&}isnumber=698{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=18626{\&}tag=1{\%}0Ahttp://ieeexplore.ieee.org/document/18626/},
volume = {77},
year = {1989}
}
@inproceedings{Raghunathan2018,
abstract = {While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most $\backslash$epsilon = 0.1 can cause more than 35{\%} test error.},
archivePrefix = {arXiv},
arxivId = {1801.09344},
author = {Raghunathan, A. and Steinhardt, J. and Liang, P.},
booktitle = {ICLR},
eprint = {1801.09344},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Raghunathan, Steinhardt, Liang/Raghunathan, Steinhardt, Liang - Certified Defenses against Adversarial Examples.pdf:pdf},
pages = {1--15},
title = {{Certified Defenses against Adversarial Examples}},
url = {http://arxiv.org/abs/1801.09344},
year = {2018}
}
@inproceedings{Rajpurkar2016,
abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0{\%}, a significant improvement over a simple baseline (20{\%}). However, human performance (86.8{\%}) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
archivePrefix = {arXiv},
arxivId = {1606.05250},
author = {Rajpurkar, P. and Zhang, J. and Lopyrev, K. and Liang, P.},
booktitle = {EMNLP},
doi = {10.18653/v1/D16-1264},
eprint = {1606.05250},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Rajpurkar et al/Rajpurkar et al. - SQuAD 100,000 Questions for Machine Comprehension of Text.pdf:pdf},
isbn = {9781941643327},
issn = {9781941643327},
keywords = {outstanding},
mendeley-tags = {outstanding},
number = {ii},
pmid = {299497},
title = {{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},
url = {http://arxiv.org/abs/1606.05250},
year = {2016}
}
@article{Ramos-Soto2015,
abstract = {We present in this paper an application that automatically generates textual short-term weather forecasts for every municipality in Galicia (NW Spain), using the real data provided by the Galician Meteorology Agency (MeteoGalicia). This solution combines in an innovative way computing with perceptions techniques and strategies for linguistic description of data, together with a natural language generation (NLG) system. The application, which is named GALiWeather, extracts relevant information from weather forecast input data and encodes it into intermediate descriptions using linguistic variables and temporal references. These descriptions are later translated into natural language texts by the NLG system. The obtained forecast results have been thoroughly validated by an expert meteorologist from MeteoGalicia using a quality assessment methodology, which covers two key dimensions of a text: the accuracy of its content and the correctness of its form. Following this validation, GALiWeather will be released as a real service, offering custom forecasts for a wide public.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4925v1},
author = {Ramos-Soto, A. and Bugar{\'{i}}n, A. and Barro, S. and Taboada, J.},
doi = {10.1109/TFUZZ.2014.2328011},
eprint = {arXiv:1411.4925v1},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ramos-Soto et al/Ramos-Soto et al. - Linguistic descriptions for automatic generation of textual short-term weather forecasts on real prediction data.pdf:pdf},
isbn = {9783642407680},
issn = {10636706},
journal = {TFS},
keywords = {Computing with perceptions (CWPs),Linguistic descriptions of data (LDD),Natural language generation (NLG),Open data},
number = {1},
pages = {44--57},
title = {{Linguistic descriptions for automatic generation of textual short-term weather forecasts on real prediction data}},
volume = {23},
year = {2015}
}
@inproceedings{Reddi2018,
abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of ex-ponential moving averages of squared past gradients. It has been empirically ob-served that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous anal-ysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with " long-term memory " of past gradi-ents, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
author = {Reddi, S. J. and Kale, S. and Kumar, S.},
booktitle = {ICLR},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Reddi, Kale, Kumar/Reddi, Kale, Kumar - On the convergence of adam and beyond.pdf:pdf},
pages = {1--23},
title = {{On the convergence of adam and beyond}},
url = {https://openreview.net/forum?id=ryQu7f-RZ{\%}0Ahttps://openreview.net/pdf?id=ryQu7f-RZ},
year = {2018}
}
@article{Reddy2014,
abstract = {In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the FREE 917 and WEBQUESTIONS benchmark datasets show our semantic parser improves over the state of the art.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Reddy, S. and Lapata, M. and Steedman, M.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Reddy, Lapata, Steedman/Reddy, Lapata, Steedman - Large-scale Semantic Parsing without Question-Answer Pairs.pdf:pdf},
isbn = {9782951740877},
issn = {2307-387X},
journal = {TACL},
pages = {377--392},
pmid = {25810777},
title = {{Large-scale Semantic Parsing without Question-Answer Pairs}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/398},
volume = {2},
year = {2014}
}
@inproceedings{Reddy2017,
abstract = {Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions. Our code and data can be downloaded at https://github.com/sivareddyg/udeplambda.},
archivePrefix = {arXiv},
arxivId = {1702.03196},
author = {Reddy, S. and T{\"{a}}ckstr{\"{o}}m, O. and Petrov, S. and Steedman, M. and Lapata, M.},
booktitle = {EMNLP},
eprint = {1702.03196},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Reddy et al/Reddy et al. - Universal Semantic Parsing.pdf:pdf},
pages = {89--101},
title = {{Universal Semantic Parsing}},
url = {http://arxiv.org/abs/1702.03196},
year = {2017}
}
@article{Reiter2005,
abstract = {One of the main challenges in automatically generating textual weather forecasts is choosing appropriate English words to communicate numeric weather data. A corpus-based analysis of how humans write forecasts showed that there were major differences in how individual writers performed this task, that is, in how they translated data into words. These differences included both different preferences between potential near-synonyms that could be used to express information, and also differences in the meanings that individual writers associated with specific words. Because we thought these differences could confuse readers, we built our SumTime-Mousam weather-forecast generator to use consistent data-to-word rules, which avoided words which were only used by a few people, and words which were interpreted differently by different people. An evaluation by forecast users suggested that they preferred SumTime-Mousam's texts to human-generated texts, in part because of better word choice; this may be the first time that an evaluation has shown that nlg texts are better than human-authored texts. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Reiter, E. and Sripada, S. and Hunter, J. and Yu, J. and Davy, I.},
doi = {10.1016/j.artint.2005.06.006},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Reiter et al/Reiter et al. - Choosing words in computer-generated weather forecasts.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {AI},
keywords = {Idiolect,Information presentation,Language and the word,Lexical choice,Natural language generation,Natural language processing,Weather forecasts,lexical choice},
mendeley-tags = {lexical choice},
number = {1-2},
pages = {137--169},
title = {{Choosing words in computer-generated weather forecasts}},
volume = {167},
year = {2005}
}
@inproceedings{Ritter2010,
author = {Ritter, A. and Cherry, C. and Dolan, W. B.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Ritter, Cherry, Dolan/Ritter, Cherry, Dolan - Data-Driven Response Generation in Social Media.pdf:pdf},
title = {{Data-Driven Response Generation in Social Media}},
year = {2010}
}
@unpublished{Rojas1997,
abstract = {This paper is a short and painless introduction to the $\lambda$ calculus. Originally developed in order to study some mathematical properties of effectively com- putable functions, this formalism has provided a strong theoretical foundation for the family of functional programming languages. We show how to perform some arithmetical computations using the $\lambda$ calculus and how to define recur- sive functions, even though functions in $\lambda$ calculus are not given names and thus cannot refer explicitly to themselves.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.09060v1},
author = {Rojas, R.},
doi = {10.1006/anbe.1999.1219},
eprint = {arXiv:1503.09060v1},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Rojas/Rojas - A Tutorial Introduction to the Lambda Calculus.pdf:pdf},
issn = {00033472},
pages = {1--9},
pmid = {10512656},
title = {{A Tutorial Introduction to the Lambda Calculus}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.136.4173{\&}rep=rep1{\&}type=pdf},
volume = {58},
year = {1997}
}
@unpublished{Rong2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.2738v4},
author = {Rong, X.},
eprint = {arXiv:1411.2738v4},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Rong/Rong - word2vec Parameter Learning Explained.pdf:pdf},
pages = {1--21},
title = {{word2vec Parameter Learning Explained}},
year = {2014}
}
@inproceedings{Roth2005,
author = {Roth, D.},
booktitle = {ICML},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Roth/Roth - Integer Linear Programming Inference for Conditional Random Fields.pdf:pdf},
title = {{Integer Linear Programming Inference for Conditional Random Fields}},
year = {2005}
}
@inproceedings{Sabour2017,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, S. and Frosst, N. and Hinton, G. E.},
booktitle = {NIPS},
eprint = {1710.09829},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Sabour, Frosst, Hinton/Sabour, Frosst, Hinton - Dynamic Routing Between Capsules.pdf:pdf},
number = {Nips},
title = {{Dynamic Routing Between Capsules}},
url = {http://arxiv.org/abs/1710.09829},
year = {2017}
}
@article{Salakhutdinov2007,
author = {Salakhutdinov, R. and Hinton, G. E.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Salakhutdinov, Hinton/Salakhutdinov, Hinton - Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure.pdf:pdf},
journal = {JMLR},
number = {1},
title = {{Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure}},
year = {2007}
}
@inproceedings{Salakhutdinov2003,
abstract = {We show a close relationship between the Expectation- Maximization (EM) algorithm and direct optimization algorithms such as gradientbased methods for parameter learning. We identify analytic conditions under which EM exhibits Newton-like behavior, and conditions under which it possesses poor, first-order convergence. Based on this analysis, we propose two novel algorithms for maximum likelihood estimation of latent variable models, and report empirical results showing that, as predicted by theory, the proposed new algorithms can substantially outperform standard EM in terms of speed of convergence in certain cases. 1.},
author = {Salakhutdinov, R. and Roweis, S. and Ghahramani, Z.},
booktitle = {ICML},
doi = {10.1145/ 1273496.1273497},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Salakhutdinov, Roweis, Ghahramani/Salakhutdinov, Roweis, Ghahramani - Optimization with EM and expectation-conjugate-gradient.pdf:pdf},
isbn = {1-57735-189-4},
keywords = {EM},
mendeley-tags = {EM},
number = {2},
pages = {672},
title = {{Optimization with EM and expectation-conjugate-gradient}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-088.pdf},
volume = {20},
year = {2003}
}
@inproceedings{Salimans2015,
abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
archivePrefix = {arXiv},
arxivId = {1410.6460},
author = {Salimans, T. and Kingma, D. P. and Welling, M.},
booktitle = {ICML},
eprint = {1410.6460},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Salimans, Kingma, Welling/Salimans, Kingma, Welling - Markov Chain Monte Carlo and Variational Inference Bridging the Gap.pdf:pdf},
isbn = {1410.6460},
title = {{Markov Chain Monte Carlo and Variational Inference: Bridging the Gap}},
url = {http://arxiv.org/abs/1410.6460},
year = {2015}
}
@inproceedings{Sarawagi2005,
abstract = {We describe semi-Markov conditional random fields (semi-CRFs), a con-ditionally trained version of semi-Markov chains. Intuitively, a semi-CRF on an input sequence x outputs a " segmentation " of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements x i of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time—often only a small constant factor slower than conventional CRFs. In experiments on five named entity recognition problems, semi-CRFs generally outper-form conventional CRFs.},
author = {Sarawagi, S. and Cohen, W. W.},
booktitle = {NIPS},
doi = {10.1.1.128.3524},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Sarawagi, Cohen/Sarawagi, Cohen - Semi-Markov Conditional Random Fields for Information Extraction.pdf:pdf},
isbn = {0262195348},
issn = {10495258},
pages = {1185--1192},
title = {{Semi-Markov Conditional Random Fields for Information Extraction}},
url = {http://papers.nips.cc/paper/2648-semi-markov-conditional-random-fields-for-information-extraction},
year = {2005}
}
@inproceedings{Sebastian2011,
author = {Sebastian, N. and Rother, C. and Bagon, S. and Sharp, T. and Yao, B. and Kohli, P.},
booktitle = {ICCV},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Sebastian et al/Sebastian et al. - Decision Tree Fields.pdf:pdf},
title = {{Decision Tree Fields}},
year = {2011}
}
@inproceedings{Sharan2017,
abstract = {We study the problem of learning overcomplete HMMs---those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results---both positive and negative---which help define the boundaries between the tractable and intractable settings. Specifically, we show positive results for a large subclass of HMMs whose transition matrices are sparse, well-conditioned, and have small probability mass on short cycles. On the other hand, we show that learning is impossible given only a polynomial number of samples for HMMs with a small output alphabet and whose transition matrices are random regular graphs with large degree. We also discuss these results in the context of learning HMMs which can capture long-term dependencies.},
archivePrefix = {arXiv},
arxivId = {1711.02309},
author = {Sharan, V. and Kakade, S. and Liang, P. and Valiant, G.},
booktitle = {NIPS},
eprint = {1711.02309},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Sharan et al/Sharan et al. - Learning Overcomplete HMMs.pdf:pdf},
pages = {1--10},
title = {{Learning Overcomplete HMMs}},
url = {http://arxiv.org/abs/1711.02309},
year = {2017}
}
@inproceedings{Shen2016,
abstract = {We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. Experiments on Chinese-English and English-French translation show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system.},
archivePrefix = {arXiv},
arxivId = {1512.02433},
author = {Shen, S. and Cheng, Y. and He, Z. and He, W. and Wu, H. and Sun, M. and Liu, Y.},
booktitle = {ACL},
eprint = {1512.02433},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Shen et al/Shen et al. - Minimum Risk Training for Neural Machine Translation.pdf:pdf},
pages = {1--9},
title = {{Minimum Risk Training for Neural Machine Translation}},
url = {http://arxiv.org/abs/1512.02433},
year = {2016}
}
@article{Shrivastava2011,
abstract = {The flow around an arrangement of two cylinders in tandem exhibits a remarkably complex behaviour that is of interest for many engineering problems, such as environmental flows or structural design. In the present paper, a Large Eddy Simulation using a staggered Cartesian grid has been performed for the flow around two cylinders in tandem of diameter D = 20 mm and height H = 50 mm submerged in an open channel with height h = 60 mm . The two axes have a streamwise spacing of 2D. The Reynolds number is 1500, based on the cylinder diameter and the free-stream velocity u . The results obtained show that no vortex shedding occurs in the gap between the two cylinders where the separated shear layers produced by the upstream cylinder reattach on the surface of the downstream one. The flow separates on the top of the first cylinder with the presence of two spiral nodes known as owl-face configuration. On top of the downstream cylinder, the flow is attached. A complex mean flow develops in the gap and also behind the second cylinder. Comparisons with PIV measurements reveal good general agreement, but there are differences concerning some details of the flow in the gap between the cylinders.},
author = {Shrivastava, A. and Malisiewicz, T. and Gupta, A. and Efros, A. A.},
doi = {10.1145/2070781.2024188},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Shrivastava et al/Shrivastava et al. - Data-driven visual similarity for cross-domain image matching.pdf:pdf},
isbn = {9781450308076},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {image matching,image re-,paintings,re-photography,saliency,sketches,trieval,visual memex,visual similarity},
number = {6},
pages = {1},
title = {{Data-driven visual similarity for cross-domain image matching}},
url = {http://dl.acm.org/citation.cfm?doid=2070781.2024188},
volume = {30},
year = {2011}
}
@article{Silver2017,
author = {Silver, D. and Huang, A. and Maddison, C. J. and Guez, A. and Sifre, L. and van den Driessche, G. and Schrittwieser, J. and Antonoglou, I. and Panneershelvam, V. and Lanctot, M. and Dieleman, S. and Grewe, D. and Nham, J. and Kalchbrenner, N. and Sutskever, I. and Lillicrap, T. and Leach, M. and Kavukcuoglu, K. and Graepel, T. and Hassabis, D.},
doi = {10.1038/nature16961},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Silver et al/Silver et al. - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7585},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2017}
}
@article{Siskind1996,
abstract = {This paper presents a computational study of part of the lexical-acquisition task faced by children, namely the acquisition of word-to-meaning mappings. It first approximates this task as a formal mathematical problem. It then presents an implemented algorithm for solving this problem, illustrating its operation on a small example. This algorithm offers one precise interpretation of the intuitive notions of cross-situational learning and the principle of contrast applied between words in an utterance. It robustly learns a homonymous lexicon despite noisy multi-word input, in the presence of referential uncertainty, with no prior knowledge that is specific to the language being learned. Computational simulations demonstrate the robustness of this algorithm and illustrate how algorithms based on cross-situational learning and the principle of contrast might be able to solve lexical-acquisition problems of the size faced by children, under weak, worst-case assumptions about the type and quantity of data available.},
author = {Siskind, J. M.},
doi = {10.1016/S0010-0277(96)00728-7},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Siskind/Siskind - A computational study of cross-situational techniques for learning word-to-meaning mappings.pdf:pdf},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
number = {1-2},
pages = {39--91},
pmid = {8990968},
title = {{A computational study of cross-situational techniques for learning word-to-meaning mappings}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027796007287},
volume = {61},
year = {1996}
}
@inproceedings{Smiley2016,
abstract = {For data-to-text tasks in Natural Language Generation (NLG), researchers are often faced with choices about the right words to express phenomena seen in the data. One common phenomenon centers around the description of trends between two data points and selecting the appropriate verb to express both the di-rection and intensity of movement. Our re-search shows that rather than simply select-ing the same verbs again and again, variation and naturalness can be achieved by quantify-ing writers' patterns of usage around verbs.},
author = {Smiley, C. and Plachouras, V. and Schilder, F. and Bretz, H. and Leidner, J. L. and Song, D.},
booktitle = {INLG},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Smiley et al/Smiley et al. - When to Plummet and When to Soar Corpus Based Verb Selection for Natural Language Generation.pdf:pdf},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {36--39},
title = {{When to Plummet and When to Soar: Corpus Based Verb Selection for Natural Language Generation}},
year = {2016}
}
@inproceedings{Snyder2007,
abstract = {This paper addresses the task of aligning a database with a corresponding text. The goal is to link individual database entries with sentences that verbalize the same information. By providing explicit semantics-to-text links, these alignments can aid the training of natural language generation and information extraction systems. Beyond these pragmatic benefits, the alignment problem is appealing from a modeling perspective: the mappings between database entries and text sentences exhibit rich structural dependencies, unique to this task. Thus, the key challenge is to make use of as many global dependencies as possible without sacrificing tractability. To this end, we cast text-database alignment as a structured multilabel classification task where each sentence is labeled with a subset of matching database entries. In contrast to existing multilabel classifiers, our approach operates over arbitrary global features of inputs and proposed labels. We compare our model with a baseline classifier that makes locally optimal decisions. Our results show that the proposed model yields a 15{\%} relative reduction in error, and compares favorably with human performance.},
author = {Snyder, B. and Barzilay, R.},
booktitle = {IJCAI},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Snyder, Barzilay/Snyder, Barzilay - Database-text alignment via structured multilabel classification.pdf:pdf},
issn = {10450823},
keywords = {information extraction,natural language processing},
pages = {1713--1718},
title = {{Database-text alignment via structured multilabel classification}},
year = {2007}
}
@unpublished{Sripada,
author = {Sripada, S. G. and Reiter, E. and Hunter, J. and Yu, J.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Sripada et al/Sripada et al. - SumTime-Meteo Parallel Corpus of Naturally Occurring Forecast Texts and Weather Data.pdf:pdf},
pages = {1--13},
title = {{SumTime-Meteo: Parallel Corpus of Naturally Occurring Forecast Texts and Weather Data}},
year = {2003}
}
@inproceedings{Srivastava2015,
abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
archivePrefix = {arXiv},
arxivId = {1505.00387},
author = {Srivastava, R. K. and Greff, K. and Schmidhuber, J.},
booktitle = {ICML},
eprint = {1505.00387},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Srivastava, Greff, Schmidhuber/Srivastava, Greff, Schmidhuber - Highway Networks.pdf:pdf},
title = {{Highway Networks}},
url = {http://arxiv.org/abs/1505.00387},
year = {2015}
}
@inproceedings{Srivastava2017,
abstract = {Natural language constitutes a predomi-nant medium for much of human learn-ing and pedagogy. We consider the prob-lem of concept learning from natural lan-guage explanations, and a small number of labeled examples of the concept. For example, in learning the concept of a phish-ing email, one might say 'this is a phishing email because it asks for your bank account number'. Solving this problem involves both learning to interpret open-ended nat-ural language statements, as well as learn-ing the concept itself. We present a joint model for (1) language interpretation (se-mantic parsing) and (2) concept learning (classification) that does not require label-ing statements with logical forms. Instead, the model prefers discriminative interpre-tations of statements in context of observ-able features of the data as a weak signal for parsing. On a dataset of email-related concepts, this approach yields across-the-board improvements in classification per-formance, with a 30{\%} relative improve-ment in F1 score over competitive classifi-cation methods in the low data regime.},
author = {Srivastava, S. and Labutov, S. and Mitchell, T.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Srivastava, Labutov, Mitchell/Srivastava, Labutov, Mitchell - Joint Concept Learning and Semantic Parsing from Natural Language Explanations.pdf:pdf},
pages = {1527--1536},
title = {{Joint Concept Learning and Semantic Parsing from Natural Language Explanations}},
url = {http://aclweb.org/anthology/D17-1161},
year = {2017}
}
@inproceedings{Stede2000,
author = {Stede, M.},
booktitle = {INLG},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Stede/Stede - The hyperonym problem revisited Conceptual and lexical hierarchies in language.pdf:pdf},
keywords = {Lexical choice,lexical choice},
mendeley-tags = {lexical choice},
pages = {93--99},
title = {{The hyperonym problem revisited: Conceptual and lexical hierarchies in language}},
year = {2000}
}
@inproceedings{Steinhardt2017,
abstract = {Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12{\%} to 23{\%} test error by adding only 3{\%} poisoned data.},
archivePrefix = {arXiv},
arxivId = {1706.03691},
author = {Steinhardt, J. and Koh, P. W. and Liang, P.},
booktitle = {NIPS},
eprint = {1706.03691},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Steinhardt, Koh, Liang/Steinhardt, Koh, Liang - Certified Defenses for Data Poisoning Attacks.pdf:pdf},
number = {i},
title = {{Certified Defenses for Data Poisoning Attacks}},
url = {http://arxiv.org/abs/1706.03691},
year = {2017}
}
@inproceedings{Strapparava2005,
author = {Strapparava, C.},
booktitle = {EMNLP-HLT},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Strapparava/Strapparava - Making Computers Laugh Investigations in Automatic Humor Recognition.pdf:pdf},
number = {October},
pages = {531--538},
title = {{Making Computers Laugh : Investigations in Automatic Humor Recognition}},
year = {2005}
}
@inproceedings{Surdeanu2012,
abstract = {Distant supervision for relation extraction (RE) -- gathering training data by aligning a database of facts with text -- is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains.},
archivePrefix = {arXiv},
arxivId = {1706.05075},
author = {Surdeanu, M. and Tibshirani, J. and Nallapati, R. and Manning, C. D.},
booktitle = {EMNLP},
doi = {10.3115/v1/D14-1200},
eprint = {1706.05075},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Surdeanu et al/Surdeanu et al. - Multi-instance Multi-label Learning for Relation Extraction.pdf:pdf},
isbn = {9781937284435},
keywords = {Relation Extraction},
number = {July},
pages = {455--465},
pmid = {91150},
title = {{Multi-instance Multi-label Learning for Relation Extraction}},
url = {http://dl.acm.org/citation.cfm?id=2390948.2391003},
year = {2012}
}
@inproceedings{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, I. and Vinyals, O. and Le, Q. V.},
booktitle = {NIPS},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Sutskever, Vinyals, Le/Sutskever, Vinyals, Le - Sequence to sequence learning with neural networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@inproceedings{Taghipour2015,
abstract = {One of the weaknesses of current supervised word sense disambiguation (WSD) systems is that they only treat a word as a discrete en-tity. However, a continuous-space represen-tation of words (word embeddings) can pro-vide valuable information and thus improve generalization accuracy. Since word embed-dings are typically obtained from unlabeled data using unsupervised methods, this method can be seen as a semi-supervised word sense disambiguation approach. This paper investi-gates two ways of incorporating word embed-dings in a word sense disambiguation setting and evaluates these two methods on some Sen-sEval/SemEval lexical sample and all-words tasks and also a domain-specific lexical sam-ple task. The obtained results show that such representations consistently improve the ac-curacy of the selected supervised WSD sys-tem. Moreover, our experiments on a domain-specific dataset show that our supervised base-line system beats the best knowledge-based systems by a large margin.},
author = {Taghipour, K.},
booktitle = {HLT},
doi = {10.3115/v1/N15-1035},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Taghipour/Taghipour - Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains.pdf:pdf},
pages = {314--323},
title = {{Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains}},
year = {2015}
}
@inproceedings{Talmor2018,
archivePrefix = {arXiv},
arxivId = {1803.06643},
author = {Talmor, A. and Berant, J.},
booktitle = {NAACL},
eprint = {1803.06643},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Talmor, Berant/Talmor, Berant - The Web as a Knowledge-base for Answering Complex Questions.pdf:pdf},
pages = {1--10},
title = {{The Web as a Knowledge-base for Answering Complex Questions}},
year = {2018}
}
@inproceedings{Tellex2011,
abstract = {This paper describes a new model for understanding natural language commands given to autonomous systems that perform navigation and mobile manipulation in semi-structured environments. Previous approaches have used models with fixed structure to infer the likelihood of a sequence of actions given the environment and the command. In contrast, our framework, called Generalized Grounding Graphs (G3), dynamically instantiates a probabilistic graphical model for a particular natural language command according to the command's hierarchical and compositional semantic structure. Our system performs inference in the model to successfully find and execute plans corresponding to natural language commands such as “Put the tire pallet on the truck.” The model is trained using a corpus of commands collected using crowdsourcing. We pair each command with robot actions and use the corpus to learn the parameters of the model. We evaluate the robot's performance by inferring plans from natural language commands, executing each plan in a realistic robot simulator, and asking users to evaluate the system's performance. We demonstrate that our system can successfully follow many natural language commands from the corpus.},
author = {Tellex, S. and Kollar, T. and Dickerson, S.},
booktitle = {AAAI},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Tellex, Kollar, Dickerson/Tellex, Kollar, Dickerson - Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation.pdf:pdf},
keywords = {Special Track on Physically Grounded Artificial In},
pages = {1507--1514},
title = {{Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation.}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewFile/3623/4113},
year = {2011}
}
@article{Theune2001,
abstract = {We present a data-to-speech system called D2S, which can be used for the creation of data-to-speech systems in different languages {\&} domains. The most important characteristic of a data-to-speech system is that it combines language {\&} speech generation: language generation is used to produce a natural language text expressing the system's input data, {\&} speech generation is used to make this text audible. In D2S, this combination is exploited by using linguistic information available in the language generation module for the computation of prosody. This allows us to achieve a better prosodic output quality than can be achieved in a plain text-to-speech system. For language generation in D2S, the use of syntactically enriched templates is guided by knowledge of the discourse context, while for speech generation pre-recorded phrases are combined in a prosodically sophisticated manner. This combination of techniques makes it possible to create linguistically sound but efficient systems with a high quality language {\&} speech output. 1 Table, 16 Figures, 69 References. Adapted from the source document},
author = {Theune, M. and Klabbers, E. and Odijk, J. and {De Pijper}, J. R. and Krahmer, E.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Theune et al/Theune et al. - From Data to Speech A General Approach.pdf:pdf},
isbn = {1469-8110},
issn = {1351-3249, 1351-3249},
journal = {Natural Language Engineering},
keywords = {*Natural Language Processing (56550),*Speech Synthesis (82900),*Suprasegmentals (85750),*Syntactic Processing (86760),5113: descriptive linguistics,6111: phonetics,article,computational and mathematical linguistics,lexical choice,speech synthesis/recognition},
mendeley-tags = {lexical choice},
number = {1},
pages = {47--86},
title = {{From Data to Speech: A General Approach}},
url = {http://findit.lib.cuhk.edu.hk/852cuhk/?url{\_}ver=Z39.88-2004{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal{\&}genre=article{\&}sid=ProQ:ProQ{\%}3Allbashell{\&}atitle=From+Data+to+Speech{\%}3A+A+General+Approach{\&}title=Natural+Language+Engineering{\&}issn=13513249{\&}date=2001-03-},
volume = {7},
year = {2001}
}
@inproceedings{Tran2016,
abstract = {In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.},
archivePrefix = {arXiv},
arxivId = {1609.09007},
author = {Tran, K. and Bisk, Y. and Vaswani, A. and Marcu, D. and Knight, K.},
booktitle = {EMNLP},
doi = {10.18653/v1/W16-5907},
eprint = {1609.09007},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Tran et al/Tran et al. - Unsupervised Neural Hidden Markov Models.pdf:pdf},
pages = {63--71},
title = {{Unsupervised Neural Hidden Markov Models}},
url = {http://arxiv.org/abs/1609.09007},
year = {2016}
}
@inproceedings{Tsatsaronis2007,
abstract = {Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data and/or do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks{\^{a}}€™ links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.},
archivePrefix = {arXiv},
arxivId = {1508.01346},
author = {Tsatsaronis, G. and Vazirgiannis, M. and Androutsopoulos, I.},
booktitle = {IJCAI},
doi = {10.1145/1459352.1459355},
eprint = {1508.01346},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Tsatsaronis, Vazirgiannis, Androutsopoulos/Tsatsaronis, Vazirgiannis, Androutsopoulos - Word sense disambiguation with spreading activation networks generated from thesauri.pdf:pdf},
isbn = {0360-0300},
issn = {10450823},
keywords = {learning,natural language processing},
pages = {1725--1730},
pmid = {18353985},
title = {{Word sense disambiguation with spreading activation networks generated from thesauri}},
year = {2007}
}
@inproceedings{Valmadre2017,
abstract = {The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.},
archivePrefix = {arXiv},
arxivId = {1704.06036},
author = {Valmadre, J. and Bertinetto, L. and Henriques, J. F. and Vedaldi, A. and Torr, P H. S.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2017.531},
eprint = {1704.06036},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Valmadre et al/Valmadre et al. - End-to-end representation learning for Correlation Filter based tracking.pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
title = {{End-to-end representation learning for Correlation Filter based tracking}},
url = {http://arxiv.org/abs/1704.06036},
year = {2017}
}
@inproceedings{Vaswani2016,
abstract = {In this paper we present new state-of-the-art performance on CCG supertagging and pars-ing. Our model outperforms existing ap-proaches by an absolute gain of 1.5{\%}. We an-alyze the performance of several neural mod-els and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.},
author = {Vaswani, A. and Bisk, Y. and Sagae, K. and Musa, R.},
booktitle = {NAACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Vaswani et al/Vaswani et al. - Supertagging With LSTMs.pdf:pdf},
isbn = {9781941643914},
pages = {232--237},
title = {{Supertagging With LSTMs}},
url = {http://www.aclweb.org/anthology/N16-1027},
year = {2016}
}
@article{Vieira2017,
abstract = {Pruning hypotheses during dynamic program- ming is commonly used to speed up inference in settings such as parsing. Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy. This poses a difficult machine learning problem, which we tackle with the LOLS algorithm. LOLS training must continually compute the ef- fects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime.},
author = {Vieira, T. and Eisner, J.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Vieira, Eisner/Vieira, Eisner - Learning to Prune Exploring the Frontier of Fast and Accurate Parsing.pdf:pdf},
issn = {2307-387X},
journal = {TACL},
number = {2011},
pages = {263--278},
title = {{Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing}},
volume = {5},
year = {2017}
}
@inproceedings{Vogel2010,
abstract = {We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.},
author = {Vogel, A. and Jurafsky, D.},
booktitle = {ACL},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Vogel, Jurafsky/Vogel, Jurafsky - Learning to Follow Navigational Directions.pdf:pdf},
isbn = {9781617388088},
number = {July},
pages = {806--814},
title = {{Learning to Follow Navigational Directions}},
year = {2010}
}
@inproceedings{Vogel1996,
abstract = {In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.},
author = {Vogel, S. and Ney, H. and Tillmann, C.},
booktitle = {COLING},
doi = {10.3115/993268.993313},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Vogel, Ney, Tillmann/Vogel, Ney, Tillmann - HMM-based Word Alignment in Statistical Machine Translation.pdf:pdf},
pages = {836--841},
title = {{HMM-based Word Alignment in Statistical Machine Translation}},
url = {http://portal.acm.org/citation.cfm?doid=993268.993313},
volume = {96pp},
year = {1996}
}
@article{Wang2017,
abstract = {We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction. While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data. The supervised learner must identify surface features of a language's POS sequence (hand-engineered or neural features) that correlate with the language's deeper structure (latent trees). In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar induction baseline by a large margin.},
archivePrefix = {arXiv},
arxivId = {1710.03877},
author = {Wang, D. and Eisner, J.},
eprint = {1710.03877},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Wang, Eisner/Wang, Eisner - Fine-Grained Prediction of Syntactic Typology Discovering Latent Structure with Supervised Learning.pdf:pdf},
journal = {TACL},
number = {2016},
pages = {147--161},
title = {{Fine-Grained Prediction of Syntactic Typology: Discovering Latent Structure with Supervised Learning}},
url = {http://arxiv.org/abs/1710.03877},
volume = {5},
year = {2017}
}
@article{Wang2016,
abstract = {We release Galactic Dependencies 1.0---a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format. This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages. Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages. We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages. As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a "nearby" source language. We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.},
archivePrefix = {arXiv},
arxivId = {1710.03838},
author = {Wang, D. and Eisner, J.},
eprint = {1710.03838},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Wang, Eisner/Wang, Eisner - The Galactic Dependencies Treebanks Getting More Data by Synthesizing New Languages.pdf:pdf},
issn = {2307-387X},
journal = {TACL},
pages = {491--505},
title = {{The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages}},
url = {http://arxiv.org/abs/1710.03838},
volume = {4},
year = {2016}
}
@inproceedings{Wang2013,
abstract = {{\textcopyright} 2013 Association for Computational Linguistics. NLP models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting. A recently re-popularized form of regularization is to generate fake training data by repeatedly adding noise to real data. We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data. We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs. We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently. The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transductive extension. Applied to text classification and NER, our method provides a {\textgreater}1{\%} absolute performance gain over use of standard L2 regularization.},
author = {Wang, S. I. and Wang, M. and Wager, S. and Liang, P. and Manning, C. D.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Wang et al/Wang et al. - Feature noising for log-linear structured prediction.pdf:pdf},
isbn = {9781937284978},
number = {October},
pages = {1170--1179},
title = {{Feature noising for log-linear structured prediction}},
year = {2013}
}
@inproceedings{Wang2013a,
abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.},
author = {Wang, S. and Manning, C.},
booktitle = {ICML},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Wang, Manning/Wang, Manning - Fast dropout training.pdf:pdf},
keywords = {I,boring formatting information,machine learning},
pages = {118--126},
title = {{Fast dropout training}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/wang13a},
volume = {28},
year = {2013}
}
@inproceedings{Wang2015,
abstract = {How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We fur- ther study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.},
author = {Wang, Y. and Berant, J. and Liang, Percy},
booktitle = {ACL},
doi = {10.3115/v1/P15-1129},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Wang, Berant, Liang/Wang, Berant, Liang - Building a Semantic Parser Overnight.pdf:pdf},
isbn = {9781941643723},
pages = {1332--1342},
pmid = {1684229},
title = {{Building a Semantic Parser Overnight}},
url = {http://aclweb.org/anthology/P15-1129},
year = {2015}
}
@inproceedings{Welling2011,
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an in-built protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
archivePrefix = {arXiv},
arxivId = {arXiv:1203.5753v5},
author = {Welling, M. and Teh, Y. W.},
booktitle = {ICML},
doi = {10.1515/jip-2012-0071},
eprint = {arXiv:1203.5753v5},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Welling, Teh/Welling, Teh - Bayesian Learning via Stochastic Gradient Langevin Dynamics.pdf:pdf},
isbn = {978-1-4503-0619-5},
issn = {10495258},
keywords = {Bayesian learning,ICML,machine learning,online learning},
pages = {681--688},
title = {{Bayesian Learning via Stochastic Gradient Langevin Dynamics}},
year = {2011}
}
@article{Weston2013,
abstract = {This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on two scoring functions that operate by learning low-dimensional embeddings of words and of entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over existing methods that rely on text features alone.},
archivePrefix = {arXiv},
arxivId = {1307.7973},
author = {Weston, J. and Bordes, A. and Yakhnenko, O. and Usunier, N.},
eprint = {1307.7973},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Weston et al/Weston et al. - Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction.pdf:pdf},
isbn = {9781937284978},
journal = {Computational Linguistics},
title = {{Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction}},
url = {http://arxiv.org/abs/1307.7973},
year = {2013}
}
@inproceedings{Weston2015,
abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term mem- ory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chainingmultiple supporting sentences to an- swer questions that require understanding the intension of verbs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.3916v10},
author = {Weston, J. and Chopra, S. and Bordes, A.},
booktitle = {ICLR},
doi = {v0},
eprint = {arXiv:1410.3916v10},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Weston, Chopra, Bordes/Weston, Chopra, Bordes - Memory networks.pdf:pdf},
isbn = {9781424469178},
issn = {1098-7576},
pages = {1--15},
pmid = {9377276},
title = {{Memory networks}},
year = {2015}
}
@article{Williams1999,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.01121v2},
author = {Williams, A. and Drozdov, A. and Bowman, S. R.},
eprint = {arXiv:1709.01121v2},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Williams, Drozdov, Bowman/Williams, Drozdov, Bowman - Do latent tree learning models identify meaningful structure in sentences.pdf:pdf},
journal = {TACL},
title = {{Do latent tree learning models identify meaningful structure in sentences ?}},
year = {1999}
}
@inproceedings{Wiseman2017,
abstract = {Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.},
archivePrefix = {arXiv},
arxivId = {1707.08052},
author = {Wiseman, S. and Shieber, S. M. and Rush, A. M.},
booktitle = {EMNLP},
eprint = {1707.08052},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Wiseman, Shieber, Rush/Wiseman, Shieber, Rush - Challenges in Data-to-Document Generation.pdf:pdf},
title = {{Challenges in Data-to-Document Generation}},
url = {http://arxiv.org/abs/1707.08052},
year = {2017}
}
@inproceedings{Wong2007,
abstract = {This paper explores the use of statistical machine translation (SMT) methods for tactical natural language generation. We present results on using phrase-based SMT for learning to map meaning representations to natural language. Improved results are obtained by inverting a semantic parser that uses SMT methods to map sentences into meaning representations. Finally, we show that hybridizing these two approaches results in still more accurate generation systems. Automatic and human evaluation of generated sentences are presented across two domains and four languages. {\textcopyright} 2007 Association for Computational Linguistics.},
author = {Wong, Y. W. and Mooney, R. J.},
booktitle = {NAACL-HLT},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Wong, Mooney/Wong, Mooney - Generation by Inverting a Semantic Parser That Uses Statistical Machine Translation.pdf:pdf},
number = {April},
pages = {172--179},
title = {{Generation by Inverting a Semantic Parser That Uses Statistical Machine Translation}},
year = {2007}
}
@inproceedings{Wong2006,
abstract = {We present a novel statistical approach to semantic parsing, WASP, for construct-ing a complete, formal meaning represen-tation of a sentence. A semantic parser is learned given a set of sentences anno-tated with their correct meaning represen-tations. The main innovation of WASP is its use of state-of-the-art statistical ma-chine translation techniques. A word alignment model is used for lexical acqui-sition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods re-quiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.},
author = {Wong, Y. W. and Mooney, R. J.},
booktitle = {HLT},
doi = {10.3115/1220835.1220891},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Wong, Mooney/Wong, Mooney - Learning for Semantic Parsing with Statistical Machine Translation.pdf:pdf},
number = {June},
pages = {439--446},
title = {{Learning for Semantic Parsing with Statistical Machine Translation}},
url = {http://www.aclweb.org/anthology/N/N06/N06-1056},
year = {2006}
}
@inproceedings{Wong2008,
abstract = {We formulate semantic parsing as a parsing problem on a synchronous context free grammar (SCFG) which is automatically built on the corpus of natural language sentences and the representation of semantic outputs. We then present an online learning fr...},
author = {Wong, Y. W. and Mooney, R. J.},
booktitle = {ICTAI},
doi = {10.1109/ICTAI.2008.96},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Wong, Mooney/Wong, Mooney - Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus.pdf:pdf},
isbn = {9780769534404},
issn = {10823409},
number = {June},
pages = {135--142},
title = {{Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus}},
volume = {2},
year = {2008}
}
@inproceedings{Wu2017,
abstract = {Combinatory Category Grammar (CCG) supertag-ging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. How-ever, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window ap-proach, which can be treated as an attention mech-anism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words di-rectly, which is superior to the regular dropout on word embeddings. We use this approach to demon-strate the state-of-the-art CCG supertagging perfor-mance on the standard test set.},
archivePrefix = {arXiv},
arxivId = {1610.02749},
author = {Wu, H. and Zhang, J. and Zong, C.},
booktitle = {AAAI},
eprint = {1610.02749},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Wu, Zhang, Zong/Wu, Zhang, Zong - A Dynamic Window Neural Network for CCG Supertagging.pdf:pdf},
keywords = {Natural Language Processing and Machine Learning},
pages = {3337--3343},
title = {{A Dynamic Window Neural Network for CCG Supertagging}},
year = {2017}
}
@inproceedings{Xue2007,
author = {Xue, N.},
booktitle = {Treebanks and Linguistic Theories},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Xue/Xue - Tapping the implicit information for the PS to DS conversion of the Chinese Treebank.pdf:pdf},
title = {{Tapping the implicit information for the PS to DS conversion of the Chinese Treebank}},
year = {2007}
}
@article{Xue2005,
abstract = {With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with different segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are difficult. As a first step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The first two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking efforts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality.},
author = {Xue, N. and Xia, F. and Chiou, F. D. and Palmer, M.},
doi = {10.1017/S135132490400364X},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Xue et al/Xue et al. - The Penn Chinese TreeBank Phrase structure annotation of a large corpus.pdf:pdf},
isbn = {1469-8110},
issn = {13513249},
journal = {Natural Language Engineering},
number = {2},
pages = {207--238},
title = {{The Penn Chinese TreeBank: Phrase structure annotation of a large corpus}},
volume = {11},
year = {2005}
}
@inproceedings{Yang2015,
abstract = {Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge. In this work, we first identify several semantic structures behind humor and design sets of features for each structure, and next employ a com- putational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations.},
author = {Yang, D. and Lavie, A. and Dyer, C. and Hovy, E.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Yang et al/Yang et al. - Humor Recognition and Humor Anchor Extraction.pdf:pdf},
isbn = {9781941643327},
number = {September},
pages = {2367--2376},
title = {{Humor Recognition and Humor Anchor Extraction}},
year = {2015}
}
@inproceedings{Yang2018,
author = {Yang, Z. and Dai, Z and Salakhutdinov, R. and Cohen, W. W.},
booktitle = {ICLR},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Yang et al/Yang et al. - Breaking the Softmax Bottleneck A High-Rank RNN Language Model.pdf:pdf},
pages = {1--13},
title = {{Breaking the Softmax Bottleneck: A High-Rank RNN Language Model}},
year = {2018}
}
@inproceedings{Yao2017,
author = {Yao, J. and Wan, X.},
booktitle = {AAAI},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Yao, Wan/Yao, Wan - Greedy Flipping for Constrained Word Deletion.pdf:pdf},
keywords = {Natural Language Processing and Text Mining},
number = {1},
pages = {3518--3524},
title = {{Greedy Flipping for Constrained Word Deletion}},
year = {2017}
}
@inproceedings{Yao2015,
abstract = {In this paper, we formulate a sparse optimization framework for extractive document summarization. The proposed framework has a decomposable con-vex objective function. We derive an efficient ADMM algorithm to solve it. To encourage di-versity in the summaries, we explicitly introduce an additional sentence dissimilarity term in the op-timization framework. We achieve significant im-provement over previous related work under sim-ilar data reconstruction framework. We then gen-eralize our formulation to the case of compressive summarization and derive a block coordinate de-scent algorithm to optimize the objective function. Performance on DUC 2006 and DUC 2007 datasets shows that our compressive summarization results are competitive against the state-of-the-art results while maintaining reasonable readability.},
author = {Yao, J. and Wan, X. and Xiao, J.},
booktitle = {IJCAI},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Yao, Wan, Xiao/Yao, Wan, Xiao - Compressive document summarization via sparse optimization.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
keywords = {Technical Papers — Natural Language Processing},
pages = {1376--1382},
title = {{Compressive document summarization via sparse optimization}},
year = {2015}
}
@inproceedings{Yarowsky1995,
abstract = {This paper presents an unsupervised learn- ing algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints - that words tend to have one sense per discourse and one sense per collocation - exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96{\%}.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Yarowsky, D.},
booktitle = {ACL},
doi = {10.3115/981658.981684},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Yarowsky/Yarowsky - Unsupervised word sense disambiguation rivaling supervised methods.pdf:pdf},
isbn = {0736-587X},
issn = {0736587X},
pages = {189--196},
pmid = {15003161},
title = {{Unsupervised word sense disambiguation rivaling supervised methods}},
url = {http://portal.acm.org/citation.cfm?doid=981658.981684},
year = {1995}
}
@inproceedings{Yih2015,
abstract = {We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowl-edge base and can be directly mapped to a logical form. Semantic parsing is re-duced to query graph generation, formu-lated as a staged search problem. Unlike traditional approaches, our method lever-ages the knowledge base in an early stage to prune the search space and thus simpli-fies the semantic matching problem. By applying an advanced entity linking sys-tem and a deep convolutional neural net-work model that matches questions and predicate sequences, our system outper-forms previous methods substantially, and achieves an F 1 measure of 52.5{\%} on the WEBQUESTIONS dataset.},
author = {Yih, W. and Chang, M. and He, X. and Gao, J.},
booktitle = {ACL},
doi = {10.3115/v1/P15-1128},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Yih et al/Yih et al. - Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base.pdf:pdf},
isbn = {9781941643723},
pages = {1321--1331},
title = {{Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base}},
url = {http://aclweb.org/anthology/P15-1128},
year = {2015}
}
@inproceedings{Yin2017,
abstract = {We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.},
archivePrefix = {arXiv},
arxivId = {1704.01696},
author = {Yin, P. and Neubig, G.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1041},
eprint = {1704.01696},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Yin, Neubig/Yin, Neubig - A Syntactic Neural Model for General-Purpose Code Generation.pdf:pdf},
isbn = {9781945626753},
pages = {440--450},
title = {{A Syntactic Neural Model for General-Purpose Code Generation}},
url = {http://arxiv.org/abs/1704.01696},
year = {2017}
}
@inproceedings{Yogatama2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.09100v1},
author = {Yogatama, D. and Blunsom, P. and Dyer, C. and Grefenstette, E. and Ling, W.},
booktitle = {ICLR},
eprint = {arXiv:1611.09100v1},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Yogatama et al/Yogatama et al. - Learning to Compose Words into Sentences with Reinforcement Learning.pdf:pdf},
pages = {1--10},
title = {{Learning to Compose Words into Sentences with Reinforcement Learning}},
year = {2017}
}
@inproceedings{Yu2013,
abstract = {We present a method that learns repre- sentations for word meanings from short video clips paired with sentences. Un- like prior work on learning language from symbolic input, our input consists of video of people interacting with multiple com- plex objects in outdoor environments. Un- like prior computer-vision approaches that learn from videos with verb labels or im- ages with noun labels, our labels are sen- tences containing nouns, verbs, preposi- tions, adjectives, and adverbs. The cor- respondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts si- multaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video.},
archivePrefix = {arXiv},
arxivId = {1306.5263},
author = {Yu, H. and Siskind, J. M.},
booktitle = {ACL},
eprint = {1306.5263},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Yu, Siskind/Yu, Siskind - Grounded Language Learning from Video Described with Sentences.pdf:pdf},
isbn = {9781937284503},
number = {August},
pages = {53--63},
title = {{Grounded Language Learning from Video Described with Sentences}},
url = {http://www.aclweb.org/anthology/P13-1006},
year = {2013}
}
@inproceedings{Zarrieß2016,
abstract = {Colour terms have been a prime phenomenon for studying language grounding, though pre-vious work focussed mostly on descriptions of simple objects or colour swatches. This paper investigates whether colour terms can be learned from more realistic and potentially noisy visual inputs, using a corpus of referring expressions to objects represented as regions in real-world images. We obtain promising re-sults from combining a classifier that grounds colour terms in visual input with a recalibra-tion model that adjusts probability distribu-tions over colour terms according to contex-tual and object-specific preferences.},
author = {Zarrie{\ss}, S. and Schlangen, D.},
booktitle = {INLG},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zarrie{\ss}, Schlangen/Zarrie{\ss}, Schlangen - Towards Generating Colour Terms for Referents in Photographs Prefer the Expected or the Unexpected.pdf:pdf},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {246--255},
title = {{Towards Generating Colour Terms for Referents in Photographs : Prefer the Expected or the Unexpected ?}},
year = {2016}
}
@inproceedings{Zelle1996,
abstract = {This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a pre-existing, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application.},
author = {Zelle, J. M. and Mooney, R. J.},
booktitle = {AAAI},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zelle, Mooney/Zelle, Mooney - Learning to Parse Database queries using inductive logic proramming.pdf:pdf},
keywords = {1996,aaai,aaai - 96 proceedings,all rights reserved,copyright,learning to parse database,m,org,queries,using inductive logic programming,www},
number = {August},
pages = {1050--1055},
title = {{Learning to Parse Database queries using inductive logic proramming}},
url = {http://www.aaai.org/Papers/AAAI/1996/AAAI96-156.pdf},
year = {1996}
}
@inproceedings{Zettlemoyer2009,
abstract = {We consider the problem of learning context-dependent mappings from sentences to logical form. The training examples are sequences of sentences annotated with lambda-calculus meaning representations. We develop an algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7{\%} accuracy.},
author = {Zettlemoyer, L. S. and Collins, M.},
booktitle = {ACL},
doi = {10.3115/1690219.1690283},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zettlemoyer, Collins/Zettlemoyer, Collins - Learning context-dependent mappings from sentences to logical form.pdf:pdf},
isbn = {9781932432466},
number = {August},
pages = {976},
title = {{Learning context-dependent mappings from sentences to logical form}},
url = {http://portal.acm.org/citation.cfm?doid=1690219.1690283},
volume = {2},
year = {2009}
}
@inproceedings{Zettlemoyer2012,
abstract = {This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.},
archivePrefix = {arXiv},
arxivId = {1207.1420},
author = {Zettlemoyer, L. S. and Collins, M.},
booktitle = {UAI},
doi = {10.1093/acprof:oso/9780199654680.003.0006},
eprint = {1207.1420},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zettlemoyer, Collins/Zettlemoyer, Collins - Learning to Map Sentences to Logical Form Structured Classification with Probabilistic Categorial Grammars.pdf:pdf},
isbn = {0974903914},
title = {{Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars}},
url = {http://arxiv.org/abs/1207.1420},
year = {2012}
}
@inproceedings{Zettlemoyer2007,
abstract = {We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammarfor example allowing flexible word order, or insertion of lexical items with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86 {\%} F-measure in recovering fully correct semantic analyses and 95.9{\%} F-measure by a partial-match criterion, a more than 5 {\%} improvement over the 90.3{\%} partial-match figure reported by He and Young (2006).},
author = {Zettlemoyer, L. S. and Collins, M.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zettlemoyer, Collins/Zettlemoyer, Collins - Online learning of relaxed CCG grammars for parsing to logical form.pdf:pdf},
number = {June},
pages = {678--687},
title = {{Online learning of relaxed CCG grammars for parsing to logical form}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.71.3926},
year = {2007}
}
@inproceedings{Zhang2017c,
abstract = {This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of CNNs with different structures. The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision. Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.},
archivePrefix = {arXiv},
arxivId = {1710.00935},
author = {Zhang, Q. and Wu, Y. N. and Zhu, S.},
booktitle = {CVPR},
eprint = {1710.00935},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zhang, Wu, Zhu/Zhang, Wu, Zhu - Interpretable Convolutional Neural Networks.pdf:pdf},
title = {{Interpretable Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1710.00935},
year = {2017}
}
@article{Zhang2017,
author = {Zhang, S. and Rudinger, R. and Duh, K. and van Durme, B.},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zhang et al/Zhang et al. - Ordinal Common-sense Inference.pdf:pdf},
journal = {TACL},
pages = {379--395},
title = {{Ordinal Common-sense Inference}},
volume = {5},
year = {2017}
}
@inproceedings{Zhang2017a,
abstract = {In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning of se-quential structured prediction problems. Our NCRF-AE consists of two parts: an encoder which is a CRF model enhanced by deep neural networks, and a decoder which is a generative model trying to re-construct the input. Our model has a uni-fied structure with different loss functions for labeled and unlabeled data with shared parameters. We developed a variation of the EM algorithm for optimizing both the encoder and the decoder simultaneously by decoupling their parameters. Our ex-perimental results over the Part-of-Speech (POS) tagging task on eight different lan-guages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised sce-narios.},
author = {Zhang, X. and Jiang, Y. and Peng, H. and Tu, K. and Goldwasser, D.},
booktitle = {EMNLP},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zhang et al/Zhang et al. - Semi-Supervised Structured Prediction with Neural CRF Autoencoder.pdf:pdf},
pages = {1702--1712},
title = {{Semi-Supervised Structured Prediction with Neural CRF Autoencoder}},
url = {http://aclweb.org/anthology/D17-1179{\%}0Ahttp://purduenlp.cs.purdue.edu/pubs/emnlp{\_}2017.pdf},
year = {2017}
}
@inproceedings{Zhang2017b,
abstract = {To learn a semantic parser from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing model to improve the state-of-the-art accuracy from 38.7{\%} to 42.7{\%}, and then use macro grammars and holistic triggering to achieve an 11x speedup and an accuracy of 43.7{\%}.},
archivePrefix = {arXiv},
arxivId = {1707.07806},
author = {Zhang, Y. and Pasupat, P. and Liang, P.},
booktitle = {EMNLP},
eprint = {1707.07806},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zhang, Pasupat, Liang/Zhang, Pasupat, Liang - Macro Grammars and Holistic Triggering for Efficient Semantic Parsing.pdf:pdf},
title = {{Macro Grammars and Holistic Triggering for Efficient Semantic Parsing}},
url = {http://arxiv.org/abs/1707.07806},
year = {2017}
}
@inproceedings{Zhao2014,
abstract = {Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GeoQuery, Jobs and Atis domains.},
archivePrefix = {arXiv},
arxivId = {1411.5379},
author = {Zhao, K. and Huang, L.},
booktitle = {NAACL},
eprint = {1411.5379},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zhao, Huang/Zhao, Huang - Type-Driven Incremental Semantic Parsing with Polymorphism.pdf:pdf},
isbn = {9781941643495},
pages = {1416--1421},
title = {{Type-Driven Incremental Semantic Parsing with Polymorphism}},
url = {http://arxiv.org/abs/1411.5379},
volume = {0041},
year = {2014}
}
@unpublished{Zhong2017,
abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9{\%} to 59.4{\%} and logical form accuracy from 23.4{\%} to 48.3{\%}.},
archivePrefix = {arXiv},
arxivId = {1709.00103},
author = {Zhong, V. and Xiong, C. and Socher, R.},
eprint = {1709.00103},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zhong, Xiong, Socher/Zhong, Xiong, Socher - Seq2SQL Generating Structured Queries from Natural Language using Reinforcement Learning.pdf:pdf},
pages = {1--12},
title = {{Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning}},
url = {http://arxiv.org/abs/1709.00103},
year = {2017}
}
@inproceedings{Zhou2017,
abstract = {Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.},
archivePrefix = {arXiv},
arxivId = {1704.01691},
author = {Zhou, C. and Neubig, G.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1029},
eprint = {1704.01691},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zhou, Neubig/Zhou, Neubig - Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction.pdf:pdf},
isbn = {9781945626753},
title = {{Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction}},
url = {http://arxiv.org/abs/1704.01691},
year = {2017}
}
@inproceedings{Zhou2013,
abstract = {Abstract How does the activity of one person affect that of another person? Does the strength of influence remain periodic or decay exponentially over time? In this paper, we study these critical questions in social network analysis quantitatively under the framework of multi- ...$\backslash$n},
author = {Zhou, K. and Zha, H. and Song, L.},
booktitle = {ICML},
file = {:C$\backslash$:/Users/t-guqin/Documents/papers/Zhou, Zha, Song/Zhou, Zha, Song - Learning Triggering Kernels for Multi-dimensional Hawkes Processes.pdf:pdf},
pages = {1301--1309},
title = {{Learning Triggering Kernels for Multi-dimensional Hawkes Processes}},
url = {http://jmlr.org/proceedings/papers/v28/zhou13.html},
volume = {28},
year = {2013}
}
