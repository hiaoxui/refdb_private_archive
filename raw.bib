
@report{2019PointingDecodersMultiObjective,
  title = {Pointing {{Decoders}} with {{Multi}}-{{Objective Training}} for {{Diversity}} in {{Web Search Recommendation}}},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KJVLSIXZ/Anonymous - 2019 - Pointing Decoders with Multi-Objective Training for Diversity in Web Search Recommendation(2).pdf},
  keywords = {review}
}

@report{2019TemporalOrderEstimation,
  title = {Temporal {{Order Estimation}} in {{Recurrent Neural Networks}}},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/34I9HXMQ/Anonymous - 2019 - Temporal Order Estimation in Recurrent Neural Networks(2).pdf},
  keywords = {review}
}

@report{2020CausalCorpusLexical,
  title = {A {{Causal Corpus}}, {{Lexical Knowledge Base}}, and {{Sentential Generator}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N57ULVXQ/Anonymous - 2020 - A Causal Corpus, Lexical Knowledge Base, and Sentential Generator(2).pdf}
}

@report{2020CausalInferenceScript,
  title = {Causal {{Inference}} of {{Script Knowledge}}},
  date = {2020},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W8U5BZR3/Anonymous - 2020 - Causal Inference of Script Knowledge(2).pdf},
  isbn = {9788578110796}
}

@report{2020ChartdrivenSequenceLabeling,
  title = {Chart-Driven {{Sequence Labeling}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KWJ3IQGQ/Anonymous - 2020 - Chart-driven Sequence Labeling(2).pdf},
  keywords = {review}
}

@report{2020DynamicKnowledgeIntegration,
  title = {Dynamic {{Knowledge Integration}} for {{Deep Text Matching}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/73Z99KX6/Submission 6020.pdf},
  keywords = {review}
}

@report{2020KnowledgeenhancedSequencetoTreeSolver,
  title = {Knowledge-Enhanced {{Sequence}}-to-{{Tree}} Solver for {{Math Word Problems}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B5LU8JVH/Submission 6848.pdf},
  keywords = {review}
}

@report{2020LearningTopicPreserving,
  title = {Learning {{Topic Preserving Embeddings}} with {{Seq2Seq}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9WVUDPIH/Anonymous - 2020 - Learning Topic Preserving Embeddings with Seq2Seq(2).pdf},
  keywords = {review}
}

@report{2020ROMaControllableText,
  title = {{{ROMa}} : {{Controllable Text Generation}} with {{Random Observation Machine}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T9USZZYP/Anonymous - 2020 - ROMa Controllable Text Generation with Random Observation Machine(2).pdf},
  keywords = {review}
}

@report{2020SemanticAnalysisAnalytic,
  title = {Semantic {{Analysis}} via {{Analytic Truth}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SN9R97H7/Anonymous - 2020 - Semantic Analysis via Analytic Truth(2).pdf}
}

@report{2020StatutoryLegalReasoning,
  title = {Statutory {{Legal Reasoning}} : {{Challenging Natural Language Systems}} with {{Understanding Prescriptive Rules}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ELQLV77Q/Anonymous - 2020 - Statutory Legal Reasoning Challenging Natural Language Systems with Understanding Prescriptive Rules(2).pdf},
  keywords = {u}
}

@report{2020TrackingProgressLanguage,
  title = {Tracking the Progress of {{Language Models}} by Extracting Their Underlying {{Knowledge Graphs}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PXMZZ989/Tracking the Progress of Language Models by Extracting Their Underlaying Knowledge Graphs.pdf},
  keywords = {u}
}

@report{2020UncertainNaturalLanguage,
  title = {Uncertain {{Natural Language Inference}}},
  date = {2020},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/43VC8JCB/Anonymous - 2020 - Uncertain Natural Language Inference(2).pdf},
  isbn = {9788578110796}
}

@report{2020UnifiedMultiIntent,
  title = {Unified {{Multi Intent Order}} and {{Slot Prediction}} Using {{Selective Learning Propagation}} 051},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZJ88SK3/Anonymous - 2020 - Unified Multi Intent Order and Slot Prediction using Selective Learning Propagation 051(2).pdf},
  keywords = {review}
}

@report{2020UniversalDecompositionalParsing,
  title = {Universal {{Decompositional Parsing}} at the {{Syntax}}-{{Semantics Interface}}},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IK5ZEKZV/Universal Decompositional Parsing at the Syntax-Se.pdf},
  keywords = {u}
}

@report{2021AnatomyCatastrophicForgetting,
  title = {Anatomy of {{Catastrophic Forgetting}}: {{Hidden Representation}} and {{Task Semantics}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E6J7SRTR/document.pdf},
  keywords = {review}
}

@report{2021ClearingPathTruly,
  title = {Clearing the {{Path}} for {{Truly Semantic Representation Learning}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W9EBW9GB/document.pdf},
  keywords = {review}
}

@report{2021ContrastiveRepresentationDistillation,
  title = {Contrastive {{Representation Distillation}} for {{Semi}}-{{Supervised Relation Extraction}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DBD5HAFB/2717_file_Paper.pdf},
  keywords = {review}
}

@report{2021Doc2DictInformationExtraction,
  title = {{{Doc2Dict}}: {{Information Extraction}} as {{Text Generation}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V3FM436N/654_file_Paper.pdf},
  keywords = {review}
}

@report{2021EmpiricalErrorModeling,
  title = {Empirical {{Error Modeling Improves Robustness}} of {{Noisy Neural Sequence Labeling}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/33KEEF39/288_file_Paper.pdf},
  keywords = {review}
}

@report{2021EndtoEndLearningCoherent,
  title = {End-to-{{End Learning}} of {{Coherent Probabilistic Forecasts}} for {{Hierarchical Time Series}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LQDWKH98/Supplementary 6359.pdf;/home/hiaoxui/.local/share/zotero_files/storage/XSVT9RHN/Submission 6359.pdf},
  keywords = {review}
}

@report{2021EnhancingKnowledgeGraph,
  title = {Enhancing {{Knowledge Graph Extraction}} with {{Multi}}-{{Label Attributes}}, {{Applied}} to {{Scientific Claims}} and {{Aspect}}-{{Based Sentiment}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MFE6JTFG/3562_file_Paper.pdf},
  keywords = {review}
}

@report{2021ExploringPatentFeatures,
  title = {Exploring {{Patent Features}} for {{Relation Extraction}} with {{Contrastive Student}}-{{Teacher Learning}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DC4CD7TT/1906_file_Paper.pdf},
  keywords = {review}
}

@report{2021HySPAHybridSpan,
  title = {{{HySPA}}: {{Hybrid Span Generation}} for {{Scalable Text}}-to-{{Graph Extraction}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/48WZVLJF/3212_file_Paper.pdf},
  keywords = {review}
}

@report{2021NeuralOpenInformation,
  title = {Neural {{Open Information Extraction}} with {{Pointer}}-{{Generator Networks}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/THH4VVZ7/2377_file_Paper.pdf},
  keywords = {review}
}

@report{2021SpanLevelEmotionCause,
  title = {Span-{{Level Emotion Cause Analysis}} by {{Neural Sequence Tagging}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q76IMYUK/3195 Submission.pdf},
  keywords = {review}
}

@report{2021WeaklySupervisedNeuroSymbolic,
  title = {Weakly {{Supervised Neuro}}-{{Symbolic Module Networks}} for {{Numerical Reasoning}}},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QEBWMS56/weakly_supervised_neuro_symbolic_module_networks_for_numerical_reasoning.pdf},
  keywords = {review}
}

@inproceedings{abend2013UniversalConceptualCognitive,
  title = {Universal {{Conceptual Cognitive Annotation}} ( {{UCCA}} )},
  booktitle = {{{ACL}}},
  author = {Abend, O. and Rappoport, A.},
  date = {2013},
  pages = {228--238},
  url = {http://www.aclweb.org/anthology/P13-1023},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EBEUE4KU/Abend, Rappoport - 2013 - Universal Conceptual Cognitive Annotation ( UCCA )(2).pdf},
  isbn = {978-1-937284-50-3},
  keywords = {u}
}

@inproceedings{abend2017StateArtSemantic,
  title = {The State of the Art in Semantic Representation},
  booktitle = {{{ACL}}},
  author = {Abend, O. and Rappoport, A.},
  date = {2017},
  pages = {77--89},
  doi = {10.18653/v1/P17-1008},
  abstract = {© 2017 Association for Computational Linguistics. Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.},
  annotation = {49 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RGTSTCBQ/Abend, Rappoport - 2017 - The state of the art in semantic representation(2).pdf},
  isbn = {978-1-945626-75-3}
}

@incollection{abney1991ParsingChunks,
  title = {Parsing {{By Chunks}}},
  booktitle = {Principle-{{Based Parsing}}},
  author = {Abney, Steven P.},
  editor = {Berwick, Robert C. and Abney, Steven P. and Tenny, Carol},
  date = {1991},
  volume = {44},
  pages = {257--278},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-011-3474-3_10},
  url = {http://link.springer.com/10.1007/978-94-011-3474-3_10},
  urldate = {2020-08-14},
  editorb = {Chierchia, Gennaro and Jacobson, Pauline and Pelletier, Francis J.},
  editorbtype = {redactor},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RCE7MH5G/Abney - 1991 - Parsing By Chunks.pdf},
  isbn = {978-0-7923-1637-4 978-94-011-3474-3},
  keywords = {u},
  langid = {english},
  series = {Studies in {{Linguistics}} and {{Philosophy}}}
}

@article{abney1996PartialParsingFinitestate,
  title = {Partial Parsing via Finite-State Cascades},
  author = {Abney, Steven},
  date = {1996-12},
  journaltitle = {Natural Language Engineering},
  shortjournal = {Nat. Lang. Eng.},
  volume = {2},
  pages = {337--344},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324997001599},
  url = {https://www.cambridge.org/core/product/identifier/S1351324997001599/type/journal_article},
  urldate = {2020-08-14},
  abstract = {Finite state cascades represent an attractive architecture for parsing unrestricted text. Deterministic parsers specified by finite state cascades are fast and reliable. They can be extended at modest cost to construct parse trees with finite feature structures. Finally, such deterministic parsers do not necessarily involve trading off accuracy against speed—they may in fact be more accurate than exhaustive search stochastic context free parsers.},
  annotation = {594 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H4UQMLY4/Abney - 1996 - Partial parsing via finite-state cascades.pdf},
  keywords = {u},
  langid = {english},
  number = {4}
}

@incollection{abney1997PartofSpeechTaggingPartial,
  title = {Part-of-{{Speech Tagging}} and {{Partial Parsing}}},
  booktitle = {Corpus-{{Based Methods}} in {{Language}} and {{Speech Processing}}},
  author = {Abney, S.},
  editor = {Young, Steve and Bloothooft, Gerrit},
  date = {1997},
  volume = {2},
  pages = {118--136},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-017-1183-8_4},
  url = {http://link.springer.com/10.1007/978-94-017-1183-8_4},
  urldate = {2020-08-14},
  editorb = {Ide, Nancy and Véronis, Jean},
  editorbtype = {redactor},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IMYJJV3P/Abney - 1997 - Part-of-Speech Tagging and Partial Parsing.pdf},
  isbn = {978-90-481-4813-4 978-94-017-1183-8},
  keywords = {u},
  langid = {english},
  series = {Text, {{Speech}} and {{Language Technology}}}
}

@report{aghajanyan2020ConversationalSemanticParsing,
  title = {Conversational {{Semantic Parsing}}},
  author = {Aghajanyan, Armen and Maillard, Jean and Shrivastava, Akshat and Diedrick, Keith and Haeger, Mike and Li, Haoran and Mehdad, Yashar and Stoyanov, Ves and Kumar, Anuj and Lewis, Mike and Gupta, Sonal},
  date = {2020-09-28},
  url = {http://arxiv.org/abs/2009.13655},
  urldate = {2020-10-21},
  abstract = {The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which achieve better or comparable performance to the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover.},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2009.13655},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KJ57UCDT/Aghajanyan et al. - 2020 - Conversational Semantic Parsing.pdf;/home/hiaoxui/.local/share/zotero_files/storage/P78XTAVF/2009.html},
  keywords = {u}
}

@report{aghajanyan2021MuppetMassiveMultitask,
  title = {Muppet: {{Massive Multi}}-Task {{Representations}} with {{Pre}}-{{Finetuning}}},
  shorttitle = {Muppet},
  author = {Aghajanyan, Armen and Gupta, Anchit and Shrivastava, Akshat and Chen, Xilun and Zettlemoyer, Luke and Gupta, Sonal},
  date = {2021-01-26},
  url = {11},
  urldate = {2021-01-29},
  abstract = {We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g.\textasciitilde RoBERTa) and generation models (e.g.\textasciitilde BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.},
  archiveprefix = {arXiv},
  eprint = {2101.11038},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ED5SNQRT/Aghajanyan et al. - 2021 - Muppet Massive Multi-task Representations with Pr.pdf;/home/hiaoxui/.local/share/zotero_files/storage/2MF35IWG/2101.html}
}

@article{agrawal2004Primes,
  title = {Primes Is in {{P}}},
  author = {Agrawal, M. and Kayal, N. and Saxena, N.},
  date = {2004},
  journaltitle = {Annals of mathematics},
  volume = {160},
  pages = {781--793},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EYTFGCW3/Agrawal, Kayal, Saxena - 2004 - Primes is in P(2).pdf},
  number = {2}
}

@inproceedings{ainslie2020ETCEncodingLong,
  title = {{{ETC}}: {{Encoding Long}} and {{Structured Inputs}} in {{Transformers}}},
  shorttitle = {{{ETC}}},
  booktitle = {{{EMNLP}}},
  author = {Ainslie, Joshua and Ontanon, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li},
  date = {2020-10-27},
  url = {http://arxiv.org/abs/2004.08483},
  urldate = {2021-03-10},
  abstract = {Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.},
  annotation = {18 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2004.08483},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/65PNYPMF/Ainslie et al. - 2020 - ETC Encoding Long and Structured Inputs in Transf.pdf;/home/hiaoxui/.local/share/zotero_files/storage/2KBE5VQN/2004.html}
}

@inproceedings{aitchison2020WhyBiggerNot,
  title = {Why Bigger Is Not Always Better : On Finite and Infinite Neural Networks},
  booktitle = {{{ICML}}},
  author = {Aitchison, L.},
  date = {2020},
  archiveprefix = {arXiv},
  eprint = {1910.08013v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/58WSYQ6P/Aitchison - 2019 - Why bigger is not always better on finite and infinite neural networks(6).pdf;/home/hiaoxui/.local/share/zotero_files/storage/6I3R6EU6/Aitchison - 2019 - Why bigger is not always better on finite and infinite neural networks(4).pdf;/home/hiaoxui/.local/share/zotero_files/storage/KUZ7VUSL/Aitchison - 2019 - Why bigger is not always better on finite and infinite neural networks(5).pdf},
  keywords = {review}
}

@inproceedings{al-shedivat2018ContinuousAdaptationMetaLearning,
  title = {Continuous {{Adaptation}} via {{Meta}}-{{Learning}} in {{Nonstationary}} and {{Competitive Environments}}},
  booktitle = {{{ICLR}}},
  author = {Al-Shedivat, M. and Bansal, T. and Burda, Y. and Sutskever, I. and Mordatch, I. and Abbeel, P.},
  date = {2018},
  pages = {1--23},
  abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of ex-ponential moving averages of squared past gradients. It has been empirically ob-served that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous anal-ysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with " long-term memory " of past gradi-ents, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E5WEB49D/Al-Shedivat et al. - 2018 - Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments(2).pdf}
}

@article{albert1993BayesianAnalysisBinary,
  title = {Bayesian {{Analysis}} of {{Binary}} and {{Polychotomous Response Data}}},
  author = {Albert, J. H. and Chib, S.},
  date = {1993},
  journaltitle = {JASA},
  volume = {88},
  pages = {669--679},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7CFJEXGU/Albert, Chib - 1993 - Bayesian Analysis of Binary and Polychotomous Response Data(2).pdf},
  keywords = {u},
  number = {422}
}

@report{alberti2019BERTBaselineNatural,
  title = {A {{BERT Baseline}} for the {{Natural Questions}}},
  author = {Alberti, C. and Lee, K. and Collins, M.},
  date = {2019},
  url = {http://arxiv.org/abs/1901.08634},
  abstract = {This technical note describes a new baseline for the Natural Questions. Our model is based on BERT and reduces the gap between the model F1 scores reported in the original dataset paper and the human upper bound by 30\% and 50\% relative for the long and short answer tasks respectively. This baseline has been submitted to the official NQ leaderboard at ai.google.com/research/NaturalQuestions. Code, preprocessed data and pretrained model are available at https://github.com/google-research/language/tree/master/language/question\_answering/bert\_joint.},
  annotation = {57 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1901.08634},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3SMS6JRY/Alberti, Lee, Collins - 2019 - A BERT Baseline for the Natural Questions(2).pdf},
  keywords = {u}
}

@inproceedings{alonso2017ParsingUniversalDependencies,
  title = {Parsing Universal Dependencies without Training},
  booktitle = {{{EACL}}},
  author = {Alonso, H. M. and Agić, Ž. and Plank, B. and Søgaard, A.},
  date = {2017},
  pages = {230--240},
  abstract = {We propose UDP, the first training-free parser for Universal Dependencies (UD). Our algorithm is based on PageRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZI6T7FFI/Alonso et al. - 2017 - Parsing universal dependencies without training(2).pdf},
  isbn = {978-1-5108-3860-4}
}

@inproceedings{alvarez-melis2017TreestructureDecodingDoublyRecurrent,
  title = {Tree-Structure Decoding with {{Doubly}}-{{Recurrent Neural Networks}}},
  booktitle = {{{ICLR}}},
  author = {Alvarez-Melis, D. and Jaakkola, T. S.},
  date = {2017},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F3BAF5EB/Alvarez-Melis, Jaakkola - 2017 - Tree-structure decoding with Doubly-Recurrent Neural Networks(2).pdf},
  keywords = {u}
}

@inproceedings{ammar2014ConditionalRandomField,
  title = {Conditional {{Random Field Autoencoders}} for {{Unsupervised Structured Prediction}}},
  booktitle = {{{NeurIPS}}},
  author = {Ammar, W. and Dyer, C. and Smith, N. A.},
  date = {2014},
  issn = {10495258},
  url = {http://arxiv.org/abs/1411.1147},
  abstract = {We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observable data using a feature-rich conditional random field. Then a reconstruction of the input is (re)generated, conditional on the latent structure, using models for which maximum likelihood estimation has a closed-form. Our autoencoder formulation enables efficient learning without making unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. We show competitive results with instantiations of the model for two canonical NLP tasks: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.},
  annotation = {68 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1411.1147},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7PGKLRY2/Ammar, Dyer, Smith - 2014 - Conditional Random Field Autoencoders for Unsupervised Structured Prediction(2).pdf},
  keywords = {u}
}

@report{andersen2018FourierDomainJerk,
  title = {A {{Fourier Domain}} "{{Jerk}}" {{Search}} for {{Binary Pulsars}}},
  author = {Andersen, B. C. and Ransom, S. M.},
  date = {2018},
  issn = {20418213},
  doi = {10.3847/2041-8213/aad59f},
  url = {http://arxiv.org/abs/1807.07900},
  abstract = {While binary pulsar systems are fantastic laboratories for a wide array of astrophysics, they are particularly difficult to detect. The orbital motion of the pulsar changes its apparent spin frequency over the course of an observation, essentially "smearing" the response of the time series in the Fourier domain. We review the Fourier domain acceleration search (FDAS), which uses a matched filtering algorithm to correct for this smearing by assuming constant acceleration for a small enough portion of the orbit. We discuss the theory and implementation of a Fourier domain "jerk" search, developed as part of the \textbackslash textsc\{PRESTO\} software package, which extends the FDAS to account for a linearly changing acceleration, or constant orbital jerk, of the pulsar. We test the performance of our algorithm on archival Green Bank Telescope observations of the globular cluster Terzan\textasciitilde 5, and show that while the jerk search has a significantly longer runtime, it improves search sensitivity to binaries when the observation duration is \$5\$ to \$15\textbackslash\%\$ of the orbital period. Finally, we present the jerk-search-enabled detection of Ter5am (PSR\textasciitilde J1748\$-\$2446am), a new highly-accelerated pulsar in a compact, eccentric, and relativistic orbit, with a likely pulsar mass of 1.649\$\^\{+0.037\}\_\{-0.11\}\$\textbackslash,\textbackslash msun.},
  annotation = {14 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1807.07900},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6CCW9Q34/Andersen, Ransom - 2018 - A Fourier Domain Jerk Search for Binary Pulsars(2).pdf}
}

@article{anderson2019MoreDifferent,
  title = {More Is {{Different}}},
  author = {Anderson, P. W.},
  date = {2019},
  journaltitle = {Science},
  volume = {177},
  pages = {393--396},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S5WFUZAZ/Anderson - 2019 - More is Different(2).pdf},
  number = {4047}
}

@inproceedings{anderson2019OptimizationAbstractionSynergistic,
  title = {Optimization and Abstraction: {{A}} Synergistic Approach for Analyzing Neural Network Robustness},
  booktitle = {Programming {{Language Design}} and {{Implementation}}},
  author = {Anderson, G. and Dillig, I. and Pailoor, S. and Chaudhuri, S.},
  date = {2019},
  pages = {731--744},
  doi = {10.1145/3314221.3314614},
  abstract = {In recent years, the notion of local robustness (or robustness for short) has emerged as a desirable property of deep neural networks. Intuitively, robustness means that small perturbations to an input do not cause the network to perform misclas-sifications. In this paper, we present a novel algorithm for verifying robustness properties of neural networks. Our method synergistically combines gradient-based optimization methods for counterexample search with abstraction-based proof search to obtain a sound and (δ-)complete decision procedure. Our method also employs a data-driven approach to learn a verification policy that guides abstract interpretation during proof search. We have implemented the proposed approach in a tool called Charon and experimentally evaluated it on hundreds of benchmarks. Our experiments show that the proposed approach significantly outperforms three state-of-the-art tools, namely AI2, Reluplex, and Reluval.},
  annotation = {25 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.09959},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LEJSSUHX/Anderson et al. - 2019 - Optimization and abstraction A synergistic approach for analyzing neural network robustness(2).pdf},
  isbn = {978-1-4503-6712-7},
  keywords = {u}
}

@article{ando2005FrameworkLearningPredictive,
  title = {A {{Framework}} for {{Learning Predictive Structures}} from {{Multiple Tasks}} and {{Unlabeled Data}}},
  author = {Ando, R. K. and Zhang, T.},
  date = {2005},
  journaltitle = {JMLR},
  volume = {6},
  pages = {1817--1853},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9NPPMQM4/Ando, Zhang - 2005 - A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data(2).pdf}
}

@inproceedings{ando2005HighperformanceSemisupervisedLearning,
  title = {A High-Performance Semi-Supervised Learning Method for Text Chunking},
  booktitle = {{{ACL}}},
  author = {Ando, R. K. and Zhang, T.},
  date = {2005},
  pages = {1--9},
  abstract = {In machine learning, whether one can\textbackslash nbuild a more accurate classifier by using\textbackslash nunlabeled data (semi-supervised learning)\textbackslash nis an important issue. Although a num-\textbackslash nber of semi-supervised methods have been\textbackslash nproposed, their effectiveness on NLP tasks\textbackslash nis not always clear. This paper presents\textbackslash na novel semi-supervised method that em-\textbackslash nploys a learning paradigm which we call\textbackslash nstructural learning. The idea is to find\textbackslash n“what good classifiers are like” by learn-\textbackslash ning from thousands of automatically gen-\textbackslash nerated auxiliary classification problems on\textbackslash nunlabeled data. By doing so, the common\textbackslash npredictive structure shared by the multiple\textbackslash nclassification problems can be discovered,\textbackslash nwhich can then be used to improve perfor-\textbackslash nmance on the target problem. The method\textbackslash nproduces performance higher than the pre-\textbackslash nvious best results on CoNLL’00 syntac-\textbackslash ntic chunking and CoNLL’03 named entity\textbackslash nchunking (English and German).},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ULX9QC65/Ando, Zhang - 2005 - A high-performance semi-supervised learning method for text chunking(2).pdf},
  isbn = {1-932432-51-5},
  keywords = {u}
}

@inproceedings{andor2016GloballyNormalizedTransitionBased,
  title = {Globally {{Normalized Transition}}-{{Based Neural Networks}}},
  booktitle = {{{ACL}}},
  author = {Andor, D. and Alberti, C. and Weiss, D. and Severyn, A. and Presta, A. and Ganchev, K. and Petrov, S. and Collins, M.},
  date = {2016},
  url = {http://arxiv.org/abs/1603.06042},
  abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.},
  annotation = {473 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1603.06042},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JRYHFTMH/Andor et al. - 2016 - Globally Normalized Transition-Based Neural Networks(2).pdf},
  keywords = {u}
}

@inproceedings{andor2019GivingBERTCalculator,
  title = {Giving {{BERT}} a {{Calculator}}: {{Finding Operations}} and {{Arguments}} with {{Reading Comprehension}}},
  booktitle = {{{EMNLP}}},
  author = {Andor, D. and He, L. and Lee, K. and Pitler, E.},
  date = {2019},
  volume = {2},
  url = {http://arxiv.org/abs/1909.00109},
  abstract = {Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers. We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning. We augment the model with a predefined set of executable 'programs' which encompass simple arithmetic as well as extraction. Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it. On the recent Discrete Reasoning Over Passages (DROP) dataset, designed to challenge reading comprehension models, we show a 33\% absolute improvement by adding shallow programs. The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.},
  annotation = {23 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.00109},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3QRSCK93/Andor et al. - 2019 - Giving BERT a Calculator Finding Operations and Arguments with Reading Comprehension(2).pdf},
  keywords = {u}
}

@report{andreas2016LearningComposeNeural,
  title = {Learning to {{Compose Neural Networks}} for {{Question Answering}}},
  author = {Andreas, J. and Rohrbach, M. and Darrell, T. and Klein, D.},
  date = {2016},
  doi = {10.18653/v1/N16-1181},
  url = {http://arxiv.org/abs/1601.01705},
  abstract = {We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.},
  annotation = {417 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1601.01705},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WT9AM6US/Andreas et al. - 2016 - Learning to Compose Neural Networks for Question Answering(2).pdf},
  isbn = {9781941643914}
}

@inproceedings{andreas2017AnalogsLinguisticStructure,
  title = {Analogs of {{Linguistic Structure}} in {{Deep Representations}}},
  booktitle = {{{EMNLP}}},
  author = {Andreas, J. and Klein, D.},
  date = {2017},
  issn = {18734197},
  doi = {10.18653/v1/D17-1311},
  url = {http://arxiv.org/abs/1707.08139},
  abstract = {We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a "syntax" with functional analogues to qualitative properties of natural language.},
  annotation = {9 citations (Semantic Scholar/DOI) [2021-03-26] 9 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1707.08139},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7J7R3R9M/Andreas, Klein - 2017 - Analogs of Linguistic Structure in Deep Representations(2).pdf},
  keywords = {u}
}

@inproceedings{andrews2011TransformationProcessPriors,
  title = {Transformation {{Process Priors}}},
  booktitle = {{{NeurIPS}}},
  author = {Andrews, N. and Eisner, J. M.},
  date = {2011},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QGWF8GE9/Andrews, Eisner - 2011 - Transformation Process Priors(2).pdf},
  keywords = {u}
}

@inproceedings{andrews2014RobustEntityClustering,
  title = {Robust Entity Clustering via Phylogenetic Inference},
  booktitle = {{{ACL}}},
  author = {Andrews, N. and Eisner, J. M. and Dredze, M.},
  date = {2014},
  doi = {10.3115/v1/p14-1073},
  abstract = {Entity clustering must determine when two named-entity mentions refer to the same entity. Typical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity. In this paper, we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data. The generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context. Clustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection process. We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets. © 2014 Association for Computational Linguistics.},
  annotation = {17 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7VXIIC7P/Andrews, Eisner, Dredze - 2014 - Robust entity clustering via phylogenetic inference(2).pdf},
  isbn = {978-1-937284-72-5},
  keywords = {u}
}

@inproceedings{andrews2017BayesianModelingLexical,
  title = {Bayesian Modeling of Lexical Resources for Low-Resource Settings},
  booktitle = {{{ACL}}},
  author = {Andrews, N. and Dredze, M. and Van Durme, B. and Eisner, J. M.},
  date = {2017},
  pages = {1029--1039},
  doi = {10.18653/v1/P17-1095},
  abstract = {Lexical resources such as dictionaries and gazetteers are often used as auxiliary data for tasks such as part-of-speech induction and named-entity recognition. However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of features which generalize better. In this paper, we investigate a more robust approach: we stipulate that the lexicon is the result of an assumed generative process. Practically, this means that we may treat the lexical resources as observations under the proposed generative model. The lexical resources provide training data for the generative model without requiring separate data to estimate lexical feature weights. We evaluate the proposed approach in two settings: part-of-speech induction and low-resource named-entity recognition.},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HXNI9GG7/Andrews et al. - 2017 - Bayesian modeling of lexical resources for low-resource settings(2).pdf}
}

@article{andrieu2010ParticleMarkovChain,
  title = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Andrieu, C. and Doucet, A. and Holenstein, R.},
  date = {2010},
  journaltitle = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
  volume = {72},
  pages = {269--342},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2009.00736.x},
  abstract = {Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a Lévy-driven stochastic volatility model. © 2010 Royal Statistical Society.},
  annotation = {1481 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D7UT6DGS/Andrieu, Doucet, Holenstein - 2010 - Particle Markov chain Monte Carlo methods(2).pdf},
  keywords = {u},
  number = {3}
}

@inproceedings{angeli2010SimpleDomainIndependentProbabilistic,
  title = {A {{Simple Domain}}-{{Independent Probabilistic Approach}} to {{Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Angeli, G. and Liang, P. and Klein, D.},
  date = {2010},
  pages = {502--512},
  url = {http://www.aclweb.org/anthology/D10-1049},
  abstract = {We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework. In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domainsRobocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-of-the-art domain-specific systems both in terms of BLEU scores and human evaluation.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GAYR4XYH/Angeli, Liang, Klein - 2010 - A Simple Domain-Independent Probabilistic Approach to Generation(2).pdf},
  isbn = {1-932432-86-8},
  issue = {October}
}

@inproceedings{angermueller2020PopulationBasedBlackBoxOptimization,
  title = {Population-{{Based Black}}-{{Box Optimization}} for {{Biological Sequence Design}}},
  booktitle = {{{ICML}}},
  author = {Angermueller, Christof and Belanger, David and Gane, Andreea and Mariet, Zelda and Dohan, David and Murphy, Kevin and Colwell, Lucy and Sculley, D.},
  date = {2020},
  volume = {168},
  pages = {757--768},
  doi = {10.1111/j.1365-246X.2006.03227.x},
  url = {http://arxiv.org/abs/2006.03227},
  urldate = {2021-01-04},
  abstract = {The use of black-box optimization for the design of new biological sequences is an emerging research area with potentially revolutionary impact. The cost and latency of wet-lab experiments requires methods that find good sequences in few experimental rounds of large batches of sequences--a setting that off-the-shelf black-box optimization methods are ill-equipped to handle. We find that the performance of existing methods varies drastically across optimization tasks, posing a significant obstacle to real-world applications. To improve robustness, we propose Population-Based Black-Box Optimization (P3BO), which generates batches of sequences by sampling from an ensemble of methods. The number of sequences sampled from any method is proportional to the quality of sequences it previously proposed, allowing P3BO to combine the strengths of individual methods while hedging against their innate brittleness. Adapting the hyper-parameters of each of the methods online using evolutionary optimization further improves performance. Through extensive experiments on in-silico optimization tasks, we show that P3BO outperforms any single method in its population, proposing higher quality sequences as well as more diverse batches. As such, P3BO and Adaptive-P3BO are a crucial step towards deploying ML to real-world sequence design.},
  annotation = {64 citations (Semantic Scholar/DOI) [2021-03-26] 16 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2006.03227},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZMK78P64/Angermueller et al. - 2007 - Population-Based Black-Box Optimization for Biolog.pdf;/home/hiaoxui/.local/share/zotero_files/storage/S89CSH3I/2006.html},
  keywords = {u}
}

@inproceedings{anonymousMultiCQAZeroShotTransfer2020,
  title = {{{MultiCQA}} : {{Zero}}-{{Shot Transfer}} of {{Self}}-{{Supervised Text Matching Models}} on a {{Massive Scale}}},
  booktitle = {{{EMNLP}}},
  author = {Ruckle, A. and Pfeiffer, J. and Gurevych, I.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9UW9Z5J7/Anonymous - 2020 - MultiCQA Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale(2).pdf},
  keywords = {review}
}

@inproceedings{arnaud2017IdentifyingCognateSets,
  title = {Identifying {{Cognate Sets Across Dictionaries}} of {{Related Languages}}},
  booktitle = {{{EMNLP}}},
  author = {Arnaud, A. S. and Beck, D.},
  date = {2017},
  pages = {2519--2528},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZVNLVPDS/Arnaud, Beck - 2017 - Identifying Cognate Sets Across Dictionaries of Related Languages(2).pdf},
  keywords = {u}
}

@inproceedings{artetxe2018UnsupervisedStatisticalMachine,
  title = {Unsupervised {{Statistical Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  date = {2018},
  pages = {3632--3642},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1399},
  url = {http://aclweb.org/anthology/D18-1399},
  urldate = {2020-10-23},
  abstract = {While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at https:// github.com/artetxem/monoses.},
  annotation = {135 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8DQNZEXL/Artetxe et al. - 2018 - Unsupervised Statistical Machine Translation.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{artetxe2019EffectiveApproachUnsupervised,
  title = {An {{Effective Approach}} to {{Unsupervised Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  date = {2019},
  pages = {194--203},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1019},
  url = {https://www.aclweb.org/anthology/P19-1019},
  urldate = {2020-10-23},
  abstract = {While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through onthe-fly back-translation. Together, we obtain large improvements over the previous stateof-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014.},
  annotation = {60 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZIYBN6QU/Artetxe et al. - 2019 - An Effective Approach to Unsupervised Machine Tran.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{arthur2016IncorporatingDiscreteTranslation,
  title = {Incorporating {{Discrete Translation Lexicons}} into {{Neural Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Arthur, P. and Neubig, G. and Nakamura, S.},
  date = {2016},
  pages = {1557--1567},
  abstract = {Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.},
  archiveprefix = {arXiv},
  eprint = {1606.02006},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CP637YFH/Arthur, Neubig, Nakamura - 2016 - Incorporating Discrete Translation Lexicons into Neural Machine Translation(2).pdf}
}

@article{artzi2013WeaklySupervisedLearning,
  title = {Weakly {{Supervised Learning}} of {{Semantic Parsers}} for {{Mapping Instructions}} to {{Actions}}},
  author = {Artzi, Y. and Zettlemoyer, L. S.},
  date = {2013},
  journaltitle = {TACL},
  volume = {1},
  pages = {49--62},
  issn = {2307-387X},
  abstract = {The context in which language is used pro- vides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCGsemantic parsing approach that learns a joint model of mean- ing and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to di- rectly influence learning. It also enables algo- rithms that learn while executing instructions, for example by trying to replicate human ac- tions. Experiments on a benchmark naviga- tional dataset demonstrate strong performance under differing forms of supervision, includ- ing correctly executing 60\% more instruction sets relative to the previous state of the art.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UZ6HJV4K/Artzi, Zettlemoyer - 2013 - Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions(2).pdf},
  keywords = {u}
}

@inproceedings{artzi2015BroadcoverageCCGSemantic,
  title = {Broad-Coverage {{CCG Semantic Parsing}} with {{AMR}}},
  booktitle = {{{EMNLP}}},
  author = {Artzi, Y. and Lee, K. and Zettlemoyer, L. S.},
  date = {2015},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/233HWHKL/Artzi, Lee, Zettlemoyer - 2015 - Broad-coverage CCG Semantic Parsing with AMR(2).pdf}
}

@article{arulampalam2002TutorialParticleFilters,
  title = {A Tutorial on Particle Filters for Online Nonlinear/Nongaussian Bayesian Tracking},
  author = {Arulampalam, M. S. and Maskell, S. and Gordon, N. and Clapp, T.},
  date = {2002},
  journaltitle = {TSP},
  volume = {50},
  pages = {723--737},
  issn = {1053587X},
  doi = {10.1109/9780470544198.ch73},
  abstract = {Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or "particle") representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example},
  annotation = {501 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {978374},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VZCLFS2J/Arulampalam et al. - 2002 - A tutorial on particle filters for online nonlinearnongaussian bayesian tracking(2).pdf},
  isbn = {9780470544198},
  number = {2}
}

@inproceedings{arutiunian2020ReproducibilityChallengeReformer,
  title = {Reproducibility {{Challenge}}: {{Reformer}}},
  booktitle = {{{NeurIPS}}},
  author = {Arutiunian, Artashes and McGuire, Morgan and Gisnås, Hallvar and Imran, Sheik Mohamed and Pleban, Dean and Negi, Priyank and Lozano, David Arnoldo Ortiz},
  date = {2020},
  pages = {10},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GL8XS69B/Arutiunian et al. - Reproducibility Challenge Reformer.pdf},
  langid = {english}
}

@inproceedings{athalye2018ObfuscatedGradientsGive,
  title = {Obfuscated {{Gradients Give}} a {{False Sense}} of {{Security}}: {{Circumventing Defenses}} to {{Adversarial Examples}}},
  booktitle = {{{ICML}}},
  author = {Athalye, A. and Carlini, N. and Wagner, D.},
  date = {2018},
  url = {http://arxiv.org/abs/1802.00420},
  abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. For each of the three types of obfuscated gradients we discover, we describe characteristic behaviors of defenses exhibiting this effect and develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 8 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely and 1 partially.},
  annotation = {1380 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1802.00420},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KCDNE9HQ/Athalye, Carlini, Wagner - 2018 - Obfuscated Gradients Give a False Sense of Security Circumventing Defenses to Adversarial Examples(2).pdf},
  keywords = {u}
}

@inproceedings{auli2013JointLanguageTranslation,
  title = {Joint {{Language}} and {{Translation Modeling}} with {{Recurrent Neural Networks}}},
  booktitle = {{{EMNLP}}},
  author = {Auli, M. and Galley, M. and Quirk, C. and Zweig, G.},
  date = {2013},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3LJFAVJK/Auli et al. - 2013 - Joint Language and Translation Modeling with Recurrent Neural Networks(2).pdf}
}

@report{ba2016LayerNormalization,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016-07-21},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2020-08-04},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  annotation = {1897 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JHCWX8C4/Ba et al. - 2016 - Layer Normalization.pdf;/home/hiaoxui/.local/share/zotero_files/storage/CEGELCJQ/1607.html}
}

@inproceedings{baevski2020ClozedrivenPretrainingSelfattention,
  title = {Cloze-Driven Pretraining of Self-Attention Networks},
  booktitle = {{{EMNLP}}-{{IJCNLP}}},
  author = {Baevski, A. and Edunov, S. and Liu, Y. and Zettlemoyer, L. S. and Auli, M.},
  date = {2020},
  pages = {5360--5369},
  doi = {10.18653/v1/d19-1539},
  abstract = {We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.},
  annotation = {102 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1903.07785},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8AKM2NF5/Baevski et al. - 2020 - Cloze-driven pretraining of self-attention networks(2).pdf},
  isbn = {978-1-950737-90-1},
  keywords = {u}
}

@article{bagchi2013DetectabilityEccentricBinary,
  title = {On the Detectability of Eccentric Binary Pulsars},
  author = {Bagchi, M. and Lorimer, D. R. and Wolfe, S.},
  date = {2013},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  volume = {432},
  pages = {1303--1314},
  issn = {00358711},
  doi = {10.1093/mnras/stt559},
  abstract = {By generalizing earlier work of Johnston \& Kulkarni, we present a detailed description of the reduction in the signal-to-noise ratio for observations of binary pulsars. We present analytical expressions, and provide software, to calculate the sensitivity reduction for orbits of arbitrary eccentricity. We find that this reduction can be quite significant, especially in the case of a massive companion like another neutron star or a black hole. On the other hand, the reduction is less for highly eccentric orbits. We also demonstrate that this loss of sensitivity can be recovered by employing "acceleration search" or "acceleration-jerk search" algorithms.},
  annotation = {19 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1302.4914v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7XRXPPMG/Bagchi, Lorimer, Wolfe - 2013 - On the detectability of eccentric binary pulsars(2).pdf},
  keywords = {u},
  number = {2}
}

@inproceedings{bahdanau2019LearningUnderstandGoal,
  title = {Learning to {{Understand Goal Specifications}} by {{Modelling Reward}}},
  booktitle = {{{ICLR}}},
  author = {Bahdanau, D. and Hill, F. and Leike, J. and Hughes, E. and Kohli, P. and Grefenstette, E.},
  date = {2019},
  pages = {1--26},
  archiveprefix = {arXiv},
  eprint = {1803.00781v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L578ZLWV/Bahdanau et al. - 2019 - Learning to Understand Goal Specifications by Modelling Reward(2).pdf}
}

@inproceedings{baker1998BerkeleyFrameNetProject,
  title = {The {{Berkeley FrameNet Project}}},
  booktitle = {{{ACL}}-{{COLING}}},
  author = {Baker, C. F. and Fillmore, C. J. and Lowe, J. B.},
  date = {1998},
  doi = {10.3115/980845.980860},
  abstract = {FrameNet is a three-year NSF-supported project in corpus-based computational lexicog-raphy, now in its second year (NSF IRI-9618838, "Tools for Lexicon Building"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generaliza-tions, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words de-scribed, and (b) the valence representation (se-mantic and syntactic) of several thousand words and phrases, each accompanied by (c) a repre-sentative collection of annotated corpus attes-tations, which jointly exemplify the observed linkings between "frame elements" and their syntactic realizations (e.g. grammatical func-tion, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computa-tional tools that have been adapted or created in-house for this work.},
  annotation = {2671 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NZ9VIL9X/Baker, Fillmore, Lowe - 1998 - The Berkeley FrameNet Project(2).pdf}
}

@inproceedings{baker2007FrameSemanticStructure,
  title = {Frame Semantic Structure Extraction},
  booktitle = {{{SemEval}}},
  author = {Baker, C. and Ellsworth, M. and Erk, K.},
  date = {2007},
  pages = {99--104},
  abstract = {This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http: //framenet.icsi.Berkeley.edu), and their semantic dependents, which are usually, but not always, their syntactic dependents (including subjects). The training data was FN annotated sentences. In testing, participants automatically annotated three previously unseen texts to match gold standard (human) annotation, including predicting previously unseen frames and roles. Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UC2QJ4FB/Baker, Ellsworth, Erk - 2007 - Frame semantic structure extraction(2).pdf}
}

@inproceedings{ballesteros2015ImprovedTransitionBasedParsing,
  title = {Improved {{Transition}}-{{Based Parsing}} by {{Modeling Characters}} Instead of {{Words}} with {{LSTMs}}},
  booktitle = {{{EMNLP}}},
  author = {Ballesteros, M. and Dyer, C. and Smith, N. A.},
  date = {2015},
  url = {http://arxiv.org/abs/1508.00657},
  abstract = {We present extensions to a continuous-state dependency parsing method that makes it applicable to morphologically rich languages. Starting with a high-performance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.},
  annotation = {270 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1508.00657},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2Z68FTXI/Ballesteros, Dyer, Smith - 2015 - Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs(2).pdf}
}

@inproceedings{banarescu2013AbstractMeaningRepresentation,
  title = {Abstract Meaning Representation for Sembanking},
  booktitle = {Linguistic {{Annotation Workshop}} \& {{Interoperability}} with {{Discourse}}},
  author = {Banarescu, L. and Bonial, C. and Cai, S. and Georgescu, M. and Griffitt, K. and Hermjakob, U. and Knight, K. and Koehn, P. and Palmer, M. and Schneider, N.},
  date = {2013},
  pages = {178--186},
  url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Abstract+Meaning+Representation+for+Sembanking#0},
  abstract = {We describe Abstract Meaning Represen- tation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sen- tences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural lan- guage understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KLYFLLSX/Banarescu et al. - 2013 - Abstract meaning representation for sembanking(2).pdf}
}

@inproceedings{bansal2014TailoringContinuousWord,
  title = {Tailoring {{Continuous Word Representations}} for {{Dependency Parsing}}},
  booktitle = {{{ACL}}},
  author = {Bansal, M. and Gimpel, K. and Livescu, K.},
  date = {2014},
  pages = {809--815},
  doi = {10.3115/v1/p14-2131},
  abstract = {Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the use of continuous word representations as features for dependency parsing. We com- pare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of oth- ers. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representa- tions achieves the best results, suggesting their complementarity.},
  annotation = {276 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XFIXB6PV/Bansal, Gimpel, Livescu - 2014 - Tailoring Continuous Word Representations for Dependency Parsing(2).pdf},
  keywords = {u}
}

@article{baran2008OptimallyAdaptiveIntegration,
  title = {Optimally Adaptive Integration of Univariate Lipschitz Functions},
  author = {Baran, I. and Demaine, E. D. and Katz, D. A.},
  date = {2008},
  journaltitle = {Algorithmica},
  volume = {50},
  pages = {255--278},
  issn = {01784617},
  doi = {10.1007/s00453-007-9093-7},
  annotation = {10 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZSB8PDW/Baran, Demaine, Katz - 2008 - Optimally adaptive integration of univariate lipschitz functions(2).pdf},
  number = {2}
}

@inproceedings{barga2007ConsistentStreamingTime,
  title = {Consistent Streaming through Time: {{A}} Vision for Event Stream Processing},
  booktitle = {{{CIDR}}},
  author = {Barga, R. S. and Goldstein, J. and Ali, M. and Hong, M.},
  date = {2007},
  pages = {363--374},
  abstract = {Event processing will play an increasingly important role in constructing enterprise applications that can immediately react to business critical events. Various technologies have been proposed in recent years, such as event processing, data streams and asynchronous messaging (e.g. pub/sub). We believe these technologies share a common processing model and differ only in target workload, including query language features and consistency requirements. We argue that integrating these technologies is the next step in a natural progression. In this paper, we present an overview and discuss the foundations of CEDR, an event streaming system that embraces a temporal stream model to unify and further enrich query language features, handle imperfections in event delivery, define correctness guarantees, and define operator semantics. We describe specific contributions made so far and outline next steps in developing the CEDR system.},
  archiveprefix = {arXiv},
  eprint = {cs/0612115},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PK8PKIUP/Barga et al. - 2007 - Consistent streaming through time A vision for event stream processing(2).pdf}
}

@inproceedings{barker2001LibraryGenericConcepts,
  title = {A Library of Generic Concepts for Composing Knowledge Bases},
  booktitle = {International {{Conference}} on {{Knowledge Capture}}},
  author = {Barker, K. and Porter, B. and Clark, P.},
  date = {2001},
  pages = {14--21},
  abstract = {Building a knowledge base for a given domain traditionally involves a subject matter expert and a knowledge engineer. One of the goals of our research is to eliminate the knowledge engineer. There are at least two ways to achieve this goal: train domain experts to write axioms (i.e., turn them into knowledge engineers) or create tools that allow users to build knowledge bases without having to write axioms. Our strategy is to create tools that allow users to build knowledge bases through instantiation and assembly of generic knowledge components from a small library. In many ways, creating such a library is like designing an ontology: What are the most general kinds of events and entities? How are these things related hierarchically? What is their meaning and how is it represented? The pressures of making the library usable by domain experts, however, leads to departures from the traditional ontology design goals of coverage, consensus and elegance. In this paper we describe our component library, a hierarchy of reusable, composable, domain-independent knowledge units. The library emphasizes coverage (what is an appropriate set of components for our task), access (how can a domain expert find appropriate components) and semantics (what knowledge and what kind of representation permit useful composition). We have begun building a library on these principles, influenced heavily by linguistic resources. In early evaluations we have put the library into the hands of domain experts (in Biology) having no experience with knowledge bases or knowledge acquisition.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZZLX39BR/Barker, Porter, Clark - 2001 - A library of generic concepts for composing knowledge bases(2).pdf},
  isbn = {1-58113-380-4}
}

@inproceedings{baroni2014DonCountPredict,
  title = {Don’t Count, Predict! {{A}} Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors},
  booktitle = {{{ACL}}},
  author = {Baroni, M. and Dinuand, G. and Kruszewski, G.},
  date = {2014},
  pages = {238--247},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7LXCDQSL/Baroni, Dinuand, Kruszewski - 2014 - Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semanti(2).pdf}
}

@inproceedings{baroni2014DonCountPredicta,
  title = {Don't Count, Predict! {{A}} Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors},
  booktitle = {{{ACL}}},
  author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, Germán},
  date = {2014},
  pages = {238--247},
  publisher = {{Association for Computational Linguistics}},
  location = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-1023},
  url = {http://aclweb.org/anthology/P14-1023},
  urldate = {2020-11-19},
  abstract = {Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts.},
  annotation = {1182 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2IE244X4/Baroni et al. - 2014 - Don't count, predict! A systematic comparison of c.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{barrett2018MeasuringAbstractReasoning,
  title = {Measuring Abstract Reasoning in Neural Networks},
  booktitle = {{{ICML}}},
  author = {Barrett, D. G. T. and Hill, F. and Santoro, A. and Morcos, A. S. and Lillicrap, T.},
  date = {2018},
  issn = {1938-7228},
  url = {http://arxiv.org/abs/1807.04225},
  abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.},
  annotation = {124 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1807.04225},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/77VCFI8X/Barrett et al. - 2018 - Measuring abstract reasoning in neural networks(2).pdf}
}

@inproceedings{barzilay2005CollectiveContentSelection,
  title = {Collective Content Selection for Concept-to-Text Generation},
  booktitle = {{{EMNLP}}},
  author = {Barzilay, R. and Lapata, M.},
  date = {2005},
  pages = {331--338},
  doi = {http://dx.doi.org/10.3115/1220575.1220617},
  url = {http://dl.acm.org/citation.cfm?id=1220617},
  abstract = {A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efficient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IEYIUBTP/Barzilay, Lapata - 2005 - Collective content selection for concept-to-text generation(2).pdf},
  issue = {October}
}

@inproceedings{barzilay2005ModelingLocalCoherence,
  title = {Modeling {{Local Coherence}}: {{An}} Entity Based Approach},
  booktitle = {{{ACL}}},
  author = {Barzilay, R. and Lapata, M.},
  date = {2005},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5VU69JCR/Barzilay, Lapata - 2005 - Modeling Local Coherence An entity based approach(2).pdf},
  keywords = {u}
}

@article{barzilay2008ModelingLocalCoherence,
  title = {Modeling {{Local Coherence}}: {{An Entity}}-{{Based Approach}}},
  author = {Barzilay, R. and Lapata, M.},
  date = {2008},
  journaltitle = {Computational Linguistics},
  volume = {34},
  pages = {1--34},
  issn = {0891-2017},
  doi = {10.1162/coli.2008.34.1.1},
  url = {http://www.mitpressjournals.org/doi/10.1162/coli.2008.34.1.1},
  abstract = {This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.},
  annotation = {607 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JRPK8MXB/Barzilay, Lapata - 2008 - Modeling Local Coherence An Entity-Based Approach(2).pdf},
  isbn = {1932432515},
  keywords = {u},
  number = {1}
}

@inproceedings{beatson2019EfficientOptimizationLoops,
  title = {Efficient {{Optimization}} of {{Loops}} and {{Limits}} with {{Randomized Telescoping Sums}}},
  booktitle = {{{ICML}}},
  author = {Beatson, A. and Adams, R. P.},
  date = {2019},
  url = {http://arxiv.org/abs/1905.07006},
  abstract = {We consider optimization problems in which the objective requires an inner loop with many steps or is the limit of a sequence of increasingly costly approximations. Meta-learning, training recurrent neural networks, and optimization of the solutions to differential equations are all examples of optimization problems with this character. In such problems, it can be expensive to compute the objective function value and its gradient, but truncating the loop or using less accurate approximations can induce biases that damage the overall solution. We propose randomized telescope (RT) gradient estimators, which represent the objective as the sum of a telescoping series and sample linear combinations of terms to provide cheap unbiased gradient estimates. We identify conditions under which RT estimators achieve optimization convergence rates independent of the length of the loop or the required accuracy of the approximation. We also derive a method for tuning RT estimators online to maximize a lower bound on the expected decrease in loss per unit of computation. We evaluate our adaptive RT estimators on a range of applications including meta-optimization of learning rates, variational inference of ODE parameters, and training an LSTM to model long sequences.},
  annotation = {10 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1905.07006},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PQTLULR5/Beatson, Adams - 2019 - Efficient Optimization of Loops and Limits with Randomized Telescoping Sums(2).pdf},
  keywords = {u}
}

@article{beldiceanu1994IntroducingGlobalConstraints,
  title = {Introducing Global Constraints in {{CHIP}}},
  author = {Beldiceanu, N. and Contejean, E.},
  date = {1994},
  journaltitle = {Mathematical and Computer Modelling},
  volume = {20},
  pages = {97--123},
  issn = {08957177},
  doi = {10.1016/0895-7177(94)90127-9},
  abstract = {The purpose of this paper is to show how the introduction of new primitive constraints (e.g., among, diffn, cycle) over finite domains in the constraint logic programming system CHIP result in finding very rapidly good solutions for a large class of difficult sequencing, scheduling, geometrical placement and vehicle routing problems. The among constraint allows us to specify sequencing constraints in a very concise way. For the first time, the diffn constraint allows us to express and to solve directly multi-dimensional placement problems, where one has to consider nonoverlapping constraints between n-dimensional objects (e.g., rectangles, parallelepipeds). The cycle constraint makes it possible to specify a wide range of graph partitioning problems that could not yet be expressed by using current constraint logic programming languages. One of the main advantages of all these new primitives is to take into account more globally a set of elementary constraints. Finally, we point out that all the previous primitive constraints enhance the power of the CHIP system significantly, allowing us to solve real life problems that were not within reach of constraint technology before. © 1994.},
  annotation = {372 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HG47VGSZ/Beldiceanu, Contejean - 1994 - Introducing global constraints in CHIP(2).pdf},
  number = {12}
}

@inproceedings{belkin2018UnderstandDeepLearning,
  title = {To Understand Deep Learning We Need to Understand Kernel Learning},
  booktitle = {{{ICML}}},
  author = {Belkin, M. and Ma, S. and Mandal, S.},
  date = {2018},
  pages = {874--882},
  abstract = {Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, which are typically heavily over-parametrized, tend to fit the training data exactly. Despite this "overfitting", they perform well on test data, a phenomenon not yet fully understood. The first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world and two synthetic datasets, we establish experimentally that kernel machines trained to have zero classification error or near zero regression error (interpolation) perform very well on test data. We proceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exponentially with data size. None of the existing bounds produce non-trivial results for interpolating solutions. We also show experimentally that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels results recently reported for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. Similar performance of overfitted Laplacian and Gaussian classifiers on test, suggests that generalization is tied to the properties of the kernel function rather than the optimization process. Some key phenomena of deep learning are manifested similarly in kernel methods in the modern "overfitted" regime. The combination of the experimental and theoretical results presented in this paper indicates a need for new theoretical ideas for understanding properties of classical kernel methods. We argue that progress on understanding deep learning will be difficult until more tractable "shallow" kernel methods are better understood.},
  archiveprefix = {arXiv},
  eprint = {1802.01396},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IFRGWYQP/Belkin, Ma, Mandal - 2018 - To understand deep learning we need to understand kernel learning(2).pdf},
  isbn = {978-1-5108-6796-3},
  keywords = {u}
}

@inproceedings{belkin2019ReconcilingModernMachinelearning,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias–Variance Trade-Off},
  booktitle = {{{PNAS}}},
  author = {Belkin, M. and Hsu, D. and Ma, S. and Mandal, S.},
  date = {2019},
  pages = {15849--15854},
  issn = {10916490},
  doi = {10.1073/pnas.1903070116},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
  annotation = {282 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1812.11118},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EB38C9XQ/Belkin et al. - 2019 - Reconciling modern machine-learning practice and the classical bias–variance trade-off(2).pdf},
  keywords = {u}
}

@report{beltagy2020LongformerLongDocumentTransformer,
  title = {Longformer: {{The Long}}-{{Document Transformer}}},
  shorttitle = {Longformer},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  date = {2020-12-02},
  url = {http://arxiv.org/abs/2004.05150},
  urldate = {2021-03-10},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  annotation = {169 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2004.05150},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5CKB38XC/Beltagy et al. - 2020 - Longformer The Long-Document Transformer.pdf;/home/hiaoxui/.local/share/zotero_files/storage/FERJWD9Q/Peters, Cohan - 2020 - Longformer The Long-Document Transformer(2).pdf;/home/hiaoxui/.local/share/zotero_files/storage/T98PFHB2/2004.html},
  keywords = {u}
}

@article{belz2008AutomaticGenerationWeather,
  title = {Automatic Generation of Weather Forecast Texts Using Comprehensive Probabilistic Generation-Space Models},
  author = {Belz, A.},
  date = {2008},
  journaltitle = {Natural Language Engineering},
  volume = {14},
  pages = {431--455},
  issn = {13513249},
  doi = {10.1017/S1351324907004664},
  url = {http://www.journals.cambridge.org/abstract_S1351324907004664},
  abstract = {Two important recent trends in natural language generation are (i) probabilistic techniques and (ii) comprehensive approaches that move away from traditional strictly modular and sequential models. This paper reports experiments in which pCRU - a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space - was used to semi-automatically create five different versions of a weather forecast generator. The generators were evaluated in terms of output quality, development time and computational efficiency against (i) human forecasters, (ii) a traditional handcrafted pipelined NLG system and (iii) a HALOGEN-style statistical generator. The most striking result is that despite acquiring all decision-making abilities automatically, the best pCRU generators produce outputs of high enough quality to be scored more highly by human judges than forecasts written by experts. © 2007 Cambridge University Press.},
  annotation = {165 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EKUQZ5GE/Belz - 2008 - Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models(2).pdf},
  isbn = {1469-8110},
  number = {4}
}

@article{bengio2003NeuralProbabilisticLanguage,
  title = {A {{Neural Probabilistic Language Model}}},
  author = {Bengio, Y. and Ducharme, R. and Vincent, P. and Jauvin, C.},
  date = {2003},
  journaltitle = {JMLR},
  issn = {15364046},
  doi = {10.1080/1536383X.2018.1448388},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/STCP399F/Bengio et al. - 2003 - A Neural Probabilistic Language Model(2).pdf}
}

@inproceedings{bengio2007GreedyLayerWiseTraining,
  title = {Greedy {{Layer}}-{{Wise Training}} of {{Deep Networks}}},
  booktitle = {{{NeurIPS}}},
  author = {Bengio, Y. and Lamblin, P. and Popovici, D. and Larochelle, H.},
  date = {2007},
  volume = {19},
  pages = {153},
  issn = {01628828},
  doi = {citeulike-article-id:4640046},
  abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
  eprint = {19018704},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SUBIJ8CN/Bengio et al. - 2007 - Greedy Layer-Wise Training of Deep Networks(2).pdf},
  isbn = {0-262-19568-2},
  number = {1}
}

@inproceedings{bengio2009CurriculumLearning,
  title = {Curriculum Learning},
  booktitle = {{{ICML}}},
  author = {Bengio, Y. and Louradour, J. and Collobert, R. and Weston, J.},
  date = {2009},
  pages = {41--48},
  issn = {0022-5193},
  doi = {10.1145/1553374.1553380},
  url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
  abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illus- trates gradually more concepts, and gradu- ally more complex ones. Here, we formal- ize such training strategies in the context of machine learning, and call them “curricu- lum learning”. In the context of recent re- search studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neu- ral networks), we explore curriculum learn- ing in various set-ups. The experiments show that significant improvements in generaliza- tion can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  annotation = {2294 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {5414602},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NFMBK6FJ/Bengio et al. - 2009 - Curriculum learning(2).pdf},
  isbn = {978-1-60558-516-1}
}

@article{bengio2013RepresentationLearningReview,
  title = {Representation Learning: {{A}} Review and New Perspectives},
  author = {Bengio, Y. and Courville, A. and Vincent, P.},
  date = {2013},
  journaltitle = {PAMI},
  volume = {35},
  pages = {1798--1828},
  issn = {01628828},
  doi = {10.1109/TPAMI.2013.50},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  annotation = {6950 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {23459267},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CQWL88GM/Bengio, Courville, Vincent - 2013 - Representation learning A review and new perspectives(2).pdf},
  isbn = {0162-8828 VO - 35},
  keywords = {u},
  number = {8}
}

@inproceedings{bengio2014DeepGenerativeStochastic,
  title = {Deep {{Generative Stochastic Networks Trainable}} by {{Backprop}}},
  booktitle = {{{ICML}}},
  author = {Bengio, Y. and Thibodeau-Laufer, É. and Alain, G. and Yosinski, J.},
  date = {2014},
  url = {http://arxiv.org/abs/1306.1091},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.},
  annotation = {314 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1306.1091},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/96C9GICH/Bengio et al. - 2014 - Deep Generative Stochastic Networks Trainable by Backprop(2).pdf}
}

@inproceedings{benmalek2019KeepingNotesConditional,
  title = {Keeping {{Notes}}: {{Conditional Natural Language Generation}} with a {{Scratchpad Encoder}}},
  booktitle = {{{ACL}}},
  author = {Benmalek, R. and Khabsa, M. and Desu, S. and Cardie, C. and Banko, M.},
  date = {2019},
  pages = {4157--4167},
  doi = {10.18653/v1/p19-1407},
  abstract = {We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a "scratchpad" memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks --- Machine Translation, Question Generation, and Text Summarization --- and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.},
  annotation = {0 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1906.05275},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5TYISUER/Benmalek et al. - 2019 - Keeping Notes Conditional Natural Language Generation with a Scratchpad Encoder(2).pdf}
}

@inproceedings{berant2013SemanticParsingFreebase,
  title = {Semantic {{Parsing}} on {{Freebase}} from {{Question}}-{{Answer Pairs}}},
  booktitle = {{{EMNLP}}},
  author = {Berant, J. and Chou, A. and Frostig, R. and Liang, P.},
  date = {2013},
  pages = {1533--1544},
  abstract = {In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai andYates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.},
  eprint = {2216100},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZQPESRCB/Berant et al. - 2013 - Semantic Parsing on Freebase from Question-Answer Pairs(2).pdf},
  isbn = {978-1-937284-97-8},
  issue = {October}
}

@inproceedings{berant2014SemanticParsingParaphrasing,
  title = {Semantic {{Parsing}} via {{Paraphrasing}}},
  booktitle = {{{ACL}}},
  author = {Berant, J. and Liang, P.},
  date = {2014},
  pages = {1415--1425},
  issn = {00219258},
  doi = {10.3115/v1/P14-1133},
  url = {http://aclweb.org/anthology/P14-1133},
  abstract = {A central challenge in semantic parsing is handling the myriadways in which knowl- edge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this pa- per, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically gener- ate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best para- phrases the input, and output the corre- sponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves state- of-the-art accuracies on two recently re- leased question-answering datasets. 1},
  annotation = {403 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {1903399},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4NK2VXLG/Berant, Liang - 2014 - Semantic Parsing via Paraphrasing(2).pdf},
  isbn = {978-1-937284-72-5},
  keywords = {u}
}

@article{berant2015ImitationLearningAgendabased,
  title = {Imitation {{Learning}} of {{Agenda}}-Based {{Semantic Parsers}}},
  author = {Berant, J. and Liang, P.},
  date = {2015},
  journaltitle = {TACL},
  volume = {3},
  pages = {545--558},
  abstract = {Semantic parsers conventionally construct logical forms bottom-up in a fixed order, re- sulting in the generation of many extraneous partial logical forms. In this paper, we com- bine ideas from imitation learning and agenda- based parsing to train a semantic parser that searches partial logical forms in a more strate- gic order. Empirically, our parser reduces the number of constructed partial logical forms by an order of magnitude, and obtains a 6x-9x speedup over fixed-order parsing, while main- taining comparable accuracy.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TSPE82XH/Berant, Liang - 2015 - Imitation Learning of Agenda-based Semantic Parsers(2).pdf},
  keywords = {u}
}

@inproceedings{berg-kirkpatrick2010PainlessUnsupervisedLearning,
  title = {Painless Unsupervised Learning with Features},
  booktitle = {{{NAACL}}},
  author = {Berg-kirkpatrick, T. and Bouchard-Côté, A. and DeNero, J. and Klein, D.},
  date = {2010},
  pages = {582--590},
  url = {http://portal.acm.org/citation.cfm?id=1858082},
  abstract = {We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3QRD3A64/Berg-kirkpatrick et al. - 2010 - Painless unsupervised learning with features(2).pdf},
  isbn = {1-932432-65-5},
  issue = {June}
}

@article{berger1996MaximumEntropyApproach,
  title = {A Maximum Entropy Approach to Natural Language Processing},
  author = {Berger, A. L. and Pietra, S. A. D. and Pietra, V. J. D.},
  date = {1996},
  journaltitle = {Computational Linguistics},
  volume = {22},
  pages = {39--71},
  issn = {08912017},
  doi = {10.3115/1075812.1075844},
  url = {http://portal.acm.org/citation.cfm?id=234285.234289},
  abstract = {The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.},
  annotation = {154 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/89GSRF9V/Berger, Pietra, Pietra - 1996 - A maximum entropy approach to natural language processing(2).pdf},
  isbn = {1558603573},
  number = {1}
}

@inproceedings{bergsma2011LearningBilingualLexicons,
  title = {Learning Bilingual Lexicons Using the Visual Similarity of Labeled Web Images},
  booktitle = {{{IJCAI}}},
  author = {Bergsma, S. and Van Durme, B.},
  date = {2011},
  pages = {1764--1769},
  issn = {10450823},
  doi = {10.5591/978-1-57735-516-8/IJCAI11-296},
  abstract = {Speakers of many different languages use the Internet. A common activity among these users is uploading images and associating these images with words (in their own language) as captions, filenames, or surrounding text. We use these explicit, monolingual, image-to-word connections to successfully learn implicit, bilingual, word-to-word translations. Bilingual pairs of words are proposed as translations if their corresponding images have similar visual features. We generate bilingual lexicons in 15 language pairs, focusing on words that have been automatically identified as physical objects. The use of visual similarity substantially improves performance over standard approaches based on string similarity: for generated lexicons with 1000 translations, including visual information leads to an absolute improvement in accuracy of 8-12\% over string edit distance alone.},
  annotation = {59 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T5JQSJHF/Bergsma, Van Durme - 2011 - Learning bilingual lexicons using the visual similarity of labeled web images(2).pdf},
  isbn = {978-1-57735-512-0},
  keywords = {u}
}

@inproceedings{bertero2016LongShortTermMemory,
  title = {A {{Long Short}}-{{Term Memory Framework}} for {{Predicting Humor}} in {{Dialogues}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Bertero, D. and Fung, P.},
  date = {2016},
  pages = {130--135},
  abstract = {We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the au- dience would react. We model the setup- punchline relation of conversational humor with a Long Short-Term Memory, with utter- ance encodings obtained from a Convolutional Neural Network. Out neural network frame- work is able to improve the F-score of8\%over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to un- derstand jokes.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CILDBLFM/Bertero, Fung - 2016 - A Long Short-Term Memory Framework for Predicting Humor in Dialogues(2).pdf},
  isbn = {978-1-941643-91-4}
}

@inproceedings{bethard2013ClearTKTimeMLMinimalistApproach,
  title = {{{ClearTK}}-{{TimeML}}: {{A}} Minimalist Approach to {{TempEval}} 2013},
  booktitle = {Joint {{Conference}} on {{Lexical}} and {{Computational Semantics}}},
  author = {Bethard, S.},
  date = {2013},
  volume = {2},
  pages = {10--14},
  abstract = {The ClearTK-TimeML submission to Temp-Eval 2013 competed in all English tasks: identi-fying events, identifying times, and identifying temporal relations. The system is a pipeline of machine-learning models, each with a small set of features from a simple morpho-syntactic an-notation pipeline, and where temporal relations are only predicted for a small set of syntac-tic constructions and relation types. ClearTK-TimeML ranked 1 st for temporal relation F1, time extent strict F1 and event tense accuracy.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6PBXSEW8/Bethard - 2013 - ClearTK-TimeML A minimalist approach to TempEval 2013(2).pdf},
  isbn = {978-1-937284-49-7},
  keywords = {u}
}

@inproceedings{bhattacharjya2018ProximalGraphicalEvent,
  title = {Proximal Graphical Event Models},
  booktitle = {{{NeurIPS}}},
  author = {Bhattacharjya, D. and Subramanian, D. and Gao, T.},
  date = {2018},
  pages = {8136--8145},
  issn = {10495258},
  abstract = {Event datasets include events that occur irregularly over the timeline and are prevalent in numerous domains. We introduce proximal graphical event models (PGEM) as a representation of such datasets. PGEMs belong to a broader family of models that characterize relationships between various types of events, where the rate of occurrence of an event type depends only on whether or not its parents have occurred in the most recent history. The main advantage over the state of the art models is that they are entirely data driven and do not require additional inputs from the user, which can require knowledge of the domain such as choice of basis functions or hyperparameters in graphical event models. We theoretically justify our learning of optimal windows for parental history and the choices of parental sets, and the algorithm are sound and complete in terms of parent structure learning. We present additional efficient heuristics for learning PGEMs from data, demonstrating their effectiveness on synthetic and real datasets.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YHZ4NZWK/Bhattacharjya, Subramanian, Gao - 2018 - Proximal graphical event models(2).pdf}
}

@inproceedings{bisk2019PIQAReasoningPhysical,
  title = {{{PIQA}}: {{Reasoning}} about {{Physical Commonsense}} in {{Natural Language}}},
  shorttitle = {{{PIQA}}},
  booktitle = {{{AAAI}}},
  author = {Bisk, Yonatan and Zellers, Rowan and Bras, Ronan Le and Gao, Jianfeng and Choi, Yejin},
  date = {2019-11-26},
  url = {http://arxiv.org/abs/1911.11641},
  urldate = {2020-11-20},
  annotation = {49 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1911.11641},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EFRA2GSM/Bisk et al. - 2019 - PIQA Reasoning about Physical Commonsense in Natu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/WQ3DQJTD/1911.html},
  keywords = {u}
}

@article{blei2003LatentDirichletAllocation,
  title = {Latent {{Dirichlet}} Allocation},
  author = {Blei, D. M. and Ng, A. Y. and Jordan, M. I.},
  date = {2003},
  journaltitle = {JMLR},
  volume = {3},
  pages = {993--1022},
  issn = {15324435},
  doi = {10.1016/b978-0-12-411519-4.00006-9},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  annotation = {9994 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IKCVBYEH/Blei, Ng, Jordan - 2003 - Latent Dirichlet allocation(2).pdf},
  number = {4-5}
}

@inproceedings{blondel2018LearningClassifiersFenchelYoung,
  title = {Learning {{Classifiers}} with {{Fenchel}}-{{Young Losses}}: {{Generalized Entropies}}, {{Margins}}, and {{Algorithms}}},
  booktitle = {{{ICML}}},
  author = {Blondel, M. and Martins, A. F. T. and Niculae, V.},
  date = {2018},
  pages = {1--21},
  url = {http://arxiv.org/abs/1805.09717},
  abstract = {We study in this paper Fenchel-Young losses, a generic way to construct convex loss functions from a convex regularizer. We provide an in-depth study of their properties in a broad setting and show that they unify many well-known loss functions. When constructed from a generalized entropy, which includes well-known entropies such as Shannon and Tsallis entropies, we show that Fenchel-Young losses induce a predictive probability distribution and develop an efficient algorithm to compute that distribution for separable entropies. We derive conditions for generalized entropies to yield a distribution with sparse support and losses with a separation margin. Finally, we present both primal and dual algorithms to learn predictive models with generic Fenchel-Young losses.},
  annotation = {21 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1805.09717},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4JLEJDVA/Blondel, Martins, Niculae - 2018 - Learning Classifiers with Fenchel-Young Losses Generalized Entropies, Margins, and Algorithms(2).pdf},
  keywords = {u}
}

@inproceedings{blum1992Training3nodeNeural,
  title = {Training a 3-Node Neural Network Is {{NP}}-Complete},
  booktitle = {{{NeurIPS}}},
  author = {Blum, A. L. and Rivest, R. L.},
  date = {1992},
  volume = {5},
  pages = {117--127},
  issn = {08936080},
  doi = {10.1016/S0893-6080(05)80010-3},
  abstract = {We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for this network so that it produces output consistent with a given set of training examples. We extend the result to other simple networks. We also present a network for which training is hard but where switching to a more powerful representation makes training easier. These results suggest that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. They also suggest the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with nonlinear functions such as sigmoids. © 1992 Pergamon Press plc.},
  annotation = {750 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2W2X6IAC/Blum, Rivest - 1992 - Training a 3-node neural network is NP-complete(2).pdf},
  isbn = {0-55869-019-5},
  number = {1}
}

@inproceedings{blum1998CombiningLabeledUnlabeled,
  title = {Combining Labeled and Unlabeled Data with Co-Training},
  booktitle = {{{COLT}}},
  author = {Blum, A. and Mitchell, T.},
  date = {1998},
  pages = {92--100},
  doi = {10.1145/279943.279962},
  abstract = {The problem of using a large unlabeled sample is considered to boost the performance of a learning algorithm when only a small set of labeled examples is available. In particular, a problem setting is considered to classify web pages, in which the description of each example can be partitioned into two distinct views. A PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data are presented. Also, empirical results on real web-page is giving, indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice.},
  annotation = {5053 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G3CTRJ36/Blum, Mitchell - 1998 - Combining labeled and unlabeled data with co-training(2).pdf}
}

@inproceedings{blunsom2011HierarchicalPitmanYorProcess,
  title = {A Hierarchical {{Pitman}}-{{Yor}} Process {{HMM}} for Unsupervised Part of Speech Induction},
  booktitle = {{{ACL}}-{{HLT}}},
  author = {Blunsom, P. and Cohn, T.},
  date = {2011},
  pages = {865--874},
  abstract = {In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages. © 2011 Association for Computational Linguistics.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YE6JBDDS/Blunsom, Cohn - 2011 - A hierarchical Pitman-Yor process HMM for unsupervised part of speech induction(2).pdf},
  isbn = {978-1-932432-87-9},
  keywords = {u}
}

@article{boito2017UnwrittenLanguagesDemand,
  title = {Unwritten {{Languages Demand Attention Too}}! {{Word Discovery}} with {{Encoder}}-{{Decoder Models}}},
  author = {Boito, M. Z. and Alexandre, B. and Villavicencio, A. and Besacier, L. and Dev, F.},
  date = {2017},
  journaltitle = {ASRU},
  archiveprefix = {arXiv},
  eprint = {1709.05631v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6D4EKYHW/Boito et al. - 2017 - Unwritten Languages Demand Attention Too! Word Discovery with Encoder-Decoder Models(2).pdf},
  keywords = {u}
}

@article{bojanowski2017EnrichingWordVectors,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, P. and Grave, E. and Joulin, A. and Mikolov, T.},
  date = {2017},
  journaltitle = {TACL},
  volume = {5},
  pages = {135--146},
  doi = {10.1162/tacl_a_00051},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  annotation = {4523 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XAHEKTHT/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information(2).pdf},
  keywords = {u}
}

@inproceedings{bollacker2008FreebaseCollaborativelyCreated,
  title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
  booktitle = {International {{Conference}} on {{Management}} of {{Data}}},
  author = {Bollacker, K. and Evans, C. and Paritosh, P. and Sturge, T. and Taylor, J.},
  date = {2008},
  pages = {1247--1250},
  issn = {07308078},
  doi = {10.1145/1376616.1376746},
  url = {http://doi.acm.org/10.1145/1376616.1376746},
  abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
  annotation = {3067 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {3105260},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4NUERHKH/Bollacker et al. - 2008 - Freebase a collaboratively created graph database for structuring human knowledge(2).pdf},
  isbn = {978-1-60558-102-6}
}

@inproceedings{bonial2013RenewingRevisingSemLink,
  title = {Renewing and Revising {{SemLink}}},
  booktitle = {The {{GenLex Workshop}} on {{Linked Data}} in {{Linguistics}}},
  author = {Bonial, C. and Stowe, K. and Palmer, M.},
  date = {2013},
  pages = {9--17},
  url = {http://www.aclweb.org/anthology/W13-5503},
  abstract = {This research describes SemLink, a compre- hensive resource for Natural Language Pro- cessing that maps and unifies several high- quality lexical resources: PropBank, VerbNet, FrameNet, and the recently added OntoNotes sense groupings. Each of these resources was created for slightly different purposes, and therefore each carries unique strengths and limitations. SemLink allows users to lever- age the strengths of each resource and provides the groundwork for incorporating these lexi- cal resources effectively into linked data re- sources. SemLink and the resources included therein are discussed with a focus on the value of using lexical resources in a complemen- tary fashion. Recent improvements to Sem- Link, including the addition of a newresource, the OntoNotes sense groupings, are described. Work to address future goals, including further expansion of SemLink, is also discussed. 1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C22FND2Y/Bonial, Stowe, Palmer - 2013 - Renewing and revising SemLink(2).pdf}
}

@inproceedings{bordes2010LabelRankingAmbiguous,
  title = {Label {{Ranking}} under {{Ambiguous Supervision}} for {{Learning Semantic Correspondences}}},
  booktitle = {{{ICML}}},
  author = {Bordes, A. and Usunier, N. and Weston, J. and Kennedy, P.},
  date = {2010},
  pages = {103--110},
  abstract = {This paper studies the problem of learning from ambiguous supervision, focusing on the task of learning semantic correspondences. A learning problem is said to be ambiguously supervised when, for a given training input, a set of output candidates is provided with no prior of which one is correct. We propose to tackle this problem by solving a related unambiguous task with a label ranking ap- proach and show how and why this performs well on the original task, via the method of task-transfer. We apply it to learning to match natural language sentences to a struc- tured representation of their meaning and empirically demonstrate that this competes with the state-of-the-art on two benchmarks.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FLKA362T/Bordes et al. - 2010 - Label Ranking under Ambiguous Supervision for Learning Semantic Correspondences(2).pdf},
  isbn = {978-1-60558-907-7},
  issue = {Icml}
}

@inproceedings{bordes2014QuestionAnsweringSubgraph,
  title = {Question {{Answering}} with {{Subgraph Embeddings}}},
  booktitle = {{{EMNLP}}},
  author = {Bordes, A. and Chopra, S. and Weston, J.},
  date = {2014},
  url = {http://arxiv.org/abs/1406.3676},
  abstract = {This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a competitive benchmark of the literature.},
  annotation = {486 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1406.3676},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K65SLZ2M/Bordes, Chopra, Weston - 2014 - Question Answering with Subgraph Embeddings(2).pdf}
}

@report{borman2009ExpectationMaximizationAlgorithm,
  title = {The {{Expectation Maximization Algorithm A}} Short Tutorial},
  author = {Borman, S.},
  date = {2009},
  volume = {25},
  pages = {1--9},
  issn = {15360229},
  doi = {10.1097/RLU.0b013e3181b06c41\\r00003072-200909000-00002},
  abstract = {This tutorial discusses the ExpectationMaximization (EM) algorithm of Demp- ster, Laird and Rubin 1. The approach taken follows that of an unpublished note by Stuart Russel, but fleshes out some of the gory details. In order to ensure that the presentation is reasonably self-contained, some of the results on which the derivation of the algorithm is based are presented prior to the main results. The EM algorithm has become a popular tool in statistical estimation problems involving incomplete data, or in problems which can be posed in a sim- ilar form, such as mixture estimation 3, 4. The EM algorithm has also been used in various motion estimation frameworks 5 and variants of it have been used in multiframe superresolution restoration methods which combine motion estimation along the lines of 2.},
  eprint = {19692813},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z4MDDUA4/Borman - 2009 - The Expectation Maximization Algorithm A short tutorial(2).pdf},
  isbn = {0387952845},
  number = {x}
}

@inproceedings{bosnjak2017ProgrammingDifferentiableForth,
  title = {Programming with a {{Differentiable Forth Interpreter}}},
  booktitle = {{{ICML}}},
  author = {Bošnjak, M. and Rocktäschel, T. and Naradowsky, J. and Riedel, S.},
  date = {2017},
  url = {http://arxiv.org/abs/1605.06640},
  abstract = {Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.},
  annotation = {101 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1605.06640},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WIBEF9XY/Bošnjak et al. - 2017 - Programming with a Differentiable Forth Interpreter(2).pdf},
  keywords = {u}
}

@inproceedings{bottou2007TradeoffsLargeScale,
  title = {The {{Tradeoffs}} of {{Large Scale Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Bottou, L. and Bousquet, O.},
  date = {2007},
  volume = {58},
  pages = {1089},
  issn = {03406245},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3BT5IACY/Bottou, Bousquet - 2007 - The Tradeoffs of Large Scale Learning(2).pdf},
  number = {4}
}

@inproceedings{bouraoui2020InducingRelationalKnowledge,
  title = {Inducing {{Relational Knowledge}} from {{BERT}}},
  booktitle = {{{AAAI}}},
  author = {Bouraoui, Zied and Camacho-Collados, Jose and Schockaert, Steven},
  date = {2020-04-03},
  volume = {34},
  pages = {7456--7463},
  doi = {10.1609/aaai.v34i05.6242},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/6242},
  urldate = {2020-11-20},
  abstract = {One of the most remarkable properties of word embeddings is the fact that they capture certain types of semantic and syntactic relationships. Recently, pre-trained language models such as BERT have achieved groundbreaking results across a wide range of Natural Language Processing tasks. However, it is unclear to what extent such models capture relational knowledge beyond what is already captured by standard word embeddings. To explore this question, we propose a methodology for distilling relational knowledge from a pre-trained language model. Starting from a few seed instances of a given relation, we first use a large text corpus to find sentences that are likely to express this relation. We then use a subset of these extracted sentences as templates. Finally, we fine-tune a language model to predict whether a given word pair is likely to be an instance of some relation, when given an instantiated template for that relation as input.},
  annotation = {31 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IWYITHS8/Bouraoui et al. - 2020 - Inducing Relational Knowledge from BERT.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{bowman2015LargeAnnotatedCorpus,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  booktitle = {{{EMNLP}}},
  author = {Bowman, S. R. and Angeli, G. and Potts, C. and Manning, C. D.},
  date = {2015},
  url = {http://arxiv.org/abs/1508.05326},
  abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
  annotation = {1699 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1508.05326},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BCZFN3J5/Bowman et al. - 2015 - A large annotated corpus for learning natural language inference(2).pdf}
}

@inproceedings{bowman2016GeneratingSentencesContinuous,
  title = {Generating {{Sentences}} from a {{Continuous Space}}},
  booktitle = {{{CoNLL}}},
  author = {Bowman, S. R. and Vilnis, L. and Vinyals, O. and Dai, A. M. and Jozefowicz, R. and Bengio, S.},
  date = {2016},
  doi = {10.18653/v1/K16-1002},
  url = {http://arxiv.org/abs/1511.06349},
  abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
  annotation = {1302 citations (Semantic Scholar/DOI) [2021-03-26] 1302 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {387149},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FAH7TGRJ/Bowman et al. - 2016 - Generating Sentences from a Continuous Space(2).pdf},
  keywords = {u}
}

@article{box1968RecentAdvancesForecasting,
  title = {Some {{Recent Advances}} in {{Forecasting}} and {{Control}}},
  author = {Box, G. E. P. and Jenkins, G. M.},
  date = {1968},
  journaltitle = {Journal of the Royal Statistical Society},
  volume = {17},
  pages = {91--109},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VXZBTCXQ/Box, Jenkins - 1968 - Some Recent Advances in Forecasting and Control(2).pdf},
  number = {2}
}

@inproceedings{boyd1998TRENDSystemGenerating,
  title = {{{TREND}}: {{A System}} for {{Generating Intelligent Descriptions}} of {{Time}}-{{Series Data}}},
  booktitle = {{{ICIPS}}},
  author = {Boyd, S.},
  date = {1998},
  pages = {1--5},
  doi = {10.1.1.57.3705},
  abstract = {| A system is described that integrates knowledge-based signal processing and natural lan-guage processing to automatically generate descrip-tions of time-series data. These descriptions are based on short and long-term trends in the data which are detected using wavelet analysis. The basic architec-ture of the system is presented and some experimental results are shown for weather data.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G6XWQQF9/Boyd - 1998 - TREND A System for Generating Intelligent Descriptions of Time-Series Data(2).pdf}
}

@inproceedings{branavan2009ReinforcementLearningMapping,
  title = {Reinforcement Learning for Mapping Instructions to Actions},
  booktitle = {{{ACL}}-{{IJCNLP}}},
  author = {Branavan, S. R. K. and Chen, H. and Zettlemoyer, L. S. and Barzilay, R.},
  date = {2009},
  volume = {1},
  pages = {82},
  issn = {1742206X},
  doi = {10.3115/1687878.1687892},
  url = {http://portal.acm.org/citation.cfm?doid=1687878.1687892},
  abstract = {In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.},
  annotation = {236 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {18493666},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K2N8388X/Branavan et al. - 2009 - Reinforcement learning for mapping instructions to actions(2).pdf},
  isbn = {978-1-932432-45-9},
  issue = {August},
  keywords = {u}
}

@inproceedings{bratieres2014ScalableGaussianProcess,
  title = {Scalable {{Gaussian}} Process Structured Prediction for Grid Factor Graph Applications},
  booktitle = {{{ICML}}},
  author = {Bratières, S. and Quadrianto, N. and Nowozin, S. and Ghahramani, Z},
  date = {2014},
  volume = {2},
  pages = {1625--1636},
  abstract = {Structured prediction is an important and well- studied problem with many applications across machine learning. GPstruct is a recently proposed structured prediction model that offers appealing properties such as being kernelised, non-parametric, and supporting Bayesian inference (Bratieres et al., 2013). The model places a Gaussian process prior over energy functions which describe relationships between input variables and structured output variables. However, the memory demand of GPstruct is quadratic in the number of latent variables and training runtime scales cubically. This prevents GPstruct from being applied to problems involving grid factor graphs, which are prevalent in computer vision and spatial statistics applications. Here we explore a scalable approach to learning GPstruct models based on ensemble learning, with weak learners (predictors) trained on subsets of the latent variables and bootstrap data, which can easily be distributed. We show experiments with 4M latent variables on image segmentation. Our method outperforms widely-used conditional random field models trained with pseudo-likelihood. Moreover, in image segmentation problems it improves over recent state-of- the-art marginal optimisation methods in terms of predictive performance and uncertainty calibration. Finally, it generalises well on all training set sizes.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TR7VMWKI/Bratières et al. - 2014 - Scalable Gaussian process structured prediction for grid factor graph applications(2).pdf},
  isbn = {978-1-63439-397-3}
}

@inproceedings{breese1998EmpiricalAnalysisPredictive,
  title = {Empirical {{Analysis}} of {{Predictive Algorithms}} for {{Collaborative Filtering}}},
  booktitle = {{{UAI}}},
  author = {Breese, John S and Heckerman, David and Kadie, Carl},
  date = {1998},
  pages = {10},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/33PNVZ36/Breese et al. - Empirical Analysis of Predictive Algorithms for Co.pdf},
  langid = {english}
}

@inproceedings{brill2002AnalysisAskMSRQuestionanswering,
  title = {An Analysis of the {{AskMSR}} Question-Answering System},
  booktitle = {{{EMNLP}}},
  author = {Brill, E. and Dumais, S. and Banko, M.},
  date = {2002},
  volume = {10},
  pages = {257--264},
  doi = {10.3115/1118693.1118726},
  url = {http://portal.acm.org/citation.cfm?doid=1118693.1118726},
  abstract = {We describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy. The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers. Because a wrong an-swer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer.},
  annotation = {328 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JKVE7UQV/Brill, Dumais, Banko - 2002 - An analysis of the AskMSR question-answering system(2).pdf},
  issue = {July},
  keywords = {u}
}

@article{brown1993MathematicsStatisticalMachine,
  title = {The Mathematics of Statistical Machine Translation: {{Parameter}} Estimation},
  author = {Brown, P. F. and Pietra, S. A. D. and Pietra, V. J. D. and Mercer, R. L.},
  date = {1993},
  journaltitle = {Computational Linguistics},
  volume = {19},
  pages = {263--311},
  issn = {08912017},
  doi = {10.1080/08839514.2011.559906},
  url = {http://www.aclweb.org/anthology/J93-2003},
  abstract = {We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.},
  annotation = {9 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {3046723},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CT2Y86KZ/Brown et al. - 1993 - The mathematics of statistical machine translation Parameter estimation(2).pdf},
  isbn = {0891-2017},
  number = {2}
}

@inproceedings{brown2017RichEventOntology,
  title = {The {{Rich Event Ontology}}},
  booktitle = {Events and {{Stories}} in the {{News Workshop}}},
  author = {Brown, S. and Bonial, C. and Obrst, L. and Palmer, M.},
  date = {2017},
  pages = {87--97},
  doi = {10.18653/v1/w17-2712},
  abstract = {In this paper we describe a new lexical semantic resource, The Rich Event Ontol-ogy, which provides an independent conceptual backbone to unify existing semantic role labeling (SRL) schemas and augment them with event-to-event causal and temporal relations. By unifying the FrameNet, VerbNet, Automatic Content Extraction, and Rich Entities, Relations and Events resources, the ontology serves as a shared hub for the disparate annotation schemas and therefore enables the combination of SRL training data into a larger, more diverse corpus. By adding temporal and causal relational information not found in any of the independent resources , the ontology facilitates reasoning on and across documents, revealing relationships between events that come together in temporal and causal chains to build more complex scenarios. We envision the open resource serving as a valuable tool for both moving from the ontology to text to query for event types and scenarios of interest, and for moving from text to the ontology to access interpretations of events using the combined semantic information housed there.},
  annotation = {10 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YJKAEC8E/Brown et al. - 2017 - The Rich Event Ontology(2).pdf}
}

@report{brown2020LanguageModelsAre,
  title = {Language {{Models}} Are {{Few}}-{{Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2020-08-07},
  annotation = {774 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5ZXNZI3D/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{bruna2013SpectralNetworksLocally,
  title = {Spectral {{Networks}} and {{Locally Connected Networks}} on {{Graphs}}},
  booktitle = {{{ICLR}}},
  author = {Bruna, J. and Zaremba, W. and Szlam, A. and LeCun, Y.},
  date = {2013},
  url = {http://arxiv.org/abs/1312.6203},
  abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
  annotation = {1926 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1312.6203},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LJR79LH3/Bruna et al. - 2013 - Spectral Networks and Locally Connected Networks on Graphs(2).pdf},
  keywords = {u}
}

@inproceedings{bulat2017SpeakingSeeingUnderstanding,
  title = {Speaking , {{Seeing}} , {{Understanding}} : {{Correlating}} Semantic Models with Conceptual Representation in the Brain},
  booktitle = {{{EMNLP}}},
  author = {Bulat, L. and Clark, S. and Shutova, E.},
  date = {2017},
  pages = {1081--1091},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3IPQZ5XH/Bulat, Clark, Shutova - 2017 - Speaking , Seeing , Understanding Correlating semantic models with conceptual representation in the br(2).pdf},
  keywords = {u},
  number = {2}
}

@inproceedings{buys2017RobustIncrementalNeural,
  title = {Robust {{Incremental Neural Semantic Graph Parsing}}},
  booktitle = {{{ACL}}},
  author = {Buys, J. and Blunsom, P.},
  date = {2017},
  pages = {1215--1226},
  doi = {10.18653/v1/P17-1112},
  url = {http://arxiv.org/abs/1704.07092},
  abstract = {Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focused almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69\% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.},
  annotation = {56 citations (Semantic Scholar/DOI) [2021-03-26] 56 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.07092},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A7CHM8SL/Buys, Blunsom - 2017 - Robust Incremental Neural Semantic Graph Parsing(2).pdf},
  isbn = {978-1-945626-75-3},
  keywords = {u}
}

@inproceedings{caglayan2019ProbingNeedVisual,
  title = {Probing the {{Need}} for {{Visual Context}} in {{Multimodal Machine Translation}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Caglayan, O. and Madhyastha, P. and Specia, L. and Barrault, L.},
  date = {2019},
  url = {http://arxiv.org/abs/1903.08678},
  abstract = {Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.},
  annotation = {42 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1903.08678},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HRGKPIAJ/Caglayan et al. - 2019 - Probing the Need for Visual Context in Multimodal Machine Translation(2).pdf},
  keywords = {u},
  number = {i}
}

@inproceedings{cai2013LargescaleSemanticParsing,
  title = {Large-Scale {{Semantic Parsing}} via {{Schema Matching}} and {{Lexicon Extension}}},
  booktitle = {{{ACL}}},
  author = {Cai, Q. and Yates, A.},
  date = {2013},
  pages = {423--433},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VUXTXDWW/Cai, Yates - 2013 - Large-scale Semantic Parsing via Schema Matching and Lexicon Extension(2).pdf},
  isbn = {978-1-937284-50-3}
}

@inproceedings{cai2017CRFAutoencoderUnsupervised,
  title = {{{CRF Autoencoder}} for {{Unsupervised Dependency Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Cai, J. and Jiang, Y. and Tu, K.},
  date = {2017},
  pages = {1638--1643},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CUXZRHR8/Cai, Jiang, Tu - 2017 - CRF Autoencoder for Unsupervised Dependency Parsing(2).pdf},
  keywords = {u}
}

@inproceedings{cai2018FullEndtoEndSemantic,
  title = {A {{Full End}}-to-{{End Semantic Role Labeler}}, {{Syntax}}-Agnostic {{Over Syntax}}-Aware?},
  booktitle = {{{COLING}}},
  author = {Cai, Jiaxun and He, Shexia and Li, Zuchao and Zhao, Hai},
  date = {2018},
  pages = {13},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZ5EB76M/Cai et al. - A Full End-to-End Semantic Role Labeler, Syntax-ag.pdf},
  langid = {english}
}

@inproceedings{cai2019CoreSemanticFirst,
  title = {Core {{Semantic First}}: {{A Top}}-down {{Approach}} for {{AMR Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Cai, D. and Lam, W.},
  date = {2019},
  pages = {3790--3800},
  url = {http://arxiv.org/abs/1909.04303},
  abstract = {We introduce a novel scheme for parsing a piece of text into its Abstract Meaning Representation (AMR): Graph Spanning based Parsing (GSP). One novel characteristic of GSP is that it constructs a parse graph incrementally in a top-down fashion. Starting from the root, at each step, a new node and its connections to existing nodes will be jointly predicted. The output graph spans the nodes by the distance to the root, following the intuition of first grasping the main ideas then digging into more details. The \textbackslash textit\{core semantic first\} principle emphasizes capturing the main ideas of a sentence, which is of great interest. We evaluate our model on the latest AMR sembank and achieve the state-of-the-art performance in the sense that no heuristic graph re-categorization is adopted. More importantly, the experiments show that our parser is especially good at obtaining the core semantics.},
  annotation = {15 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.04303},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J9GECFFG/Cai, Lam - 2019 - Core Semantic First A Top-down Approach for AMR Parsing(2).pdf},
  keywords = {u}
}

@inproceedings{cai2019GraphTransformerGraphtoSequence,
  title = {Graph {{Transformer}} for {{Graph}}-to-{{Sequence Learning}}},
  booktitle = {{{AAAI}}},
  author = {Cai, D. and Lam, W.},
  date = {2019},
  url = {http://arxiv.org/abs/1911.07470},
  abstract = {The dominant graph-to-sequence transduction models employ graph neural networks for graph representation learning, where the structural information is reflected by the receptive field of neurons. Unlike graph neural networks that restrict the information exchange between immediate neighborhood, we propose a new model, known as Graph Transformer, that uses explicit relation encoding and allows direct communication between two distant nodes. It provides a more efficient way for global graph structure modeling. Experiments on the applications of text generation from Abstract Meaning Representation (AMR) and syntax-based neural machine translation show the superiority of our proposed model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU on LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art results by up to 2.2 points. On the syntax-based translation tasks, our model establishes new single-model state-of-the-art BLEU scores, 21.3 for English-to-German and 14.1 for English-to-Czech, improving over the existing best results, including ensembles, by over 1 BLEU.},
  annotation = {31 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1911.07470},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DGMN36VD/Cai, Lam - 2019 - Graph Transformer for Graph-to-Sequence Learning(2).pdf},
  keywords = {u}
}

@report{cai2020AMRParsingGraphSequence,
  title = {{{AMR Parsing}} via {{Graph}}-{{Sequence Iterative Inference}}},
  author = {Cai, D. and Lam, W.},
  date = {2020},
  url = {http://arxiv.org/abs/2004.05572},
  abstract = {We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input \textbackslash textit\{sequence\} to abstract; and (2) where in the output \textbackslash textit\{graph\} to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported \textbackslash textsc\{Smatch\} scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2\textbackslash\% on LDC2017T10 (AMR 2.0) and 75.4\textbackslash\% on LDC2014T12 (AMR 1.0).},
  annotation = {21 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2004.05572},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N665ZDQU/Cai, Lam - 2020 - AMR Parsing via Graph-Sequence Iterative Inference(2).pdf},
  keywords = {u}
}

@article{camacho-collados2018WordSenseEmbeddings,
  title = {From Word to Sense Embeddings: {{A}} Survey on Vector Representations of Meaning},
  author = {Camacho-Collados, J. and Pilehvar, M. T.},
  date = {2018},
  journaltitle = {JAIR},
  volume = {63},
  pages = {743--788},
  issn = {10769757},
  doi = {10.1613/jair.1.11259},
  abstract = {Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.},
  annotation = {142 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZKSBXBH4/Camacho-Collados, Pilehvar - 2018 - From word to sense embeddings A survey on vector representations of meaning(2).pdf}
}

@inproceedings{caraballo1999DeterminingSpecificityNouns,
  title = {Determining the Specificity of Nouns from Text},
  booktitle = {{{EMNLP}}},
  author = {Caraballo, S. A. and Charniak, E.},
  date = {1999},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WU8F3ZFI/Caraballo, Charniak - 1999 - Determining the specificity of nouns from text(2).pdf}
}

@inproceedings{cassidy2014AnnotationFrameworkDense,
  title = {An Annotation Framework for Dense Event Ordering},
  booktitle = {{{ACL}}},
  author = {Cassidy, T. and McDowell, B. and Chambers, N. and Bethard, S.},
  date = {2014},
  volume = {2},
  pages = {501--506},
  abstract = {Today's event ordering research is heavily dependent on annotated corpora. Current corpora influence shared evaluations and drive algorithm development. Partly due to this dependence, most research focuses on partial orderings of a document's events. For instance, the TempEval competitions and the TimeBank only annotate small portions of the event graph, focusing on the most salient events or on specific types of event pairs (e.g., only events in the same sentence). Deeper temporal reasoners struggle with this sparsity because the entire temporal picture is not represented. This paper proposes a new annotation process with a mechanism to force annotators to label connected graphs. It generates 10 times more relations per document than the TimeBank, and our TimeBank-Dense corpus is larger than all current corpora. We hope this process and its dense corpus encourages research on new global models with deeper reasoning. © 2014 Association for Computational Linguistics.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IP4BLMS6/Cassidy et al. - 2014 - An annotation framework for dense event ordering(2).pdf},
  isbn = {978-1-937284-73-2},
  keywords = {u}
}

@inproceedings{chaganty2016HowMuch131,
  title = {How {{Much}} Is 131 {{Million Dollars}}? {{Putting Numbers}} in {{Perspective}} with {{Compositional Descriptions}}},
  booktitle = {{{ACL}}},
  author = {Chaganty, A. and Liang, P.},
  date = {2016},
  pages = {578--587},
  doi = {10.18653/v1/P16-1055},
  url = {http://aclweb.org/anthology/P16-1055},
  abstract = {How much is 131 million US dollars? To help readers put such numbers in con-text, we propose a new task of automati-cally generating short descriptions known as perspectives, e.g. " \$131 million is about the cost to employ everyone in Texas over a lunch period " . First, we collect a dataset of numeric mentions in news arti-cles, where each mention is labeled with a set of rated perspectives. We then pro-pose a system to generate these descrip-tions consisting of two steps: formula con-struction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on fa-miliarity, numeric proximity and seman-tic compatibility. In generation, we con-vert a formula into natural language us-ing a sequence-to-sequence recurrent neu-ral network. Our system obtains a 15.2\% F 1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.},
  annotation = {13 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1609.00070},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4BAFWNHM/Chaganty, Liang - 2016 - How Much is 131 Million Dollars Putting Numbers in Perspective with Compositional Descriptions(2).pdf},
  isbn = {978-1-5108-2758-5}
}

@inproceedings{chahuneau2013KnowledgerichMorphologicalPriors,
  title = {Knowledge-Rich Morphological Priors for Bayesian Language Models},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Chahuneau, V. and Smith, N. A. and Dyer, C.},
  date = {2013},
  pages = {1206--1215},
  abstract = {We present a morphology-Aware nonparametric Bayesian model of language whose prior distribution uses manually constructed finitestate transducers to capture the word formation processes of particular languages. This relaxes the word independence assumption and enables sharing of statistical strength across, for example, stems or inflectional paradigms in different contexts. Our model can be used in virtually any scenario where multinomial distributions over words would be used. We obtain state-of-The-Art results in language modeling, word alignment, and unsupervised morphological disambiguation for a variety of morphologically rich languages.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L6N2TCLV/Chahuneau, Smith, Dyer - 2013 - Knowledge-rich morphological priors for bayesian language models(2).pdf},
  isbn = {978-1-937284-47-3},
  issue = {June}
}

@inproceedings{chalier2020JointReasoningMultiFaceted,
  title = {Joint {{Reasoning}} for {{Multi}}-{{Faceted Commonsense Knowledge}}},
  booktitle = {{{AKBC}}},
  author = {Chalier, Y. and Razniewski, S. and Weikum, G.},
  date = {2020},
  archiveprefix = {arXiv},
  eprint = {2001.04170v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BBF4JZH7/Chalier, Razniewski, Weikum - 2020 - Joint Reasoning for Multi-Faceted Commonsense Knowledge(4).pdf;/home/hiaoxui/.local/share/zotero_files/storage/LX9FGCAC/Chalier, Razniewski, Weikum - 2020 - Joint Reasoning for Multi-Faceted Commonsense Knowledge(3).pdf},
  keywords = {review}
}

@article{chambers2014DenseEventOrdering,
  title = {Dense {{Event Ordering}} with a {{Multi}}-{{Pass Architecture}}},
  author = {Chambers, N. and Cassidy, T. and McDowell, B. and Bethard, S},
  date = {2014},
  journaltitle = {TACL},
  volume = {2},
  pages = {273--284},
  doi = {10.1162/tacl_a_00182},
  abstract = {The past 10 years of event ordering research has focused on learning partial orderings over document events and time expressions. The most popular corpus, the TimeBank, contains a small subset of the possible ordering graph. Many evaluations follow suit by only testing certain pairs of events (e.g., only main verbs of neighboring sentences). This has led most research to focus on specific learners for partial labelings. This paper attempts to nudge the discussion from identifying some relations to all relations. We present new experiments on strongly connected event graphs that contain ∼10 times more relations per document than the TimeBank. We also describe a shift away from the single learner to a sieve-based architecture that naturally blends multiple learners into a precision-ranked cascade of sieves. Each sieve adds labels to the event graph one at a time, and earlier sieves inform later ones through transitive closure. This paper thus describes innovations in both approach and task. We experiment on the densest event graphs to date and show a 14\% gain over state-of-the-art.},
  annotation = {105 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TK9TF6PH/Chambers et al. - 2014 - Dense Event Ordering with a Multi-Pass Architecture(2).pdf},
  keywords = {u}
}

@inproceedings{chang2015LearningSearchBetter,
  title = {Learning to Search Better than Your Teacher},
  booktitle = {{{ICML}}},
  author = {Chang, K. W. and Krishnamurthy, A. and Agarwal, A. and Daumé III, H. and Langford, J.},
  date = {2015},
  pages = {2058--2066},
  abstract = {Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications.},
  archiveprefix = {arXiv},
  eprint = {1502.02206},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/728HF6SA/Chang et al. - 2015 - Learning to search better than your teacher(2).pdf},
  isbn = {978-1-5108-1058-7},
  keywords = {u}
}

@inproceedings{chen2008LearningSportscastTest,
  title = {Learning to Sportscast: A Test of Grounded Language Acquisition},
  booktitle = {{{ICML}}},
  author = {Chen, D. L. and Mooney, R. J.},
  date = {2008},
  pages = {128--135},
  doi = {http://doi.acm.org/10.1145/1390156.1390173},
  abstract = {We present a novel commentator system that learns language from sportscasts\textbackslash nof simulated soccer games. The system learns to parse and generate\textbackslash ncommentaries without any engineered knowledge about the English language.\textbackslash nTraining is done using only ambiguous supervision in the form of\textbackslash ntextual human commentaries and simulation states of the soccer games.\textbackslash nThe system simultaneously tries to establish correspondences between\textbackslash nthe commentaries and the simulation states as well as build a translation\textbackslash nmodel. We also present a novel algorithm, Iterative Generation Strategy\textbackslash nLearning (IGSL), for deciding which events to comment on. Human evaluations\textbackslash nof the generated commentaries indicate they are of reasonable quality\textbackslash ncompared to human commentaries.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RLPPG6NW/Chen, Mooney - 2008 - Learning to sportscast a test of grounded language acquisition(2).pdf},
  isbn = {978-1-60558-205-4},
  issue = {July}
}

@article{chen2010TrainingMultilingualSportscaster,
  title = {Training a Multilingual Sportscaster: {{Using}} Perceptual Context to Learn Language},
  author = {Chen, D. L. and Kim, J. and Mooney, R. J.},
  date = {2010},
  journaltitle = {JAIR},
  volume = {37},
  pages = {397--435},
  issn = {10769757},
  doi = {10.1613/jair.2962},
  abstract = {We present a novel framework for learning to interpret and generate language using only per-ceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descrip-tive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.},
  annotation = {111 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2T7P4KQI/Chen, Kim, Mooney - 2010 - Training a multilingual sportscaster Using perceptual context to learn language(2).pdf},
  isbn = {1076-9757}
}

@inproceedings{chen2011LearningInterpretNatural,
  title = {Learning to {{Interpret Natural Language Navigation Instructions}} from {{Observations}}},
  booktitle = {{{AAAI}}},
  author = {Chen, D. L. and Mooney, R. J.},
  date = {2011},
  pages = {859--865},
  issn = {1938-7228},
  doi = {10.1.1.221.8069/???},
  url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/download/3701/3974},
  abstract = {The ability to understand natural-language instructions is crit- ical to building intelligent agents that interact with humans. We present a systemthat learns to transformnatural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environ- ments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation in- structions for these environments is used for training and test- ing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the sys- tem is able to automatically learn to correctly interpret a rea- sonable fraction of the complex instructions in this corpus. 1},
  archiveprefix = {arXiv},
  eprint = {23459267},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MUILCJU9/Chen, Mooney - 2011 - Learning to Interpret Natural Language Navigation Instructions from Observations(2).pdf},
  isbn = {978-1-57735-508-3},
  issue = {August},
  keywords = {u}
}

@inproceedings{chen2011PanningGoldFinding,
  title = {Panning for Gold: Finding Relevant Semantic Content for Grounded Language Learning},
  booktitle = {{{MLSLP}}},
  author = {Chen, D. L. and Mooney, R. J.},
  date = {2011},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WN58ZJDT/Chen, Mooney - 2011 - Panning for gold finding relevant semantic content for grounded language learning(2).pdf},
  issue = {June}
}

@report{chen2012LearningLanguageAmbiguous,
  title = {Learning Language from Ambiguous Perceptual Context},
  author = {Chen, D. L.},
  date = {2012},
  pages = {197},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CVZH5S2T/Chen - 2012 - Learning language from ambiguous perceptual context(2).pdf}
}

@inproceedings{chen2014FastAccurateDependency,
  title = {A {{Fast}} and {{Accurate Dependency Parser}} Using {{Neural Networks}}},
  booktitle = {{{EMNLP}}},
  author = {Chen, D. and Manning, C. D.},
  date = {2014},
  doi = {10.3115/v1/d14-1082},
  abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2\% improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2\% unlabeled attachment score on the English Penn Treebank.},
  annotation = {1460 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TF3GQNQT/Chen, Manning - 2014 - A Fast and Accurate Dependency Parser using Neural Networks(2).pdf}
}

@inproceedings{chen2014UnifiedModelWord,
  title = {A {{Unified Model}} for {{Word Sense Representation}} and {{Disambiguation}}},
  booktitle = {{{EMNLP}}},
  author = {Chen, X. and Liu, Z. and Sun, M.},
  date = {2014},
  doi = {10.3115/v1/d14-1110},
  abstract = {Most word representation methods assume that each word owns a single semantic vec-tor. This is usually problematic because lexical ambiguity is ubiquitous, which is also the problem to be resolved by word sense disambiguation. In this paper, we present a unified model for joint word sense representation and disambiguation, which will assign distinct representation-s for each word sense. 1 The basic idea is that both word sense representation (WS-R) and word sense disambiguation (WS-D) will benefit from each other: (1) high-quality WSR will capture rich informa-tion about words and senses, which should be helpful for WSD, and (2) high-quality WSD will provide reliable disambiguat-ed corpora for learning better sense rep-resentations. Experimental results show that, our model improves the performance of contextual word similarity compared to existing WSR methods, outperforms state-of-the-art supervised methods on domain-specific WSD, and achieves competitive performance on coarse-grained all-words WSD.},
  annotation = {293 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JYMUKEQ9/Chen, Liu, Sun - 2014 - A Unified Model for Word Sense Representation and Disambiguation(2).pdf}
}

@inproceedings{chen2017DiscriminativeInformationRetrieval,
  title = {Discriminative {{Information Retrieval}} for {{Question Answering Sentence Selection}}},
  booktitle = {{{EACL}}},
  author = {Chen, T. and Van Durme, B.},
  date = {2017},
  volume = {2},
  pages = {719--725},
  doi = {10.18653/v1/e17-2114},
  abstract = {We propose a framework for discriminative IR atop linguistic features, trained to improve the recall of answer candidate pas-sage retrieval, the initial step in text-based question answering. We formalize this as an instance of linear feature-based IR, demonstrating a 34\% -43\% improvement in recall for candidate triage for QA.},
  annotation = {15 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VN6RS8YK/Chen, Van Durme - 2017 - Discriminative Information Retrieval for Question Answering Sentence Selection(2).pdf}
}

@inproceedings{chen2017EnhancedLSTMNatural,
  title = {Enhanced {{LSTM}} for {{Natural Language Inference}}},
  booktitle = {{{ACL}}},
  author = {Chen, Q. and Wei, S. and Inkpen, D.},
  date = {2017},
  archiveprefix = {arXiv},
  eprint = {1609.06038v3},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FU2C2LBA/Chen, Wei, Inkpen - 2017 - Enhanced LSTM for Natural Language Inference(2).pdf},
  number = {2017}
}

@inproceedings{chen2017VariationalLossyAutoencoder,
  title = {Variational {{Lossy Autoencoder}}},
  booktitle = {{{ICLR}}},
  author = {Chen, X. and Kingma, D. P. and Salimans, T. and Duan, Y. and Dhariwal, P. and Schulman, J. and Sutskever, I. and Abbeel, P.},
  date = {2017},
  pages = {1--17},
  url = {http://arxiv.org/abs/1611.02731},
  abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution \$p(z)\$ and decoding distribution \$p(x|z)\$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
  annotation = {398 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1611.02731},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LMHQ89GP/Chen et al. - 2017 - Variational Lossy Autoencoder(2).pdf},
  keywords = {u}
}

@inproceedings{chen2018LearningKwayDdimensional,
  title = {Learning {{K}}-Way {{D}}-Dimensional {{Discrete Codes}} for {{Compact Embedding Representations}}},
  booktitle = {{{ICML}}},
  author = {Chen, T. and Min, M. R. and Sun, Y.},
  date = {2018},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VYF2DMR9/Chen, Min, Sun - 2018 - Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations(2).pdf}
}

@report{chen2018NeuralOrdinaryDifferential,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, T. Q. and Rubanova, Y. and Bettencourt, J. and Duvenaud, D.},
  date = {2018},
  pages = {1--19},
  archiveprefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BQWY55JL/Chen et al. - 2018 - Neural Ordinary Differential Equations(2).pdf},
  keywords = {u}
}

@inproceedings{chen2018RecurrentNeuralNetworks,
  title = {Recurrent {{Neural Networks}} as {{Weighted Language Recognizers}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Chen, Y. and Gilroy, S. and Maletti, A. and May, J. and Knight, K.},
  date = {2018},
  pages = {2261--2271},
  doi = {10.18653/v1/N18-1205},
  url = {http://arxiv.org/abs/1711.05408},
  abstract = {We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete and APX-hard. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.},
  annotation = {30 citations (Semantic Scholar/DOI) [2021-03-26] 30 citations (Semantic Scholar/arXiv) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KC9RD459/Chen et al. - 2018 - Recurrent Neural Networks as Weighted Language Recognizers(2).pdf}
}

@inproceedings{chen2018VariationalSequentialLabelers,
  title = {Variational Sequential Labelers for Semi-Supervised Learning},
  booktitle = {{{EMNLP}}},
  author = {Chen, M. and Tang, Q. and Livescu, K. and Gimpel, K.},
  date = {2018},
  pages = {215--226},
  doi = {10.18653/v1/d18-1020},
  abstract = {We introduce a family of multitask variational methods for semi-supervised sequence labeling. Our model family consists of a latent-variable generative model and a discriminative labeler. The generative models use latent variables to define the conditional probability of a word given its context, drawing inspiration from word prediction objectives commonly used in learning word embeddings. The labeler helps inject discriminative information into the latent space. We explore several latent variable configurations, including ones with hierarchical structure, which enables the model to account for both label-specific and word-specific information. Our models consistently outperform standard sequential baselines on 8 sequence labeling datasets, and improve further with unlabeled data.},
  annotation = {19 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1906.09535},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DXLMEG3N/Chen et al. - 2018 - Variational sequential labelers for semi-supervised learning(2).pdf},
  isbn = {978-1-948087-84-1},
  keywords = {u}
}

@inproceedings{chen2019EnhancingNeuralDataToText,
  title = {Enhancing {{Neural Data}}-{{To}}-{{Text Generation Models}} with {{External Background Knowledge}}},
  booktitle = {{{EMNLP}}},
  author = {Chen, S. and Wang, J. and Feng, X. and Jiang, F. and Qin, B. and Lin, C.},
  date = {2019},
  pages = {3022--3032},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NXHGVM5Q/Chen et al. - 2019 - Enhancing Neural Data-To-Text Generation Models with External Background Knowledge(2).pdf},
  keywords = {u}
}

@report{chen2020DifferentiableProductQuantization,
  title = {Differentiable {{Product Quantization}} for {{Learning Compact Embedding Layers}}},
  author = {Chen, T. and Li, L and Sun, Y.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LE43Y5VI/Chen, Li, Sun - 2020 - Differentiable Product Quantization for Learning Compact Embedding Layers(5).pdf;/home/hiaoxui/.local/share/zotero_files/storage/MTQKQS7D/Chen, Li, Sun - 2020 - Differentiable Product Quantization for Learning Compact Embedding Layers(6).pdf;/home/hiaoxui/.local/share/zotero_files/storage/NIYLQ59Q/Chen, Li, Sun - 2020 - Differentiable Product Quantization for Learning Compact Embedding Layers(4).pdf},
  keywords = {review}
}

@report{chen2020HierarchicalEntityTyping,
  title = {Hierarchical {{Entity Typing}} via {{Multi}}-Level {{Learning}} to {{Rank}}},
  author = {Chen, T. and Chen, Y. and Van Durme, B.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CZ9MXXXT/Chen, Chen, Van Durme - 2020 - Hierarchical Entity Typing via Multi-level Learning to Rank(2).pdf}
}

@inproceedings{chen2020JointModelingArguments,
  title = {Joint {{Modeling}} of {{Arguments}} for {{Event Understanding}}},
  booktitle = {Workshop on {{Computational Approaches}} to {{Discourse}}},
  author = {Chen, Yunmo and Chen, Tongfei and Van Durme, Benjamin},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CSE565LD/2020.codi-1.10.pdf}
}

@inproceedings{cheng2013RelationalInferenceWikification,
  title = {Relational {{Inference}} for {{Wikiﬁcation}}},
  booktitle = {{{EMNLP}}},
  author = {Cheng, X. and Roth, D.},
  date = {2013},
  abstract = {Wikification, commonly referred to as Disam- biguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific correspondingWikipedia pages. Previous ap- proaches to D2W focused on the use of lo- cal and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these meth- ods fail (often, embarrassingly), when some level of text understanding is needed to sup- port Wikification. In this paper we introduce a novel approach to Wikification by incorpo- rating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Lin- ear Programming (ILP) formulation of Wik- ification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant im- provements in bothWikification and the TAC Entity Linking task.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4ZTSFAUE/Cheng, Roth - 2013 - Relational Inference for Wikiﬁcation(2).pdf}
}

@inproceedings{cheng2016LongShortTermMemoryNetworks,
  title = {Long {{Short}}-{{Term Memory}}-{{Networks}} for {{Machine Reading}}},
  booktitle = {{{EMNLP}}},
  author = {Cheng, J. and Dong, L. and Lapata, M.},
  date = {2016},
  url = {http://arxiv.org/abs/1601.06733},
  abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
  annotation = {582 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1601.06733},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K4XQEWFQ/Cheng, Dong, Lapata - 2016 - Long Short-Term Memory-Networks for Machine Reading(2).pdf}
}

@inproceedings{cheng2020AttendingEntitiesBetter,
  title = {Attending to {{Entities}} for {{Better Text Understanding}}},
  booktitle = {{{AAAI}}},
  author = {Cheng, P. and Erk, K.},
  date = {2020},
  url = {http://arxiv.org/abs/1911.04361},
  abstract = {Recent progress in NLP witnessed the development of large-scale pre-trained language models (GPT, BERT, XLNet, etc.) based on Transformer (Vaswani et al. 2017), and in a range of end tasks, such models have achieved state-of-the-art results, approaching human performance. This demonstrates the power of the stacked self-attention architecture when paired with a sufficient number of layers and a large amount of pre-training data. However, on tasks that require complex and long-distance reasoning where surface-level cues are not enough, there is still a large gap between the pre-trained models and human performance. Strubell et al. (2018) recently showed that it is possible to inject knowledge of syntactic structure into a model through supervised self-attention. We conjecture that a similar injection of semantic knowledge, in particular, coreference information, into an existing model would improve performance on such complex problems. On the LAMBADA (Paperno et al. 2016) task, we show that a model trained from scratch with coreference as auxiliary supervision for self-attention outperforms the largest GPT-2 model, setting the new state-of-the-art, while only containing a tiny fraction of parameters compared to GPT-2. We also conduct a thorough analysis of different variants of model architectures and supervision configurations, suggesting future directions on applying similar techniques to other problems.},
  annotation = {6 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1911.04361},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NZCJ9CQH/Cheng, Erk - 2020 - Attending to Entities for Better Text Understanding(2).pdf}
}

@report{child2019GeneratingLongSequences,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  date = {2019-04-23},
  url = {http://arxiv.org/abs/1904.10509},
  urldate = {2021-03-28},
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n \textbackslash sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  annotation = {266 citations (Semantic Scholar/arXiv) [2021-03-27]},
  archiveprefix = {arXiv},
  eprint = {1904.10509},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RP2BQQMG/Child et al. - 2019 - Generating Long Sequences with Sparse Transformers.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7YF2T6TM/1904.html}
}

@inproceedings{chiticariu2013RulebasedInformationExtraction,
  title = {Rule-Based {{Information Extraction}} Is {{Dead}}! {{Long Live Rule}}-Based {{Information Extraction Systems}}!},
  booktitle = {{{EMNLP}}},
  author = {Chiticariu, L. and Li, Y. and Reiss, F. R.},
  date = {2013},
  url = {https://www.aclweb.org/anthology/D13-1079},
  abstract = {Abstract The rise of “Big Data” analytics over unstructured text has led to renewed interest in information extraction (IE). We surveyed the landscape of IE technologies and identified a major disconnect between industry and academia: while rule - based IE dominates the ...},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YVBWI8HQ/Chiticariu, Li, Reiss - 2013 - Rule-based Information Extraction is Dead! Long Live Rule-based Information Extraction Systems!(2).pdf}
}

@inproceedings{cho2014LearningPhraseRepresentations,
  title = {Learning Phrase Representations Using {{RNN}} Encoder-Decoder for Statistical Machine Translation},
  booktitle = {{{EMNLP}}},
  author = {Cho, K. and Van Merriënboer, B. and Gulcehre, C. and Bahdanau, D. and Bougares, F. and Schwenk, H. and Bengio, Y.},
  date = {2014},
  pages = {1724--1734},
  issn = {09205691},
  doi = {10.3115/v1/d14-1179},
  url = {http://arxiv.org/abs/1406.1078},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  annotation = {9995 citations (Semantic Scholar/DOI) [2021-03-26] 9995 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2079951},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P7TG7XF5/Cho et al. - 2014 - Learning phrase representations using RNN encoder-decoder for statistical machine translation(2).pdf},
  isbn = {978-1-937284-96-1}
}

@inproceedings{cho2014PropertiesNeuralMachine,
  title = {On the {{Properties}} of {{Neural Machine Translation}} : {{Encoder}} – {{Decoder Approaches}}},
  booktitle = {Eighth {{Workshop}} on {{Syntax}}, {{Semantics}} and {{Structure}} in {{Statistical Translation}}},
  author = {Cho, K. and van Merrienboer, B. and Bahdanau, D. and Bengio, Y.},
  date = {2014},
  pages = {103--111},
  doi = {10.3115/v1/W14-4012},
  abstract = {Neural machine translation is a relatively new approach to statistical machine trans-lation based purely on neural networks. The neural machine translation models of-ten consist of an encoder and a decoder. The encoder extracts a fixed-length repre-sentation from a variable-length input sen-tence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the proper-ties of the neural machine translation us-ing two models; RNN Encoder–Decoder and a newly proposed gated recursive con-volutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance de-grades rapidly as the length of the sentence and the number of unknown words in-crease. Furthermore, we find that the pro-posed gated recursive convolutional net-work learns a grammatical structure of a sentence automatically.},
  annotation = {3118 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1409.1259},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KBQVB6RS/Cho et al. - 2014 - On the Properties of Neural Machine Translation Encoder – Decoder Approaches(2).pdf},
  isbn = {978-1-937284-96-1},
  keywords = {u},
  options = {useprefix=true}
}

@inproceedings{choi2018LearningComposeTaskSpecific,
  title = {Learning to {{Compose Task}}-{{Specific Tree Structures}}},
  booktitle = {{{AAAI}}},
  author = {Choi, J. and Yoo, K. M. and Lee, S.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1707.02786v4},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ENETHYQ8/Choi, Yoo, Lee - 2018 - Learning to Compose Task-Specific Tree Structures(2).pdf},
  keywords = {u}
}

@inproceedings{choi2018QuACQuestionAnswering,
  title = {{{QuAC}}: {{Question Answering}} in {{Context}}},
  booktitle = {{{EMNLP}}},
  author = {Choi, E. and He, H. and Iyyer, M. and Yatskar, M. and Yih, W. and Choi, Y. and Liang, P. and Zettlemoyer, L. S.},
  date = {2018},
  url = {http://arxiv.org/abs/1808.07036},
  abstract = {We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.},
  annotation = {248 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1808.07036},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2TUETN4K/Choi et al. - 2018 - QuAC Question Answering in Context(2).pdf},
  keywords = {u}
}

@inproceedings{christodoulopoulos2010TwoDecadesUnsupervised,
  title = {Two Decades of Unsupervised {{POS}} Induction: {{How}} Far Have We Come?},
  booktitle = {{{EMNLP}}},
  author = {Christodoulopoulos, C. and Goldwater, S. and Steedman, M.},
  date = {2010},
  pages = {575--584},
  abstract = {Part-of-speech (POS) induction is one of the most popular tasks in research on unsuper-vised NLP. Many different methods have been proposed, yet comparisons are difficult to make since there is little consensus on evaluation framework, and many papers evaluate against only one or two competitor systems. Here we evaluate seven different POS induction systems spanning nearly 20 years of work, using a variety of measures. We show that some of the oldest (and simplest) systems stand up surprisingly well against more recent approaches. Since most of these systems were developed and tested using data from the WSJ corpus, we compare their generalization abilities by testing on both WSJ and the multilingual Multext-East corpus. Finally, we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype-driven learner. In most cases, the prototype-driven learner outperforms the unsupervised system used to initialize it, yielding state-of-the-art results on WSJ and improvements on non-English corpora. © 2010 Association for Computational Linguistics.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E93S7IMU/Christodoulopoulos, Goldwater, Steedman - 2010 - Two decades of unsupervised POS induction How far have we come(2).pdf},
  isbn = {1-932432-86-8},
  issue = {October},
  keywords = {u}
}

@report{chun2020LearningExplainCausal,
  title = {Learning to {{Explain Causal Rationale}} of {{Stock Price Changes}} in {{Financial Reports}}},
  author = {Chun, Y. E.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y8HCRWBX/Anonymous - 2020 - Learning to Explain Causal Rationale of Stock Price Changes in Financial Reports(2).pdf},
  keywords = {review}
}

@inproceedings{chung2015RecurrentLatentVariable,
  title = {A {{Recurrent Latent Variable Model}} for {{Sequential Data}}},
  booktitle = {{{NeurIPS}}},
  author = {Chung, J. and Kastner, K. and Dinh, L. and Goel, K. and Courville, A. and Bengio, Y.},
  date = {2015},
  pages = {8},
  issn = {10495258},
  url = {http://arxiv.org/abs/1506.02216},
  abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
  annotation = {647 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1506.02216},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UWIVDBHY/Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential Data(2).pdf},
  keywords = {u}
}

@inproceedings{church1988StochasticPartsProgram,
  title = {A {{Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text}}},
  booktitle = {{{ANLC}}},
  author = {Church, K. W.},
  date = {1988},
  pages = {136--143},
  url = {http://www.aclweb.org/anthology/A88-1019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J6XNV7RG/Church - 1988 - A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text(2).pdf},
  keywords = {u}
}

@article{church1990WordAssociationNorms,
  title = {Word Association Norms, Mutual Information, and Lexicography},
  author = {Church, K. W. and Hanks, P.},
  date = {1990},
  journaltitle = {Computational Linguistics},
  volume = {16},
  pages = {22--29},
  doi = {10.3115/981623.981633},
  abstract = {The term word association is used in a very particular sense in the\textbackslash npsycholinguistic literature. \{(Generally\} speaking, subjects respond\textbackslash nquicker than normal to the word nurse if it follows a highly associated\textbackslash nword such as doctor. ) We will extend the term to provide the basis\textbackslash nfor a statistical description of a variety of interesting linguistic\textbackslash nphenomena, ranging from semantic relations of the doctor/nurse type\textbackslash n(content word/content word) to lexico-syntactic co-occurrence constraints\textbackslash nbetween verbs and prepositions (content word/function word). This\textbackslash npaper will propose an objective measure based on the information\textbackslash ntheoretic notion of mutual information, for estimating word association\textbackslash nnorms from computer readable corpora. \{(The\} standard method of obtaining\textbackslash nword association norms, testing a few thousand subjects on a few\textbackslash nhundred words, is both costly and unreliable.) The proposed measure,\textbackslash nthe association ratio, estimates word association norms directly\textbackslash nfrom computer readable corpora, making it possible to estimate norms\textbackslash nfor tens of thousands of words.},
  annotation = {4049 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5LZ2JU2U/Church, Hanks - 1990 - Word association norms, mutual information, and lexicography(2).pdf},
  keywords = {u},
  number = {1}
}

@inproceedings{cianflone2018LetItAgain,
  title = {Let ’ s Do It “ Again ”: {{A First Computational Approach}} to {{Detecting Adverbial Presupposition Triggers}}},
  booktitle = {{{ACL}}},
  author = {Cianflone, A. and Chi, J. and Cheung, K.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1806.04262},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QBHU3MCU/Cianflone, Chi, Cheung - 2018 - Let ’ s do it “ again ” A First Computational Approach to Detecting Adverbial Presupposition Triggers(2).pdf}
}

@inproceedings{clark2018NeuralTextGeneration,
  title = {Neural {{Text Generation}} in {{Stories Using Entity Representations}} as {{Context}}},
  booktitle = {{{NAACL}}},
  author = {Clark, E. and Ji, Y. and Smith, N. A.},
  date = {2018},
  pages = {2250--2260},
  doi = {10.18653/v1/n18-1204},
  abstract = {We introduce an approach to neural text gen-eration that explicitly represents entities men-tioned in the text. Entity representations are vectors that are updated as the text proceeds; they are designed specifically for narrative text like fiction or news stories. Our experiments demonstrate that modeling entities offers a benefit in two automatic evaluations: mention generation (in which a model chooses which entity to mention next and which words to use in the mention) and selection between a correct next sentence and a distractor from later in the same story. We also conduct a human evalu-ation on automatically generated text in story contexts; this study supports our emphasis on entities and suggests directions for further re-search.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FAHC8R67/Clark, Ji, Smith - 2018 - Neural Text Generation in Stories Using Entity Representations as Context(2).pdf},
  keywords = {u}
}

@inproceedings{clark2018SemiSupervisedSequenceModeling,
  title = {Semi-{{Supervised Sequence Modeling}} with {{Cross}}-{{View Training}}},
  booktitle = {{{EMNLP}}},
  author = {Clark, Kevin and Luong, Minh-Thang and Manning, Christopher D. and Le, Quoc V.},
  date = {2018-09-21},
  url = {http://arxiv.org/abs/1809.08370},
  urldate = {2021-01-04},
  abstract = {Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.},
  annotation = {160 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1809.08370},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/47EDQN8X/Clark et al. - 2018 - Semi-Supervised Sequence Modeling with Cross-View .pdf;/home/hiaoxui/.local/share/zotero_files/storage/CBZTXD9W/1809.html},
  keywords = {u}
}

@inproceedings{clark2018SimpleEffectiveMultiParagraph,
  title = {Simple and {{Effective Multi}}-{{Paragraph Reading Comprehension}}},
  booktitle = {{{ACL}}},
  author = {Clark, Christopher and Gardner, Matt},
  date = {2018},
  abstract = {We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a sharednormalization training objective that encourages the model to produce globally correct output. We combine this method with a stateof-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.},
  archiveprefix = {arXiv},
  eprint = {1710.10723},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E64JWYTQ/Clark and Gardner - 2017 - Simple and Effective Multi-Paragraph Reading Compr.pdf},
  langid = {english}
}

@inproceedings{clark2019DonTakeEasy,
  title = {Don't {{Take}} the {{Easy Way Out}} : {{Ensemble Based Methods}} for {{Avoiding Known Dataset Biases}}},
  booktitle = {{{EMNLP}}},
  author = {Clark, C. and Yatskar, M. and Zettlemoyer, L. S.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TGXZX9YK/Clark, Yatskar, Zettlemoyer - 2019 - Don't Take the Easy Way Out Ensemble Based Methods for Avoiding Known Dataset Biases(2).pdf},
  keywords = {review}
}

@inproceedings{clark2019WhatDoesBERT,
  title = {What {{Does BERT Look At}}? {{An Analysis}} of {{BERT}}'s {{Attention}}},
  shorttitle = {What {{Does BERT Look At}}?},
  booktitle = {{{EMNLP}}},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  date = {2019},
  url = {http://arxiv.org/abs/1906.04341},
  urldate = {2021-02-23},
  abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
  annotation = {380 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1906.04341},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W268X4FJ/Clark et al. - 2019 - What Does BERT Look At An Analysis of BERT's Atte.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JR4YGD3N/1906.html}
}

@report{clark2020TransformersSoftReasoners,
  title = {Transformers as {{Soft Reasoners}} over {{Language}}},
  author = {Clark, P. and Tafjord, O. and Richardson, K.},
  date = {2020},
  url = {http://arxiv.org/abs/2002.05867},
  abstract = {AI has long pursued the goal of having systems reason over *explicitly provided* knowledge, but building suitable representations has proved challenging. Here we explore whether transformers can similarly learn to reason (or emulate reasoning), but using rules expressed in language, thus bypassing a formal representation. We provide the first demonstration that this is possible, and characterize the extent of this capability. To do this, we use a collection of synthetic datasets that test increasing levels of reasoning complexity (number of rules, presence of negation, and depth of chaining). We find transformers appear to learn rule-based reasoning with high (99\%) accuracy on these datasets, and in a way that generalizes to test data requiring substantially deeper chaining than in the training data (95\%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as a limited "soft theorem prover" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering. All datasets and a live demo are available at http://rule-reasoning.apps.allenai.org/},
  annotation = {25 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2002.05867},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D8LQVAAP/Clark, Tafjord, Richardson - 2020 - Transformers as Soft Reasoners over Language(2).pdf},
  keywords = {u}
}

@inproceedings{clarke2010DrivingSemanticParsing,
  title = {Driving {{Semantic Parsing}} from the {{World}}’s {{Response}}},
  booktitle = {{{CoNLL}}},
  author = {Clarke, J. and Goldwasser, D. and Chang, M. and Roth, D.},
  date = {2010},
  pages = {18--27},
  abstract = {Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ERPEU79Q/Clarke et al. - 2010 - Driving Semantic Parsing from the World’s Response(2).pdf},
  isbn = {978-1-932432-83-1},
  issue = {July}
}

@inproceedings{cocos2017WordSenseFiltering,
  title = {Word {{Sense Filtering Improves Embedding}}-{{Based Lexical Substitution}}},
  booktitle = {Workshop on {{Sense}}, {{Concept}} and {{Entity Representations}} and Their {{Applications}}},
  author = {Cocos, A. and Apidianaki, M. and Callison-Burch, C.},
  date = {2017},
  volume = {April},
  pages = {110--119},
  issn = {0893-0341, 0893-0341},
  doi = {http://dx.doi.org/10.1097/00002093-199700112-00003},
  abstract = {The role of word sense disambiguation in lexical substitution has been questioned due to the high performance of vector space models which propose good sub-stitutes without explicitly accounting for sense. We show that a filtering mecha-nism based on a sense inventory optimized for substitutability can improve the results of these models. Our sense inventory is constructed using a clustering method which generates paraphrase clusters that are congruent with lexical substitution an-notations in a development set. The re-sults show that lexical substitution can still benefit from senses which can improve the output of vector space paraphrase ranking models.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5744SMH7/Cocos, Apidianaki, Callison-Burch - 2017 - Word Sense Filtering Improves Embedding-Based Lexical Substitution(2).pdf},
  keywords = {u}
}

@inproceedings{cohen2018SphericalCNNs,
  title = {Spherical {{CNNs}}},
  booktitle = {{{ICLR}}},
  author = {Cohen, T. S. and Geiger, M. and Köhler, J. and Welling, M.},
  date = {2018},
  pages = {1--15},
  archiveprefix = {arXiv},
  eprint = {1801.10130v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJ3LLIGC/Cohen et al. - 2018 - Spherical CNNs(2).pdf},
  number = {3}
}

@report{cohen2019DifferentiableRepresentationsMultihop,
  title = {Differentiable {{Representations For Multihop Inference Rules}}},
  author = {Cohen, W. W. and Sun, H. and Hofer, R. A. and Siegler, M.},
  date = {2019},
  volume = {1},
  pages = {1--10},
  url = {http://arxiv.org/abs/1905.10417},
  abstract = {We present efficient differentiable implementations of second-order multi-hop reasoning using a large symbolic knowledge base (KB). We introduce a new operation which can be used to compositionally construct second-order multi-hop templates in a neural model, and evaluate a number of alternative implementations, with different time and memory trade offs. These techniques scale to KBs with millions of entities and tens of millions of triples, and lead to simple models with competitive performance on several learning tasks requiring multi-hop reasoning.},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1905.10417},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2SRMCBWC/Cohen et al. - 2019 - Differentiable Representations For Multihop Inference Rules(2).pdf},
  keywords = {u}
}

@inproceedings{collell2018NeuralNetworkCrossModal,
  title = {Do {{Neural Network Cross}}-{{Modal Mappings Really Bridge Modalities}}?},
  booktitle = {{{ACL}}},
  author = {Collell, G. and Moens, M.},
  date = {2018},
  pages = {1--7},
  doi = {arXiv:1805.07616v1},
  url = {http://arxiv.org/abs/1805.07616},
  abstract = {Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space. The predicted vectors are then used to perform e.g., retrieval or labeling. Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors. However, whether this is achieved has not been investigated yet. Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue. In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions. Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors. In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.},
  annotation = {11 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1805.07616},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8GFE3SC6/Collell, Moens - 2018 - Do Neural Network Cross-Modal Mappings Really Bridge Modalities(2).pdf},
  keywords = {u}
}

@thesis{collins1999HeadDrivenStatisticalModels,
  title = {Head-{{Driven Statistical Models}} for {{Natural Language Parsing}}},
  author = {Collins, M.},
  date = {1999},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XE9ENIDX/Collins - 1999 - Head-Driven Statistical Models for Natural Language Parsing(2).pdf}
}

@report{collins2000MEMMsLogLinearTagging,
  title = {{{MEMMs}} ( {{Log}}-{{Linear Tagging Models}} )},
  author = {Collins, M.},
  date = {2000},
  pages = {1--12},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3V5BRYBC/Collins - 2000 - MEMMs ( Log-Linear Tagging Models )(2).pdf}
}

@inproceedings{collins2002DiscrimativeTrainingMethods,
  title = {Discrimative {{Training Methods}} for {{Hidden Markov Models}}: {{Theory}} and {{Experiments}} with {{Perceptron Algorithms}}},
  booktitle = {{{EMNLP}}},
  author = {Collins, M.},
  date = {2002},
  pages = {1--8},
  doi = {10.3115/1118693.1118694},
  url = {http://www.aclweb.org/anthology/W02-1001},
  abstract = {New algorithms for training tagging models, as an alternative to MEM and CRF models},
  annotation = {2229 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CN8G83MF/Collins - 2002 - Discrimative Training Methods for Hidden Markov Models Theory and Experiments with Perceptron Algorithms(2).pdf},
  issue = {July},
  keywords = {u}
}

@inproceedings{collins2005ClauseRestructuringStatistical,
  title = {Clause Restructuring for Statistical Machine Translation},
  booktitle = {{{ACL}}},
  author = {Collins, M. and Koehn, P. and Kučerová, I.},
  date = {2005},
  pages = {531--540},
  doi = {10.3115/1219840.1219906},
  url = {http://portal.acm.org/citation.cfm?doid=1219840.1219906},
  abstract = {We describe a method for incorporating syntactic information in statistical\textbackslash nmachine translation systems. The first step of the method is to parse\textbackslash nthe source language string that is being translated. The second step\textbackslash nis to apply a series of transformations to the parse tree, effectively\textbackslash nreordering the surface string on the source language side of the\textbackslash ntranslation system. The goal of this step is to recover an underlying\textbackslash nword order that is closer to the target language word-order than\textbackslash nthe original string. The reordering approach is applied as a pre-processing\textbackslash nstep in both the training and decoding phases of a phrase-based statistical\textbackslash nMT system. We describe experiments on translation from German to\textbackslash nEnglish, showing an improvement from 25.2\% Bleu score for a baseline\textbackslash nsystem to 26.8\% Bleu score for the system with reordering, a statistically\textbackslash nsignificant improvement.},
  annotation = {615 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ESCQBQ42/Collins, Koehn, Kučerová - 2005 - Clause restructuring for statistical machine translation(2).pdf},
  isbn = {1-932432-51-5}
}

@report{collins2011ProbabilisticContextFreeGrammars,
  title = {Probabilistic {{Context}}-{{Free Grammars}} ({{PCFGs}})},
  author = {Collins, M.},
  date = {2011},
  pages = {1--18},
  issn = {0387307680},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3FCKRDGQ/Collins - 2011 - Probabilistic Context-Free Grammars (PCFGs)(2).pdf}
}

@report{collins2011StatisticalMachineTranslation,
  title = {Statistical {{Machine Translation}}: {{IBM Models}} 1 and 2},
  author = {Collins, M.},
  date = {2011},
  pages = {1--22},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HPF48MLJ/Collins - 2011 - Statistical Machine Translation IBM Models 1 and 2(2).pdf}
}

@report{collins2011TaggingProblemsHidden,
  title = {Tagging {{Problems}}, and {{Hidden Markov Models}}},
  author = {Collins, M.},
  date = {2011},
  volume = {1},
  pages = {1--22},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KE6AHTQX/Collins - 2011 - Tagging Problems, and Hidden Markov Models(2).pdf},
  number = {1}
}

@report{collins2013ForwardBackwardAlgorithm,
  title = {The {{Forward}}-{{Backward Algorithm}}},
  author = {Collins, M.},
  date = {2013},
  pages = {1--4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LR59RVL7/Collins - 2013 - The Forward-Backward Algorithm(2).pdf}
}

@report{collins2013InsideOutsideAlgorithm,
  title = {The {{Inside}}-{{Outside Algorithm}}},
  author = {Collins, M.},
  date = {2013},
  pages = {1--15},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YISTQPET/Collins - 2013 - The Inside-Outside Algorithm(2).pdf}
}

@report{collins2013LanguageModeling,
  title = {Language {{Modeling}}},
  author = {Collins, M.},
  date = {2013},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FG4PT4QS/Collins - 2013 - Language Modeling(2).pdf}
}

@report{collins2013LexicalizedProbabilisticContextFree,
  title = {Lexicalized {{Probabilistic Context}}-{{Free Grammars Weaknesses}} of {{PCFGs}} as {{Parsing Models}}},
  author = {Collins, M.},
  date = {2013},
  pages = {1--22},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QT2QVMJS/Collins - 2013 - Lexicalized Probabilistic Context-Free Grammars Weaknesses of PCFGs as Parsing Models(2).pdf},
  number = {1}
}

@report{collins2013LogLinearModels,
  title = {Log-{{Linear Models}}},
  author = {Collins, M.},
  date = {2013},
  volume = {1},
  pages = {1--20},
  doi = {10.1007/978-1-4614-2299-0},
  abstract = {Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers a few seconds beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger. Anticipation requires modeling the driver’s action space, events inside the vehicle such as their head movements, and also the outside environment. Performing this joint modeling makes anticipation a challenging problem. In this work we anticipate driving maneuvers a few seconds before they occur. For this purpose we equip a car with cameras and a computing device to capture the context from both inside and outside of the car. We represent the context with expressive features and propose an Autoregressive Input-Output HMM to model the contextual information. We evaluate our approach on a diverse data set with 1180 miles of natural freeway and city driving and show that we can anticipate maneuvers 3.5 seconds before they occur with over 80\% F1-score. Our computation time during inference is under 3.6 milliseconds.},
  archiveprefix = {arXiv},
  eprint = {1504.02789v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HN4FVIG8/Collins - 2013 - Log-Linear Models(2).pdf},
  isbn = {9781461422990},
  number = {1}
}

@report{collins2013LogLinearModelsMEMMs,
  title = {Log-{{Linear Models}}, {{MEMMs}}, and {{CRFs}}},
  author = {Collins, M.},
  date = {2013},
  pages = {1--11},
  abstract = {Throughout this note I'll use underline to denote vectors. For example, w ∈ R d will be a vector with components w 1 , w 2 , . . . w d . We use exp(x) for the exponential function, i.e., exp(x) = e x . 2 Log-linear models We have sets X and Y: we will assume that Y is a finite set. Our goal is to build a model that estimates the conditional probability p(y|x) of a label y ∈ Y given an input x ∈ X . For example, x might be a word, and y might be a candidate part-of-speech (noun, verb, preposition etc.) for that word. We have a feature-vector definition φ : X × Y → R d . We also assume a parameter vector w ∈ R d . Given these definitions, log-linear models take the following form: p(y|x; w) =},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JQXGHB8G/Collins - 2013 - Log-Linear Models, MEMMs, and CRFs(2).pdf}
}

@report{collins2013NaiveBayesModel,
  title = {The {{Naive Bayes Model}}, {{Maximum}}-{{Likelihood Estimation}}, and the {{EM Algorithm}}},
  author = {Collins, M.},
  date = {2013},
  pages = {1--21},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y2MJSVBW/Collins - 2013 - The Naive Bayes Model, Maximum-Likelihood Estimation, and the EM Algorithm(2).pdf}
}

@report{collins2013PhraseBasedTranslationModels,
  title = {Phrase-{{Based Translation Models}}},
  author = {Collins, M.},
  date = {2013},
  pages = {1--12},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZEMCNUJN/Collins - 2013 - Phrase-Based Translation Models(2).pdf}
}

@report{cong2020FewShotEventDetection,
  title = {Few-{{Shot Event Detection}} with {{Prototypical Amortized Conditional Random Field}}},
  author = {Cong, Xin and Cui, Shiyao and Yu, Bowen and Liu, Tingwen and Wang, Yubin and Wang, Bin},
  date = {2020-12-03},
  url = {http://arxiv.org/abs/2012.02353},
  urldate = {2020-12-07},
  abstract = {Event Detection, a fundamental task of Information Extraction, tends to struggle when it needs to recognize novel event types with a few samples, i.e. Few-Shot Event Detection (FSED). Previous identify-then-classify paradigm attempts to solve this problem in the pipeline manner but ignores the trigger discrepancy between event types, thus suffering from the error propagation. In this paper, we present a novel unified joint model which converts the task to a few-shot tagging problem with a double-part tagging scheme. To this end, we first design the Prototypical Amortized Conditional Random Field (PA-CRF) to model the label dependency in the few-shot scenario, which builds prototypical amortization networks to approximate the transition scores between labels based on the label prototypes. Then Gaussian distribution is introduced for the modeling of the transition scores in PA-CRF to alleviate the uncertain estimation resulting from insufficient data. We conduct experiments on the benchmark dataset FewEvent and the experimental results show that the tagging based methods are better than existing pipeline and joint learning methods. In addition, the proposed PA-CRF achieves the best results on the public dataset.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2012.02353},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FRDVZBFN/Cong et al. - 2020 - Few-Shot Event Detection with Prototypical Amortiz.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JATKC8PH/2012.html},
  keywords = {u}
}

@article{copestake2005MinimalRecursionSemantics,
  title = {Minimal Recursion Semantics: {{An}} Introduction},
  author = {Copestake, A. and Flickinger, D. and Pollard, C. and Sag, I. A.},
  date = {2005},
  journaltitle = {Research on Language and Computation},
  volume = {3},
  pages = {281--332},
  issn = {15707075},
  doi = {10.1007/s11168-006-6327-9},
  abstract = {Minimal recursion semantics (MRS) is a framework for computational semantics that is suitable for parsing and generation and that can be implemented in typed feature structure formalisms. We discuss why, in general, a semantic representation with minimal structure is desirable and illustrate how a descriptively adequate representation with a nonrecursive structure may be achieved. MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition. We have integrated MRS with a broad-coverage HPSG grammar.},
  annotation = {1134 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6TNNQETB/Copestake et al. - 2005 - Minimal recursion semantics An introduction(2).pdf},
  keywords = {u},
  number = {4}
}

@inproceedings{cotterell2017ProbabilisticTypologyDeep,
  title = {Probabilistic {{Typology}}: {{Deep Generative Models}} of {{Vowel Inventories}}},
  booktitle = {{{ACL}}},
  author = {Cotterell, R. and Eisner, J. M.},
  date = {2017},
  doi = {10.18653/v1/P17-1109},
  url = {http://arxiv.org/abs/1705.01684},
  abstract = {Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.},
  annotation = {20 citations (Semantic Scholar/DOI) [2021-03-26] 20 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1705.01684},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S6DSAFAK/Cotterell, Eisner - 2017 - Probabilistic Typology Deep Generative Models of Vowel Inventories(2).pdf},
  isbn = {978-1-945626-75-3}
}

@inproceedings{cotterell2018AreAllLanguages,
  title = {Are {{All Languages Equally Hard}} to {{Language}}-{{Model}}?},
  booktitle = {{{NAACL}}},
  author = {Cotterell, R. and Mielke, S. J. and Eisner, J. M. and Roark, B.},
  date = {2018},
  pages = {536--541},
  doi = {10.18653/v1/n18-2085},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1806.03743v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N9H2G64H/Cotterell et al. - 2018 - Are All Languages Equally Hard to Language-Model(2).pdf}
}

@article{cour2011LearningPartialLabels,
  title = {Learning from Partial Labels},
  author = {Cour, T. and Sapp, B. and Taskar, B.},
  date = {2011},
  journaltitle = {JMLR},
  volume = {12},
  pages = {1501--1536},
  issn = {15324435},
  doi = {10.1007/978-0-387-31439-6_100092},
  abstract = {We address the problem of partially-labeled multiclass classification, where instead of a single label per instance, the algorithm is given a candidate set of labels, only one of which is correct. Our setting is motivated by a common scenario in many image and video collections, where only partial access to labels is available. The goal is to learn a classifier that can disambiguate the partiallylabeled training instances, and generalize to unseen data. We define an intuitive property of the data distribution that sharply characterizes the ability to learn in this setting and show that effective learning is possible even when all the data is only partially labeled. Exploiting this property of the data, we propose a convex learning formulation based on minimization of a loss function appropriate for the partial label setting. We analyze the conditions under which our loss function is asymptotically consistent, as well as its generalization and transductive performance. We apply our framework to identifying faces culled from web news sources and to naming characters in TV series and movies; in particular, we annotated and experimented on a very large video data set and achieve 6\% error for character naming on 16 episodes of the TV series Lost. © 2011 Timothee Cour, Ben Sapp and Ben Taskar.},
  annotation = {247 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UUIXR77S/Cour, Sapp, Taskar - 2011 - Learning from partial labels(2).pdf}
}

@inproceedings{creutz2002UnsupervisedDiscoveryMorphemes,
  title = {Unsupervised {{Discovery}} of {{Morphemes}}},
  booktitle = {Workshop of the {{ACL Special Interest Group}} in {{Computational Phonology}}},
  author = {Creutz, M. and Lagus, K.},
  date = {2002},
  pages = {21--30},
  url = {http://arxiv.org/abs/cs/0205057},
  abstract = {We present two methods for unsupervised segmentation of words into morpheme-like units. The model utilized is especially suited for languages with a rich morphology, such as Finnish. The first method is based on the Minimum Description Length (MDL) principle and works online. In the second method, Maximum Likelihood (ML) optimization is used. The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis. Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current state-of-the-art system.},
  annotation = {354 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {cs/0205057},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TPBU89JT/Creutz, Lagus - 2002 - Unsupervised Discovery of Morphemes(2).pdf},
  issue = {July}
}

@inproceedings{croce2019AuditingDeepLearning,
  title = {Auditing {{Deep Learning}} Processes through {{Kernel}}-Based {{Explanatory Models}}},
  booktitle = {{{EMNLP}}},
  author = {Croce, D. and Rossini, D. and Basili, R.},
  date = {2019},
  pages = {4037--4046},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KAUV2HT5/Croce, Rossini, Basili - 2019 - Auditing Deep Learning processes through Kernel-based Explanatory Models(2).pdf},
  keywords = {u}
}

@report{culkin2020IterativeParaphrasticAugmentation,
  title = {Iterative {{Paraphrastic Augmentation}} with {{Discriminative Span}}-Based {{Alignment}}},
  author = {Culkin, R. and Hu, J. E. and Stengel-Eskin, E. and Qin, G. and Van Durme, B.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I32LMWPQ/Culkin et al. - 2020 - Iterative Paraphrastic Augmentation with Discriminative Span-based Alignment(2).pdf}
}

@article{dahab2008TextOntoExAutomaticOntology,
  title = {{{TextOntoEx}}: {{Automatic}} Ontology Construction from Natural {{English}} Text},
  author = {Dahab, M. Y. and Hassan, H. A. and Rafea, A.},
  date = {2008},
  journaltitle = {Expert Systems with Applications},
  volume = {34},
  pages = {1474--1480},
  issn = {09574174},
  doi = {10.1016/j.eswa.2007.01.043},
  abstract = {Most of existing ontologies construction tools support construction of ontological relations (e.g., taxonomy, equivalence, etc.) but they do not support construction of domain relations, non-taxonomic conceptual relationships (e.g., causes, caused by, treat, treated by, has-member, contain, material-of, operated-by, controls, etc.). Domain relations are found mainly in text sources. TextOntoEx constructs ontology from natural domain text using semantic pattern-based approach. TextOntoEx is a chain between linguistic analysis and ontology engineering. TextOntoEx analyses natural domain text to extract candidate relations and then maps them into meaning representation to facilitate constructing ontology. The paper explains this approach in more details and discusses some experiments on deriving ontology from natural text. © 2007.},
  annotation = {119 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2HIGTQPH/Dahab, Hassan, Rafea - 2008 - TextOntoEx Automatic ontology construction from natural English text(2).pdf},
  keywords = {u},
  number = {2}
}

@inproceedings{dai2015SemisupervisedSequenceLearning,
  title = {Semi-Supervised {{Sequence Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Dai, A. M. and Le, Q. V.},
  date = {2015},
  pages = {1--10},
  issn = {10495258},
  url = {http://arxiv.org/abs/1511.01432},
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  annotation = {709 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {414454},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z93ZB6NP/Dai, Le - 2015 - Semi-supervised Sequence Learning(2).pdf}
}

@inproceedings{dai2017DeformableConvolutionalNetworks,
  title = {Deformable {{Convolutional Networks}}},
  booktitle = {{{ICCV}}},
  author = {Dai, J. and Qi, H. and Xiong, Y. and Li, Y. and Zhang, G. and Hu, H. and Wei, Y.},
  date = {2017},
  issn = {0004-6361},
  doi = {10.1051/0004-6361/201527329},
  url = {http://arxiv.org/abs/1703.06211},
  abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation. The code would be released.},
  annotation = {144 citations (Semantic Scholar/DOI) [2021-03-26] 1434 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {23459267},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H9HMGMM4/Dai et al. - 2017 - Deformable Convolutional Networks(2).pdf},
  isbn = {2-00-401243-9}
}

@inproceedings{dai2019TransformerXLAttentiveLanguage,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed}}-{{Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  booktitle = {{{ACL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  date = {2019-06-02},
  url = {http://arxiv.org/abs/1901.02860},
  urldate = {2020-11-11},
  annotation = {898 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2WZVCLRF/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/home/hiaoxui/.local/share/zotero_files/storage/RF7BGPER/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/home/hiaoxui/.local/share/zotero_files/storage/76C4WBNU/1901.html;/home/hiaoxui/.local/share/zotero_files/storage/EEC7VLT4/1901.html},
  keywords = {u}
}

@inproceedings{das2011SemisupervisedFramesemanticParsing,
  title = {Semi-Supervised Frame-Semantic Parsing for Unknown Predicates},
  booktitle = {{{ACL}}-{{HLT}}},
  author = {Das, D. and Smith, N. A.},
  date = {2011},
  pages = {1435--1444},
  abstract = {We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15\% absolute improvement in frame identification accuracy and over 13\% absolute improvement in full frame-semantic parsing F 1 score on a blind test set, over a state-of-the-art supervised baseline. © 2011 Association for Computational Linguistics.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UZQRRLTH/Das, Smith - 2011 - Semi-supervised frame-semantic parsing for unknown predicates(2).pdf}
}

@article{das2014FrameSemanticParsing,
  title = {Frame-{{Semantic Parsing}}},
  author = {Das, D. and Chen, D. and Martins, A. F. T. and Scneider, N. and Smith, N. A.},
  date = {2014},
  journaltitle = {Computational Linguistics},
  volume = {40},
  issn = {04194217},
  doi = {10.1162/COLI},
  abstract = {We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in nat- ural language texts. The architecture com- bines various linguistically-motivated clas- sification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually de- fine linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost en- tirely unsupervised and completely language- independent; it relies on few language re- sources and is thus suitable for a large num- ber of languages. Furthermore, unlike much recent work, our approach can identify ex- pressions of various types and syntactic con- structions. We demonstrate a significant im- provement in identification accuracy, com- pared with less sophisticated baselines.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z6DQJTTF/Das et al. - 2014 - Frame-Semantic Parsing(2).pdf},
  number = {1}
}

@inproceedings{daumeiii2004PhrasebasedHMMApproach,
  title = {A Phrase-Based {{HMM}} Approach to Document/Abstract Alignment.},
  booktitle = {{{EMNLP}}},
  author = {Daumé III, H. and Marcu, D.},
  date = {2004},
  pages = {119--126},
  abstract = {We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical sum-marization systems that can be trained on large cor-pora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outper-forms both the Cut \& Paste alignment model (Jing, 2002) and models developed in the context of ma-chine translation (Brown et al., 1993).},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F3A2KBGU/Daumé III, Marcu - 2004 - A phrase-based HMM approach to documentabstract alignment(2).pdf}
}

@inproceedings{daumeiii2007FrustratinglyEasyDomain,
  title = {Frustratingly {{Easy Domain Adaptation}}},
  booktitle = {{{ACL}}},
  author = {Daumé III, H.},
  date = {2007},
  pages = {256--263},
  url = {http://arxiv.org/abs/0907.1815},
  abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
  annotation = {1487 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {0907.1815},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/787QKURD/Daumé III - 2007 - Frustratingly Easy Domain Adaptation(2).pdf},
  issue = {June}
}

@article{daumeiii2009SearchbasedStructuredPrediction,
  title = {Search-Based Structured Prediction},
  author = {Daumé III, H. and Langford, J. and Marcu, D.},
  date = {2009},
  journaltitle = {Machine Learning},
  volume = {75},
  pages = {297--325},
  issn = {08856125},
  doi = {10.1007/s10994-009-5106-x},
  abstract = {We present Searn, an algorithm for integrating search and learning to solve complex structured prediction problems such as those that occur in natural language, speech, computational biology, and vision. Searn is a meta-algorithm that transforms these complex problems into simple classification problems to which any binary classifier may be applied. Unlike current algorithms for structured learning that require decomposition of both the loss function and the feature functions over the predicted structure, Searn is able to learn prediction functions for any loss function and any class of features. Moreover, Searn comes with a strong, natural theoretical guarantee: good performance on the derived classification problems implies good performance on the structured prediction problem. © 2009 Springer Science+Business Media, LLC.},
  annotation = {522 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZQ4W22MQ/Daumé III, Langford, Marcu - 2009 - Search-based structured prediction(2).pdf},
  number = {3}
}

@inproceedings{dawborn2014DocrepLightweightEfficient,
  title = {Docrep: {{A}} Lightweight and Efficient Document Representation Framework},
  booktitle = {{{COLING}}},
  author = {Dawborn, Tim and Curran, James R},
  date = {2014},
  pages = {10},
  abstract = {Modelling linguistic phenomena requires highly structured and complex data representations. Document representation frameworks (DRFs) provide an interface to store and retrieve multiple annotation layers over a document. Researchers face a difficult choice: using a heavy-weight DRF or implement a custom DRF. The cost is substantial, either learning a new complex system, or continually adding features to a home-grown system that risks overrunning its original scope.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UW4DI5EH/Dawborn and Curran - docrep A lightweight and efficient document repre.pdf},
  langid = {english}
}

@report{day2021MessagePassingNeural,
  title = {Message {{Passing Neural Processes}}},
  author = {Day, B. and Cangea, C. and Jamasb, A. R. and Liò, P.},
  date = {2021},
  pages = {10},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TWSFSYVP/Authors - Message Passing Neural Processes.pdf},
  keywords = {review},
  langid = {english}
}

@inproceedings{daza2019TranslateLabelEncoderDecoder,
  title = {Translate and {{Label}}! {{An Encoder}}-{{Decoder Approach}} for {{Cross}}-Lingual {{Semantic Role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Daza, A. and Frank, A.},
  date = {2019},
  pages = {603--615},
  url = {http://arxiv.org/abs/1908.11326},
  abstract = {We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our model does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages.},
  annotation = {8 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1908.11326},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/22F92S7H/Daza, Frank - 2019 - Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling(2).pdf},
  keywords = {u}
}

@inproceedings{demarcken1996LinguisticStructureComposition,
  title = {Linguistic {{Structure}} as {{Composition}} and {{Perturbation}}},
  booktitle = {{{ACL}}},
  author = {de Marcken, C. G.},
  date = {1996},
  pages = {335--341},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F9XMLJSH/de Marcken - 1996 - Linguistic Structure as Composition and Perturbation(2).pdf},
  options = {useprefix=true}
}

@thesis{demarcken1996UnsupervisedLanguageAcquisition,
  title = {Unsupervised {{Language Acquisition}}},
  author = {de Marcken, C. G.},
  date = {1996},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YP2RVQ7Y/de Marcken - 1996 - Unsupervised Language Acquisition(2).pdf},
  options = {useprefix=true}
}

@article{dempster1977MaximumLikelihoodIncomplete,
  title = {Maximum {{Likelihood}} from {{Incomplete Data}} via the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  date = {1977},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {39},
  pages = {1--22},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J57J73P9/Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via the EM.pdf},
  keywords = {u},
  number = {1}
}

@report{demszky2018TransformingQuestionAnswering,
  title = {Transforming {{Question Answering Datasets}}},
  author = {Demszky, D. and Guu, K.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1809.02922v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2VMAAT3Y/Demszky, Guu - 2018 - Transforming Question Answering Datasets(2).pdf}
}

@inproceedings{denero2008SamplingAlignmentStructure,
  title = {Sampling Alignment Structure under a {{Bayesian}} Translation Model},
  booktitle = {{{EMNLP}}},
  author = {DeNero, J. and Bouchard-Côté, A. and Klein, D.},
  date = {2008},
  pages = {314--323},
  doi = {10.3115/1613715.1613758},
  url = {http://dl.acm.org/citation.cfm?id=1613758},
  abstract = {We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previ- ous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrase- based translation systems.},
  annotation = {102 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q83FVKQ7/DeNero, Bouchard-Côté, Klein - 2008 - Sampling alignment structure under a Bayesian translation model(2).pdf},
  issue = {October},
  keywords = {u}
}

@inproceedings{denero2009FastConsensusDecoding,
  title = {Fast Consensus Decoding over Translation Forests},
  booktitle = {{{ACL}}-{{IJCNLP}}},
  author = {DeNero, J. and Chiang, D. and Knight, K.},
  date = {2009},
  volume = {3},
  pages = {1--177},
  issn = {0149645X},
  doi = {10.1109/MWSYM.2003.1211046},
  abstract = {The minimum Bayes risk (MBR) decoding objective\textbackslash nimproves BLEU scores for machine translation\textbackslash noutput relative to the standard Viterbi objective\textbackslash nof maximizing model score. However,\textbackslash nMBR targeting BLEU is prohibitively slow to optimize\textbackslash nover k-best lists for large k. In this paper,\textbackslash nwe introduce and analyze an alternative to\textbackslash nMBR that is equally effective at improving performance,\textbackslash nyet is asymptotically faster—running\textbackslash n80 times faster than MBR in experiments with\textbackslash n1000-best lists. Furthermore, our fast decoding\textbackslash nprocedure can select output sentences based on\textbackslash ndistributions over entire forests of translations, in\textbackslash naddition to k-best lists. We evaluate our procedure\textbackslash non translation forests from two large-scale,\textbackslash nstate-of-the-art hierarchical machine translation\textbackslash nsystems. Our forest-based decoding objective\textbackslash nconsistently outperforms k-best list MBR, giving\textbackslash nimprovements of up to 1.0 BLEU.\textbackslash n1 Introduction\textbackslash nIn statistical},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FHVHN9K8/DeNero, Chiang, Knight - 2009 - Fast consensus decoding over translation forests(2).pdf},
  isbn = {0-7803-7694-3},
  keywords = {u},
  number = {1}
}

@inproceedings{deng2017RandomForestBased,
  title = {The {{Random Forest Based Detection}} of {{Shadowsock}}'s {{Traffic}}},
  booktitle = {{{IHMSC}}},
  author = {Deng, Z. and Liu, Z. and Chen, Z. and Guo, Y.},
  date = {2017},
  pages = {75--78},
  doi = {10.1109/IHMSC.2017.132},
  url = {http://ieeexplore.ieee.org/document/8048116/},
  abstract = {—With the development of anonymous communi-cation technology, it has led to the fact that the network monitoring is becoming more and more difficult. If the anonymous traffic can be effectively identified, the abuse of such technology can be prevented. Since the study of machine learning is rapidly developing these years, this paper applies the Random Forest Algorithm ---a semi-supervised learning method ---into the traffic detection of Shadowsocks. We can get over 85\% detection accuracy rate in our experiments after applying Random Forest Algorithm by collecting train set, gathering features, training models and predicting results. With the scale of train set and test set increase, the detection accuracy rate gradually increases until it becomes constant. We will make several adjustments on train set, test set and feature set to reduce the false alarm rate and false rate when detecting.},
  annotation = {13 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DF3H2HQE/Deng et al. - 2017 - The Random Forest Based Detection of Shadowsock's Traffic(2).pdf},
  isbn = {978-1-5386-3021-1}
}

@inproceedings{deng2018LatentAlignmentVariational,
  title = {Latent Alignment and Variational Attention},
  booktitle = {{{NeurIPS}}},
  author = {Deng, Y. and Kim, Y. and Chiu, J. and Guo, D. and Rush, A. M.},
  date = {2018},
  pages = {9712--9724},
  issn = {10495258},
  abstract = {Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training. On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.},
  archiveprefix = {arXiv},
  eprint = {1807.03756},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HLZBVFYX/Deng et al. - 2018 - Latent alignment and variational attention(2).pdf},
  keywords = {u}
}

@inproceedings{deutsch2018DistributionalOrthographicAggregation,
  title = {A {{Distributional}} and {{Orthographic Aggregation Model}} for {{English Derivational Morphology}}},
  booktitle = {{{ACL}}},
  author = {Deutsch, D. and Hewitt, J. and Roth, D.},
  date = {2018},
  pages = {1--10},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TFWPK7SK/Deutsch, Hewitt, Roth - 2018 - A Distributional and Orthographic Aggregation Model for English Derivational Morphology(2).pdf}
}

@inproceedings{devlin2014FastRobustNeural,
  title = {Fast and {{Robust Neural Network Joint Models}} for {{Statistical Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Devlin, J. and Zbib, R. and Huang, Z. and Lamar, T. and Schwartz, R. and Makhoul, J.},
  date = {2014},
  pages = {1370--1380},
  doi = {10.3115/v1/p14-1129},
  abstract = {Recent work has shown success in us-ing neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexi-calized and can be integrated into any MT decoder. We also present several varia-tions of the NNJM which provide signif-icant additive improvements. Although the model is quite simple, it yields strong empirical results. On the NIST OpenMT12 Arabic-English condi-tion, the NNJM features produce a gain of +3.0 BLEU on top of a powerful, feature-rich baseline which already includes a target-only NNLM. The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chi-ang's (2007) original Hiero implementa-tion. Additionally, we describe two novel tech-niques for overcoming the historically high cost of using NNLM-style models in MT decoding. These techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM.},
  annotation = {492 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7SU3HKM9/Devlin et al. - 2014 - Fast and Robust Neural Network Joint Models for Statistical Machine Translation(2).pdf},
  keywords = {u}
}

@inproceedings{devlin2019BERTPretrainingDeep,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  booktitle = {{{NAACL}}},
  author = {Devlin, J. and Chang, M. and Lee, K. and Toutanova, K.},
  date = {2019},
  url = {http://arxiv.org/abs/1810.04805},
  annotation = {9994 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XQV8W66S/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding(2).pdf}
}

@inproceedings{diab2002UnsupervisedMethodWord,
  title = {An Unsupervised Method for Word Sense Tagging Using Parallel Corpora},
  booktitle = {{{ACL}}},
  author = {Diab, M. and Resnik, P.},
  date = {2002},
  doi = {10.3115/1073083.1073126},
  abstract = {With an increasing number of languages making their way to our desktops everyday via the Internet, researchers have come to realize the lack of linguistic knowledge resources for scarcely represented/studied languages. In an attempt to bootstrap some of the required linguistic resources for some of those languages, this paper presents an unsupervised method for automatic multilingual word sense tagging using parallel corpora. The method is evaluated on the English Brown corpus and its translation into three different languages: French, German and Spanish. A preliminary evaluation of the proposed method yielded results of up to 79\% accuracy rate for the English data on 81.8\% of the SemCor manually tagged data.},
  annotation = {274 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/647TD6KP/Diab, Resnik - 2002 - An unsupervised method for word sense tagging using parallel corpora(2).pdf}
}

@inproceedings{ding2019GreedyStrategyWorks,
  title = {Greedy Strategy Works for K-Center Clustering with Outliers and Coreset Construction},
  booktitle = {Leibniz {{International Proceedings}} in {{Informatics}}},
  author = {Ding, H. and Yu, H. and Wang, Z.},
  date = {2019},
  issn = {18688969},
  doi = {10.4230/LIPIcs.ESA.2019.40},
  abstract = {We study the problem of k-center clustering with outliers in arbitrary metrics and Euclidean space. Though a number of methods have been developed in the past decades, it is still quite challenging to design quality guaranteed algorithm with low complexity for this problem. Our idea is inspired by the greedy method, Gonzalez’s algorithm, for solving the problem of ordinary k-center clustering. Based on some novel observations, we show that this greedy strategy actually can handle k-center clustering with outliers efficiently, in terms of clustering quality and time complexity. We further show that the greedy approach yields small coreset for the problem in doubling metrics, so as to reduce the time complexity significantly. Our algorithms are easy to implement in practice. We test our method on both synthetic and real datasets. The experimental results suggest that our algorithms can achieve near optimal solutions and yield lower running times comparing with existing methods.},
  annotation = {7 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1901.08219},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/73LH76FF/Ding, Yu, Wang - 2019 - Greedy strategy works for k-center clustering with outliers and coreset construction(2).pdf},
  isbn = {978-3-95977-124-5}
}

@inproceedings{dodge2019RNNArchitectureLearning,
  title = {{{RNN Architecture Learning}} with {{Sparse Regularization}}},
  booktitle = {{{EMNLP}}},
  author = {Dodge, J. and Schwartz, R. and Peng, H. and Smith, N. A.},
  date = {2019},
  abstract = {Neural models for NLP typically use large numbers of parameters to reach state-of-the-art performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learning sparse, parameter-efficient NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018), a family of models that is closely connected to weighted finite-state automata (WFSAs). We take advantage of rational RNNs' natural grouping of the weights, so the group lasso penalty directly removes WFSA states, substantially reducing the number of parameters in the model. Our experiments on a number of sentiment analysis datasets, using both GloVe and BERT embeddings, show that our approach learns neural structures which have fewer parameters without sacrificing performance relative to parameter-rich baselines. Our method also highlights the interpretable properties of rational RNNs. We show that sparsifying such models makes them easier to visualize, and we present models that rely exclusively on as few as three WFSAs after pruning more than 90\% of the weights. We publicly release our code.},
  archiveprefix = {arXiv},
  eprint = {1909.03011},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BKZNN62W/Dodge et al. - 2019 - RNN Architecture Learning with Sparse Regularization(2).pdf},
  keywords = {u}
}

@inproceedings{dodge2019ShowYourWork,
  title = {Show {{Your Work}}: {{Improved Reporting}} of {{Experimental Results}}},
  booktitle = {{{EMNLP}}},
  author = {Dodge, J. and Gururangan, S. and Card, D. and Schwartz, R. and Smith, N. A.},
  date = {2019},
  pages = {2185--2194},
  url = {http://arxiv.org/abs/1909.03004},
  abstract = {Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.},
  annotation = {78 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.03004},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F7U5AB4H/Dodge et al. - 2019 - Show Your Work Improved Reporting of Experimental Results(2).pdf},
  keywords = {u}
}

@report{doersch2016TutorialVariationalAutoencoders,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, C.},
  date = {2016},
  pages = {1--23},
  archiveprefix = {arXiv},
  eprint = {1606.05908v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QSD6WTR8/Doersch - 2016 - Tutorial on Variational Autoencoders(2).pdf}
}

@book{donaldson1978ChildrenMinds,
  title = {Children's {{Minds}}},
  author = {Donaldson, M. C.},
  date = {1978}
}

@inproceedings{dong2015QuestionAnsweringFreebase,
  title = {Question {{Answering}} over {{Freebase}} with {{Multi}}-{{Column Convolutional Neural Networks}}},
  booktitle = {{{ACL}}},
  author = {Dong, L. and Wei, F. and Zhou, M. and Xu, K.},
  date = {2015},
  pages = {260--269},
  abstract = {Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct exten- sive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.},
  eprint = {1683577},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FS8IQWKX/Dong et al. - 2015 - Question Answering over Freebase with Multi-Column Convolutional Neural Networks(2).pdf},
  isbn = {978-1-941643-72-3},
  keywords = {u}
}

@inproceedings{dong2016LanguageLogicalForm,
  title = {Language to {{Logical Form}} with {{Neural Attention}}},
  booktitle = {{{ACL}}},
  author = {Dong, L. and Lapata, M.},
  date = {2016},
  pages = {33--43},
  doi = {10.18653/v1/P16-1004},
  url = {http://arxiv.org/abs/1601.01280},
  abstract = {Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.},
  annotation = {431 citations (Semantic Scholar/DOI) [2021-03-26] 431 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1601.01280},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WQ6TDSTI/Dong, Lapata - 2016 - Language to Logical Form with Neural Attention(2).pdf},
  isbn = {978-1-5108-2758-5}
}

@inproceedings{dong2018CoarsetoFineDecodingNeural,
  title = {Coarse-to-{{Fine Decoding}} for {{Neural Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Dong, L. and Lapata, M.},
  date = {2018},
  pages = {1--12},
  url = {http://arxiv.org/abs/1805.04793},
  abstract = {Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.},
  annotation = {188 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1805.04793},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2QA59852/Dong, Lapata - 2018 - Coarse-to-Fine Decoding for Neural Semantic Parsing(2).pdf},
  keywords = {u}
}

@inproceedings{dong2019NeuralLogicMachines,
  title = {Neural Logic Machines},
  booktitle = {{{ICLR}}},
  author = {Dong, H. and Mao, J. and Lin, T. and Wang, C. and Li, L. and Zhou, D.},
  date = {2019},
  abstract = {We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks-as function approximators, and logic programming-as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.},
  archiveprefix = {arXiv},
  eprint = {1904.11694},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W3MCLUUG/Dong et al. - 2019 - Neural logic machines(2).pdf},
  keywords = {u}
}

@inproceedings{dou2018Data2TextStudioAutomated,
  title = {{{Data2Text Studio}} : {{Automated Text Generation}} from {{Structured Data}}},
  booktitle = {{{EMNLP}}},
  author = {Dou, L. and Qin, G. and Wang, J. and Yao, J. and Lin, C.},
  date = {2018},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3GLSZ3JC/Dou et al. - 2018 - Data2Text Studio Automated Text Generation from Structured Data(2).pdf}
}

@inproceedings{dou2019InvestigatingMetaLearningAlgorithms,
  title = {Investigating {{Meta}}-{{Learning Algorithms}} for {{Low}}-{{Resource Natural Language Understanding Tasks}}},
  booktitle = {{{EMNLP}}},
  author = {Dou, Z. and Yu, K. and Anastasopoulos, A.},
  date = {2019},
  pages = {1192--1197},
  abstract = {Learning general representations of text is a fundamental problem for many natural language understanding (NLU) tasks. Previously, researchers have proposed to use language model pre-training and multi-task learning to learn robust representations. However, these methods can achieve sub-optimal performance in low-resource scenarios. Inspired by the recent success of optimization-based meta-learning algorithms, in this paper, we explore the model-agnostic meta-learning algorithm (MAML) and its variants for low-resource NLU tasks. We validate our methods on the GLUE benchmark and show that our proposed models can outperform several strong baselines. We further empirically demonstrate that the learned representations can be adapted to new tasks efficiently and effectively.},
  archiveprefix = {arXiv},
  eprint = {1908.10423},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TTAP78IN/Dou, Yu, Anastasopoulos - 2019 - Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks(2).pdf},
  keywords = {u}
}

@incollection{doucet2009TutorialParticleFiltering,
  title = {A Tutorial on Particle Filtering and Smoothing: Fifteen Years Later},
  booktitle = {Handbook of {{Nonlinear Filtering}}},
  author = {Doucet, A. and Johansen, A. M.},
  date = {2009},
  issn = {01677152},
  doi = {10.1.1.157.772},
  abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QESU3G6B/Doucet, Johansen - 2009 - A tutorial on particle filtering and smoothing fifteen years later(2).pdf},
  isbn = {978-0-19-953290-2}
}

@article{downey2010AnalysisProbabilisticModel,
  title = {Analysis of a Probabilistic Model of Redundancy in Unsupervised Information Extraction},
  author = {Downey, Doug and Etzioni, Oren and Soderland, Stephen},
  date = {2010-07},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {174},
  pages = {726--748},
  issn = {00043702},
  doi = {10.1016/j.artint.2010.04.024},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370210000688},
  urldate = {2020-09-27},
  abstract = {Unsupervised Information Extraction (UIE) is the task of extracting knowledge from text without the use of hand-labeled training examples. Because UIE systems do not require human intervention, they can recursively discover new relations, attributes, and instances in a scalable manner. When applied to massive corpora such as the Web, UIE systems present an approach to a primary challenge in artificial intelligence: the automatic accumulation of massive bodies of knowledge.},
  annotation = {28 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7BVYKLDU/Downey et al. - 2010 - Analysis of a probabilistic model of redundancy in.pdf},
  keywords = {u},
  langid = {english},
  number = {11}
}

@inproceedings{dozat2017DeepBiaffineAttention,
  title = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {{{ICLR}}},
  author = {Dozat, T. and Manning, C. D.},
  date = {2017},
  archiveprefix = {arXiv},
  eprint = {1611.01734},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5KTSH7M3/Dozat, Manning - 2017 - Deep biaffine attention for neural dependency parsing(2).pdf}
}

@inproceedings{dozat2017SimplerMoreAccurate,
  title = {Simpler but {{More Accurate Semantic Dependency Parsing}}},
  booktitle = {{{ACL}}},
  author = {Dozat, T. and Manning, C. D},
  date = {2017},
  archiveprefix = {arXiv},
  eprint = {1807.01396v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M2QYAIM3/Dozat, Manning - 2017 - Simpler but More Accurate Semantic Dependency Parsing(2).pdf}
}

@inproceedings{dredze2008ConfidenceweightedLinearClassification,
  title = {Confidence-Weighted Linear Classification},
  booktitle = {{{ICML}}},
  author = {Dredze, M. and Crammer, K. and Pereira, F.},
  date = {2008},
  pages = {264--271},
  doi = {10.1145/1390156.1390190},
  abstract = {We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and eovarianee of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training. Copyright 2008 by the author(s)/owner(s).},
  annotation = {391 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L5V8HCHA/Dredze, Crammer, Pereira - 2008 - Confidence-weighted linear classification(2).pdf}
}

@inproceedings{dror2020DeepDominanceHow,
  title = {Deep Dominance - {{How}} to Properly Compare Deep Neural Models},
  booktitle = {{{ACL}}},
  author = {Dror, R. and Shlomov, S. and Reichart, R.},
  date = {2020},
  pages = {2773--2785},
  abstract = {Comparing between Deep Neural Network (DNN) models based on their performance on unseen data is crucial for the progress of the NLP field. However, these models have a large number of hyper-parameters and, being non-convex, their convergence point depends on the random values chosen at initialization and during training. Proper DNN comparison hence requires a comparison between their empirical score distributions on unseen data, rather than between single evaluation scores as is standard for more simple, convex models. In this paper, we propose to adapt to this problem a recently proposed test for the Almost Stochastic Dominance relation between two distributions. We define the criteria for a high quality comparison method between DNNs, and show, both theoretically and through analysis of extensive experimental results with leading DNN models for sequence tagging tasks, that the proposed test meets all criteria while previously proposed methods fail to do so. We hope the test we propose here will set a new working practice in the NLP community.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C7QF9KCX/Dror, Shlomov, Reichart - 2020 - Deep dominance - How to properly compare deep neural models(2).pdf},
  isbn = {978-1-950737-48-2},
  keywords = {u}
}

@inproceedings{drozdov2019UnsupervisedLatentTree,
  title = {Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Drozdov, A. and Verga, P. and Yadav, M. and Iyyer, M. and McCallum, A.},
  date = {2019},
  pages = {1129--1141},
  abstract = {We introduce deep inside-outside recursive autoencoders (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence and uses inside-outside dynamic programming to consider all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI.},
  archiveprefix = {arXiv},
  eprint = {1904.02142},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2JIFG7S4/4645-Article Text-7684-1-10-20190707.pdf;/home/hiaoxui/.local/share/zotero_files/storage/73PMWJUG/Drozdov et al. - 2019 - Unsupervised latent tree induction with deep inside-outside recursive autoencoders(2).pdf},
  isbn = {978-1-950737-13-0}
}

@inproceedings{dsouza2013ClassifyingTemporalRelations,
  title = {Classifying Temporal Relations with Rich Linguistic Knowledge},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {D'Souza, J. and Ng, V.},
  date = {2013},
  pages = {918--927},
  abstract = {We examine the task of temporal relation clas- sification. Unlike existing approaches to this task, we (1) classify an event-event or event- time pair as one of the 14 temporal relations defined in the TimeBank corpus, rather than as one of the six relations collapsed from the original 14; (2) employ sophisticated linguis- tic knowledge derived from a variety of se- mantic and discourse relations, rather than fo- cusing on morpho-syntactic knowledge; and (3) leverage a novel combination of rule-based and learning-based approaches, rather than re- lying solely on one or the other. Experiments with the TimeBank corpus demonstrate that our knowledge-rich, hybrid approach yields a 15–16\% relative reduction in error over a state-of-the-art learning-based baseline sys- tem.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q5CIUBG2/D'Souza, Ng - 2013 - Classifying temporal relations with rich linguistic knowledge(2).pdf},
  isbn = {978-1-937284-47-3},
  keywords = {u}
}

@inproceedings{du2016RecurrentMarkedTemporal,
  title = {Recurrent {{Marked Temporal Point Processes}} : {{Embedding Event History}} to {{Vector}}},
  booktitle = {{{KDD}}},
  author = {Du, N. and Tech, G. and Gomez-rodriguez, M. and Tech, G.},
  date = {2016},
  pages = {1555--1564},
  doi = {10.1145/2939672.2939875},
  url = {http://dl.acm.org/citation.cfm?doid=2939672.2939875},
  abstract = {Large volumes of event data are becoming increasingly available in a wide variety of applications, such as healthcare analytics, smart cities and social network analysis. The precise time interval or the exact distance between two events carries a great deal of information about the dynamics of the underlying systems. These characteristics make such data fundamentally different from independently and identically distributed data and time-series data where time and space are treated as indexes rather than random variables. Marked temporal point processes are the mathematical framework for modeling event data with covariates. However, typical point process models often make strong assumptions about the generative processes of the event data, which may or may not reflect the reality, and the specifically fixed parametric assumptions also have restricted the expressive power of the respective processes. Can we obtain a more expressive model of marked temporal point processes? How can we learn such a model from massive data? In this paper, we propose the Recurrent Marked Temporal Point Process (RMTPP) to simultaneously model the event timings and the markers. The key idea of our approach is to view the intensity function of a temporal point process as a nonlinear function of the history, and use a recurrent neural network to automatically learn a representation of influences from the event history. We develop an efficient stochastic gradient algorithm for learning the model parameters which can readily scale up to millions of events. Using both synthetic and real world datasets, we show that, in the case where the true models have parametric specifications, RMTPP can learn the dynamics of such models without the need to know the actual parametric forms; and in the case where the true models are unknown, RMTPP can also learn the dynamics and achieve better predictive performance than other parametric alternatives based on particular prior assumptions.},
  annotation = {294 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RPL8DNSK/Du et al. - 2016 - Recurrent Marked Temporal Point Processes Embedding Event History to Vector(2).pdf},
  isbn = {978-1-4503-4232-2}
}

@inproceedings{du2019GradientDescentProvably,
  title = {Gradient {{Descent Provably Optimizes Over}}-Parameterized {{Neural Networks}}},
  booktitle = {{{ICLR}}},
  author = {Du, S. S. and Zhai, X. and Poczos, B. and Singh, A.},
  date = {2019},
  pages = {1--15},
  url = {http://arxiv.org/abs/1810.02054},
  abstract = {One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an \$m\$ hidden node shallow neural network with ReLU activation and \$n\$ training data, we show as long as \$m\$ is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function. Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.},
  annotation = {499 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1810.02054},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D22FB9NU/Du et al. - 2019 - Gradient Descent Provably Optimizes Over-parameterized Neural Networks(2).pdf},
  keywords = {u}
}

@inproceedings{dua2019DROPReadingComprehension,
  title = {{{DROP}}: {{A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs}}},
  shorttitle = {{{DROP}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  date = {2019-04-16},
  url = {http://arxiv.org/abs/1903.00161},
  urldate = {2020-10-28},
  abstract = {Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7\% F1 on our generalized accuracy metric, while expert human performance is 96.0\%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0\% F1.},
  annotation = {171 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1903.00161},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5YGSIZU8/Dua et al. - 2019 - DROP A Reading Comprehension Benchmark Requiring .pdf;/home/hiaoxui/.local/share/zotero_files/storage/KI5VIG8P/1903.html},
  keywords = {u}
}

@article{duchi2011AdaptiveSubgradientMethods,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, J. and Hazan, E. and Singer, Y.},
  date = {2011},
  journaltitle = {JMLR},
  volume = {12},
  pages = {2121--2159},
  issn = {15324435},
  doi = {10.1109/CDC.2012.6426698},
  url = {http://jmlr.org/papers/v12/duchi11a.html},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  annotation = {48 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2868127},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B5NVWDUF/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization(2).pdf},
  isbn = {9780982252925},
  keywords = {u}
}

@inproceedings{dufter2019AnalyticalMethodsInterpretable,
  title = {Analytical {{Methods}} for {{Interpretable Ultradense Word Embeddings}}},
  booktitle = {{{EMNLP}}},
  author = {Dufter, P. and Schütze, H.},
  date = {2019},
  pages = {1185--1191},
  url = {http://arxiv.org/abs/1904.08654},
  abstract = {Word embeddings are useful for a wide variety of tasks, but they lack interpretability. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the embeddings without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. While DensRay is very closely related to the Densifier, it can be computed in closed form, is hyperparameter-free and thus more robust than the Densifier. We evaluate the methods on lexicon induction and set-based word analogy and conclude that analytical methods such as DensRay and SVMs are preferable. For word analogy we propose a new method to solve the task which outperforms the previous state of the art by large margins.},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.08654},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JH97JXJX/Dufter, Schütze - 2019 - Analytical Methods for Interpretable Ultradense Word Embeddings(2).pdf},
  keywords = {u}
}

@article{durrett2014JointModelEntity,
  title = {A {{Joint Model}} for {{Entity Analysis}}: {{Coreference}}, {{Typing}}, and {{Linking}}},
  shorttitle = {A {{Joint Model}} for {{Entity Analysis}}},
  author = {Durrett, Greg and Klein, Dan},
  date = {2014-12},
  journaltitle = {TACL},
  shortjournal = {TACL},
  volume = {2},
  pages = {477--490},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00197},
  url = {https://direct.mit.edu/tacl/article/43330},
  urldate = {2021-03-28},
  abstract = {We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.},
  annotation = {202 citations (Semantic Scholar/DOI) [2021-03-28]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4GLE38EW/Durrett and Klein - 2014 - A Joint Model for Entity Analysis Coreference, Ty.pdf},
  langid = {english}
}

@inproceedings{duvenaud2013StructureDiscoveryNonparametric,
  title = {Structure Discovery in Nonparametric Regression through Compositional Kernel Search},
  booktitle = {{{ICML}}},
  author = {Duvenaud, D. and Lloyd, J. R. and Grosse, R. and Tenenbaum, J. B. and Ghahramani, Z.},
  date = {2013},
  volume = {28},
  pages = {2203--2211},
  abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KBDFJZF9/Duvenaud et al. - 2013 - Structure discovery in nonparametric regression through compositional kernel search(2).pdf},
  issue = {PART 3}
}

@inproceedings{dyer2015TransitionBasedDependencyParsing,
  title = {Transition-{{Based Dependency Parsing}} with {{Stack Long Short}}-{{Term Memory}}},
  booktitle = {{{ACL}}},
  author = {Dyer, C. and Ballesteros, M. and Ling, W. and Matthews, A. and Smith, N. A.},
  date = {2015},
  url = {http://arxiv.org/abs/1505.08075},
  abstract = {We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.},
  annotation = {664 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1505.08075},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HJ73DBHJ/Dyer et al. - 2015 - Transition-Based Dependency Parsing with Stack Long Short-Term Memory(2).pdf},
  keywords = {u}
}

@inproceedings{dyer2016RecurrentNeuralNetwork,
  title = {Recurrent Neural Network Grammars},
  booktitle = {{{NAACL}}},
  author = {Dyer, C. and Kuncoro, A. and Ballesteros, M. and Smith, N. A.},
  date = {2016},
  pages = {199--209},
  doi = {10.18653/v1/n16-1024},
  annotation = {344 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1602.07776},
  eprinttype = {arxiv},
  isbn = {978-1-941643-91-4},
  keywords = {u}
}

@inproceedings{dziedzic2019BandlimitedTrainingInference,
  title = {Band-Limited {{Training}} and {{Inference}} for {{Convolutional Neural Networks}}},
  booktitle = {{{ICML}}},
  author = {Dziedzic, A. and Paparrizos, J. and Krishnan, S. and Elmore, A. and Franklin, M.},
  date = {2019},
  pages = {1745--1754},
  url = {http://proceedings.mlr.press/v97/dziedzic19a.html},
  abstract = {The convolutional layers are core building blocks of neural network architectures. In general, a convolutional filter applies to the entire frequency spectrum of the input data. We explore artificially constraining the frequency spectra of these filters and data, called band-limiting, during training. The frequency domain constraints apply to both the feed-forward and back-propagation steps. Experimentally, we observe that Convolutional Neural Networks (CNNs) are resilient to this compression scheme and results suggest that CNNs learn to leverage lower-frequency components. In particular, we found: (1) band-limited training can effectively control the resource usage (GPU and memory); (2) models trained with band-limited layers retain high prediction accuracy; and (3) requires no modification to existing training algorithms or neural network architectures to use unlike other compression schemes.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/25ENWSRN/Dziedzic et al. - 2019 - Band-limited Training and Inference for Convolutional Neural Networks(2).pdf},
  keywords = {u}
}

@inproceedings{ebner2019BagofWordsTransferNonContextual,
  title = {Bag-of-{{Words Transfer}}: {{Non}}-{{Contextual Techniques}} for {{Multi}}-{{Task Learning}}},
  booktitle = {{{EMNLP}}},
  author = {Ebner, S. and Wang, F. and Van Durme, B.},
  date = {2019},
  pages = {40--46},
  doi = {10.18653/v1/d19-6105},
  abstract = {Many architectures for multi-task learning (MTL) have been proposed to take advantage of transfer among tasks, often involving complex models and training procedures. In this paper, we ask if the sentence-level representations learned in previous approaches provide …},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2WXDMJUB/Ebner, Wang, Van Durme - 2019 - Bag-of-Words Transfer Non-Contextual Techniques for Multi-Task Learning(2).pdf},
  keywords = {u}
}

@inproceedings{ebner2020MultiSentenceArgumentLinking,
  title = {Multi-{{Sentence Argument Linking}}},
  booktitle = {{{ACL}}},
  author = {Ebner, Seth and Xia, Patrick and Culkin, Ryan and Rawlins, Kyle and Van Durme, Benjamin},
  date = {2020},
  url = {http://arxiv.org/abs/1911.03766},
  abstract = {We introduce a dataset with annotated Roles Across Multiple Sentences (RAMS), consisting of over 9,000 annotated events. This enables the development of a novel span-based labeling framework that operates at the document level, which connects related ideas in sentence-level semantic role labeling and coreference resolution. We achieve 68.1 F1 on RAMS when given argument span boundaries and 73.2 F1 when also given gold event types. We additionally illustrate the applicability of the approach to the slot filling task in the Gun Violence Database.},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1911.03766},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZNGBR2K/2020.acl-main.718.pdf}
}

@article{edmonds2002NearSynonymyLexicalChoice,
  title = {Near-{{Synonymy}} and {{Lexical Choice}}},
  author = {Edmonds, P. and Hirst, G.},
  date = {2002},
  journaltitle = {Computational Linguistics},
  volume = {28},
  pages = {105--144},
  issn = {0891-2017},
  doi = {10.1162/089120102760173625},
  url = {http://www.mitpressjournals.org/doi/10.1162/089120102760173625},
  abstract = {We develop a new computational model for representing the fine-grained meanings of near-synonyms and the differences between them. We also develop a lexical-choice process that can decide which of several near-synonyms is most appropriate in a particular situation. This research has direct applications in machine translation and text generation.We first identify the problems of representing near-synonyms in a computational lexicon and show that no previous model adequately accounts for near-synonymy. We then propose a preliminary theory to account for near-synonymy, relying crucially on the notion of granularity of representation, in which the meaning of a word arises out of a context-dependent combination of a context-independent core meaning and a set of explicit differences to its near-synonyms. That is, near-synonyms cluster together.We then develop a clustered model of lexical knowledge, derived from the conventional ontological model. The model cuts off the ontology at a coarse grain, thus avoiding an awkward proliferation of language-dependent concepts in the ontology, yet maintaining the advantages of efficient computation and reasoning. The model groups near-synonyms into subconceptual clusters that are linked to the ontology. A cluster differentiates near-synonyms in terms of fine-grained aspects of denotation, implication, expressed attitude, and style. The model is general enough to account for other types of variation, for instance, in collocational behavior.An efficient, robust, and flexible fine-grained lexical-choice process is a consequence of a clustered model of lexical knowledge. To make it work, we formalize criteria for lexical choice as preferences to express certain concepts with varying indirectness, to express attitudes, and to establish certain styles. The lexical-choice process itself works on two tiers: between clusters and between near-synonyns of clusters. We describe our prototype implementation of the system, called I-Saurus.},
  annotation = {226 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/843XAR76/Edmonds, Hirst - 2002 - Near-Synonymy and Lexical Choice(2).pdf},
  isbn = {0891-2017},
  number = {2}
}

@inproceedings{edunov2017ClassicalStructuredPrediction,
  title = {Classical {{Structured Prediction Losses}} for {{Sequence}} to {{Sequence Learning}}},
  booktitle = {{{NAACL}}},
  author = {Edunov, S. and Ott, M. and Auli, M. and Grangier, D. and Ranzato, M.},
  date = {2017},
  url = {http://arxiv.org/abs/1711.04956},
  abstract = {There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT'14 German-English translation as well as Gigaword abstractive summarization. On the larger WMT'14 English-French translation task, sequence-level training achieves 41.5 BLEU which is on par with the state of the art.},
  annotation = {107 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1711.04956},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FPNDBYAP/Edunov et al. - 2017 - Classical Structured Prediction Losses for Sequence to Sequence Learning(2).pdf},
  keywords = {u}
}

@inproceedings{eisenstein2008BayesianUnsupervisedTopic,
  title = {Bayesian Unsupervised Topic Segmentation},
  booktitle = {{{EMNLP}}},
  author = {Eisenstein, J. and Barzilay, R.},
  date = {2008},
  pages = {334},
  doi = {10.3115/1613715.1613760},
  url = {http://portal.acm.org/citation.cfm?doid=1613715.1613760},
  abstract = {This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven by lexical cohesion: the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the Bayesian framework. 1},
  annotation = {227 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AAGIZF3Q/Eisenstein, Barzilay - 2008 - Bayesian unsupervised topic segmentation(2).pdf},
  issue = {October},
  keywords = {u}
}

@inproceedings{eisner2004DynaDeclarativeLanguage,
  title = {Dyna: {{A Declarative Language}} for {{Implementing Dynamic Programs}}},
  booktitle = {{{ACL}}},
  author = {Eisner, J. M. and Goldlust, E. and Smith, N. A.},
  date = {2004},
  pages = {218--221},
  abstract = {We present the first version of a new declarative programming language. Dyna has many uses but was designed especially for rapid development of new statistical NLP systems. A Dyna program is a small set of equations, resembling Prolog inference rules, that specify the abstract structure of a dynamic programming algorithm. It compiles into efficient, portable, C++ classes that can be easily invoked from a larger application. By default, these classes run a generalization of agenda-based parsing, prioritizing the partial parses by some figure of merit. The classes can also perform an exact backward (outside) pass in the service of parameter training. The compiler already knows several implementation tricks, algorithmic transforms, and numerical optimization techniques. It will acquire more over time: we intend for it to generalize and encapsulate best practices, and serve as a testbed for new practices. Dyna is now being used for parsing, machine translation, morphological analysis, grammar induction, and finite-state modeling.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B3CB9RQZ/Eisner, Goldlust, Smith - 2004 - Dyna A Declarative Language for Implementing Dynamic Programs(2).pdf},
  isbn = {0-00-140110-6},
  issue = {July}
}

@inproceedings{eisner2008DynaNonProbabilisticProgramming,
  title = {Dyna: {{A Non}}-{{Probabilistic Programming Language}} for {{Probabilistic AI}}},
  booktitle = {{{NeurIPS}}},
  author = {Eisner, J. M.},
  date = {2008},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6TK62GEM/Eisner - 2008 - Dyna A Non-Probabilistic Programming Language for Probabilistic AI(2).pdf}
}

@incollection{eisner2010DynaExtendingDatalog,
  title = {Dyna: {{Extending Datalog}} for Modern {{AI}}},
  booktitle = {Datalog {{Reloaded}}},
  author = {Eisner, J. M. and Filardo, N. W.},
  date = {2010},
  abstract = {Abstract. Modern statistical AI systems are quite large and complex; this interferes with research, development, and education. We point out that most of the computation involves database-like queries and updates on complex views of the data. Specifically, recursive ...},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5LLZWB4S/Eisner, Filardo - 2010 - Dyna Extending Datalog for modern AI(2).pdf}
}

@inproceedings{eisner2016InsideOutsideForwardBackwardAlgorithms,
  title = {Inside-{{Outside}} and {{Forward}}-{{Backward Algorithms Are Just Backprop}}},
  booktitle = {{{EMNLP}}},
  author = {Eisner, J. M.},
  date = {2016},
  pages = {1--17},
  abstract = {A probabilistic or weighted grammar implies a posterior probability distribution over possi-ble parses of a given input sentence. One often needs to extract information from this distri-bution, by computing the expected counts (in the unknown parse) of various grammar rules, constituents, transitions, or states. This re-quires an algorithm such as inside-outside or forward-backward that is tailored to the gram-mar formalism. Conveniently, each such al-gorithm can be obtained by automatically dif-ferentiating an " inside " algorithm that merely computes the log-probability of the evidence (the sentence). This mechanical procedure produces correct and efficient code. As for any other instance of back-propagation, it can be carried out manually or by software. This pedagogical paper carefully spells out the con-struction and relates it to traditional and non-traditional views of these algorithms.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WNQ6XIXL/Eisner - 2016 - Inside-Outside and Forward-Backward Algorithms Are Just Backprop(2).pdf}
}

@inproceedings{ejerhed1988FindingClausesUnrestricted,
  title = {Finding {{Clauses In Unrestricted Text By Finitary And Stochastic Methods}}},
  booktitle = {{{ANLC}}},
  author = {Ejerhed, E. I.},
  date = {1988},
  pages = {219--227},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {The Working Group I contribution to the IPCC's Fifth Assessment Report (AR5) considers new evidence of climate change based on many independent scientific analyses from observations of the climate system, paleoclimate archives, theoretical studies of climate processes and simulations using climate models. It builds upon the Working Group I contribution to the IPCC’s Fourth Assessment Report (AR4), and incorporates subsequent new findings of research. As a component of the fifth assessment cycle, the IPCC Special Report on Managing the Risks of Extreme Events to Advance Climate Change Adaptation (SREX) is an important basis for information on changing weather and climate extremes. This Summary for Policymakers (SPM) follows the structure of the Working Group I report. The narrative is supported by a series of overarching highlighted conclusions which, taken together, provide a concise summary. Main sections are introduced with a brief paragraph in italics which outlines the methodological basis of the assessment. The degree of certainty in key findings in this assessment is based on the author teams’ evaluations of underlying scientific understanding and is expressed as a qualitative level of confidence (from very low to very high) and, when possible, probabilistically with a quantified likelihood (from exceptionally unlikely to virtually certain). Confidence in the validity of a finding is based on the type, amount, quality, and consistency of evidence (e.g., data, mechanistic understanding, theory, models, expert judgment) and the degree of agreement1. Probabilistic estimates of quantified measures of uncertainty in a finding are based on statistical analysis of observations or model results, or both, and expert judgment2. Where appropriate, findings are also formulated as statements of fact without using uncertainty qualifiers. (See Chapter 1 and Box TS.1 for more details about the specific language the IPCC uses to communicate uncertainty) The basis for substantive paragraphs in this Summary for Policymakers can be found in the chapter sections of the underlying report and in the Technical Summary. These references are given},
  annotation = {1013 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NE3VNLL8/Ejerhed - 1988 - Finding Clauses In Unrestricted Text By Finitary And Stochastic Methods(2).pdf},
  isbn = {978-85-7811-079-6},
  keywords = {u}
}

@inproceedings{elsahar2018TRExLargeScale,
  title = {T-{{REx}}: {{A Large Scale Alignment}} of {{Natural Language}} with {{Knowledge Base Triples}}},
  booktitle = {{{LREC}}},
  author = {Elsahar, Hady and Vougiouklis, Pavlos and Remaci, Arslen and Gravier, Christophe and Hare, Jonathon and Simperl, Elena and Laforest, Frederique},
  date = {2018},
  pages = {5},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XMPBV7H6/Elsahar et al. - T-REx A Large Scale Alignment of Natural Language.pdf},
  langid = {english}
}

@inproceedings{estival2004OntologyBasedNaturalLanguage,
  title = {Towards {{Ontology}}-{{Based Natural Language Processing}}},
  booktitle = {Workshop on {{NLP}} and {{XML}}},
  author = {Estival, D. and Nowak, C. and Zschorn, A.},
  date = {2004},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z8DXCBDB/Estival, Nowak, Zschorn - 2004 - Towards Ontology-Based Natural Language Processing(2).pdf},
  keywords = {u}
}

@report{etesami2016LearningNetworkMultivariate,
  title = {Learning {{Network}} of {{Multivariate Hawkes Processes}}: {{A Time Series Approach}}},
  author = {Etesami, J. and Kiyavash, N. and Zhang, K. and Singhal, K.},
  date = {2016},
  pages = {1--14},
  url = {http://arxiv.org/abs/1603.04319},
  abstract = {Learning the influence structure of multiple time series data is of great interest to many disciplines. This paper studies the problem of recovering the causal structure in network of multivariate linear Hawkes processes. In such processes, the occurrence of an event in one process affects the probability of occurrence of new events in some other processes. Thus, a natural notion of causality exists between such processes captured by the support of the excitation matrix. We show that the resulting causal influence network is equivalent to the Directed Information graph (DIG) of the processes, which encodes the causal factorization of the joint distribution of the processes. Furthermore, we present an algorithm for learning the support of excitation matrix (or equivalently the DIG). The performance of the algorithm is evaluated on synthesized multivariate Hawkes networks as well as a stock market and MemeTracker real-world dataset.},
  annotation = {36 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1603.04319},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U4JKF6JH/Etesami et al. - 2016 - Learning Network of Multivariate Hawkes Processes A Time Series Approach(2).pdf},
  isbn = {9781510827806},
  keywords = {u}
}

@inproceedings{ethayarajh2019HowContextualAre,
  title = {How {{Contextual}} Are {{Contextualized Word Representations}}? {{Comparing}} the {{Geometry}} of {{BERT}}, {{ELMo}}, and {{GPT}}-2 {{Embeddings}}},
  booktitle = {{{EMNLP}}},
  author = {Ethayarajh, K.},
  date = {2019},
  pages = {55--65},
  url = {http://arxiv.org/abs/1909.00512},
  abstract = {Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5\% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.},
  annotation = {84 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.00512},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PURY538B/Ethayarajh - 2019 - How Contextual are Contextualized Word Representations Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings(2).pdf}
}

@report{faez2020DeepGraphGenerators,
  title = {Deep {{Graph Generators}}: {{A Survey}}},
  shorttitle = {Deep {{Graph Generators}}},
  author = {Faez, Faezeh and Ommi, Yassaman and Baghshah, Mahdieh Soleymani and Rabiee, Hamid R.},
  date = {2020-12-31},
  url = {http://arxiv.org/abs/2012.15544},
  urldate = {2021-01-16},
  abstract = {Deep generative models have achieved great success in areas such as image, speech, and natural language processing in the past few years. Thanks to the advances in graph-based deep learning, and in particular graph representation learning, deep graph generation methods have recently emerged with new applications ranging from discovering novel molecular structures to modeling social networks. This paper conducts a comprehensive survey on deep learning-based graph generation approaches and classifies them into five broad categories, namely, autoregressive, autoencoder-based, RL-based, adversarial, and flow-based graph generators, providing the readers a detailed description of the methods in each class. We also present publicly available source codes, commonly used datasets, and the most widely utilized evaluation metrics. Finally, we highlight the existing challenges and discuss future research directions.},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2012.15544},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/875PXZ7C/Faez et al. - 2020 - Deep Graph Generators A Survey.pdf;/home/hiaoxui/.local/share/zotero_files/storage/L965ZVL3/2012.html}
}

@inproceedings{falke2017BringingStructureSummaries,
  title = {Bringing {{Structure}} into {{Summaries}}: {{Crowdsourcing}} a {{Benchmark Corpus}} of {{Concept Maps}}},
  booktitle = {{{EMNLP}}},
  author = {Falke, T. and Gurevych, I.},
  date = {2017},
  pages = {2951--2961},
  url = {http://arxiv.org/abs/1704.04452},
  abstract = {Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.},
  annotation = {20 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.04452},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6XNFARUG/Falke, Gurevych - 2017 - Bringing Structure into Summaries Crowdsourcing a Benchmark Corpus of Concept Maps(2).pdf},
  keywords = {u}
}

@inproceedings{fan2018HierarchicalNeuralStory,
  title = {Hierarchical {{Neural Story Generation}}},
  booktitle = {{{ACL}}},
  author = {Fan, A. and Lewis, M. and Dauphin, Y.},
  date = {2018},
  pages = {1--10},
  url = {http://arxiv.org/abs/1805.04833},
  abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
  annotation = {309 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1805.04833},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ITY66JGK/Fan, Lewis, Dauphin - 2018 - Hierarchical Neural Story Generation(2).pdf},
  keywords = {u}
}

@report{fedus2021SwitchTransformersScaling,
  title = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle = {Switch {{Transformers}}},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  date = {2021-01-11},
  url = {http://arxiv.org/abs/2101.03961},
  urldate = {2021-01-16},
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  annotation = {17 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2101.03961},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I46AM4TA/Fedus et al. - 2021 - Switch Transformers Scaling to Trillion Parameter.pdf;/home/hiaoxui/.local/share/zotero_files/storage/3HVGBBCU/2101.html}
}

@article{feldman2004EmbodiedMeaningNeural,
  title = {Embodied Meaning in a Neural Theory of Language},
  author = {Feldman, J. and Narayanan, S.},
  date = {2004},
  journaltitle = {Brain and Language},
  volume = {89},
  pages = {385--392},
  issn = {0093934X},
  doi = {10.1016/S0093-934X(03)00355-9},
  abstract = {In this paper, we outline an explicitly neural theory of language (NTL) that attempts to explain how many brain functions (including emotion and social cognition) work together to understand and learn language. The focus will be on the required representations and computations, although there will be some discussion of results on specific brain structures. In this approach, one does not expect to find brain areas specialized only for language or to find language processing confined to only a few areas.},
  annotation = {326 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {15068922},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MJM5VP4J/Feldman, Narayanan - 2004 - Embodied meaning in a neural theory of language(2).pdf},
  isbn = {0093-934X, 0093-934X},
  keywords = {u},
  number = {2}
}

@inproceedings{feldman2019CommonsenseKnowledgeMining,
  title = {Commonsense {{Knowledge Mining}} from {{Pretrained Models}}},
  booktitle = {{{EMNLP}}},
  author = {Feldman, J. and Davison, J. and Rush, A. M.},
  date = {2019},
  pages = {1173--1178},
  url = {http://arxiv.org/abs/1909.00505},
  abstract = {Inferring commonsense knowledge is a key challenge in natural language processing, but due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple's validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though this method performs worse on a test set than models explicitly trained on a corresponding training set, it outperforms these methods when mining commonsense knowledge from new sources, suggesting that unsupervised techniques may generalize better than current supervised approaches.},
  annotation = {54 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.00505},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AEUJV3PD/Feldman, Davison, Rush - 2019 - Commonsense Knowledge Mining from Pretrained Models(2).pdf},
  keywords = {u}
}

@inproceedings{feng2018PathologiesNeuralModels,
  title = {Pathologies of {{Neural Models Make Interpretations Difficult}}},
  booktitle = {{{EMNLP}}},
  author = {Feng, S. and Wallace, E. and Grissom, A. and Iyyer, M. and Rodriguez, P. and Boyd-Graber, J.},
  date = {2018},
  pages = {3719--3728},
  issn = {19342608},
  doi = {http://dx.doi.org/10.1016/0255-2701(91)80005-A},
  url = {http://arxiv.org/abs/1804.07781},
  abstract = {One way to interpret neural model predictions is to highlight the most important input features---for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word's importance is determined by either input perturbation---measuring the decrease in model confidence when that word is removed---or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.},
  annotation = {108 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.07781},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z4AFGKUC/Feng et al. - 2018 - Pathologies of Neural Models Make Interpretations Difficult(2).pdf},
  isbn = {0255-2701}
}

@inproceedings{feng2020ExploringEndtoEndDifferentiable,
  title = {Exploring {{End}}-to-{{End Differentiable Natural Logic Modeling}}},
  booktitle = {{{COLING}}},
  author = {Feng, Yufei and Zheng, Zi’ou and Liu, Quan and Greenspan, Michael and Zhu, Xiaodan},
  date = {2020},
  pages = {1172--1185},
  publisher = {{International Committee on Computational Linguistics}},
  location = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.101},
  url = {https://www.aclweb.org/anthology/2020.coling-main.101},
  urldate = {2021-02-12},
  abstract = {We explore end-to-end trained differentiable models that integrate natural logic with neural networks, aiming to keep the backbone of natural language reasoning based on the natural logic formalism while introducing subsymbolic vector representations and neural components. The proposed model adapts module networks to model natural logic operations, which is enhanced with a memory component to model contextual information. Experiments show that the proposed framework can effectively model monotonicity-based reasoning, compared to the baseline neural network models without built-in inductive bias for monotonicity-based reasoning. Our proposed model shows to be robust when transferred from upward to downward inference. We perform further analyses on the performance of the proposed model on aggregation, showing the effectiveness of the proposed subcomponents on helping achieve better intermediate aggregation performance.},
  eventtitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CXFI78CN/Feng et al. - 2020 - Exploring End-to-End Differentiable Natural Logic .pdf},
  langid = {english}
}

@inproceedings{ferreira2016MoreVariationText,
  title = {Towards More Variation in Text Generation: {{Developing}} and Evaluating Variation Models for Choice of Referential Form},
  booktitle = {{{ACL}}},
  author = {Ferreira, T. C. and Krahmer, E. and Wubben, S.},
  date = {2016},
  pages = {568--577},
  url = {http://www.aclweb.org/anthology/P16-1054},
  abstract = {In this study, we introduce a nondeterministic method for referring expression generation. We describe two models that account for individual variation in the choice of referential form in automatically generated text: a Naive Bayes model and a Recurrent Neural Network. Both are evaluated using the VaREG corpus. Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those produced by a random baseline model.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VHNHUW7D/Ferreira, Krahmer, Wubben - 2016 - Towards more variation in text generation Developing and evaluating variation models for choice of(2).pdf},
  isbn = {978-1-5108-2758-5}
}

@inproceedings{ferreira2019NeuralDatatotextGeneration,
  title = {Neural Data-to-Text Generation: {{A}} Comparison between Pipeline and End-to-End Architectures},
  booktitle = {{{EMNLP}}},
  author = {Ferreira, T. C. and van der Lee, C. and van Miltenburg, E. and Krahmer, E.},
  date = {2019},
  pages = {552--562},
  url = {http://arxiv.org/abs/1908.09022},
  abstract = {Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. In contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in-between. This study introduces a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Both architectures were implemented making use of state-of-the art deep learning methods as the encoder-decoder Gated-Recurrent Units (GRU) and Transformer. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available.},
  annotation = {45 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1908.09022},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W68ITM6P/Ferreira et al. - 2019 - Neural data-to-text generation A comparison between pipeline and end-to-end architectures(2).pdf},
  keywords = {u},
  options = {useprefix=true}
}

@inproceedings{fevry2020EmpiricalEvaluationPretraining,
  title = {Empirical {{Evaluation}} of {{Pretraining Strategies}} for {{Supervised Entity Linking}}},
  booktitle = {{{AKBC}}},
  author = {Févry, Thibault and FitzGerald, Nicholas and Soares, Livio Baldini and Kwiatkowski, Tom},
  date = {2020-05-28},
  url = {http://arxiv.org/abs/2005.14253},
  urldate = {2020-09-29},
  abstract = {In this work, we present an entity linking model which combines a Transformer architecture with large scale pretraining from Wikipedia links. Our model achieves the state-of-the-art on two commonly used entity linking datasets: 96.7\% on CoNLL and 94.9\% on TAC-KBP. We present detailed analyses to understand what design choices are important for entity linking, including choices of negative entity candidates, Transformer architecture, and input perturbations. Lastly, we present promising results on more challenging settings such as end-to-end entity linking and entity linking without in-domain training data.},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2005.14253},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CW2AECH9/Févry et al. - 2020 - Empirical Evaluation of Pretraining Strategies for.pdf;/home/hiaoxui/.local/share/zotero_files/storage/287LSD43/2005.html},
  keywords = {u}
}

@article{fillmore1976FrameSemanticsNature,
  title = {Frame {{Semantics}} and the {{Nature}} of {{Language}}},
  author = {Fillmore, C. J.},
  date = {1976},
  journaltitle = {Annals of the New York Academy of Sciences},
  volume = {280},
  pages = {20--32},
  issn = {17496632},
  doi = {10.1111/j.1749-6632.1976.tb25467.x},
  abstract = {When the question of the origin of language is considered from an evolutionary perspective, it loses much of its clarity and simplicity. Should we be looking for the first step in the chain of events that led to what we now see as human language? the first step away from what? Or should we be trying to determine the last step},
  annotation = {862 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/894QX9D4/Fillmore - 1976 - Frame Semantics and the Nature of Language(2).pdf},
  keywords = {u},
  number = {1}
}

@incollection{fillmore1982FrameSemantics,
  title = {Frame {{Semantics}}},
  booktitle = {Linguistics in the {{Morning Calm}}},
  author = {Fillmore, C. J.},
  date = {1982},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YSZSD9AD/Fillmore - 1982 - Frame Semantics(2).pdf},
  keywords = {u}
}

@inproceedings{finkel2009NestedNamedEntity,
  title = {Nested Named Entity Recognition},
  booktitle = {{{EMNLP}}},
  author = {Finkel, Jenny Rose and Manning, Christopher D.},
  date = {2009},
  volume = {1},
  pages = {141},
  publisher = {{Association for Computational Linguistics}},
  location = {{Singapore}},
  doi = {10.3115/1699510.1699529},
  annotation = {200 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BQRRYKK8/Finkel and Manning - 2009 - Nested named entity recognition.pdf},
  isbn = {978-1-932432-59-6},
  langid = {english}
}

@inproceedings{fitzgerald2013LearningDistributionsLogical,
  title = {Learning {{Distributions}} over {{Logical Forms}} for {{Referring Expression Generation}}},
  booktitle = {{{EMNLP}}},
  author = {FitzGerald, N. and Artzi, Y. and Zettlemoyer, L. S.},
  date = {2013},
  pages = {1914--1925},
  abstract = {We present a new approach to referring expression generation, casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world. Despite an extremely large space of possible expressions, we demonstrate effective learning of a globally normalized log-linear distribution. This learning is enabled by a new, multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms. We train and evaluate the approach on a new corpus of references to sets of visual objects. Experiments show the approach is able to learn accurate models, which generate over 87\% of the expressions people used. Additionally, on the previously studied special case of single object reference, we show a 35\% relative error reduction over previous state of the art.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y6TX2H7T/FitzGerald, Artzi, Zettlemoyer - 2013 - Learning Distributions over Logical Forms for Referring Expression Generation(2).pdf},
  isbn = {978-1-937284-97-8},
  issue = {October},
  keywords = {u}
}

@inproceedings{fitzgerald2015SemanticRoleLabeling,
  title = {Semantic Role Labeling with Neural Network Factors},
  booktitle = {{{EMNLP}}},
  author = {Fitzgerald, N. and Täckström, O. and Ganchev, K. and Das, D.},
  date = {2015},
  pages = {960--970},
  doi = {10.18653/v1/d15-1112},
  abstract = {We present a new method for semantic role labeling in which arguments and semantic roles are jointly embedded in a shared vector space for a given predicate. These embeddings belong to a neural network, whose output represents the potential functions of a graphical model designed for the SRL task. We consider both local and structured learning methods and obtain strong results on standard PropBank and FrameNet corpora with a straightforward product-of-experts model. We further show how the model can learn jointly from PropBank and FrameNet annotations to obtain additional improvements on the smaller FrameNet dataset.},
  annotation = {115 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7ZVR6SNI/Fitzgerald et al. - 2015 - Semantic role labeling with neural network factors(2).pdf},
  isbn = {978-1-941643-32-7},
  keywords = {u}
}

@inproceedings{fitzgerald2018LargeScaleQASRLParsing,
  title = {Large-{{Scale QA}}-{{SRL Parsing}}},
  booktitle = {{{ACL}}},
  author = {FitzGerald, N. and Michael, J. and He, L. and Zettlemoyer, L. S.},
  date = {2018},
  pages = {1--10},
  archiveprefix = {arXiv},
  eprint = {1805.05377},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/75PSZ2ZQ/FitzGerald et al. - 2018 - Large-Scale QA-SRL Parsing(2).pdf},
  keywords = {u}
}

@inproceedings{fleischman2002EmotionalVariationNatural,
  title = {Towards Emotional Variation in Natural Language Generation},
  booktitle = {{{INLG}}},
  author = {Fleischman, M. and Hovy, E.},
  date = {2002},
  pages = {1--8},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJIQY7WY/Fleischman, Hovy - 2002 - Towards emotional variation in natural language generation(2).pdf}
}

@report{frankle2018LotteryTicketHypothesis,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Small}}, {{Trainable Neural Networks}}},
  author = {Frankle, J. and Carbin, M.},
  date = {2018},
  doi = {arXiv:1803.03635v1},
  url = {http://arxiv.org/abs/1803.03635},
  abstract = {Neural network compression techniques are able to reduce the parameter counts of trained networks by over 90\%--decreasing storage requirements and improving inference performance--without compromising accuracy. However, contemporary experience is that it is difficult to train small architectures from scratch, which would similarly improve training performance. We articulate a new conjecture to explain why it is easier to train large networks: the "lottery ticket hypothesis." It states that large networks that train successfully contain subnetworks that--when trained in isolation--converge in a comparable number of iterations to comparable accuracy. These subnetworks, which we term "winning tickets," have won the initialization lottery: their connections have initial weights that make training particularly effective. We find that a standard technique for pruning unnecessary network weights naturally uncovers a subnetwork which, at the start of training, comprised a winning ticket. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis. We consistently find winning tickets that are less than 20\% of the size of several fully-connected, convolutional, and residual architectures for MNIST and CIFAR10. Furthermore, winning tickets at moderate levels of pruning (20-50\% of the original network size) converge up to 6.7x faster than the original network and exhibit higher test accuracy.},
  annotation = {671 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JM44HNSG/Frankle, Carbin - 2018 - The Lottery Ticket Hypothesis Finding Small, Trainable Neural Networks(2).pdf},
  keywords = {u}
}

@inproceedings{frankle2019LotteryTicketHypothesis,
  title = {The Lottery Ticket Hypothesis: {{Finding}} Sparse, Trainable Neural Networks},
  booktitle = {{{ICLR}}},
  author = {Frankle, J. and Carbin, M.},
  date = {2019},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  archiveprefix = {arXiv},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F5HD3BJ9/Frankle, Carbin - 2019 - The lottery ticket hypothesis Finding sparse, trainable neural networks(2).pdf},
  keywords = {u}
}

@inproceedings{fu2021NestedNamedEntity,
  title = {Nested {{Named Entity Recognition}} with {{Partially}}-{{Observed TreeCRFs}}},
  booktitle = {{{AAAI}}},
  author = {Fu, Yao and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Huang, Fei},
  date = {2021},
  url = {http://arxiv.org/abs/2012.08478},
  urldate = {2020-12-17},
  abstract = {Named entity recognition (NER) is a well-studied task in natural language processing. However, the widely-used sequence labeling framework is difficult to detect entities with nested structures. In this work, we view nested NER as constituency parsing with partially-observed trees and model it with partially-observed TreeCRFs. Specifically, we view all labeled entity spans as observed nodes in a constituency tree, and other spans as latent nodes. With the TreeCRF we achieve a uniform way to jointly model the observed and the latent nodes. To compute the probability of partial trees with partial marginalization, we propose a variant of the Inside algorithm, the \textbackslash textsc\{Masked Inside\} algorithm, that supports different inference operations for different nodes (evaluation for the observed, marginalization for the latent, and rejection for nodes incompatible with the observed) with efficient parallelized implementation, thus significantly speeding up training and inference. Experiments show that our approach achieves the state-of-the-art (SOTA) F1 scores on the ACE2004, ACE2005 dataset, and shows comparable performance to SOTA models on the GENIA dataset. Our approach is implemented at: \textbackslash url\{https://github.com/FranxYao/Partially-Observed-TreeCRFs\}.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2012.08478},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N392BR4N/Fu et al. - 2020 - Nested Named Entity Recognition with Partially-Obs.pdf;/home/hiaoxui/.local/share/zotero_files/storage/AUJ2E4D2/2012.html}
}

@inproceedings{fukui2016MultimodalCompactBilinear,
  title = {Multimodal {{Compact Bilinear Pooling}} for {{Visual Question Answering}} and {{Visual Grounding}}},
  booktitle = {{{EMNLP}}},
  author = {Fukui, A. and Park, D. H. and Yang, D. and Rohrbach, A. and Darrell, T. and Rohrbach, M.},
  date = {2016},
  url = {http://arxiv.org/abs/1606.01847},
  abstract = {Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.},
  annotation = {840 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1606.01847},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8LK2D5YK/Fukui et al. - 2016 - Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding(2).pdf}
}

@inproceedings{gaddy2018WhatGoingNeural,
  title = {What’s {{Going On}} in {{Neural Constituency Parsers}}? {{An Analysis}}},
  shorttitle = {What’s {{Going On}} in {{Neural Constituency Parsers}}?},
  booktitle = {{{NAACL}}},
  author = {Gaddy, David and Stern, Mitchell and Klein, Dan},
  date = {2018},
  pages = {999--1010},
  publisher = {{Association for Computational Linguistics}},
  location = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1091},
  url = {http://aclweb.org/anthology/N18-1091},
  urldate = {2020-08-07},
  abstract = {A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and featurerich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.},
  annotation = {40 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CKB5WXGN/Gaddy et al. - 2018 - What’s Going On in Neural Constituency Parsers An.pdf},
  langid = {english}
}

@inproceedings{gal2016DropoutBayesianApproximation,
  title = {Dropout as a {{Bayesian Approximation}} : {{Representing Model Uncertainty}} in {{Deep Learning}}},
  booktitle = {{{ICML}}},
  author = {Gal, Y. and Ghahramani, Z.},
  date = {2016},
  volume = {48},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QKP9CHQH/Gal, Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning(2).pdf},
  keywords = {u}
}

@article{ganchev2010PosteriorRegularizationStructured,
  title = {Posterior {{Regularization}} for {{Structured Latent Variable Models}}},
  author = {Ganchev, K. and Graça, J. V. and Gillenwater, J. and Taskar, B.},
  date = {2010},
  journaltitle = {JMLR},
  volume = {11},
  issn = {15324435},
  url = {http://dl.acm.org/citation.cfm?id=1756006.1859918},
  abstract = {We present Posterior Regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efficiently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior Regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efficiency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efficient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HW777KE9/Ganchev et al. - 2010 - Posterior Regularization for Structured Latent Variable Models(2).pdf},
  isbn = {1532-4435}
}

@inproceedings{gangal2020BERTeringRAMSWhat,
  title = {{{BERTering RAMS}}: {{What}} and {{How Much}} Does {{BERT Already Know About Event Arguments}}? -- {{A Study}} on the {{RAMS Dataset}}},
  shorttitle = {{{BERTering RAMS}}},
  booktitle = {{{EMNLP}}},
  author = {Gangal, Varun and Hovy, Eduard},
  date = {2020-10-09},
  url = {http://arxiv.org/abs/2010.04098},
  urldate = {2021-02-17},
  abstract = {Using the attention map based probing frame-work from (Clark et al., 2019), we observe that, on the RAMS dataset (Ebner et al., 2020), BERT's attention heads have modest but well above-chance ability to spot event arguments sans any training or domain finetuning, vary-ing from a low of 17.77\% for Place to a high of 51.61\% for Artifact. Next, we find that linear combinations of these heads, estimated with approx 11\% of available total event argument detection supervision, can push performance well-higher for some roles - highest two being Victim (68.29\% Accuracy) and Artifact(58.82\% Accuracy). Furthermore, we investigate how well our methods do for cross-sentence event arguments. We propose a procedure to isolate "best heads" for cross-sentence argument detection separately of those for intra-sentence arguments. The heads thus estimated have superior cross-sentence performance compared to their jointly estimated equivalents, albeit only under the unrealistic assumption that we already know the argument is present in an-other sentence. Lastly, we seek to isolate to what extent our numbers stem from lexical frequency based associations between gold arguments and roles. We propose NONCE, a scheme to create adversarial test examples by replacing gold arguments with randomly generated "nonce" words. We find that learnt linear combinations are robust to NONCE, though individual best heads can be more sensitive.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2010.04098},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N5YU8ZE5/Gangal and Hovy - 2020 - BERTering RAMS What and How Much does BERT Alread.pdf;/home/hiaoxui/.local/share/zotero_files/storage/54SBLGAP/2010.html}
}

@inproceedings{gao2015CompactBilinearPooling,
  title = {Compact {{Bilinear Pooling}}},
  booktitle = {{{CVPR}}},
  author = {Gao, Y. and Beijbom, O. and Zhang, N. and Darrell, T.},
  date = {2015},
  pages = {317--326},
  issn = {10636919},
  doi = {10.1109/CVPR.2016.41},
  url = {http://arxiv.org/abs/1511.06062},
  abstract = {Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.},
  annotation = {462 citations (Semantic Scholar/DOI) [2021-03-26] 462 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1511.06062},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FMULMNXN/Gao et al. - 2015 - Compact Bilinear Pooling(2).pdf},
  isbn = {978-1-4673-8851-1},
  number = {2}
}

@inproceedings{gardner2018NeuralSemanticParsing,
  title = {Neural {{Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Gardner, M. and Dasigi, P. and Iyer, S. and Suhr, A. and Zettlemoyer, L. S.},
  date = {2018},
  pages = {17--18},
  doi = {10.1523/JNEUROSCI.5952-09.2010.Orbitofrontal},
  eprint = {6142362},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2X723RDP/Gardner et al. - 2018 - Neural Semantic Parsing(2).pdf}
}

@inproceedings{garnelo2018NeuralProcesses,
  title = {Neural {{Processes}}},
  booktitle = {{{ICML}}},
  author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  date = {2018-07-04},
  url = {http://arxiv.org/abs/1807.01622},
  urldate = {2021-03-01},
  abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  annotation = {91 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1807.01622},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YLJNPRJP/Garnelo et al. - 2018 - Neural Processes.pdf;/home/hiaoxui/.local/share/zotero_files/storage/525YPWCZ/1807.html}
}

@inproceedings{gasthaus2019ProbabilisticForecastingSpline,
  title = {Probabilistic {{Forecasting}} with {{Spline Quantile Function RNNs}}},
  booktitle = {{{AISTATS}}},
  author = {Gasthaus, J. and Benidis, K. and Wang, Y. and Rangapuram, S. S. and Salinas, D. and Flunkert, V. and Januschowski, T.},
  date = {2019},
  volume = {89},
  pages = {1901--1910},
  url = {http://proceedings.mlr.press/v89/gasthaus19a.html},
  abstract = {In this paper, we propose a flexible method for probabilistic modeling with conditional quantile functions using monotonic regression splines. The shape of the spline is parameterized by a neural network whose parameters are learned by minimizing the continuous ranked probability score. Within this framework, we propose a method for probabilistic time series forecasting, which combines the modeling capacity of recurrent neural networks with the flexibility of a spline-based representation of the output distribution. Unlike methods based on parametric probability density functions and maximum likelihood estimation, the proposed method can flexibly adapt to different output distributions without manual intervention. We empirically demonstrate the effectiveness of the approach on synthetic and real-world data sets.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S6BYQHN7/Gasthaus et al. - 2019 - Probabilistic Forecasting with Spline Quantile Function RNNs(2).pdf}
}

@report{gatt2017SurveyStateArt,
  title = {Survey of the {{State}} of the {{Art}} in {{Natural Language Generation}}: {{Core}} Tasks, Applications and Evaluation},
  author = {Gatt, A. and Krahmer, E.},
  date = {2017},
  pages = {1--111},
  url = {http://arxiv.org/abs/1703.09902},
  abstract = {This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artificial intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of Natural Language Processing, with an emphasis on different evaluation methods and the relationships between them.},
  annotation = {331 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1703.09902},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BEYJT6NF/Gatt, Krahmer - 2017 - Survey of the State of the Art in Natural Language Generation Core tasks, applications and evaluation(2).pdf},
  number = {c}
}

@inproceedings{ge2005StatisticalSemanticParser,
  title = {A {{Statistical Semantic Parser}} That {{Integrates Syntax}} and {{Semantics}}},
  booktitle = {{{CoNLL}}},
  author = {Ge, R. and Mooney, R.},
  date = {2005},
  pages = {9--16},
  doi = {10.3115/1706543.1706546},
  abstract = {We introduce a learning semantic parser, SCISSOR, thatmaps natural-language sentences to a detailed, formal, meaning-representation language. It first uses an integrated statistical parser to produce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We present experimental results demonstrating that SCISSOR produces more accurate semantic representations than several previous approaches.},
  annotation = {189 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V7EMI57P/Ge, Mooney - 2005 - A Statistical Semantic Parser that Integrates Syntax and Semantics(2).pdf},
  issue = {June},
  keywords = {u}
}

@inproceedings{ge2006DiscriminativeRerankingSemantic,
  title = {Discriminative Reranking for Semantic Parsing},
  booktitle = {{{ACL}}},
  author = {Ge, R. and Mooney, R. J.},
  date = {2006},
  pages = {263},
  doi = {10.3115/1273073.1273107},
  url = {http://portal.acm.org/citation.cfm?id=1273107},
  abstract = {Semantic parsing is the task of mapping natural language sentences to complete formal meaning representations. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. In this paper, we investigate discriminative reranking upon a baseline semantic parser, SCISSOR, where the composition of meaning representations is guided by syntax. We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics. We report experimental results on two real applications, an interpreter for coaching instructions in robotic soccer and a natural-language database interface. The results show that reranking can improve the performance on the coaching interpreter, but not on the database interface.},
  annotation = {54 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B6PQ8QEG/Ge, Mooney - 2006 - Discriminative reranking for semantic parsing(2).pdf},
  issue = {July},
  keywords = {u}
}

@article{gildea2000AutomaticLabelingSemantic,
  title = {Automatic Labeling of Semantic Roles},
  author = {Gildea, D. and Jurafsky, D.},
  date = {2000},
  journaltitle = {Computational Linguistics},
  pages = {512--520},
  issn = {0891-2017},
  doi = {10.3115/1075218.1075283},
  url = {http://portal.acm.org/citation.cfm?doid=1075218.1075283},
  abstract = {We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data. 1},
  annotation = {931 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/93TKTRIA/Gildea, Jurafsky - 2000 - Automatic labeling of semantic roles(2).pdf},
  isbn = {0891-2017},
  keywords = {u},
  number = {1972}
}

@article{giles1992LearningExtractingFinite,
  title = {Learning and {{Extracting Finite State Automata}} with {{Second}}-{{Order Recurrent Neural Networks}}},
  author = {Giles, C. L. and Miller, C. B. and Chen, D. and Chen, H. H. and Sun, G. Z. and Lee, Y. C.},
  date = {1992},
  journaltitle = {Neural Computation},
  volume = {4},
  pages = {393--405},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.3.393},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.3.393#.Vr3oBfnhAuU},
  abstract = {We show that a recurrent, second-order neural network using a real-time, forward training algorithm readily learns to infer small regular grammars from positive and negative string training samples. We present simulations that show the effect of initial conditions, training set size and order, and neural network architecture. All simulations were performed with random initial weight strengths and usually converge after approximately a hundred epochs of training. We discuss a quantization algorithm for dynamically extracting finite state automata during and after training. For a well-trained neural net, the extracted automata constitute an equivalence class of state machines that are reducible to the minimal machine of the inferred grammar. We then show through simulations that many of the neural net state machines are dynamically stable, that is, they correctly classify many long unseen strings. In addition, some of these extracted automata actually outperform the trained neural network for classification...},
  annotation = {472 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JT6YWNRL/Giles et al. - 1992 - Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks(2).pdf},
  keywords = {u},
  number = {3}
}

@inproceedings{girshick2014RichFeatureHierarchies,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  booktitle = {{{CVPR}}},
  author = {Girshick, R. and Donahue, J. and Darrell, T. and Malik, J.},
  date = {2014},
  pages = {2--9},
  issn = {10636919},
  doi = {10.1109/CVPR.2014.81},
  annotation = {9996 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {26656583},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6PIBPW5Y/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation(2).pdf},
  isbn = {978-1-4799-5118-5}
}

@inproceedings{girshick2015FastRCNN,
  title = {Fast {{R}}-{{CNN}}},
  booktitle = {{{ICCV}}},
  author = {Girshick, R.},
  date = {2015},
  volume = {2015 Inter},
  pages = {1440--1448},
  issn = {15505499},
  doi = {10.1109/ICCV.2015.169},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  annotation = {9993 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {23739795},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NW8SARPG/Girshick - 2015 - Fast R-CNN(2).pdf},
  isbn = {978-1-4673-8391-2}
}

@report{gkatzia2016ContentSelectionDatatoText,
  title = {Content {{Selection}} in {{Data}}-to-{{Text Systems}}: {{A Survey}}},
  author = {Gkatzia, D.},
  date = {2016},
  archiveprefix = {arXiv},
  eprint = {1610.08375v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UVT99BDT/Gkatzia - 2016 - Content Selection in Data-to-Text Systems A Survey(2).pdf}
}

@report{goldberg2019AssessingBERTSyntactic,
  title = {Assessing {{BERT}}'s {{Syntactic Abilities}}},
  author = {Goldberg, Yoav},
  date = {2019-01-16},
  url = {http://arxiv.org/abs/1901.05287},
  urldate = {2020-11-19},
  annotation = {190 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1901.05287},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ILFTMEJI/Goldberg - 2019 - Assessing BERT's Syntactic Abilities.pdf;/home/hiaoxui/.local/share/zotero_files/storage/TTWDDREB/1901.html}
}

@inproceedings{gong2018HeavyKeeperAccurateAlgorithm,
  title = {{{HeavyKeeper}}: {{An Accurate Algorithm}} for {{Finding Top}}-k {{Elephant Flows}}},
  booktitle = {{{USENIX Annual Technical Conference}}},
  author = {Gong, J. and Yang, T. and Zhang, H. and Hao, L. and Uhlig, S. and Chen, S. and Uden, L. and Li, X.},
  date = {2018},
  issn = {1063-6692},
  doi = {10.1109/tnet.2019.2933868},
  abstract = {Finding top-k elephant flows is a critical task in network traffic measurement, with many applications in congestion control, anomaly detection and traffic engineering. As the line rates keep increasing in today's networks, designing accurate and fast algorithms for online identification of elephant flows becomes more and more challenging. The prior algorithms are seriously limited in achieving accuracy under the constraints of heavy traffic and small on-chip memory in use. We observe that the basic strategies adopted by these algorithms either require significant space overhead to measure the sizes of all flows or incur significant inaccuracy when deciding which flows to keep track of. In this paper, we adopt a new strategy, called count-with-exponential-decay, to achieve space-accuracy balance by actively removing small flows through decaying, while minimizing the impact on large flows, so as to achieve high precision in finding top-k elephant flows. Moreover, the proposed algorithm called HeavyKeeper incurs small, constant processing overhead per packet and thus supports high line rates. Experimental results show that HeavyKeeper algorithm achieves 99.99\% precision with a small memory size, and reduces the error by around 3 orders of magnitude on average compared to the state-of-the-art.},
  annotation = {21 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4GWWS9RJ/Gong et al. - 2018 - HeavyKeeper An Accurate Algorithm for Finding Top-k Elephant Flows(2).pdf},
  isbn = {978-1-939133-02-1}
}

@inproceedings{goodfellow2014GenerativeAdversarialNets,
  title = {Generative {{Adversarial Nets}}},
  booktitle = {{{NeurIPS}}},
  author = {Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
  date = {2014},
  pages = {2672--2680},
  issn = {10495258},
  doi = {10.1017/CBO9781139058452},
  abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
  archiveprefix = {arXiv},
  eprint = {1000183096},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M6UCMDKC/Goodfellow et al. - 2014 - Generative Adversarial Nets(2).pdf},
  isbn = {1406.2661}
}

@inproceedings{goodman2016UCLSheffieldSemEval2016,
  title = {{{UCL}}+{{Sheffield}} at {{SemEval}}-2016 Task 8: {{Imitation}} Learning for Amr Parsing with an α-Bound},
  booktitle = {{{SemEval}}},
  author = {Goodman, J. and Vlachos, A. and Naradowsky, J.},
  date = {2016},
  pages = {1167--1172},
  doi = {10.18653/v1/s16-1180},
  abstract = {We develop a novel transition-based parsing algorithm for the abstract meaning representation parsing task using exact imitation learning, in which the parser learns a statistical model by imitating the actions of an expert on the training data. We then use the imitation learning algorithm DAgger to improve the performance, and apply an α-bound as a simple noise reduction technique. Our performance on the test set was 60\% in F-score, and the performance gains on the development set due to DAgger was up to 1.1 points of F-score. The α-bound improved performance by up to 1.8 points.},
  annotation = {17 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HITSGN75/Goodman, Vlachos, Naradowsky - 2016 - UCLSheffield at SemEval-2016 task 8 Imitation learning for amr parsing with an α-bound(2).pdf}
}

@inproceedings{gormley2014LowresourceSemanticRole,
  title = {Low-Resource Semantic Role Labeling},
  booktitle = {{{ACL}}},
  author = {Gormley, M. R. and Mitchell, M. and Van Durme, B. and Dredze, Mark},
  date = {2014},
  pages = {1177--1187},
  abstract = {We explore the extent to which highresource manual annotations such as treebanks are necessary for the task of semantic role labeling (SRL). We examine how performance changes without syntactic supervision, comparing both joint and pipelined methods to induce latent syntax. This work highlights a new application of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax. Our best models obtain competitive results in the high-resource setting and state-ofthe- art results in the low resource setting, reaching 72.48\% F1 averaged across languages. We release our code for this work along with a larger toolkit for specifying arbitrary graphical structure. © 2014 Association for Computational Linguistics.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZGJZ7X2A/Gormley et al. - 2014 - Low-resource semantic role labeling(2).pdf}
}

@article{gorniak2007SituatedLanguageUnderstanding,
  title = {Situated Language Understanding as Filtering Perceived Affordances.},
  author = {Gorniak, P. and Roy, D.},
  date = {2007},
  journaltitle = {Cognitive Science},
  volume = {31},
  pages = {197--231},
  issn = {0364-0213},
  doi = {10.1080/15326900701221199},
  abstract = {We introduce a computational theory of situated language understanding in which the meaning of words and utterances depends on the physical environment and the goals and plans of communication partners. According to the theory, concepts that ground linguistic meaning are neither internal nor external to language users, but instead span the objective-subjective boundary. To model the possible interactions between subject and object, the theory relies on the notion of perceived affordances: structured units of interaction that can be used for prediction at multiple levels of abstraction. Language understanding is treated as a process of filtering perceived affordances. The theory accounts for many aspects of the situated nature of human language use and provides a unified solution to a number of demands on any theory of language understanding including conceptual combination, prototypicality effects, and the generative nature of lexical items. To support the theory, we describe an implemented system that understands verbal commands situated in a virtual gaming environment. The implementation uses probabilistic hierarchical plan recognition to generate perceived affordances. The system has been evaluated on its ability to correctly interpret free-form spontaneous verbal commands recorded from unrehearsed game play between human players. The system is able to "step into the shoes" of human players and correctly respond to a broad range of verbal commands in which linguistic meaning depends on social and physical context. We quantitatively compare the system's predictions in response to direct player commands with the actions taken by human players and show generalization to unseen data across a range of situations and verbal constructions.},
  annotation = {62 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {21635295},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5RE95XFD/Gorniak, Roy - 2007 - Situated language understanding as filtering perceived affordances(2).pdf},
  isbn = {0364-0213 1551-6709},
  keywords = {u},
  number = {2}
}

@inproceedings{goulding2016EventSeriesPrediction,
  title = {Event Series Prediction via Non-Homogeneous Poisson Process Modelling},
  booktitle = {{{ICDM}}},
  author = {Goulding, J. and Preston, S. and Smith, G.},
  date = {2016},
  publisher = {{IEEE}},
  issn = {15504786},
  doi = {10.1109/ICDM.2016.150},
  abstract = {© 2016 IEEE. Data streams whose events occur at random arrival times rather than at the regular, tick-Tock intervals of traditional time series are increasingly prevalent. Event series are continuous, irregular and often highly sparse, differing greatly in nature to the regularly sampled time series traditionally the concern of hard sciences. As mass sets of such data have become more common, so interest in predicting future events in them has grown. Yet repurposing of traditional forecasting approaches has proven ineffective, in part due to issues such as sparsity, but often due to inapplicable underpinning assumptions such as stationarity and ergodicity. In this paper we derive a principled new approach to forecasting event series that avoids such assumptions, based upon: 1. The processing of event series datasets in order to produce a first parameterized mixture model of non-homogeneous Poisson processes, and 2. Application of a technique called parallel forecasting that uses these processes' rate functions to directly generate accurate temporal predictions for new query realizations. This approach uses forerunners of a stochastic process to shed light on the distribution of future events, not for themselves, but for realizations that subsequently follow in their footsteps.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HT47K5FF/Goulding, Preston, Smith - 2016 - Event series prediction via non-homogeneous poisson process modelling(2).pdf},
  isbn = {978-1-5090-5472-5}
}

@report{govindarajan2019DecomposingGeneralizationModels,
  title = {Decomposing {{Generalization}}: {{Models}} of {{Generic}}, {{Habitual}}, and {{Episodic Statements}}},
  author = {Govindarajan, V. and Van Durme, B. and White, A. S.},
  date = {2019},
  url = {http://arxiv.org/abs/1901.11429},
  abstract = {We present a novel semantic framework for modeling linguistic expressions of generalization---generic, habitual, and episodic statements---as combinations of simple, real-valued referential properties of predicates and their arguments. We use this framework to construct a dataset covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to probe the efficacy of type-level and token-level information---including hand-engineered features and static (GloVe) and contextual (ELMo) word embeddings---for predicting expressions of generalization. Data and code are available at decomp.io.},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1901.11429},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JEEFAPZC/Govindarajan, Van Durme, White - 2019 - Decomposing Generalization Models of Generic, Habitual, and Episodic Statements(2).pdf}
}

@inproceedings{goyal2011ApproximateScalableBounded,
  title = {Approximate Scalable Bounded Space Sketch for Large Data {{NLP}}},
  booktitle = {{{EMNLP}}},
  author = {Goyal, A. and Daumé III, H.},
  date = {2011},
  pages = {250--261},
  abstract = {We exploit sketch techniques, especially the Count-Min sketch, a memory, and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself. These methods use hashing to deal with massive amounts of streaming text. We apply Count-Min sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory). © 2011 Association for Computational Linguistics.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IREKBSAD/Goyal, Daumé III - 2011 - Approximate scalable bounded space sketch for large data NLP(2).pdf},
  isbn = {1-937284-11-5},
  keywords = {u}
}

@inproceedings{goyal2011LossyConservativeUpdate,
  title = {Lossy Conservative Update ({{LCU}}) Sketch: {{Succinct}} Approximate Count Storage},
  booktitle = {{{AAAI}}},
  author = {Goyal, A. and Daumé III, H.},
  date = {2011},
  pages = {878--883},
  abstract = {In this paper, we propose a variant of the conservative-update Count-Min sketch to further reduce the over-estimation error incurred. Inspired by ideas from lossy counting, we divide a stream of items into multiple windows, and decrement certain counts in the sketch at window boundaries. We refer to this approach as a lossy conservative update (LCU). The reduction in over-estimation error of counts comes at the cost of introducing under-estimation error in counts. However, in our intrinsic evaluations, we show that the reduction in over-estimation is much greater than the under-estimation error introduced by our method LCU. We apply our LCU framework to scale distributional similarity computations to web-scale corpora. We show that this technique is more efficient in terms of memory, and time, and more robust than conservative update with Count-Min (CU) sketch on this task. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9D2PP5D3/Goyal, Daumé III - 2011 - Lossy conservative update (LCU) sketch Succinct approximate count storage(2).pdf},
  isbn = {978-1-57735-508-3}
}

@inproceedings{graca2008ExpectationMaximizationPosterior,
  title = {Expectation {{Maximization}} and {{Posterior Constraints}}},
  booktitle = {{{NeurIPS}}},
  author = {Graça, J. V. and Ganchev, K. and Taskar, B.},
  date = {2008},
  abstract = {The expectation maximization (EM) algorithm is a widely used\textbackslash nmaximum likeli- hood estimation procedure for statistical models\textbackslash nwhen the values of some of the variables in the model are not\textbackslash nobserved. Very often, however, our aim is primar- ily to find a\textbackslash nmodel that assigns values to the latent variables that have\textbackslash nintended meaning for our data and maximizing expected likelihood\textbackslash nonly sometimes ac- complishes this. Unfortunately, it is\textbackslash ntypically difficult to add even simple a-priori information about\textbackslash nlatent variables in graphical models without making the models\textbackslash noverly complex or intractable. In this paper, we present an\textbackslash nefficient, principled way to inject rich constraints on the\textbackslash nposteriors of latent variables into the EM algorithm. Our method\textbackslash ncan be used to learn tractable graphical models that sat- isfy\textbackslash nadditional, otherwise intractable constraints. Focusing on\textbackslash nclustering and the alignment problem for statistical machine\textbackslash ntranslation, we show that simple, in- tuitive posterior\textbackslash nconstraints can greatly improve the performance over standard\textbackslash nbaselines and be competitive with more complex, intractable\textbackslash nmodels.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/59B5RJCT/Graça, Ganchev, Taskar - 2008 - Expectation Maximization and Posterior Constraints(2).pdf},
  isbn = {1-60560-352-X}
}

@article{graca2010LearningTractableWord,
  title = {Learning {{Tractable Word Alignment Models}} with {{Complex Constraints}}},
  author = {Graça, J. V. and Ganchev, K. and Taskar, B.},
  date = {2010},
  journaltitle = {Computational Linguistics},
  volume = {36},
  pages = {481--504},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00007},
  abstract = {Word-level alignment of bilingual text is a critical resource for a growing variety of tasks. Proba- bilistic models for word alignment present a fundamental trade-off between richness of captured constraints and correlations versus efficiency and tractability of inference. In this article, we use the Posterior Regularization framework (Gra¸ ca, Ganchev, and Taskar 2007) to incorporate complex constraints into probabilistic models during learning without changing the efficiency of the underlying model. We focus on the simple and tractable hidden Markov model, and present an efficient learning algorithm for incorporating approximate bijectivity and symmetry constraints. Models estimated with these constraints produce a significant boost in performance as measured by both precision and recall of manually annotated alignments for six language pairs. We also report experiments on two different tasks where word alignments are required: phrase-based machine translation and syntax transfer, and show promising improvements over standard methods},
  annotation = {32 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V8WBDPEH/Graça, Ganchev, Taskar - 2010 - Learning Tractable Word Alignment Models with Complex Constraints(2).pdf},
  isbn = {0891-2017},
  number = {3}
}

@inproceedings{grachev2017NeuralNetworksCompression,
  title = {Neural {{Networks Compression}} for {{Language Modeling}}},
  booktitle = {International {{Conference}} on {{Pattern Recognition}} and {{Artificial Intelligence}}},
  author = {Grachev, A. M. and Ignatov, D. I. and Savchenko, A. V.},
  date = {2017},
  archiveprefix = {arXiv},
  eprint = {1708.05963v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JR6SV26J/Grachev, Ignatov, Savchenko - 2017 - Neural Networks Compression for Language Modeling(2).pdf}
}

@inproceedings{grave2015ConvexFeaturerichDiscriminative,
  title = {A Convex and Feature-Rich Discriminative Approach to Dependency Grammar Induction},
  booktitle = {{{ACL}}-{{IJCNLP}}},
  author = {Grave, É. and Elhadad, N.},
  date = {2015},
  pages = {1375--1384},
  abstract = {In this paper, we introduce a new method for the problem of unsupervised depen-dency parsing. Most current approaches are based on generative models. Learning the parameters of such models relies on solving a non-convex optimization prob-lem, thus making them sensitive to initial-ization. We propose a new convex formu-lation to the task of dependency grammar induction. Our approach is discriminative, allowing the use of different kinds of fea-tures. We describe an efficient optimiza-tion algorithm to learn the parameters of our model, based on the Frank-Wolfe algo-rithm. Our method can easily be general-ized to other unsupervised learning prob-lems. We evaluate our approach on ten languages belonging to four different fam-ilies, showing that our method is competi-tive with other state-of-the-art methods.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VIHHQUPE/Grave, Elhadad - 2015 - A convex and feature-rich discriminative approach to dependency grammar induction(2).pdf},
  isbn = {978-1-941643-72-3}
}

@report{graves2017AutomatedCurriculumLearning,
  title = {Automated {{Curriculum Learning}} for {{Neural Networks}}},
  author = {Graves, A. and Bellemare, M. G. and Menick, J. and Munos, R. and Kavukcuoglu, K.},
  date = {2017},
  issn = {1938-7228},
  url = {http://arxiv.org/abs/1704.03003},
  abstract = {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.},
  annotation = {229 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.03003},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UZEFR3B3/Graves et al. - 2017 - Automated Curriculum Learning for Neural Networks(2).pdf},
  isbn = {9781510855144}
}

@article{green1995ReversibleJumpMarkov,
  title = {Reversible {{Jump Markov Chain Monte Carlo Computation}} and {{Bayesian Model Determination}}},
  author = {Green, P. J.},
  date = {1995},
  journaltitle = {Biometrika},
  volume = {82},
  pages = {711--732},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SAEHPA6D/Green - 1995 - Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination(2).pdf},
  keywords = {u},
  number = {4}
}

@inproceedings{grenager2005UnsupervisedLearningField,
  title = {Unsupervised {{Learning}} of {{Field Segmentation Models}} for {{Information Extraction}}},
  booktitle = {{{ACL}}},
  author = {Grenager, T. and Klein, D. and Manning, C. D.},
  date = {2005},
  pages = {371--378},
  doi = {10.3115/1219840.1219886},
  url = {http://portal.acm.org/citation.cfm?id=1219886&amp;dl=},
  abstract = {The applicability of many current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.},
  annotation = {88 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H9UE7SLR/Grenager, Klein, Manning - 2005 - Unsupervised Learning of Field Segmentation Models for Information Extraction(2).pdf},
  isbn = {1-932432-51-5},
  issue = {June},
  keywords = {u}
}

@article{grosz1986AttentionIntentionsStructure,
  title = {Attention, Intentions, and the Structure of Discourse},
  author = {Grosz, B. J. and Sidner, C. L.},
  date = {1986},
  journaltitle = {Computational Linguistics},
  volume = {12},
  pages = {175--204},
  issn = {08912017},
  doi = {10.14348/molcells.2014.0104},
  url = {http://dl.acm.org/citation.cfm?id=12458},
  abstract = {In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse. In this theory, discourse structure is composed of three separate but interre- lated components: the structure of the sequence of utterances (called the linguistic structure), a struc- ture of purposes (called the intentional structure), and the state of focus of attention (called the attentional state). The linguistic structure consists of segments of the discourse into which the utter- ances naturally aggregate. The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds. The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse. The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions. The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses. Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored. This theory provides a framework for describing the processing of utterances in a discourse. Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and track- ing the discourse through the operation of the mechanisms associated with attentional state. This processing description specifies in these recognition tasks the role of information from the discourse and from the participants' knowledge of the domain.},
  annotation = {92 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T9JY45CH/Grosz, Sidner - 1986 - Attention, intentions, and the structure of discourse(2).pdf},
  isbn = {0891-2017},
  keywords = {u},
  number = {3}
}

@inproceedings{gu2018NonautoregressiveNeuralMachine,
  title = {Non-Autoregressive Neural Machine Translation},
  booktitle = {{{ICLR}}},
  author = {Gu, J. and Bradbury, J. and Xiong, C. and Li, V. O. K. and Socher, R.},
  date = {2018},
  doi = {10.18653/v1/d18-1044},
  abstract = {Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.},
  annotation = {28 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1808.08583},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G6ECYTZM/Gu et al. - 2018 - Non-autoregressive neural machine translation(2).pdf},
  keywords = {u}
}

@inproceedings{gu2019LevenshteinTransformer,
  title = {Levenshtein {{Transformer}}},
  booktitle = {{{NeurIPS}}},
  author = {Gu, J. and Wang, C. and Zhao, J},
  date = {2019},
  url = {http://arxiv.org/abs/1905.11006},
  abstract = {Modern neural sequence generation models are built to either generate tokens step-by-step from scratch or (iteratively) modify a sequence of tokens bounded by a fixed length. In this work, we develop Levenshtein Transformer, a new partially autoregressive model devised for more flexible and amenable sequence generation. Unlike previous approaches, the atomic operations of our model are insertion and deletion. The combination of them facilitates not only generation but also sequence refinement allowing dynamic length changes. We also propose a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable performance but much-improved efficiency on both generation (e.g. machine translation, text summarization) and refinement tasks (e.g. automatic post-editing). We further confirm the flexibility of our model by showing a Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing.},
  annotation = {79 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1905.11006},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KPXPBLUY/Gu, Wang, Zhao - 2019 - Levenshtein Transformer(2).pdf},
  keywords = {u}
}

@article{gu2020InsertionbasedDecodingAutomatically,
  title = {Insertion-Based {{Decoding}} with Automatically {{Inferred Generation Order}}},
  author = {Gu, J. and Liu, Q. and Cho, K.},
  date = {2020},
  journaltitle = {TACL},
  url = {http://arxiv.org/abs/1902.01370},
  abstract = {Conventional neural autoregressive decoding commonly assumes a fixed left-to-right generation order, which may be sub-optimal. In this work, we propose a novel decoding algorithm -- InDIGO -- which supports flexible sequence generation in arbitrary orders through insertion operations. We extend Transformer, a state-of-the-art sequence generation model, to efficiently implement the proposed approach, enabling it to be trained with either a pre-defined generation order or adaptive orders obtained from beam-search. Experiments on four real-world tasks, including word order recovery, machine translation, image caption and code generation, demonstrate that our algorithm can generate sequences following arbitrary orders, while achieving competitive or even better performance compared to the conventional left-to-right generation. The generated sequences show that InDIGO adopts adaptive generation orders based on input information.},
  annotation = {60 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1902.01370},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S6IS7H3U/Gu, Liu, Cho - 2020 - Insertion-based Decoding with automatically Inferred Generation Order(2).pdf},
  keywords = {u}
}

@inproceedings{gui2019LexiconBasedGraphNeural,
  title = {A {{Lexicon}}-{{Based Graph Neural Network}} for {{Chinese NER}}},
  booktitle = {{{EMNLP}}},
  author = {Gui, T. and Zou, Y. and Zhang, Q.},
  date = {2019},
  pages = {1040--1050},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UUPHLP85/Gui, Zou, Zhang - 2019 - A Lexicon-Based Graph Neural Network for Chinese NER(2).pdf},
  keywords = {u}
}

@inproceedings{guo2018DialogtoActionConversationalQuestion,
  title = {Dialog-to-{{Action}}: {{Conversational Question Answering Over}} a {{Large}}-{{Scale Knowledge Base}}},
  booktitle = {{{NeurIPS}}},
  author = {Guo, Daya and Tang, Duyu and Duan, Nan and Zhou, Ming and Yin, Jian},
  date = {2018},
  pages = {10},
  abstract = {We present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. To handle enormous ellipsis phenomena in conversation, we introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances. Dialog memory management is embodied in a generative model, in which a logical form is interpreted in a top-down manner following a small and flexible grammar. We learn the model from denotations without explicit annotation of logical forms, and evaluate it on a large-scale dataset consisting of 200K dialogs over 12.8M entities. Results verify the benefits of modeling dialog memory, and show that our semantic parsing-based approach outperforms a memory network based encoder-decoder model by a huge margin.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2NBHMT7Y/Guo et al. - Dialog-to-Action Conversational Question Answerin.pdf;/home/hiaoxui/.local/share/zotero_files/storage/DZD86BWW/Guo et al. - Dialog-to-Action Conversational Question Answerin.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{gupta2020NeuralModuleNetworks,
  title = {Neural {{Module Networks}} for {{Reasoning}} over {{Text}}},
  booktitle = {{{ICLR}}},
  author = {Gupta, Nitish and Lin, Kevin and Roth, Dan and Singh, Sameer and Gardner, Matt},
  date = {2020-02-15},
  url = {http://arxiv.org/abs/1912.04971},
  urldate = {2021-03-02},
  abstract = {Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains. However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning. Our proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules.},
  annotation = {25 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1912.04971},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8XQPZ5I7/Gupta et al. - 2020 - Neural Module Networks for Reasoning over Text.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KUXIYYEH/1912.html}
}

@inproceedings{gururangan2018AnnotationArtifactsNatural,
  title = {Annotation {{Artifacts}} in {{Natural Language Inference Data}}},
  booktitle = {{{NAACL}}},
  author = {Gururangan, S. and Swayamdipta, S. and Levy, O. and Schwartz, R. and Bowman, S. R. and Smith, N. A.},
  date = {2018},
  issn = {1702.00887},
  doi = {10.18653/v1/N18-2017},
  url = {http://arxiv.org/abs/1803.02324},
  abstract = {Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67\% of SNLI (Bowman et. al, 2015) and 53\% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.},
  annotation = {379 citations (Semantic Scholar/DOI) [2021-03-26] 379 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1803.02324},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EMD4CGKX/Gururangan et al. - 2018 - Annotation Artifacts in Natural Language Inference Data(2).pdf},
  isbn = {978-1-5108-2758-5}
}

@article{gutmann2012NoiseContrastiveEstimationUnnormalized,
  title = {Noise-{{Contrastive Estimation}} of {{Unnormalized Statistical Models}}, with {{Applications}} to {{Natural Image Statistics}}},
  author = {Gutmann, M. U. and Hyvärine, A.},
  date = {2012},
  journaltitle = {JMLR},
  volume = {13},
  pages = {307--361},
  issn = {1532-4435},
  abstract = {We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, themodel is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective func- tion for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially gener- ated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to be- have like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimationmethods for unnormalizedmodels. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6TSLL5SE/Gutmann, Hyvärine - 2012 - Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statis(2).pdf},
  isbn = {1532-4435},
  keywords = {u}
}

@inproceedings{guu2017LanguageProgramsBridging,
  title = {From {{Language}} to {{Programs}}: {{Bridging Reinforcement Learning}} and {{Maximum Marginal Likelihood}}},
  booktitle = {{{ACL}}},
  author = {Guu, K. and Pasupat, P. and Liu, E. Z. and Liang, P.},
  date = {2017},
  pages = {1051--1062},
  doi = {10.18653/v1/P17-1097},
  url = {http://arxiv.org/abs/1704.07926},
  abstract = {Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.},
  annotation = {122 citations (Semantic Scholar/DOI) [2021-03-26] 122 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.07926},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5VYI6PF3/Guu et al. - 2017 - From Language to Programs Bridging Reinforcement Learning and Maximum Marginal Likelihood(2).pdf},
  isbn = {978-1-945626-75-3},
  keywords = {u}
}

@inproceedings{hahn2016KnowledgeAcceleratorBig,
  title = {The {{Knowledge Accelerator}}: {{Big Picture Thinking}} in {{Small Pieces}}},
  booktitle = {Human {{Factors}} in {{Computing Systems}}},
  author = {Hahn, N. and Chang, J. and Kim, J. E. and Kittur, A.},
  date = {2016},
  pages = {2258--2270},
  doi = {10.1145/2858036.2858364},
  abstract = {Crowdsourcing offers a powerful new paradigm for online work. However, real world tasks are often interdependent, requiring a big picture view of the difference pieces involved. Existing crowdsourcing approaches that support such tasks -- ranging from Wikipedia to flash teams -- are bottlenecked by relying on a small number of individuals to maintain the big picture. In this paper, we explore the idea that a computational system can scaffold an emerging interdependent, big picture view entirely through the small contributions of individuals, each of whom sees only a part of the whole. To investigate the viability, strengths, and weaknesses of this approach we instantiate the idea in a prototype system for accomplishing distributed information synthesis and evaluate its output across a variety of topics. We also contribute a set of design patterns that may be informative for other systems aimed at supporting big picture thinking in small pieces.},
  annotation = {40 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZEXGTKXQ/Hahn et al. - 2016 - The Knowledge Accelerator Big Picture Thinking in Small Pieces(2).pdf},
  isbn = {978-1-4503-3362-7},
  keywords = {u}
}

@inproceedings{hajishirzi2011ReasoningRoboCupSoccer,
  title = {Reasoning about {{RoboCup Soccer Narratives}}},
  booktitle = {{{UAI}}},
  author = {Hajishirzi, H. and Hockenmaier, J. and Mueller, E. T. and Amir, E.},
  date = {2011},
  url = {http://arxiv.org/abs/1202.3728},
  abstract = {This paper presents an approach for learning to translate simple narratives, i.e., texts (sequences of sentences) describing dynamic systems, into coherent sequences of events without the need for labeled training data. Our approach incorporates domain knowledge in the form of preconditions and effects of events, and we show that it outperforms state-of-the-art supervised learning systems on the task of reconstructing RoboCup soccer games from their commentaries.},
  annotation = {21 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1202.3728},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V6EFWPVE/Hajishirzi et al. - 2011 - Reasoning about RoboCup Soccer Narratives(2).pdf},
  isbn = {978-0-9749039-7-2}
}

@inproceedings{hale2018FindingSyntaxHuman,
  title = {Finding {{Syntax}} in {{Human Encephalography}} with {{Beam Search}}},
  booktitle = {{{ACL}}},
  author = {Hale, J. and Dyer, C. and Kuncoro, A. and Brennan, J. R.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1806.04127},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ABMQ7EPJ/Hale et al. - 2018 - Finding Syntax in Human Encephalography with Beam Search(2).pdf},
  keywords = {u},
  number = {2014}
}

@article{hallett2007ComposingQuestionsConceptual,
  title = {Composing {{Questions}} through {{Conceptual Authoring}}},
  author = {Hallett, C. and Scott, D. and Power, R.},
  date = {2007},
  journaltitle = {Computational Linguistics},
  volume = {33},
  pages = {105--133},
  issn = {0891-2017},
  doi = {10.1162/coli.2007.33.1.105},
  url = {http://www.mitpressjournals.org/doi/10.1162/coli.2007.33.1.105},
  abstract = {This article describes a method for composing fluent and complex natural language questions, while avoiding the standard pitfalls of free text queries. The method, based on Conceptual Authoring, is targeted at question-answering systems where reliability and transparency are critical, and where users cannot be expected to undergo extensive training in question composition. This scenario is found in most corporate domains, especially in applications that are risk-averse. We present a proof-of-concept system we have developed: a question-answering interface to a large repository of medical histories in the area of cancer. We show that the method allows users to successfully and reliably compose complex queries with minimal training.},
  annotation = {91 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DXVF75WV/Hallett, Scott, Power - 2007 - Composing Questions through Conceptual Authoring(2).pdf},
  isbn = {0891-2017},
  number = {1}
}

@inproceedings{hamilton2017InductiveRepresentationLearning,
  title = {Inductive {{Representation Learning}} on {{Large Graphs}}},
  booktitle = {{{NeurIPS}}},
  author = {Hamilton, W. L. and Ying, R. and Leskovec, J.},
  date = {2017},
  pages = {1--11},
  url = {http://arxiv.org/abs/1710.09471},
  abstract = {Graphs (networks) are ubiquitous and allow us to model entities (nodes) and the dependencies (edges) between them. Learning a useful feature representation from graph data lies at the heart and success of many machine learning tasks such as classification, anomaly detection, link prediction, among many others. Many existing techniques use random walks as a basis for learning features or estimating the parameters of a graph model for a downstream prediction task. Examples include recent node embedding methods such as DeepWalk, node2vec, as well as graph-based deep learning algorithms. However, the simple random walk used by these methods is fundamentally tied to the identity of the node. This has three main disadvantages. First, these approaches are inherently transductive and do not generalize to unseen nodes and other graphs. Second, they are not space-efficient as a feature vector is learned for each node which is impractical for large graphs. Third, most of these approaches lack support for attributed graphs. To make these methods more generally applicable, we propose a framework for inductive network representation learning based on the notion of attributed random walk that is not tied to node identity and is instead based on learning a function \$\textbackslash Phi : \textbackslash mathrm\{\textbackslash rm \textbackslash bf x\} \textbackslash rightarrow w\$ that maps a node attribute vector \$\textbackslash mathrm\{\textbackslash rm \textbackslash bf x\}\$ to a type \$w\$. This framework serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many other previous methods that leverage traditional random walks.},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1710.09471},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BTR5N8JL/Hamilton, Ying, Leskovec - 2017 - Inductive Representation Learning on Large Graphs(2).pdf},
  issue = {Nips}
}

@inproceedings{hancock2018TrainingClassifiersNatural,
  title = {Training Classifiers with Natural Language Explanations},
  booktitle = {{{ACL}}},
  author = {Hancock, B. and Bringmann, M. and Varma, P. and Liang, P. and Wang, S. and Ré, C.},
  date = {2018},
  volume = {1},
  pages = {1884--1895},
  abstract = {Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100× faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.},
  archiveprefix = {arXiv},
  eprint = {1805.03818v4},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/55HL335D/Hancock et al. - 2018 - Training classifiers with natural language explanations(2).pdf},
  isbn = {978-1-948087-32-2},
  keywords = {u}
}

@inproceedings{hartmann2017OutofdomainFrameNetSemantic,
  title = {Out-of-Domain {{FrameNet}} Semantic Role Labeling},
  booktitle = {{{EACL}}},
  author = {Hartmann, S. and Kuznetsov, I. and Martin, T. and Gurevych, I.},
  date = {2017},
  doi = {10.18653/v1/e17-1045},
  abstract = {Domain dependence of NLP systems is one of the major obstacles to their application in large-scale text analysis, also restricting the applicability of FrameNet semantic role labeling (SRL) systems. Yet, current FrameNet SRL systems are still only evaluated on a single in-domain test set. For the first time, we study the domain dependence of FrameNet SRL on a wide range of benchmark sets. We create a novel test set for FrameNet SRL based on user-generated web text and find that the major bottleneck for out-of-domain FrameNet SRL is the frame identification step. To address this problem, we develop a simple, yet efficient system based on distributed word representations. Our system closely approaches the state-of-the-art in-domain while outperforming the best available frame identification system out-of-domain. We publish our system and test data for research purposes.},
  annotation = {30 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G2RECFFX/Hartmann et al. - 2017 - Out-of-domain FrameNet semantic role labeling(2).pdf},
  isbn = {978-1-5108-3860-4}
}

@inproceedings{hashimoto2018FairnessDemographicsRepeated,
  title = {Fairness {{Without Demographics}} in {{Repeated Loss Minimization}}},
  booktitle = {{{ICML}}},
  author = {Hashimoto, T. B. and Srivastava, M. and Namkoong, H. and Liang, P.},
  date = {2018},
  url = {http://arxiv.org/abs/1806.08010},
  abstract = {Machine learning models (e.g., speech recognizers) are usually trained to minimize average loss, which results in representation disparity---minority groups (e.g., non-native speakers) contribute less to the training objective and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even make initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.},
  annotation = {112 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1806.08010},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M6QQPJBY/Hashimoto et al. - 2018 - Fairness Without Demographics in Repeated Loss Minimization(2).pdf},
  keywords = {u}
}

@inproceedings{havrylov2017EmergenceLanguageMultiagent,
  title = {Emergence of {{Language}} with {{Multi}}-Agent {{Games}}: {{Learning}} to {{Communicate}} with {{Sequences}} of {{Symbols}}},
  booktitle = {{{NeurIPS}}},
  author = {Havrylov, S. and Titov, I.},
  date = {2017},
  url = {http://arxiv.org/abs/1705.11192},
  abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
  annotation = {129 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1705.11192},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GFRCA2E7/Havrylov, Titov - 2017 - Emergence of Language with Multi-agent Games Learning to Communicate with Sequences of Symbols(2).pdf},
  keywords = {u}
}

@inproceedings{hayashi2019NoncommutativeBilinearModel,
  title = {A {{Non}}-Commutative {{Bilinear Model}} for {{Answering Path Queries}} in {{Knowledge Graphs}}},
  booktitle = {{{EMNLP}}},
  author = {Hayashi, K. and Shimbo, M.},
  date = {2019},
  pages = {2422--2430},
  doi = {10.18653/v1/d19-1246},
  abstract = {Bilinear diagonal models for knowledge graph embedding (KGE), such as DistMult and ComplEx, balance expressiveness and computational efficiency by representing relations as diagonal matrices. Although they perform well in predicting atomic relations, composite relations (relation paths) cannot be modeled naturally by the product of relation matrices, as the product of diagonal matrices is commutative and hence invariant with the order of relations. In this paper, we propose a new bilinear KGE model, called BlockHolE, based on block circulant matrices. In BlockHolE, relation matrices can be non-commutative, allowing composite relations to be modeled by matrix product. The model is parameterized in a way that covers a spectrum ranging from diagonal to full relation matrices. A fast computation technique is developed on the basis of the duality of the Fourier transform of circulant matrices.},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.01567},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BIZBXF96/Hayashi, Shimbo - 2019 - A Non-commutative Bilinear Model for Answering Path Queries in Knowledge Graphs(2).pdf},
  keywords = {u}
}

@inproceedings{he2013DynamicFeatureSelection,
  title = {Dynamic Feature Selection for Dependency Parsing},
  booktitle = {{{EMNLP}}},
  author = {He, H. and Daumé III, H. and Eisner, J. M.},
  date = {2013},
  pages = {1455--1464},
  abstract = {Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30\% of the feature templates.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I3HM72PT/He, Daumé III, Eisner - 2013 - Dynamic feature selection for dependency parsing(2).pdf},
  isbn = {978-1-937284-97-8}
}

@inproceedings{he2014LearningSearchBranchandbound,
  title = {Learning to Search in Branch-and-Bound Algorithms},
  booktitle = {{{NeurIPS}}},
  author = {He, H. and Daumé III, H. and Eisner, J. M.},
  date = {2014},
  pages = {3293--3301},
  issn = {10495258},
  abstract = {Branch-and-bound is a widely used method in combinatorial optimization, including mixed integer programming, structured prediction and MAP inference. While most work has been focused on developing problem-specific techniques, little is known about how to systematically design the node searching strategy on a branch-and-bound tree. We address the key challenge of learning an adaptive node searching order for any class of problem solvable by branch-and-bound. Our strategies are learned by imitation learning. We apply our algorithm to linear programming based branch-and-bound for solving mixed integer programs (MIP). We compare our method with one of the fastest open-source solvers, SCIP; and a very efficient commercial solver, Gurobi. We demonstrate that our approach achieves better solutions faster on four MIP libraries.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LMQFIUZN/He, Daumé III, Eisner - 2014 - Learning to search in branch-and-bound algorithms(2).pdf}
}

@inproceedings{he2015QuestionanswerDrivenSemantic,
  title = {Question-Answer Driven Semantic Role Labeling: {{Using}} Natural Language to Annotate Natural Language},
  booktitle = {{{EMNLP}}},
  author = {He, L. and Lewis, M. and Zettlemoyer, L. S.},
  date = {2015},
  pages = {643--653},
  doi = {10.18653/v1/d15-1076},
  abstract = {This paper introduces the task of questionanswer driven semantic role labeling (QA-SRL), where question-answer pairs are used to represent predicate-argument structure. For example, the verb "introduce" in the previous sentence would be labeled with the questions "What is introduced?", and "What introduces something?", each paired with the phrase from the sentence that gives the correct answer. Posing the problem this way allows the questions themselves to define the set of possible roles, without the need for predefined frame or thematic role ontologies. It also allows for scalable data collection by annotators with very little training and no linguistic expertise. We gather data in two domains, newswire text and Wikipedia articles, and introduce simple classifierbased models for predicting which questions to ask and what their answers should be. Our results show that non-expert annotators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task.},
  annotation = {113 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4JCG8S65/He, Lewis, Zettlemoyer - 2015 - Question-answer driven semantic role labeling Using natural language to annotate natural language(2).pdf},
  isbn = {978-1-941643-32-7},
  keywords = {u}
}

@article{he2015SpatialPyramidPooling,
  title = {Spatial {{Pyramid Pooling}} in {{Deep Convolutional Networks}} for {{Visual Recognition}}},
  author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
  date = {2015},
  journaltitle = {PAMI},
  volume = {37},
  pages = {1904--1916},
  issn = {01628828},
  doi = {10.1109/TPAMI.2015.2389824},
  abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224x224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170x faster than the recent leading method R-CNN (and 24-64x faster overall), while achieving better or comparable accuracy on Pascal VOC 2007.},
  annotation = {787 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {26353135},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6HLA27KL/He et al. - 2015 - Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(2).pdf},
  isbn = {9783319105772},
  keywords = {u},
  number = {9}
}

@inproceedings{he2017DeepResidualLearning,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {{{CVPR}}},
  author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
  date = {2017},
  pages = {770--778},
  issn = {1664-1078},
  doi = {10.1109/CVPR.2016.90},
  url = {http://arxiv.org/abs/1703.10722},
  abstract = {We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 23.36.},
  annotation = {9995 citations (Semantic Scholar/DOI) [2021-03-26] 75 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {23554596},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W3F89Y6H/He et al. - 2017 - Deep Residual Learning for Image Recognition(2).pdf},
  isbn = {978-1-4673-8851-1}
}

@inproceedings{he2017DeepSemanticRole,
  title = {Deep {{Semantic Role Labeling}}: {{What Works}} and {{What}}'s {{Next}}},
  booktitle = {{{ACL}}},
  author = {He, L. and Lee, K. and Lewis, M. and Zettlemoyer, L. S.},
  date = {2017},
  issn = {1881803X},
  abstract = {In this paper, we have proposed a non intrusive fatigue detection systembased on eye tracking of drivers. Initially, the face is located through skincolor information to ensure the presence of driver in video frame. This isfollowed by the detection of pupils on the basis of radii, inter pupil distanceand angle between the pupils. The pupils are constantly being tracked throughKalrnan filter to predict their future position in successive frames. Ondetection of low vigilance, monitored through eye state, the driver is informedabout his/her poor state of driving through an alarm. The system has been testedusing real data, with different sequences recorded in day and night drivingconditions, and with users belonging to different race and gender. ICIC International ?? 2010.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4G5WMRS2/He et al. - 2017 - Deep Semantic Role Labeling What Works and What's Next(2).pdf}
}

@inproceedings{he2017DeepSemanticRolea,
  title = {Deep {{Semantic Role Labeling}}: {{What Works}} and {{What}}’s {{Next}}},
  booktitle = {{{ACL}}},
  author = {He, L. and Lee, K. and Lewis, M. and Zettlemoyer, L. S.},
  date = {2017},
  pages = {473--483},
  doi = {10.18653/v1/P17-1044},
  url = {http://aclweb.org/anthology/P17-1044},
  abstract = {We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10\% relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.},
  annotation = {301 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3HZKZK3M/He et al. - 2017 - Deep Semantic Role Labeling What Works and What’s Next(2).pdf},
  isbn = {978-1-945626-75-3}
}

@inproceedings{he2017LearningSymmetricCollaborative,
  title = {Learning {{Symmetric Collaborative Dialogue Agents}} with {{Dynamic Knowledge Graph Embeddings}}},
  booktitle = {{{ACL}}},
  author = {He, H. and Balakrishnan, A. and Eric, M. and Liang, P.},
  date = {2017},
  doi = {10.18653/v1/P17-1162},
  url = {http://arxiv.org/abs/1704.07130},
  abstract = {We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.},
  annotation = {88 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.07130},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9K8ANVVW/He et al. - 2017 - Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings(2).pdf},
  isbn = {978-1-945626-75-3},
  keywords = {u}
}

@inproceedings{he2018JointlyPredictingPredicates,
  title = {Jointly {{Predicting Predicates}} and {{Arguments}} in {{Neural Semantic Role Labeling}}},
  booktitle = {{{ACL}}},
  author = {He, L. and Lee, K. and Levy, O. and Zettlemoyer, L. S.},
  date = {2018},
  pages = {1--6},
  abstract = {Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the relations between them. The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates.},
  archiveprefix = {arXiv},
  eprint = {1805.04787},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LCRBNMXE/He et al. - 2018 - Jointly Predicting Predicates and Arguments in Neu.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QXF692KJ/He et al. - 2018 - Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling(2).pdf}
}

@inproceedings{he2018SyntaxSemanticRole,
  title = {Syntax for {{Semantic Role Labeling}}, {{To Be}}, {{Or Not To Be}}},
  booktitle = {{{ACL}}},
  author = {He, Shexia and Li, Zuchao and Zhao, Hai and Bai, Hongxiao},
  date = {2018},
  pages = {11},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CPN8R35I/He et al. - Syntax for Semantic Role Labeling, To Be, Or Not T.pdf},
  langid = {english}
}

@inproceedings{he2019PunGenerationSurprise,
  title = {Pun {{Generation}} with {{Surprise}}},
  booktitle = {{{NAACL}}},
  author = {He, H. and Peng, N. and Liang, P.},
  date = {2019},
  pages = {1734--1744},
  doi = {10.18653/v1/n19-1172},
  abstract = {We tackle the problem of generating a pun sentence given a pair of homophones (e.g., "died" and "dyed"). Supervised text generation is inappropriate due to the lack of a large corpus of puns, and even if such a corpus existed, mimicry is at odds with generating novel content. In this paper, we propose an unsupervised approach to pun generation using a corpus of unhumorous text and what we call the local-global surprisal principle: we posit that in a pun sentence, there is a strong association between the pun word (e.g., "dyed") and the distant context, as well as a strong association between the alternative word (e.g., "died") and the immediate context. This contrast creates surprise and thus humor. We instantiate this principle for pun generation in two ways: (i) as a measure based on the ratio of probabilities under a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Human evaluation shows that our retrieve-and-edit approach generates puns successfully 31\% of the time, tripling the success rate of a neural generation baseline.},
  annotation = {20 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.06828v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BPL3LA94/He, Peng, Liang - 2019 - Pun Generation with Surprise(2).pdf}
}

@inproceedings{heafield2013ScalableModifiedKneserNey,
  title = {Scalable {{Modified Kneser}}-{{Ney Language Model Estimation}}},
  booktitle = {{{ACL}}},
  author = {Heafield, K. and Pouzyrevsky, I. and Clark, J. H. and Koehn, P.},
  date = {2013},
  pages = {690--696},
  abstract = {We present an efficient algorithm to es-timate large modified Kneser-Ney mod-els including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7\% of the RAM and 14.0\% of the wall time taken by SRILM. The code is open source as part of KenLM.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B7JMPXEV/Heafield et al. - 2013 - Scalable Modified Kneser-Ney Language Model Estimation(2).pdf},
  isbn = {978-1-937284-51-0},
  keywords = {u}
}

@inproceedings{hearst1992AutomaticAcquisitionHyponyms,
  title = {Automatic {{Acquisition}} of {{Hyponyms}} from {{Large Text Corpora}}},
  booktitle = {{{COLING}}},
  author = {Hearst, M. A.},
  date = {1992},
  abstract = {We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoid- ance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily rec- ognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discov- ering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the struc- ture of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ML9UMBIE/Hearst - 1992 - Automatic Acquisition of Hyponyms from Large Text Corpora(2).pdf}
}

@inproceedings{heinzerling2017TrustVerifyBetter,
  title = {Trust, but {{Verify}}! {{Better Entity Linking}} through {{Automatic Verification}}},
  booktitle = {{{EACL}}},
  author = {Heinzerling, B. and Strube, M. and Lin, C.},
  date = {2017},
  volume = {1},
  pages = {828--838},
  abstract = {We introduce automatic verification as a post-processing step for entity linking (EL). The proposed method trusts EL sys-tem results collectively, by assuming en-tity mentions are mostly linked correctly, in order to create a semantic profile of the given text using geospatial and temporal information, as well as fine-grained entity types. This profile is then used to auto-matically verify each linked mention indi-vidually, i.e., to predict whether it has been linked correctly or not. Verification allows leveraging a rich set of global and pairwise features that would be prohibitively expen-sive for EL systems employing global in-ference. Evaluation shows consistent im-provements across datasets and systems. In particular, when applied to state-of-the-art systems, our method yields an abso-lute improvement in linking performance of up to 1.7 F 1 on AIDA/CoNLL'03 and up to 2.4 F 1 on the English TAC KBP 2015 TEDL dataset.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H927UJLC/Heinzerling, Strube, Lin - 2017 - Trust, but Verify! Better Entity Linking through Automatic Verification(2).pdf},
  isbn = {978-1-5108-3860-4}
}

@inproceedings{henaff2015DeepConvolutionalNetworks,
  title = {Deep {{Convolutional Networks}} on {{Graph}}-{{Structured Data}}},
  booktitle = {{{NeurIPS}}},
  author = {Henaff, M. and Bruna, J. and LeCun, Y.},
  date = {2015},
  pages = {1--10},
  url = {http://arxiv.org/abs/1506.05163},
  abstract = {Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.},
  annotation = {894 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1506.05163},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TYDFRWAU/Henaff, Bruna, LeCun - 2015 - Deep Convolutional Networks on Graph-Structured Data(2).pdf},
  keywords = {u}
}

@inproceedings{hermann2014SemanticFrameIdentification,
  title = {Semantic Frame Identification with Distributed Word Representations},
  booktitle = {{{ACL}}},
  author = {Hermann, K. M. and Das, D. and Weston, J. and Ganchev, K.},
  date = {2014},
  pages = {1448--1458},
  doi = {10.3115/v1/p14-1136},
  abstract = {We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-ofthe- art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. © 2014 Association for Computational Linguistics.},
  annotation = {104 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4CLJUFD4/Hermann et al. - 2014 - Semantic frame identification with distributed word representations(2).pdf},
  isbn = {9781937284725}
}

@inproceedings{hershcovich2017TransitionBasedDirectedAcyclic,
  title = {A {{Transition}}-{{Based Directed Acyclic Graph Parser}} for {{UCCA}}},
  booktitle = {{{ACL}}},
  author = {Hershcovich, D. and Abend, O. and Rappoport, A.},
  date = {2017},
  pages = {1127--1138},
  doi = {10.18653/v1/P17-1104},
  url = {http://arxiv.org/abs/1704.00552},
  abstract = {We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.},
  annotation = {72 citations (Semantic Scholar/DOI) [2021-03-26] 72 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.00552},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E7ZNZYFV/Hershcovich, Abend, Rappoport - 2017 - A Transition-Based Directed Acyclic Graph Parser for UCCA(2).pdf},
  isbn = {978-1-945626-75-3},
  keywords = {u}
}

@inproceedings{herzig2019DonParaphraseDetect,
  title = {Don’t Paraphrase, Detect! {{Rapid}} and {{Effective Data Collection}} for {{Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Herzig, J. and Berant, J.},
  date = {2019},
  pages = {3808--3818},
  doi = {10.18653/v1/d19-1394},
  abstract = {A major hurdle on the road to conversational interfaces is the difficulty in collecting data that maps language utterances to logical forms. One prominent approach for data collection has been to automatically generate pseudo-language paired with logical forms, and paraphrase the pseudo-language to natural language through crowdsourcing (Wang et al., 2015). However, this data collection procedure often leads to low performance on real data, due to a mismatch between the true distribution of examples and the distribution induced by the data collection procedure. In this paper, we thoroughly analyze two sources of mismatch in this process: the mismatch in logical form distribution and the mismatch in language distribution between the true and induced distributions. We quantify the effects of these mismatches, and propose a new data collection approach that mitigates them. Assuming access to unlabeled utterances from the true distribution, we combine crowdsourcing with a paraphrase model to detect correct logical forms for the unlabeled utterances. On two datasets, our method leads to 70.6 accuracy on average on the true distribution, compared to 51.3 in paraphrasing-based data collection.},
  annotation = {11 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1908.09940},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J5MTT44J/Herzig, Berant - 2019 - Don’t paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing(2).pdf},
  keywords = {u},
  number = {2015}
}

@inproceedings{herzig2019SpanbasedSemanticParsing,
  title = {Span-Based {{Semantic Parsing}} for {{Compositional Generalization}}},
  booktitle = {{{EMNLP}}},
  author = {Herzig, Jonathan and Berant, Jonathan},
  date = {2019},
  url = {http://arxiv.org/abs/2009.06040},
  urldate = {2020-09-27},
  abstract = {Despite the success of sequence-to-sequence (seq2seq) models in semantic parsing, recent work has shown that they fail in compositional generalization, i.e., the ability to generalize to new structures built of components observed during training. In this work, we posit that a span-based parser should lead to better compositional generalization. we propose SpanBasedSP, a parser that predicts a span tree over an input utterance, explicitly encoding how partial programs compose over spans in the input. SpanBasedSP extends Pasupat et al. (2019) to be comparable to seq2seq models by (i) training from programs, without access to gold trees, treating trees as latent variables, (ii) parsing a class of non-projective trees through an extension to standard CKY. On GeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong seq2seq baselines on random splits, but dramatically improves performance compared to baselines on splits that require compositional generalization: from \$69.8 \textbackslash rightarrow 95.3\$ average accuracy.},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2009.06040},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6TATT6EB/Herzig and Berant - 2020 - Span-based Semantic Parsing for Compositional Gene.pdf;/home/hiaoxui/.local/share/zotero_files/storage/GENHN3NL/2009.html},
  keywords = {u}
}

@inproceedings{hewitt2018LearningTranslationsImages,
  title = {Learning Translations via Images with a Massively Multilingual Image Dataset},
  booktitle = {{{ACL}}},
  author = {Hewitt, J. and Ippolito, D. and Callahan, B. and Kriz, R. and Wijaya, D. and Callison-Burch, C.},
  date = {2018},
  pages = {2566--2576},
  doi = {10.18653/v1/p18-1239},
  abstract = {We conduct the most comprehensive study to date into translating words via images. To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents. Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings. In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages. We run experiments on a dozen high resource languages and 20 low resources languages, demonstrating the effect of word concreteness and part-of-speech on translation quality. To improve image-based translation, we introduce a novel method of predicting word concreteness from images, which improves on a previous state-of-the-art unsupervised technique. This allows us to predict when image-based translation may be effective, enabling consistent improvements to a state-of-the-art text-based word translation system. Our code and the Massively Multilingual Image Dataset (MMID) are available at http://multilingual-images.org/.},
  annotation = {17 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZYKV2727/Hewitt et al. - 2018 - Learning translations via images with a massively multilingual image dataset(2).pdf},
  isbn = {978-1-948087-32-2},
  keywords = {u}
}

@inproceedings{hewitt2019DesigningInterpretingProbes,
  title = {Designing and {{Interpreting Probes}} with {{Control Tasks}}},
  booktitle = {{{EMNLP}}},
  author = {Hewitt, J. and Liang, P.},
  date = {2019},
  url = {http://arxiv.org/abs/1909.03368},
  abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe's capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.},
  annotation = {91 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.03368},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9J9IR8DI/Hewitt, Liang - 2019 - Designing and Interpreting Probes with Control Tasks(2).pdf;/home/hiaoxui/.local/share/zotero_files/storage/GFETH4QL/Hewitt and Liang - 2019 - Designing and Interpreting Probes with Control Tas.pdf},
  keywords = {u}
}

@inproceedings{hinton1983OptimalPerceptualInference,
  title = {Optimal {{Perceptual Inference}}},
  booktitle = {{{CVPR}}},
  author = {Hinton, G. E. and Sejnowski, T. J.},
  date = {1983},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/799X5B2A/Hinton, Sejnowski - 1983 - Optimal Perceptual Inference(2).pdf}
}

@inproceedings{hinton1994AutoencodersMinimumDescription,
  title = {Autoencoders, {{Minimum Description Length}} and {{Helmholtz}} Free {{Energy}}},
  booktitle = {{{NeurIPS}}},
  author = {Hinton, G. E. and Zemel, R. S.},
  date = {1994},
  issn = {15205207},
  doi = {10.1021/jp906511z},
  abstract = {An autoencoder network uses a aset of recognition weights to convert the input veecotre into a code vectore. It then uses set of generative weights to convert the code vector inot an approximate reconstruction of the input vector. We derive and objective function for training autoencoderss based on the minimum descrption length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconsrtuction error. WE show that this information is minimzed by choosing code vectors stochastiacally according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approixmation gives an upper bound on the description length. Even hen this bound is poor, it can be used a Lyapuov function for learning both the generative and recognition weights. We demonstrate that this approach can be used to learn factorial codes.},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {20148535},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M3FZ6Q5S/Hinton, Zemel - 1994 - Autoencoders, Minimum Description Length and Helmholtz free Energy(2).pdf},
  isbn = {1049-5258}
}

@article{hinton2006FastLearningAlgorithm,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, G. E. and Osindero, S. and Teh, Y.},
  date = {2006},
  journaltitle = {Neural Computation},
  volume = {18},
  pages = {1527--1554},
  issn = {0899-7667},
  doi = {10.1162/neco.2006.18.7.1527},
  abstract = {We show how to use complementary priors to eliminate the explaining- away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa- tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive ver- sion of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribu- tion of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning al- gorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  annotation = {9995 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {16764513},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TPGCRIE5/Hinton, Osindero, Teh - 2006 - A Fast Learning Algorithm for Deep Belief Nets(2).pdf},
  isbn = {0899-7667}
}

@article{hinton2006ReducingDimensionalityData,
  title = {Reducing the Dimensionality of Data with Neural Networks},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  date = {2006},
  journaltitle = {Science},
  volume = {313},
  pages = {504--507},
  issn = {00368075},
  doi = {10.1126/science.1127647},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  annotation = {9990 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KMPTDSUW/Hinton, Salakhutdinov - 2006 - Reducing the dimensionality of data with neural networks(2).pdf},
  number = {5786}
}

@inproceedings{hinton2015DistillingKnowledgeNeural,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  booktitle = {{{NeurIPS}}},
  author = {Hinton, G. E. and Vinyals, O. and Dean, J.},
  date = {2015},
  pages = {1--9},
  issn = {0022-2488},
  doi = {10.1063/1.4931082},
  url = {http://arxiv.org/abs/1503.02531},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-03-26] 5306 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {18249735},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8QYUXCVG/Hinton, Vinyals, Dean - 2015 - Distilling the Knowledge in a Neural Network(2).pdf},
  isbn = {3-531-20785-7}
}

@report{hinton2021HowRepresentPartwhole,
  title = {How to Represent Part-Whole Hierarchies in a Neural Network},
  author = {Hinton, Geoffrey},
  date = {2021-02-24},
  url = {http://arxiv.org/abs/2102.12627},
  urldate = {2021-02-27},
  abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2102.12627},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LT5RPTPN/Hinton - 2021 - How to represent part-whole hierarchies in a neura.pdf;/home/hiaoxui/.local/share/zotero_files/storage/967TCEN7/2102.html}
}

@article{hochreiter1997LongShortTermMemory,
  title = {Long {{Short}}-{{Term Memory}}},
  author = {Hochreiter, S. and Schmidhuber, J.},
  date = {1997},
  journaltitle = {Neural Computation},
  volume = {1780},
  pages = {1735--1780},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WP4VGP83/Hochreiter, Schmidhuber - 1997 - Long Short-Term Memory(2).pdf},
  keywords = {u}
}

@inproceedings{hong2016BuildingCrossdocumentEventEvent,
  title = {Building a {{Cross}}-Document {{Event}}-{{Event Relation Corpus}}},
  booktitle = {Linguistic {{Annotation Workshop}}},
  author = {Hong, Y. and Zhang, T. and O'Gorman, T. and Horowit-Hendler, S. and Ji, H. and Palmer, M.},
  date = {2016},
  pages = {1--6},
  doi = {10.18653/v1/w16-1701},
  abstract = {We propose a new task of extracting event-event relations across documents. We present our efforts at designing an anno-tation schema and building a corpus for this task. Our schema includes five main types of relations: Inheritance, Expan-sion, Contingency, Comparison and Tem-porality, along with 21 subtypes. We also lay out the main challenges based on de-tailed inter-annotator disagreement and er-ror analysis. We hope these resources can serve as a benchmark to encourage re-search on this new problem.},
  annotation = {14 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J9CIBF3S/Hong et al. - 2016 - Building a Cross-document Event-Event Relation Corpus(2).pdf},
  keywords = {u}
}

@inproceedings{hope2018AcceleratingInnovationAnalogy,
  title = {Accelerating Innovation through Analogy Mining},
  booktitle = {{{KDD}}},
  author = {Hope, T. and Chan, J. and Kittur, A. and Shahaf, D.},
  date = {2018},
  volume = {2018-July},
  pages = {5274--5278},
  issn = {10450823},
  doi = {10.1145/3097983.3098038},
  abstract = {The availability of large idea repositories (e.g., patents) could significantly accelerate innovation and discovery by providing people inspiration from solutions to analogous problems. However, finding useful analogies in these large, messy, real-world repositories remains a persistent challenge for both humans and computers. Previous approaches include costly hand-created databases that do not scale, or machine-learning similarity metrics that struggle to account for structural similarity, which is central to analogy. In this paper we explore the viability and value of learning simple structural representations. Our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions. We demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional methods. In an ideation experiment, analogies retrieved by our models significantly increased people's likelihood of generating creative ideas.},
  annotation = {17 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1706.05585v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EI4DQKL2/Hope et al. - 2018 - Accelerating innovation through analogy mining(2).pdf},
  isbn = {978-0-9992411-2-7},
  keywords = {u}
}

@inproceedings{hovy2006Ontonotes90Solution,
  title = {Ontonotes: {{The}} 90\% Solution},
  booktitle = {{{NAACL}}},
  author = {Hovy, E. and Marcus, M. and Palmer, M. and Ramshaw, L. and Weischedel, R.},
  date = {2006},
  pages = {4},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4SSGUZXL/Palmer et al. - Proceedings of the....pdf},
  langid = {english}
}

@report{hrinchuk2016TensorizedEmbeddingLayers,
  title = {Tensorized {{Embedding Layers}}},
  author = {Hrinchuk, O. and Khrulkov, V. and Mirvakhabova, L. and Orlova, E. and Oseledets, I.},
  date = {2016},
  archiveprefix = {arXiv},
  eprint = {1901.10787v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BVNRLLSM/Hrinchuk et al. - 2016 - Tensorized Embedding Layers(2).pdf},
  keywords = {u}
}

@inproceedings{hu2016DeepNeuralNetworks,
  title = {Deep {{Neural Networks}} with {{Massive Learned Knowledge}}},
  booktitle = {{{EMNLP}}},
  author = {Hu, Z. and Yang, Z. and Salakhutdinov, R. and Xing, E. P.},
  date = {2016},
  issn = {00192120},
  abstract = {Regulating deep neural networks (DNNs) with human structured knowledge has shown to be of great benefit for improved accuracy and in-terpretability. We develop a general framework that enables learning knowledge and its confidence jointly with the DNNs, so that the vast amount of fuzzy knowledge can be incorporated and automatically optimized with little manual efforts. We apply the framework to sentence sentiment analysis, augmenting a DNN with massive linguistic constraints on discourse and polarity structures. Our model substantially enhances the performance using less training data, and shows improved inter-pretability. The principled framework can also be applied to posterior regularization for regulating other statistical models.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NIAUJ3SD/Hu et al. - 2016 - Deep Neural Networks with Massive Learned Knowledge(2).pdf},
  keywords = {u}
}

@inproceedings{hu2016HarnessingDeepNeural,
  title = {Harnessing {{Deep Neural Networks}} with {{Logic Rules}}},
  booktitle = {{{ACL}}},
  author = {Hu, Z. and Ma, X. and Liu, Z. and Hovy, E. and Xing, E. P.},
  date = {2016},
  pages = {2410--2420},
  issn = {1541-1672},
  doi = {10.18653/v1/P16-1228},
  url = {http://arxiv.org/abs/1603.06318},
  abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
  annotation = {350 citations (Semantic Scholar/DOI) [2021-03-26] 350 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {18925972},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X9Z7BSPH/Hu et al. - 2016 - Harnessing Deep Neural Networks with Logic Rules(2).pdf},
  isbn = {978-1-5108-2758-5}
}

@inproceedings{hu2017ControlledGenerationText,
  title = {Toward {{Controlled Generation}} of {{Text}}},
  booktitle = {{{ICML}}},
  author = {Hu, Z. and Yang, Z. and Liang, X. and Salakhutdinov, R. and Xing, E. P.},
  date = {2017},
  doi = {arXiv:1},
  url = {http://arxiv.org/abs/1703.00955},
  abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
  annotation = {534 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1703.00955},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L73ACW9M/Hu et al. - 2017 - Toward Controlled Generation of Text(2).pdf},
  keywords = {u}
}

@inproceedings{hu2018ReadVerifyMachine,
  title = {Read + {{Verify}}: {{Machine Reading Comprehension}} with {{Unanswerable Questions}}},
  booktitle = {{{AAAI}}},
  author = {Hu, M. and Wei, F. and Peng, Y. and Huang, Z. and Yang, N. and Li, D.},
  date = {2018},
  issn = {0006-3592},
  doi = {10.1002/bit.260440802},
  url = {http://arxiv.org/abs/1810.06638},
  abstract = {Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. A key subtask is to reliably predict whether the question is unanswerable. In this paper, we propose a unified model, called U-Net, with three important components: answer pointer, no-answer pointer, and answer verifier. We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. The universal node encodes the fused information from both the question and passage, and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U-Net. Different from the state-of-art pipeline models, U-Net can be learned in an end-to-end fashion. The experimental results on the SQuAD 2.0 dataset show that U-Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0.},
  annotation = {57 citations (Semantic Scholar/DOI) [2021-03-26] 31 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {18618904},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HALJPUFT/Hu et al. - 2018 - Read Verify Machine Reading Comprehension with Unanswerable Questions(2).pdf},
  number = {1}
}

@inproceedings{hu2018SqueezeandExcitationNetworks,
  title = {Squeeze-and-{{Excitation Networks}}},
  booktitle = {{{CVPR}}},
  author = {Hu, J. and Shen, L. and Sun, G.},
  date = {2018},
  pages = {7132--7141},
  issn = {10636919},
  doi = {10.1109/CVPR.2018.00745},
  url = {http://arxiv.org/abs/1709.01507},
  abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the 'Squeeze-and-Excitation' (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251\%, achieving a \textasciitilde 25\% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.},
  annotation = {1536 citations (Semantic Scholar/DOI) [2021-03-26] 1536 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1709.01507},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7P465Z56/Hu, Shen, Sun - 2018 - Squeeze-and-Excitation Networks(2).pdf},
  isbn = {978-1-5386-6420-9}
}

@inproceedings{hu2019ActiveLearningPartial,
  title = {Active Learning with Partial Feedback},
  booktitle = {{{ICLR}}},
  author = {Hu, P. and Lipton, Z. C. and Anandkumar, A. and Ramanan, D.},
  date = {2019},
  abstract = {While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback). To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available. To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask. At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class. Each answer eliminates some classes, leaving the learner with a partial label. The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled. Active learning with partial labels requires (i) a sampling strategy to choose (example, class) pairs, and (ii) learning from partial labels between rounds. Experiments on Tiny ImageNet demonstrate that our most effective method improves 26\% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30\% of the annotation budget that would be required (naively) to annotate the dataset. Moreover, ALPF-learners fully annotate TinyImageNet at 42\% lower cost. Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples.},
  archiveprefix = {arXiv},
  eprint = {1802.07427},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GLSE9V8W/Hu et al. - 2019 - Active learning with partial feedback(2).pdf},
  keywords = {u}
}

@inproceedings{hu2019ParaBankMonolingualBitext,
  title = {{{ParaBank}}: {{Monolingual Bitext Generation}} and {{Sentential Paraphrasing}} via {{Lexically}}-Constrained {{Neural Machine Translation}}},
  booktitle = {{{AAAI}}},
  author = {Hu, J. E. and Rudinger, R. and Post, M. and Van Durme, B.},
  date = {2019},
  url = {http://arxiv.org/abs/1901.03644},
  abstract = {We present ParaBank, a large-scale English paraphrase dataset that surpasses prior work in both quantity and quality. Following the approach of ParaNMT, we train a Czech-English neural machine translation (NMT) system to generate novel paraphrases of English reference sentences. By adding lexical constraints to the NMT decoding procedure, however, we are able to produce multiple high-quality sentential paraphrases per source sentence, yielding an English paraphrase resource with more than 4 billion generated tokens and exhibiting greater lexical diversity. Using human judgments, we also demonstrate that ParaBank's paraphrases improve over ParaNMT on both semantic similarity and fluency. Finally, we use ParaBank to train a monolingual NMT model with the same support for lexically-constrained decoding for sentence rewriting tasks.},
  annotation = {23 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1901.03644},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RVCYSJ8I/Hu et al. - 2019 - ParaBank Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-constrained Neural Machine Transl(2).pdf}
}

@report{huang1989SemiContinuousHiddenMarkov,
  title = {Semi-{{Continuous Hidden Markov Models}}},
  author = {Huang, X.},
  date = {1989},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HM35FBR3/Huang - 1989 - Semi-Continuous Hidden Markov Models(2).pdf}
}

@report{huang2015BidirectionalLSTMCRFModels,
  title = {Bidirectional {{LSTM}}-{{CRF Models}} for {{Sequence Tagging}}},
  author = {Huang, Z. and Xu, W. and Yu, K.},
  date = {2015},
  issn = {1098-6596},
  doi = {10.18653/v1/P16-1101},
  url = {http://arxiv.org/abs/1508.01991},
  abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
  annotation = {1727 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/97EQ5U9Z/Huang, Xu, Yu - 2015 - Bidirectional LSTM-CRF Models for Sequence Tagging(2).pdf},
  isbn = {9781510827585}
}

@inproceedings{huang2017LearningFineGrainedExpressions,
  title = {Learning {{Fine}}-{{Grained Expressions}} to {{Solve Math Word Problems}}},
  booktitle = {{{EMNLP}}},
  author = {Huang, D. and Shi, S. and Yin, J. and Lin, C.},
  date = {2017},
  pages = {816--825},
  url = {http://www.aclweb.org/anthology/D17-1084},
  abstract = {This paper presents a novel template-based method to solve math word prob-lems. This method learns the mappings between math concept phrases in math word problems and their math expressions from training data. For each equation tem-plate, we automatically construct a rich template sketch by aggregating informa-tion from various problems with the same template. Our approach is implemented in a two-stage system. It first retrieves a few relevant equation system templates and aligns numbers in math word problems to those templates for candidate equation generation. It then does a fine-grained in-ference to obtain the final answer. Ex-periment results show that our method achieves an accuracy of 28.4\% on the lin-ear Dolphin18K benchmark, which is 10\% (54\% relative) higher than previous state-of-the-art systems while achieving an ac-curacy increase of 12\% (59\% relative) on the TS6 benchmark subset.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SFQCIDNB/Huang et al. - 2017 - Learning Fine-Grained Expressions to Solve Math Word Problems(2).pdf}
}

@inproceedings{huang2019COSMOSQAMachine,
  title = {{{COSMOS QA}}: {{Machine Reading Comprehension}} with {{Contextual Commonsense Reasoning}}},
  booktitle = {{{EMNLP}}},
  author = {Huang, L. and Bras, R. L. and Bhagavatula, C. and Choi, Y.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZFV7DZDF/Huang et al. - 2019 - COSMOS QA Machine Reading Comprehension with Contextual Commonsense Reasoning(2).pdf},
  keywords = {u}
}

@inproceedings{humeau2020PolyencodersTransformerArchitectures,
  title = {Poly-Encoders: {{Transformer Architectures}} and {{Pre}}-Training {{Strategies}} for {{Fast}} and {{Accurate Multi}}-Sentence {{Scoring}}},
  shorttitle = {Poly-Encoders},
  booktitle = {{{ICLR}}},
  author = {Humeau, Samuel and Shuster, Kurt and Lachaux, Marie-Anne and Weston, Jason},
  date = {2020-03-25},
  url = {http://arxiv.org/abs/1905.01969},
  urldate = {2021-03-26},
  abstract = {The use of deep pre-trained bidirectional transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on three existing tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.},
  annotation = {40 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1905.01969},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4CJHQSDX/Humeau et al. - 2020 - Poly-encoders Transformer Architectures and Pre-t.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JAXE5AHI/1905.html}
}

@report{inan2019ImprovingSemanticParsing,
  title = {Improving {{Semantic Parsing}} with {{Neural Generator}}-{{Reranker Architecture}}},
  author = {Inan, H. A. and Tomar, G. S. and Pan, H.},
  date = {2019},
  url = {http://arxiv.org/abs/1909.12764},
  abstract = {Semantic parsing is the problem of deriving machine interpretable meaning representations from natural language utterances. Neural models with encoder-decoder architectures have recently achieved substantial improvements over traditional methods. Although neural semantic parsers appear to have relatively high recall using large beam sizes, there is room for improvement with respect to one-best precision. In this work, we propose a generator-reranker architecture for semantic parsing. The generator produces a list of potential candidates and the reranker, which consists of a pre-processing step for the candidates followed by a novel critic network, reranks these candidates based on the similarity between each candidate and the input sentence. We show the advantages of this approach along with how it improves the parsing performance through extensive analysis. We experiment our model on three semantic parsing datasets (GEO, ATIS, and OVERNIGHT). The overall architecture achieves the state-of-the-art results in all three datasets.},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.12764},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/376EZ5LA/Inan, Tomar, Pan - 2019 - Improving Semantic Parsing with Neural Generator-Reranker Architecture(2).pdf},
  keywords = {review}
}

@article{ioffe2015BatchNormalizationAccelerating,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  author = {Ioffe, S. and Szegedy, C.},
  date = {2015},
  journaltitle = {JMLR},
  issn = {0717-6163},
  doi = {10.1007/s13398-014-0173-7.2},
  url = {http://arxiv.org/abs/1502.03167},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  annotation = {43 citations (Semantic Scholar/DOI) [2021-03-26] 9993 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {15003161},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IGACLL98/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift(2).pdf},
  isbn = {9780874216561},
  keywords = {u}
}

@inproceedings{irsoy2014OpinionMiningDeep,
  title = {Opinion {{Mining}} with {{Deep Recurrent Neural Networks}}},
  booktitle = {{{EMNLP}}},
  author = {Irsoy, O. and Cardie, C.},
  date = {2014},
  doi = {10.3115/v1/d14-1080},
  abstract = {Recurrent neural networks (RNNs) are con-nectionist models of sequential data that are naturally applicable to the analysis of natural language. Recently, " depth in space " — as an orthogonal notion to " depth in time " — in RNNs has been investigated by stacking mul-tiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architec-ture. In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task. Experimental results show that deep, narrow RNNs outperform traditional shallow, wide RNNs with the same number of parame-ters. Furthermore, our approach outperforms previous CRF-based baselines, including the state-of-the-art semi-Markov CRF model, and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF, as well as without the standard layer-by-layer pre-training typically required of RNN architectures.},
  annotation = {301 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XWPDJIJ5/Irsoy, Cardie - 2014 - Opinion Mining with Deep Recurrent Neural Networks(2).pdf}
}

@inproceedings{iyer2019LearningProgrammaticIdioms,
  title = {Learning {{Programmatic Idioms}} for {{Scalable Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Iyer, S. and Cheung, A. and Zettlemoyer, L. S.},
  date = {2019},
  pages = {5425--5434},
  doi = {10.18653/v1/d19-1545},
  abstract = {Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art (SOTA) semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and train semantic parsers to apply these idioms during decoding. Applying idiom-based decoding on a recent context-dependent semantic parsing task improves the SOTA by 2.2\textbackslash\% BLEU score while reducing training time by more than 50\textbackslash\%. This improved speed enables us to scale up the model by training on an extended training set that is 5\$\textbackslash times\$ larger, to further move up the SOTA by an additional 2.3\textbackslash\% BLEU and 0.9\textbackslash\% exact match. Finally, idioms also significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.},
  annotation = {12 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.09086},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YBWF358G/Iyer, Cheung, Zettlemoyer - 2019 - Learning Programmatic Idioms for Scalable Semantic Parsing(2).pdf},
  keywords = {u}
}

@inproceedings{iyyer2014NeuralNetworkFactoid,
  title = {A {{Neural Network}} for {{Factoid Question Answering}} over {{Paragraphs}}},
  booktitle = {{{EMNLP}}},
  author = {Iyyer, M. and Boyd-Graber, J. and Claudino, L. and Socher, R. and Daumé III, H.},
  date = {2014},
  doi = {10.3115/v1/d14-1070},
  abstract = {Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations. These methods are ineffective when question text contains very few individual words (e.g., named entities) that are indicative of the answer. We introduce a recursive neural network (rnn) model that can reason over such input by modeling textual compositionality. We apply our model, qanta, to a dataset of questions from a trivia competition called quiz bowl. Unlike previous rnn models, qanta learns word and phrase-level representations that combine across sentences to reason about entities. The model outperforms multiple baselines and, when combined with information retrieval methods, rivals the best human players.},
  annotation = {280 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U4KHU9SF/Iyyer et al. - 2014 - A Neural Network for Factoid Question Answering over Paragraphs(2).pdf}
}

@inproceedings{jain2019ScalableRuleLearning,
  title = {Scalable {{Rule Learning}} in {{Probabilistic Knowledge Bases}}},
  booktitle = {Automated {{Knowledge Base Construction}}},
  author = {Jain, A. and Friedman, T. and Kuzelka, O. and Van den Broeck, G. and Raedt, L. D.},
  date = {2019},
  pages = {1--17},
  abstract = {Knowledge Bases (KBs) are becoming increasingly large, sparse and probabilistic. These KBs are typically used to perform query inferences and rule mining. But their efficacy is only as high as their completeness. Efficiently utilizing incomplete KBs remains a major challenge as the current KB completion techniques either do not take into account the inherent uncertainty associated with each KB tuple or do not scale to large KBs. Probabilistic rule learning not only considers the probability of every KB tuple but also tackles the problem of KB completion in an explainable way. For any given probabilistic KB, it learns probabilistic first-order rules from its relations to identify interesting patterns. But, the current probabilistic rule learning techniques perform grounding to do probabilistic inference for evaluation of candidate rules. It does not scale well to large KBs as the time complexity of inference using grounding is exponential over the size of the KB. In this paper, we present SafeLearner-a scalable solution to probabilistic KB completion that performs probabilistic rule learning using lifted probabilistic inference-as faster approach instead of grounding. We compared SafeLearner to the state-of-the-art probabilistic rule learner ProbFOIL + and to its deterministic contemporary AMIE+ on standard probabilistic KBs of NELL (Never-Ending Language Learner) and Yago. Our results demonstrate that SafeLearner scales as good as AMIE+ when learning simple rules and is also significantly faster than ProbFOIL + .},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VE4446PQ/Jain et al. - 2019 - Scalable Rule Learning in Probabilistic Knowledge Bases(2).pdf},
  keywords = {u}
}

@inproceedings{jang2017CategoricalReparameterizationGumbelSoftmax,
  title = {Categorical {{Reparameterization}} with {{Gumbel}}-{{Softmax}}},
  booktitle = {{{ICLR}}},
  author = {Jang, E. and Gu, S. and Poole, B.},
  date = {2017},
  pages = {1--13},
  issn = {1611.01144},
  url = {http://arxiv.org/abs/1611.01144},
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  annotation = {1681 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1611.01144},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D8X9XAFU/Jang, Gu, Poole - 2017 - Categorical Reparameterization with Gumbel-Softmax(2).pdf}
}

@inproceedings{jean2015UsingVeryLarge,
  title = {On {{Using Very Large Target Vocabulary}} for {{Neural Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Jean, S. and Cho, K. and Memisevic, R. and Bengio, Y.},
  date = {2015},
  volume = {000},
  url = {http://arxiv.org/abs/1412.2007},
  abstract = {Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English-{$>$}German translation and almost as high performance as state-of-the-art English-{$>$}French translation system.},
  annotation = {767 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1412.2007},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NWZKX48T/Jean et al. - 2015 - On Using Very Large Target Vocabulary for Neural Machine Translation(2).pdf}
}

@inproceedings{jersak2001CombiningComplexEvent,
  title = {Combining Complex Event Models and Timing Constraints},
  booktitle = {High-{{Level Design Validation}} and {{Test Workshop}}},
  author = {Jersak, M. and Richter, K. and Ernst, R.},
  date = {2001},
  pages = {89--94},
  issn = {15526674},
  doi = {10.1109/HLDVT.2001.972813},
  abstract = {Sophisticated models of event streams including jitter and bursts as well as the possibility to specify a variety of system-level timing constraints are prerequisites for modem analysis and synthesis techniques in the area of embedded real-time systems. Currently, there is no commonly used specification that models events and timing constraints in a sufficiently general way. In this paper, we first identify a duality between event models and timing constraints and as a result present a specification that can be used for both. Our specification covers most current analysis and synthesis techniques and is easily extensible. We then show how the duality between event models and timing constraints can be applied at different points in a design flow. A real-time video transmission is used as an example.},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WKEYZ2FY/Jersak, Richter, Ernst - 2001 - Combining complex event models and timing constraints(2).pdf},
  isbn = {0-7695-1411-1}
}

@inproceedings{jia2016DataRecombinationNeural,
  title = {Data {{Recombination}} for {{Neural Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Jia, R. and Liang, P.},
  date = {2016},
  pages = {12--22},
  issn = {04194217},
  doi = {10.18653/v1/P16-1002},
  url = {http://arxiv.org/abs/1606.03622},
  abstract = {Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.},
  annotation = {281 citations (Semantic Scholar/DOI) [2021-03-26] 281 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {22251136},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AR3FDQAW/Jia, Liang - 2016 - Data Recombination for Neural Semantic Parsing(2).pdf},
  isbn = {978-1-5108-2758-5}
}

@inproceedings{jia2017AdversarialExamplesEvaluating,
  title = {Adversarial {{Examples}} for {{Evaluating Reading Comprehension Systems}}},
  booktitle = {{{EMNLP}}},
  author = {Jia, R. and Liang, P.},
  date = {2017},
  pages = {2021--2031},
  url = {http://arxiv.org/abs/1707.07328},
  abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of \$75\textbackslash\%\$ F1 score to \$36\textbackslash\%\$; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to \$7\textbackslash\%\$. We hope our insights will motivate the development of new models that understand language more precisely.},
  annotation = {711 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1707.07328},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TSEX85J8/Jia, Liang - 2017 - Adversarial Examples for Evaluating Reading Comprehension Systems(2).pdf}
}

@inproceedings{jia2019CertifiedRobustnessAdversarial,
  title = {Certified {{Robustness}} to {{Adversarial Word Substitutions}}},
  booktitle = {{{EMNLP}}},
  author = {Jia, R. and Raghunathan, A. and Göksel, K. and Liang, P.},
  date = {2019},
  url = {http://arxiv.org/abs/1909.00986},
  abstract = {State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain \$75\textbackslash\%\$ adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI. In comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only \$8\textbackslash\%\$ and \$35\textbackslash\%\$, respectively.},
  annotation = {61 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.00986},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5I9MJE7I/Jia et al. - 2019 - Certified Robustness to Adversarial Word Substitutions(2).pdf},
  keywords = {u}
}

@inproceedings{jia2019DocumentLevelNaryRelation,
  title = {Document-{{Level N}}-Ary {{Relation Extraction}} with {{Multiscale Representation Learning}}},
  booktitle = {{{NAACL}}},
  author = {Jia, R. and Wong, C. and Poon, H.},
  date = {2019},
  pages = {3693--3704},
  doi = {10.18653/v1/n19-1370},
  abstract = {Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, \$n\$-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level \$n\$-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system's purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous \$n\$-ary relation extraction methods.},
  annotation = {30 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.02347},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2K3RP6TV/Jia, Wong, Poon - 2019 - Document-Level N-ary Relation Extraction with Multiscale Representation Learning(2).pdf},
  keywords = {u}
}

@inproceedings{jiang2014SelfPacedLearningDiversity,
  title = {Self-{{Paced Learning}} with {{Diversity}}},
  booktitle = {{{NeurIPS}}},
  author = {Jiang, L. and Meng, D. and Yu, S. and Lan, Z. and Shan, S. and Hauptmann, A.},
  date = {2014},
  pages = {2078--2086},
  issn = {10495258},
  url = {http://papers.nips.cc/paper/5568-self-paced-learning-with-diversity},
  abstract = {Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which for-malizes the preference for both easy and diverse samples into a general regularizer. This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method signifi-cantly outperforms the conventional SPL on three real-world datasets. Specifical-ly, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RS4LJ7QY/Jiang et al. - 2014 - Self-Paced Learning with Diversity(2).pdf}
}

@inproceedings{jiang2016UnsupervisedNeuralDependency,
  title = {Unsupervised {{Neural Dependency Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Jiang, Y. and Han, W. and Tu, K.},
  date = {2016},
  pages = {763--771},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T34BH9YU/Jiang, Han, Tu - 2016 - Unsupervised Neural Dependency Parsing(2).pdf},
  keywords = {u},
  number = {61503248}
}

@inproceedings{jiang2019MultiInputMultiOutputSequence,
  title = {Multi-{{Input Multi}}-{{Output Sequence Labeling}} for {{Joint Extraction}} of {{Fact}} and {{Condition Tuples}} from {{Scientific Text}}},
  booktitle = {{{EMNLP}}},
  author = {Jiang, T. and Zhao, T. and Qin, B. and Liu, T. and Chawla, N. and Jiang, M.},
  date = {2019},
  pages = {302--312},
  doi = {10.18653/v1/d19-1029},
  abstract = {Condition is essential in scientific statement. Without the conditions (e.g., equipment, environment) that were precisely specified, facts (e.g., observations) in the statements may no longer be valid. Existing ScienceIE methods, which aim at extracting factual tuples from scientific text, do not consider the conditions. In this work, we propose a new sequence labeling framework (as well as a new tag schema) to jointly extract the fact and condition tuples from statement sentences. The framework has (1) a multi-output module to generate one or multiple tuples and (2) a multi-input module to feed in multiple types of signals as sequences. It improves F1 score relatively by 4.2\% on BioNLP2013 and by 6.2\% on a new bio-text dataset for tuple extraction.},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SYCTJJMJ/Jiang et al. - 2019 - Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and Condition Tuples from Scientific Te(2).pdf},
  keywords = {u}
}

@inproceedings{jiang2019YouKnowThat,
  title = {Do {{You Know That Florence Is Packed}} with {{Visitors}}? {{Evaluating State}}-of-the-Art {{Models}} of {{Speaker Commitment}}},
  booktitle = {{{ACL}}},
  author = {Jiang, N. and de Marneffe, M.},
  date = {2019},
  pages = {4208--4213},
  doi = {10.18653/v1/p19-1412},
  abstract = {When a speaker, Mary, asks Do you know that Florence is packed with visitors?, we take her to believe that Florence is packed with visitors, but not if she asks Do you think that Florence is packed with visitors? Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here we explore the hypothesis that linguistic deficits drive the error patterns of speaker commitment models by analyzing the linguistic correlates of model errors on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The Com-mitmentBank is annotated with speaker commitment towards the content of the complement (Florence is packed with visitors in our example) of clause-embedding verbs (know, think) under four entailment-canceling environments. We found that a linguistically-informed model outperforms a LSTM-based one, suggesting that linguistic knowledge is needed to capture such challenging naturalis-tic data. A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WI7HUJDY/Jiang, de Marneffe - 2019 - Do You Know That Florence Is Packed with Visitors Evaluating State-of-the-art Models of Speaker Commitmen(2).pdf},
  keywords = {u},
  options = {useprefix=true}
}

@article{jiang2020HowCanWe,
  title = {How {{Can We Know What Language Models Know}}?},
  author = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
  date = {2020-05-03},
  journaltitle = {TACL},
  url = {http://arxiv.org/abs/1911.12543},
  urldate = {2020-10-21},
  abstract = {Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as "Obama is a \_ by profession". These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as "Obama worked as a \_" may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1\% to 39.6\%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.},
  annotation = {48 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1911.12543},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/URXQNDFQ/Jiang et al. - 2020 - How Can We Know What Language Models Know.pdf;/home/hiaoxui/.local/share/zotero_files/storage/XHG8CD2P/1911.html},
  keywords = {u}
}

@inproceedings{jin2019FineGrainedEntityTyping,
  title = {Fine-{{Grained Entity Typing}} via {{Hierarchical Multi Graph Convolutional Networks}}},
  booktitle = {{{EMNLP}}},
  author = {Jin, H. and Hou, L.},
  date = {2019},
  pages = {4969--4978},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5EETI4R8/Jin, Hou - 2019 - Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks(2).pdf},
  keywords = {u}
}

@report{jing2017NeuralStyleTransfer,
  title = {Neural {{Style Transfer}}: {{A Review}}},
  author = {Jing, Y. and Yang, Y. and Feng, Z. and Ye, J. and Song, M.},
  date = {2017},
  url = {http://arxiv.org/abs/1705.04058},
  abstract = {The recent work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNN) in creating artistic fantastic imagery by separating and recombing the image content and style. This process of using CNN to migrate the semantic content of one image to different styles is referred to as Neural Style Transfer. Since then, Neural Style Transfer has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention from computer vision researchers and several methods are proposed to either improve or extend the original neural algorithm proposed by Gatys et al. However, there is no comprehensive survey presenting and summarizing recent Neural Style Transfer literature. This review aims to provide an overview of the current progress towards Neural Style Transfer, as well as discussing its various applications and open problems for future research.},
  annotation = {208 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1705.04058},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BB3BHMII/Jing et al. - 2017 - Neural Style Transfer A Review(2).pdf}
}

@inproceedings{jing2017NeuralSurvivalRecommender,
  title = {Neural Survival Recommender},
  booktitle = {{{WSDM}}},
  author = {Jing, H. and Smola, A. J.},
  date = {2017},
  pages = {515--524},
  doi = {10.1145/3018661.3018719},
  abstract = {The ability to predict future user activity is invaluable when it comes to content recommendation and personalization. For instance, knowing when users will return to an online music service and what they will listen to increases user satisfaction and therefore user retention. We present a model based on Long-Short Term Memory to estimate when a user will return to a site and what their future listening behavior will be. In doing so, we aim to solve the problem of Just-In-Time recommendation, that is, to recommend the right items at the right time. We use tools from survival analysis for return time prediction and exponential families for future activity analysis. We show that the resulting multitask problem can be solved accurately, when applied to two real-world datasets.},
  annotation = {89 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DJJHACR7/Jing, Smola - 2017 - Neural survival recommender(2).pdf},
  isbn = {978-1-4503-4675-7}
}

@inproceedings{johansson2007ExtendedConstituenttodependencyConversion,
  title = {Extended Constituent-to-Dependency Conversion for {{English}}},
  booktitle = {{{NODALIDA}}},
  author = {Johansson, R. and Nugues, P.},
  date = {2007},
  pages = {105--112},
  abstract = {We describe a new method to convert En- glish constituent trees using the Penn Tree- bank annotation style into dependency trees. The new format was inspired by annota- tion practices used in other dependency tree- banks with the intention to produce a better interface to further semantic processing than existing methods. In particular, we used a richer set of edge labels and introduced links to handle long-distance phenomena such as wh-movement and topicalization. The resulting trees generally have a more complex dependency structure. For exam- ple, 6\%of the trees contain at least one non- projective link, which is difficult for many parsing algorithms. As can be expected, the more complex structure and the enriched set of edge labels make the trees more difficult to predict, and we observed a decrease in parsing accuracy when applying two depen- dency parsers to the new corpus. However, the richer information contained in the new trees resulted in a 23\% error reduction in a baseline FrameNet semantic role labeler that relied on dependency arc labels only.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UP9CYSQQ/Johansson, Nugues - 2007 - Extended constituent-to-dependency conversion for English(2).pdf},
  isbn = {978-9985-4-0514-7}
}

@inproceedings{johansson2007LTHSemanticStructure,
  title = {{{LTH}}: {{Semantic}} Structure Extraction Using Nonprojective Dependency Trees},
  booktitle = {{{SemEval}}},
  author = {Johansson, R. and Nugues, P.},
  date = {2007},
  pages = {227--230},
  abstract = {We describe our contribution to the SemEval task on Frame-Semantic Structure Extraction. Unlike most previous systems described in literature, ours is based on dependency syntax. We also describe a fully automatic method to add words to the FrameNet lexical database, which gives an improvement in the recall of frame detection.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YX495HN6/Johansson, Nugues - 2007 - LTH Semantic structure extraction using nonprojective dependency trees(2).pdf}
}

@inproceedings{johnson2016ComposingGraphicalModels,
  title = {Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference},
  booktitle = {{{NeurIPS}}},
  author = {Johnson, M. J. and Duvenaud, D. and Wiltschko, A. B. and Datta, S. R. and Adams, R. P.},
  date = {2016},
  archiveprefix = {arXiv},
  eprint = {1603.06277v5},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3FCF6ZIQ/Johnson et al. - 2016 - Composing graphical models with neural networks for structured representations and fast inference(2).pdf}
}

@inproceedings{joshi2018ExtendingParserDistant,
  title = {Extending a {{Parser}} to {{Distant Domains Using}} a {{Few Dozen Partially Annotated Examples}}},
  booktitle = {{{ACL}}},
  author = {Joshi, Vidur and Peters, Matthew and Hopkins, Mark},
  date = {2018-05-16},
  url = {http://arxiv.org/abs/1805.06556},
  urldate = {2020-08-14},
  abstract = {We revisit domain adaptation for parsers in the neural era. First we show that recent advances in word representations greatly diminish the need for domain adaptation when the target domain is syntactically similar to the source domain. As evidence, we train a parser on the Wall Street Jour- nal alone that achieves over 90\% F1 on the Brown corpus. For more syntactically dis- tant domains, we provide a simple way to adapt a parser using only dozens of partial annotations. For instance, we increase the percentage of error-free geometry-domain parses in a held-out set from 45\% to 73\% using approximately five dozen training examples. In the process, we demon- strate a new state-of-the-art single model result on the Wall Street Journal test set of 94.3\%. This is an absolute increase of 1.7\% over the previous state-of-the-art of 92.6\%.},
  annotation = {31 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1805.06556},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DYTV5TYT/Joshi et al. - 2018 - Extending a Parser to Distant Domains Using a Few .pdf;/home/hiaoxui/.local/share/zotero_files/storage/89MQGYSK/1805.html},
  keywords = {u}
}

@inproceedings{joshi2019BERTCoreferenceResolution,
  title = {{{BERT}} for {{Coreference Resolution}}: {{Baselines}} and {{Analysis}}},
  shorttitle = {{{BERT}} for {{Coreference Resolution}}},
  booktitle = {{{EMNLP}}},
  author = {Joshi, Mandar and Levy, Omer and Weld, Daniel S. and Zettlemoyer, Luke},
  date = {2019-12-22},
  url = {http://arxiv.org/abs/1908.09091},
  urldate = {2021-03-10},
  abstract = {We apply BERT to coreference resolution, achieving strong improvements on the OntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available.},
  annotation = {81 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1908.09091},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6WHJAGHW/Joshi et al. - 2019 - BERT for Coreference Resolution Baselines and Ana.pdf;/home/hiaoxui/.local/share/zotero_files/storage/JYK5YUWE/1908.html}
}

@article{joshi2020SpanBERTImprovingPretraining,
  title = {{{SpanBERT}}: {{Improving Pre}}-Training by {{Representing}} and {{Predicting Spans}}},
  shorttitle = {{{SpanBERT}}},
  author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
  date = {2020-01-17},
  journaltitle = {TACL},
  volume = {8},
  pages = {64--77},
  url = {http://arxiv.org/abs/1907.10529},
  urldate = {2020-08-04},
  annotation = {356 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1907.10529},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3R73GZQD/Joshi et al. - 2020 - SpanBERT Improving Pre-training by Representing a.pdf},
  langid = {english}
}

@inproceedings{kalchbrenner2013RecurrentContinuousTranslation,
  title = {Recurrent {{Continuous Translation Models}}},
  booktitle = {{{EMNLP}}},
  author = {Kalchbrenner, N. and Blunsom, P.},
  date = {2013},
  url = {https://www.aclweb.org/anthology/D13-1176},
  abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is {$>$} 43\% lower than that of state-of-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and mean- ing of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2GIB29NS/Kalchbrenner, Blunsom - 2013 - Recurrent Continuous Translation Models(2).pdf}
}

@inproceedings{kalchbrenner2014ConvolutionalNeuralNetwork,
  title = {A {{Convolutional Neural Network}} for {{Modelling Sentences}}},
  booktitle = {{{ACL}}},
  author = {Kalchbrenner, N. and Grefenstette, E. and Blunsom, P.},
  date = {2014},
  url = {http://arxiv.org/abs/1404.2188},
  abstract = {The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25\% error reduction in the last task with respect to the strongest baseline.},
  annotation = {2561 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1404.2188},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CU5TLIPS/Kalchbrenner, Grefenstette, Blunsom - 2014 - A Convolutional Neural Network for Modelling Sentences(2).pdf}
}

@article{kamper2017EmbeddedSegmentalKMeans,
  title = {An {{Embedded Segmental K}}-{{Means Model}} for {{Unsupervised Segmentation}} and {{Clustering}} of {{Speech}}},
  author = {Kamper, H. and Livescu, K. and Goldwater, S.},
  date = {2017},
  journaltitle = {Computational Linguistics},
  archiveprefix = {arXiv},
  eprint = {1703.08135v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J3P37GME/Kamper, Livescu, Goldwater - 2017 - An Embedded Segmental K-Means Model for Unsupervised Segmentation and Clustering of Speech(2).pdf},
  keywords = {u}
}

@inproceedings{karpathy2016VisualizingUnderstandingRecurrent,
  title = {Visualizing and {{Understanding Recurrent Networks}}},
  booktitle = {{{ICLR}}},
  author = {Karpathy, A. and Johnson, J. and Fei-Fei, L.},
  date = {2016},
  archiveprefix = {arXiv},
  eprint = {1506.02078v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S54VR6AS/Karpathy, Johnson, Fei-Fei - 2016 - Visualizing and Understanding Recurrent Networks(2).pdf},
  keywords = {u}
}

@article{karras2012MaximizingParallelismConstruction,
  title = {Maximizing {{Parallelism}} in the {{Construction}} of {{BVHs}} , {{Octrees}} , and k-d {{Trees}}},
  author = {Karras, T.},
  date = {2012},
  journaltitle = {High Performance Graphics},
  pages = {33--37},
  issn = {00043702},
  doi = {10.2312/EGGH/HPG12/033-037},
  abstract = {A number of methods for constructing bounding volume hierarchies and point-based octrees on the GPU are based on the idea of ordering primitives along a space-filling curve. A major shortcoming with these methods is that they construct levels of the tree sequentially, which limits the amount of parallelism that they can achieve. We present a novel approach that improves scalability by constructing the entire tree in parallel. Our main contribution is an in-place algorithm for constructing binary radix trees, which we use as a building block for other types of trees.},
  annotation = {148 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {cs/9903011},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/84XU3XQ7/Karras - 2012 - Maximizing Parallelism in the Construction of BVHs , Octrees , and k-d Trees(2).pdf},
  isbn = {978-3-905674-41-5}
}

@article{kass1995BayesFactors,
  title = {Bayes {{Factors}}},
  author = {Kass, R. E. and Raftery, A. E.},
  date = {1995},
  journaltitle = {Journal of the American Statistical Association},
  volume = {90},
  pages = {773--795},
  issn = {1537274X},
  doi = {10.1080/01621459.1995.10476572},
  abstract = {Abstract In a 1935 paper and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: •From Jeffreys' Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory.•Bayes factors offer a way of evaluating evidence in favor of a null hypothesis.•Bayes factors provide a way of incorporating external information into the evalu...},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HZ3E98W4/Kass, Raftery - 1995 - Bayes Factors(2).pdf},
  keywords = {u},
  number = {430}
}

@inproceedings{kate2005LearningTransformNatural,
  title = {Learning to Transform Natural to Formal Languages},
  booktitle = {{{AAAI}}},
  author = {Kate, R. J. and Wong, Y. W. and Mooney, R. J.},
  date = {2005},
  volume = {20},
  pages = {1062--1068},
  url = {http://www.aaai.org/Library/AAAI/2005/aaai05-168.php},
  abstract = {This paper presents a method for inducing transformation rules that map natural-language sentences into a formal query or command language. The approach assumes a formal gram- mar for the target representation language and learns trans- formation rules that exploit the non-terminal symbols in this grammar. The learned transformation rules incrementally map a natural-language sentence or its syntactic parse tree into a parse-tree for the target formal language. Experimental results are presented for two corpora, one which maps En- glish instructions into an existing formal coaching language for simulated RoboCup soccer agents, and another which maps EnglishU.S.-geography questions into a database query language. We show that our method performs overall better and faster than previous approaches in both domains.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ITUMQP79/Kate, Wong, Mooney - 2005 - Learning to transform natural to formal languages(2).pdf},
  isbn = {1-57735-236-X},
  number = {3}
}

@inproceedings{kate2006UsingStringkernelsLearning,
  title = {Using String-Kernels for Learning Semantic Parsers},
  booktitle = {{{ACL}}},
  author = {Kate, R. J. and Mooney, R. J.},
  date = {2006},
  pages = {913--920},
  doi = {10.3115/1220175.1220290},
  url = {http://dl.acm.org/citation.cfm?id=1220290},
  annotation = {255 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8H3Z5IHY/Kate, Mooney - 2006 - Using string-kernels for learning semantic parsers(2).pdf},
  isbn = {1-932432-65-5},
  issue = {July}
}

@inproceedings{kate2007LearningLanguageSemantics,
  title = {Learning Language Semantics from Ambiguous Supervision},
  booktitle = {{{AAAI}}},
  author = {Kate, R. J. and Mooney, R. J.},
  date = {2007},
  pages = {895--900},
  abstract = {This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XWLEMXQI/Kate, Mooney - 2007 - Learning language semantics from ambiguous supervision(2).pdf},
  isbn = {978-1-57735-323-2},
  issue = {July}
}

@inproceedings{kaushik2018HowMuchReading,
  title = {How {{Much Reading Does Reading Comprehension Require}} ?},
  booktitle = {{{EMNLP}}},
  author = {Kaushik, D. and Lipton, Z. C.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1808.04926v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VGT3ENQH/Kaushik, Lipton - 2018 - How Much Reading Does Reading Comprehension Require(2).pdf},
  keywords = {u}
}

@inproceedings{kazemzadeh2014ReferItGameReferringObjects,
  title = {{{ReferItGame}}: {{Referring}} to {{Objects}} in {{Photographs}} of {{Natural Scenes}}},
  booktitle = {{{EMNLP}}},
  author = {Kazemzadeh, S. and Ordonez, V. and Matten, M. and Berg, T.},
  date = {2014},
  doi = {10.3115/v1/d14-1086},
  abstract = {In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.},
  annotation = {406 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CDNXAK9M/Kazemzadeh et al. - 2014 - ReferItGame Referring to Objects in Photographs of Natural Scenes(2).pdf}
}

@article{kennedy1999EventStructureScale,
  title = {From Event Structure to Scale Structure: {{Degree}} Modification in Deverbal Adjectives},
  author = {Kennedy, C. and McNally, L.},
  date = {1999},
  journaltitle = {SALT},
  pages = {163--180},
  url = {http://elanguage.net/journals/salt/article/download/9.163/1681},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NBVMM46X/Kennedy, McNally - 1999 - From event structure to scale structure Degree modification in deverbal adjectives(2).pdf},
  number = {2}
}

@article{kennedy2005ScaleStructureDegree,
  title = {Scale {{Structure}}, {{Degree Modification}} and the {{Semantics}} of {{Gradable Predicates}}},
  author = {Kennedy, C. and McNally, L.},
  date = {2005},
  journaltitle = {Language},
  volume = {81},
  pages = {345--381},
  issn = {1535-0665},
  doi = {10.1353/lan.2005.0071},
  abstract = {In this article we develop a semantic typology of gradable predicates, with special emphasis on deverbal adjectives. We argue for the linguistic relevance of this typology by demonstrating that the distribution and interpretation of degree modifiers is sensitive to its twomajor classificatory parameters: (1) whether a gradable predicate is associated with what we call an open or closed scale, and (2) whether the standard of comparison for the applicability of the predicate is absolute or relative to a context. We further showthat the classification of an important subclass of adjectives within the typology is largely predictable. Specifically, the scale structure of a deverbal gradable adjective correlates either with the algebraic part structure of the event denoted by its source verb or with the part structure of the entities to which the adjective applies. These correla- tions underscore the fact that gradability is characteristic not only of adjectives but also of verbs and nouns, and that scalar properties are shared by categorially distinct but derivationally related expressions.},
  annotation = {768 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H2GM5GWU/Kennedy, McNally - 2005 - Scale Structure, Degree Modification and the Semantics of Gradable Predicates(2).pdf},
  isbn = {00978507},
  number = {2}
}

@inproceedings{kerenidis2019QmeansQuantumAlgorithm,
  title = {Q-Means: {{A}} Quantum Algorithm for Unsupervised Machine Learning},
  booktitle = {{{NeurIPS}}},
  author = {Kerenidis, I. and Landman, J. and Luongo, A. and Prakash, A.},
  date = {2019},
  url = {http://arxiv.org/abs/1812.03584},
  abstract = {Quantum machine learning is one of the most promising applications of a full-scale quantum computer. Over the past few years, many quantum machine learning algorithms have been proposed that can potentially offer considerable speedups over the corresponding classical algorithms. In this paper, we introduce q-means, a new quantum algorithm for clustering which is a canonical problem in unsupervised machine learning. The \$q\$-means algorithm has convergence and precision guarantees similar to \$k\$-means, and it outputs with high probability a good approximation of the \$k\$ cluster centroids like the classical algorithm. Given a dataset of \$N\$ \$d\$-dimensional vectors \$v\_i\$ (seen as a matrix \$V \textbackslash in \textbackslash mathbb\{R\}\^\{N \textbackslash times d\})\$ stored in QRAM, the running time of q-means is \$\textbackslash widetilde\{O\}\textbackslash left( k d \textbackslash frac\{\textbackslash eta\}\{\textbackslash delta\^2\}\textbackslash kappa(V)(\textbackslash mu(V) + k \textbackslash frac\{\textbackslash eta\}\{\textbackslash delta\}) + k\^2 \textbackslash frac\{\textbackslash eta\^\{1.5\}\}\{\textbackslash delta\^2\} \textbackslash kappa(V)\textbackslash mu(V) \textbackslash right)\$ per iteration, where \$\textbackslash kappa(V)\$ is the condition number, \$\textbackslash mu(V)\$ is a parameter that appears in quantum linear algebra procedures and \$\textbackslash eta = \textbackslash max\_\{i\} ||v\_\{i\}||\^\{2\}\$. For a natural notion of well-clusterable datasets, the running time becomes \$\textbackslash widetilde\{O\}\textbackslash left( k\^2 d \textbackslash frac\{\textbackslash eta\^\{2.5\}\}\{\textbackslash delta\^3\} + k\^\{2.5\} \textbackslash frac\{\textbackslash eta\^2\}\{\textbackslash delta\^3\} \textbackslash right)\$ per iteration, which is linear in the number of features \$d\$, and polynomial in the rank \$k\$, the maximum square norm \$\textbackslash eta\$ and the error parameter \$\textbackslash delta\$. Both running times are only polylogarithmic in the number of datapoints \$N\$. Our algorithm provides substantial savings compared to the classical \$k\$-means algorithm that runs in time \$O(kdN)\$ per iteration, particularly for the case of large datasets.},
  annotation = {44 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1812.03584},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/77FABXJB/Kerenidis et al. - 2019 - q-means A quantum algorithm for unsupervised machine learning(2).pdf}
}

@inproceedings{kerenidis2020QuantumExpectationMaximizationGaussian,
  title = {Quantum {{Expectation}}-{{Maximization}} for {{Gaussian Mixture Models}}},
  booktitle = {{{ICML}}},
  author = {Kerenidis, I. and Luongo, A. and Prakash, A.},
  date = {2020},
  archiveprefix = {arXiv},
  eprint = {1908.06657v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BVL4JFFV/Kerenidis, Luongo, Prakash - 2019 - Quantum Expectation-Maximization for Gaussian Mixture Models(4).pdf;/home/hiaoxui/.local/share/zotero_files/storage/KZNYC95U/Kerenidis, Luongo, Prakash - 2019 - Quantum Expectation-Maximization for Gaussian Mixture Models(6).pdf;/home/hiaoxui/.local/share/zotero_files/storage/U2NJWRW8/Kerenidis, Luongo, Prakash - 2019 - Quantum Expectation-Maximization for Gaussian Mixture Models(5).pdf},
  keywords = {review}
}

@inproceedings{khandelwal2020GeneralizationMemorizationNearest,
  title = {Generalization through {{Memorization}}: {{Nearest Neighbor Language Models}}},
  booktitle = {{{ICLR}}},
  author = {Khandelwal, U. and Levy, O. and Jurafsky, D. and Zettlemoyer, L. S. and Lewis, M.},
  date = {2020},
  url = {http://arxiv.org/abs/1911.00172},
  abstract = {We introduce \$k\$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a \$k\$-nearest neighbors (\$k\$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our \$k\$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.},
  annotation = {50 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1911.00172},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/33HBQNV3/Khandelwal et al. - 2020 - Generalization through Memorization Nearest Neighbor Language Models(2).pdf},
  keywords = {u}
}

@inproceedings{khani2016UnanimousPrediction100,
  title = {Unanimous {{Prediction}} for 100\% {{Precision}} with {{Application}} to {{Learning Semantic Mappings}}},
  booktitle = {{{ACL}}},
  author = {Khani, F. and Rinard, M. and Liang, P.},
  date = {2016},
  url = {http://arxiv.org/abs/1606.06368},
  abstract = {Can we train a system that, on any new input, either says "don't know" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100\% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.},
  annotation = {9 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1606.06368},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9FT7UTLB/Khani, Rinard, Liang - 2016 - Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings(2).pdf},
  isbn = {978-1-5108-2758-5}
}

@inproceedings{kiddon2016GloballyCoherentText,
  title = {Globally Coherent Text Generation with Neural Checklist Models},
  booktitle = {{{EMNLP}}},
  author = {Kiddon, C. and Zettlemoyer, L. and Choi, Y.},
  date = {2016},
  pages = {329--339},
  doi = {10.18653/v1/d16-1032},
  abstract = {Recurrent neural networks can generate locally coherent text but often have difficulties representing what has already been generated and what still needs to be said - especially when constructing long texts. We present the neural checklist model, a recurrent neural network that models global coherence by storing and updating an agenda of text strings which should be mentioned somewhere in the output. The model generates output by dynamically adjusting the interpolation among a language model and a pair of attention models that encourage references to agenda items. Evaluations on cooking recipes and dialogue system responses demonstrate high coherence with greatly improved semantic coverage of the agenda.},
  annotation = {137 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PUFXL7ZP/Kiddon, Zettlemoyer, Choi - 2016 - Globally coherent text generation with neural checklist models(2).pdf}
}

@article{kilgarriff1997DonBelieveWord,
  title = {I Don't Believe in Word Senses},
  author = {Kilgarriff, A.},
  date = {1997},
  journaltitle = {Computers and the Humanities},
  volume = {31},
  pages = {91--113},
  issn = {1574020X},
  doi = {10.1023/A:1000583911091},
  abstract = {Word sense disambiguation assumes word senses. Within the lexicography and linguistics literature, they are known to be very slippery entities. The paper looks at problems with existing accounts of 'word sense' and describes the various kinds of ways in which a word's meaning can deviate from its core meaning. An analysis is presented in which word senses are abstractions from clusters of corpus citations, in accordance with current lexicographic practice. The corpus citations, not the word senses, are the basic objects in the ontology. The corpus citations will be clustered into senses according to the purposes of whoever or whatever does the clustering. In the absence of such purposes, word senses do not exist. Word sense disambiguation also needs a set of word senses to disambiguate between. In most recent work, the set has been taken from a general-purpose lexical resource, with the assumption that the lexical resource describes the word senses of English/French/..., between which NLP applications will need to disambiguate. The implication of the paper is, by contrast, that word senses exist only relative to a task. © 1997 Kluwer Academic Publishers.},
  annotation = {429 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {cmp-lg/9712006},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8JISK6SI/Kilgarriff - 1997 - I don't believe in word senses(2).pdf},
  isbn = {9783110895698},
  keywords = {u},
  number = {2}
}

@inproceedings{kim2010GenerativeAlignmentSemantic,
  title = {Generative Alignment and Semantic Parsing for Learning from Ambiguous Supervision},
  booktitle = {{{COLING}}},
  author = {Kim, J. and Mooney, R. J.},
  date = {2010},
  volume = {2},
  pages = {543--551},
  url = {http://dl.acm.org/citation.cfm?id=1944628%5Cnpapers3://publication/uuid/EEC4CDC8-A12E-4837-8EF4-57020ABF3026},
  abstract = {We present a probabilistic generative model for learning semantic parsers from ambiguous supervision. Our approach learns from natural language sentences paired with world states consisting of multiple potential logical meaning representations. It disambiguates the meaning of each sentence while simultaneously learning a semantic parser that maps sentences into logical form. Compared to a previous generative model for semantic alignment, it also supports full semantic parsing. Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DT6BIU82/Kim, Mooney - 2010 - Generative alignment and semantic parsing for learning from ambiguous supervision(2).pdf}
}

@inproceedings{kim2012UnsupervisedPCFGInduction,
  title = {Unsupervised {{PCFG}} Induction for Grounded Language Learning with Highly Ambiguous Supervision},
  booktitle = {{{EMNLP}}},
  author = {Kim, J. and Mooney, R. J.},
  date = {2012},
  pages = {433--444},
  abstract = {“Grounded” language learning employs train- ing data in the form of sentences paired with relevant but ambiguous perceptual contexts. B¨ orschinger et al. (2011) introduced an ap- proach to grounded language learning based on unsupervised PCFG induction. Their ap- proach works well when each sentence po- tentially refers to one of a small set of pos- sible meanings, such as in the sportscasting task. However, it does not scale to prob- lems with a large set of potential meanings for each sentence, such as the navigation in- struction following task studied by Chen and Mooney (2011). This paper presents an en- hancement of the PCFG approach that scales to such problems with highly-ambiguous su- pervision. Experimental results on the naviga- tion task demonstrates the effectiveness of our approach.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QNK9SELX/Kim, Mooney - 2012 - Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision(2).pdf},
  isbn = {978-1-937284-43-5},
  issue = {July}
}

@inproceedings{kim2014ConvolutionalNeuralNetworks,
  title = {Convolutional {{Neural Networks}} for {{Sentence Classification}}},
  booktitle = {{{EMNLP}}},
  author = {Kim, Y.},
  date = {2014},
  url = {http://arxiv.org/abs/1408.5882},
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  annotation = {7755 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1408.5882},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N6L5TRJZ/Kim - 2014 - Convolutional Neural Networks for Sentence Classification(2).pdf}
}

@inproceedings{kim2016FrustratinglyEasyNeural,
  title = {Frustratingly {{Easy Neural Domain Adaptation}}},
  booktitle = {{{COLING}}},
  author = {Kim, Y. and Stratos, K. and Sarikaya, R.},
  date = {2016},
  pages = {387--396},
  url = {http://arxiv.org/abs/0907.1815},
  abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
  annotation = {1487 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {0907.1815},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6CZAIJ96/Kim, Stratos, Sarikaya - 2016 - Frustratingly Easy Neural Domain Adaptation(2).pdf}
}

@inproceedings{kim2017StructuredAttentionNetworks,
  title = {Structured {{Attention Networks}}},
  booktitle = {{{ICLR}}},
  author = {Kim, Y. and Denton, C. and Hoang, L. and Rush, A. M.},
  date = {2017},
  pages = {1--21},
  issn = {0271-678X},
  doi = {10.1007/978-1-4615-5533-9_4},
  url = {http://arxiv.org/abs/1702.00887},
  abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
  annotation = {22 citations (Semantic Scholar/DOI) [2021-03-26] 237 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2830291},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9M8WYQU4/Kim et al. - 2017 - Structured Attention Networks(2).pdf},
  isbn = {978-1-4613-7529-6},
  keywords = {u}
}

@inproceedings{kim2019ProbingWhatDifferent,
  title = {Probing {{What Different NLP Tasks Teach Machines}} about {{Function Word Comprehension}}},
  booktitle = {Joint {{Conference}} on {{Lexical}} and {{Computational Semantics}}},
  author = {Kim, N. and Patel, R. and Poliak, A. and Wang, A. and Xia, P. and McCoy, R. T. and Tenney, I. and Ross, A. and Linzen, T. and Van Durme, B. and Bowman, S. R. and Pavlick, E.},
  date = {2019},
  pages = {235--249},
  url = {http://arxiv.org/abs/1904.11544},
  abstract = {We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, CCG supertagging and natural language inference (NLI)) on the learned representations. Our results show that pretraining on CCG---our most syntactic objective---performs the best on average across our probing tasks, suggesting that syntactic knowledge helps function word comprehension. Language modeling also shows strong performance, supporting its widespread use for pretraining state-of-the-art NLP models. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that NLI helps the comprehension of negation.},
  annotation = {34 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.11544},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JUEYI5QG/Kim et al. - 2019 - Probing What Different NLP Tasks Teach Machines about Function Word Comprehension(2).pdf}
}

@inproceedings{kimmig2012ShortIntroductionProbabilistic,
  title = {A {{Short Introduction}} to {{Probabilistic Soft Logic}}},
  booktitle = {{{NeurIPS}}},
  author = {Kimmig, A. and Bach, S. H. and Broecheler, M. and Huang, B. and Getoor, L.},
  date = {2012},
  pages = {1--4},
  abstract = {Probabilistic soft logic (PSL) is a framework for collective, probabilistic reasoning in relational domains. PSL uses first order logic rules as a template language for graphical models over random variables with soft truth values from the interval [0, 1]. Inference in this setting is a continuous optimization task, which can be solved efficiently. This paper provides an overview of the PSL language and its techniques for inference and weight learning. An implementation of PSL is available at http://psl.umiacs.umd.edu/.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XISIQI68/Kimmig et al. - 2012 - A Short Introduction to Probabilistic Soft Logic(2).pdf},
  number = {1}
}

@inproceedings{kingma2014AutoEncodingVariationalBayes,
  title = {Auto-{{Encoding Variational Bayes}}},
  booktitle = {{{ICLR}}},
  author = {Kingma, D. P. and Welling, M.},
  date = {2014},
  pages = {1--14},
  archiveprefix = {arXiv},
  eprint = {1312.6114v10},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/593X8AXM/Kingma, Welling - 2014 - Auto-Encoding Variational Bayes(2).pdf}
}

@inproceedings{kingma2014SemisupervisedLearningDeep,
  title = {Semi-Supervised {{Learning}} with {{Deep Generative Models}}},
  booktitle = {{{NeurIPS}}},
  author = {Kingma, D. P. and Rezende, D. J. and Mohamed, S. and Welling, M.},
  date = {2014},
  pages = {1--9},
  issn = {10495258},
  url = {http://arxiv.org/abs/1406.5298},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  annotation = {1624 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1406.5298v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZNB3MGKV/Kingma et al. - 2014 - Semi-supervised Learning with Deep Generative Models(2).pdf}
}

@inproceedings{kingma2015AdamMethodStochastic,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  booktitle = {{{ICLR}}},
  author = {Kingma, D. P. and Ba, J. L.},
  date = {2015},
  pages = {1--15},
  issn = {09252312},
  doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
  url = {http://arxiv.org/abs/1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  annotation = {9988 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {172668},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P4HVI7R7/Kingma, Ba - 2015 - Adam A Method for Stochastic Optimization(2).pdf},
  isbn = {978-1-4503-0072-8},
  keywords = {u}
}

@inproceedings{kingma2015VariationalDropoutLocal,
  title = {Variational {{Dropout}} and the {{Local Reparameterization Trick}}},
  booktitle = {{{NeurIPS}}},
  author = {Kingma, D. P. and Salimans, T. and Welling, M.},
  date = {2015},
  pages = {1--14},
  issn = {10495258},
  doi = {10.1016/S0733-8619(03)00096-3},
  url = {http://arxiv.org/abs/1506.02557},
  abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
  annotation = {0 citations (Semantic Scholar/DOI) [2021-03-26] 90 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {15062530},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MV5RJ2CP/Kingma, Salimans, Welling - 2015 - Variational Dropout and the Local Reparameterization Trick(2).pdf},
  isbn = {1506.02557},
  keywords = {u}
}

@inproceedings{kipf2016VariationalGraphAutoEncoders,
  title = {Variational {{Graph Auto}}-{{Encoders}}},
  booktitle = {{{NeurIPS}}},
  author = {Kipf, T. N. and Welling, M.},
  date = {2016},
  pages = {1--3},
  url = {http://arxiv.org/abs/1611.07308},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  annotation = {726 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1611.07308},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZVHUTKTR/Kipf, Welling - 2016 - Variational Graph Auto-Encoders(2).pdf},
  number = {2}
}

@inproceedings{kipf2017SemiSupervisedClassificationGraph,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  booktitle = {{{ICLR}}},
  author = {Kipf, T. N. and Welling, M.},
  date = {2017},
  pages = {1--14},
  url = {http://arxiv.org/abs/1609.02907},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  annotation = {6206 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XBXRS4XF/Kipf, Welling - 2017 - Semi-Supervised Classification with Graph Convolutional Networks(2).pdf},
  keywords = {u}
}

@article{kirkpatrick2017OvercomingCatastrophicForgetting,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  date = {2017-01-25},
  journaltitle = {PNAS},
  url = {http://arxiv.org/abs/1612.00796},
  urldate = {2020-10-21},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
  annotation = {1560 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1612.00796},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HWG9E2BE/Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural netwo.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{kitaev2020ReformerEfficientTransformer,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  booktitle = {{{ICLR}}},
  author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  date = {2020-02-18},
  url = {http://arxiv.org/abs/2001.04451},
  urldate = {2021-02-09},
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L\^2\$) to O(\$L\textbackslash log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  annotation = {224 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M9SVK8D3/Kitaev et al. - 2020 - Reformer The Efficient Transformer.pdf;/home/hiaoxui/.local/share/zotero_files/storage/U5DE45QR/2001.html}
}

@article{klauer2010HierarchicalMultinomialProcessing,
  title = {Hierarchical Multinomial Processing Tree Models: {{A}} Latent-Trait Approach},
  author = {Klauer, K. C.},
  date = {2010},
  journaltitle = {Psychometrika},
  volume = {75},
  pages = {70--98},
  issn = {00333123},
  doi = {10.1007/s11336-009-9141-0},
  abstract = {Multinomial processing tree models are widely used in many areas of psychology. A hierarchical extension of the model class is proposed, using a multivariate normal distribution of person-level parameters with the mean and covariance matrix to be estimated from the data. The hierarchical model allows one to take variability between persons into account and to assess parameter correlations. The model is estimated using Bayesian methods with weakly informative hyperprior distribution and a Gibbs sampler based on two steps of data augmentation. Estimation, model checks, and hypotheses tests are discussed. The new method is illustrated using a real data set, and its performance is evaluated in a simulation study. © 2009 The Psychometric Society.},
  annotation = {165 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NJVVAWTF/Klauer - 2010 - Hierarchical multinomial processing tree models A latent-trait approach(2).pdf},
  number = {1}
}

@inproceedings{klein2002CorpusBasedInductionSyntactic,
  title = {Corpus-{{Based Induction}} of {{Syntactic Structure}} : {{Models}} of {{Dependency}} and {{Constituency}}},
  booktitle = {{{ACL}}},
  author = {Klein, D. and Manning, C. D.},
  date = {2002},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C7EUSBJ9/Klein, Manning - 2002 - Corpus-Based Induction of Syntactic Structure Models of Dependency and Constituency(2).pdf},
  keywords = {u}
}

@article{knuth1972MathematicalAnalysisAlgorithms,
  title = {Mathematical {{Analysis}} of {{Algorithms}}},
  author = {Knuth, D. E.},
  date = {1972},
  journaltitle = {Information Processing},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K8TKBHEB/Knuth - 1972 - Mathematical Analysis of Algorithms(2).pdf}
}

@article{koch2018BeginnerGuideAnalysis,
  title = {A {{Beginner}}'s {{Guide}} to {{Analysis}} of {{RNA Sequencing Data}}},
  author = {Koch, Clarissa M. and Chiu, Stephen F. and Akbarpour, Mahzad and Bharat, Ankit and Ridge, Karen M. and Bartom, Elizabeth T. and Winter, Deborah R.},
  date = {2018-08},
  journaltitle = {American Journal of Respiratory Cell and Molecular Biology},
  shortjournal = {Am J Respir Cell Mol Biol},
  volume = {59},
  pages = {145--157},
  issn = {1044-1549, 1535-4989},
  doi = {10.1165/rcmb.2017-0430TR},
  url = {https://www.atsjournals.org/doi/10.1165/rcmb.2017-0430TR},
  urldate = {2021-01-04},
  annotation = {14 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S5PS8RG6/Koch et al. - 2018 - A Beginner’s Guide to Analysis of RNA Sequencing D.pdf},
  keywords = {u},
  langid = {english},
  number = {2}
}

@inproceedings{koehn2003StatisticalPhrasebasedTranslation,
  title = {Statistical Phrase-Based Translation},
  booktitle = {{{NAACL}}},
  author = {Koehn, P. and Och, F. J. and Marcu, D.},
  date = {2003},
  pages = {48--54},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D5CMF656/Koehn, Och, Marcu - 2003 - Statistical phrase-based translation(2).pdf}
}

@inproceedings{koh2017UnderstandingBlackboxPredictions,
  title = {Understanding {{Black}}-Box {{Predictions}} via {{Influence Functions}}},
  booktitle = {{{ICML}}},
  author = {Koh, P. W. and Liang, P.},
  date = {2017},
  issn = {1938-7228},
  url = {http://arxiv.org/abs/1703.04730},
  abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
  annotation = {992 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1703.04730},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y6NUNQI8/Koh, Liang - 2017 - Understanding Black-box Predictions via Influence Functions(2).pdf}
}

@inproceedings{kolluru2020IMoJIEIterativeMemoryBased,
  title = {{{IMoJIE}}: {{Iterative Memory}}-{{Based Joint Open Information Extraction}}},
  shorttitle = {{{IMoJIE}}},
  booktitle = {{{ACL}}},
  author = {Kolluru, Keshav and Aggarwal, Samarth and Rathore, Vipul and Mausam and Chakrabarti, Soumen},
  date = {2020-05-17},
  url = {http://arxiv.org/abs/2005.08178},
  urldate = {2021-03-16},
  abstract = {While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task. Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et. al., 2018). Our analysis reveals that CopyAttention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information. We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples. This approach overcomes both shortcomings of CopyAttention, resulting in a variable number of diverse extractions per sentence. We train IMoJIE on training data bootstrapped from extractions of several non-neural systems, which have been automatically filtered to reduce redundancy and noise. IMoJIE outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task.},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2005.08178},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2ZBAEHP2/Kolluru et al. - 2020 - IMoJIE Iterative Memory-Based Joint Open Informat.pdf;/home/hiaoxui/.local/share/zotero_files/storage/8TAXNCDY/2005.html}
}

@inproceedings{koncel-kedziorski2014MultiResolutionLanguageGrounding,
  title = {Multi-{{Resolution Language Grounding}} with {{Weak Supervision}}},
  booktitle = {{{EMNLP}}},
  author = {Koncel-Kedziorski, R. and Hajishirzi, H. and Farhadi, A.},
  date = {2014},
  pages = {386--396},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VWJHJTY3/Koncel-Kedziorski, Hajishirzi, Farhadi - 2014 - Multi-Resolution Language Grounding with Weak Supervision(2).pdf},
  isbn = {978-1-937284-96-1}
}

@inproceedings{kong2014DependencyParserTweets,
  title = {A {{Dependency Parser}} for {{Tweets}}},
  booktitle = {{{EMNLP}}},
  author = {Kong, L. and Schneider, N. and Swayamdipta, S. and Bhatia, A. and Dyer, C. and Smith, N. A.},
  date = {2014},
  doi = {10.3115/v1/d14-1108},
  abstract = {We describe a new dependency parser for English tweets, TWEEBOPARSER. The parser builds on several contributions: new syntactic annotations for a corpus of tweets (TWEEBANK), with conventions informed by the domain; adaptations to a statistical parsing algorithm; and a new approach to exploiting out-of-domain Penn Treebank data. Our experiments show that the parser achieves over 80\% unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contribu- tions.},
  annotation = {195 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WK8Z48QL/Kong et al. - 2014 - A Dependency Parser for Tweets(2).pdf}
}

@inproceedings{konstas2012ConcepttotextGenerationDiscriminative,
  title = {Concept-to-Text {{Generation}} via {{Discriminative Reranking}}},
  booktitle = {{{ACL}}},
  author = {Konstas, I. and Lapata, M.},
  date = {2012},
  pages = {369--378},
  abstract = {This paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. A key insight in our ap-proach is to reduce the tasks of content se-lection (" what to say ") and surface realization (" how to say ") into a common parsing prob-lem. We define a probabilistic context-free grammar that describes the structure of the in-put (a corpus of database records and text de-scribing some of them) and represent it com-pactly as a weighted hypergraph. The hyper-graph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. We propose a novel decoding algorithm for finding the best scoring derivation and generating in this set-ting. Experimental evaluation on the ATIS do-main shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W6IHWRFP/Konstas, Lapata - 2012 - Concept-to-text Generation via Discriminative Reranking(2).pdf},
  isbn = {978-1-937284-24-4},
  issue = {July}
}

@article{konstas2013GlobalModelConcepttotext,
  title = {A Global Model for Concept-to-Text Generation},
  author = {Konstas, I. and Lapata, M.},
  date = {2013},
  journaltitle = {JAIR},
  volume = {48},
  pages = {305--346},
  issn = {10769757},
  doi = {10.1613/jair.4025},
  abstract = {Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. We present a joint model that captures content selection (" what to say ") and surface realization (" how to say ") in an unsupervised domain-independent fashion. Rather than breaking up the generation process into a sequence of local decisions, we define a probabilis-tic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). We recast generation as the task of finding the best derivation tree for a set of database records and describe an algorithm for decoding in this framework that allows to intersect the grammar with additional information capturing fluency and syntactic well-formedness constraints. Experimental evaluation on several domains achieves re-sults competitive with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.},
  annotation = {78 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D6M2SF2Y/Konstas, Lapata - 2013 - A global model for concept-to-text generation(2).pdf}
}

@inproceedings{korattikara2015BayesianDarkKnowledge,
  title = {Bayesian {{Dark Knowledge}}},
  booktitle = {{{NeurIPS}}},
  author = {Korattikara, A. and Rathod, V. and Murphy, K. and Welling, M.},
  date = {2015},
  pages = {1--9},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/899BQBWB/Korattikara et al. - 2015 - Bayesian Dark Knowledge(2).pdf}
}

@inproceedings{kornblith2019SimilarityNeuralNetwork,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  booktitle = {{{ICML}}},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  date = {2019-07-19},
  url = {http://arxiv.org/abs/1905.00414},
  urldate = {2020-10-21},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  annotation = {132 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1905.00414},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MIXBQQKL/Kornblith et al. - 2019 - Similarity of Neural Network Representations Revis.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{kottur2017NaturalLanguageDoes,
  title = {Natural {{Language Does Not Emerge}} '{{Naturally}}' in {{Multi}}-{{Agent Dialog}}},
  booktitle = {{{EMNLP}}},
  author = {Kottur, S. and Moura, J. M. F. and Lee, S. and Batra, D.},
  date = {2017},
  url = {http://arxiv.org/abs/1706.08502},
  abstract = {A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.},
  annotation = {109 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1706.08502},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M4GUG746/Kottur et al. - 2017 - Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog(2).pdf},
  keywords = {u}
}

@inproceedings{kovatchev2019QualitativeEvaluationFramework,
  title = {A Qualitative Evaluation Framework for Paraphrase Identification},
  booktitle = {Recent {{Advances}} in {{Natural Language Processing}}},
  author = {Kovatchev, V. and Antònia Martí, M. and Salamó, M. and Beltran, J.},
  date = {2019},
  pages = {568--577},
  issn = {13138502},
  doi = {10.26615/978-954-452-056-4_067},
  abstract = {In this paper, we present a new approach for the evaluation, error analysis, and interpretation of supervised and unsupervised Paraphrase Identification (PI) systems. Our evaluation framework makes use of a PI corpus annotated with linguistic phenomena to provide a better understanding and interpretation of the performance of various PI systems. Our approach allows for a qualitative evaluation and comparison of the PI models using human interpretable categories. It does not require modification of the training objective of the systems and does not place additional burden on the developers. We replicate several popular supervised and unsupervised PI systems. Using our evaluation framework we show that: 1) Each system performs differently with respect to a set of linguistic phenomena and makes qualitatively different kinds of errors; 2) Some linguistic phenomena are more challenging than others across all systems.},
  annotation = {2 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IMC4KLUZ/Kovatchev et al. - 2019 - A qualitative evaluation framework for paraphrase identification(2).pdf},
  isbn = {978-954-452-055-7}
}

@inproceedings{krifka2007ApproximateInterpretationsNumber,
  title = {Approximate {{Interpretations}} of {{Number Words}}: {{A}} Case for Strategic Communication},
  booktitle = {Cognitive Foundations of Interpretation},
  author = {Krifka, M.},
  date = {2007},
  pages = {1--16},
  abstract = {This paper gives an explanation of the well-known phenomenon that round numbers in measure terms (like one hundred meters) are interpreted in a more approximate way than non-round numbers (like one hundred and three meters). Several possible explanations are considered: First, a preference for short expressions and approximate interpretations; second, a conditional preference for short expressions under approximate interpretations; third, an explanation in terms of strategic communication that makes use of the fact that approximate interpretations, even if not favored initially, turn out to be more likely once the probability of the reported values are factored in. These explanations are shown to be flawed, in particular because the complexity of expressions does not always matter. The theory that is put forward makes use of scales that differ insofar as they are more or less fine-grained, and proposes a principle that a number expression is interpreted on the most coarse-grained scale that it occurs on. This principle can be motivated by strategic communication that factors in the overall likelihood of the message. The emerging theory is refined in various ways. In particular, it will be shown that complexity of expressions is important after all, but mainly on the evolutionary level, where it can be shown to lead to characteristic patterns of language change. The paper ends with the discussion of some surprising facts about the influence that the number system of a language has on which numbers are actually expressed in that language.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NAUPGI3I/Krifka - 2007 - Approximate Interpretations of Number Words A case for strategic communication(2).pdf}
}

@inproceedings{krishnamurthy2012WeaklySupervisedTraining,
  title = {Weakly {{Supervised Training}} of {{Semantic Parsers}}},
  booktitle = {{{EMNLP}}},
  author = {Krishnamurthy, J. and Mitchell, T. M.},
  date = {2012},
  pages = {754--765},
  abstract = {We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependency- parsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-the- art accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80\% precision and 56\% recall, despite never having seen an annotated logical form.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D6U7DLBG/Krishnamurthy, Mitchell - 2012 - Weakly Supervised Training of Semantic Parsers(2).pdf},
  isbn = {978-1-937284-43-5},
  issue = {July},
  keywords = {u}
}

@inproceedings{krishnamurthy2013JointlyLearningParse,
  title = {Jointly {{Learning}} to {{Parse}} and {{Perceive}}: {{Connecting Natural Language}} to the {{Physical World}}},
  booktitle = {{{ACL}}},
  author = {Krishnamurthy, J. and Kollar, T.},
  date = {2013},
  volume = {1},
  pages = {193--206},
  issn = {2307-387X},
  abstract = {This paper introduces Logical Semantics with Perception (LSP), a model for grounded lan- guage acquisition that learns to map natu- ral language statements to their referents in a physical environment. For example, given an image, LSP can map the statement “blue mug on the table” to the set of image seg- ments showing blue mugs on tables. LSP learns physical representations for both cate- gorical (“blue,” “mug”) and relational (“on”) language, and also learns to compose these representations to produce the referents of en- tire statements. We further introduce a weakly supervised training procedure that estimates LSP's parameters using annotated referents for entire statements, without annotated ref- erents for individual words or the parse struc- ture of the statement. We perform experiments on two applications: scene understanding and geographical question answering. We find that LSP outperforms existing, less expressive models that cannot represent relational lan- guage. We further find that weakly supervised training is competitive with fully supervised training while requiring significantly less an- notation effort.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LZE9QR2V/Krishnamurthy, Kollar - 2013 - Jointly Learning to Parse and Perceive Connecting Natural Language to the Physical World(2).pdf}
}

@inproceedings{kshirsagar2015FramesemanticRoleLabeling,
  title = {Frame-Semantic Role Labeling with Heterogeneous Annotations},
  booktitle = {{{ACL}}-{{IJCNLP}}},
  author = {Kshirsagar, M. and Thomson, S. and Schneider, N. and Carbonell, J. and Smith, N. A. and Dyer, C.},
  date = {2015},
  pages = {218--224},
  doi = {10.3115/v1/p15-2036},
  abstract = {We consider the task of identifying and la-beling the semantic arguments of a predi-cate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach aug-ments an existing model with features de-rived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4\% absolute in-crease in F1versus the original model.},
  annotation = {61 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F954X5QG/Kshirsagar et al. - 2015 - Frame-semantic role labeling with heterogeneous annotations(2).pdf},
  isbn = {978-1-941643-73-0}
}

@inproceedings{kumar2002MinimumBayesRiskDecoding,
  title = {Minimum {{Bayes}}-{{Risk Decoding}} for {{Statistical Machine Translation}}},
  booktitle = {{{HLT}}-{{NAACL}}},
  author = {Kumar, S. and Byrne, W.},
  date = {2002},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AVVB7844/Kumar, Byrne - 2002 - Minimum Bayes-Risk Decoding for Statistical Machine Translation(2).pdf},
  isbn = {0-00-140110-6},
  number = {0121285}
}

@inproceedings{kuncoro2017WhatRecurrentNeural,
  title = {What {{Do Recurrent Neural Network Grammars Learn About Syntax}} ?},
  booktitle = {{{EACL}}},
  author = {Kuncoro, A. and Ballesteros, M. and Kong, L. and Dyer, C. and Neubig, G. and Smith, N. A.},
  date = {2017},
  volume = {1},
  pages = {1249--1258},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MVZS5KAK/Kuncoro et al. - 2017 - What Do Recurrent Neural Network Grammars Learn About Syntax(2).pdf},
  keywords = {u}
}

@inproceedings{kuru2016CharNERCharacterLevelNamed,
  title = {{{CharNER}} : {{Character}}-{{Level Named Entity Recognition}}},
  booktitle = {{{COLING}}},
  author = {Kuru, O. and Can, O. A. and Deniz, Y.},
  date = {2016},
  pages = {911--921},
  abstract = {We describe and evaluate a character-level tagger for language-independent Named Entity Recognition (NER). Instead of words, a sentence is represented as a sequence of characters. The model consists of stacked bidirectional LSTMs which inputs characters and outputs tag probabilities for each character. These probabilities are then converted to consistent word level named entity tags using a Viterbi decoder. We are able to achieve close to state-of-the-art NER performance in seven languages with the same basic model using only labeled NER data and no hand-engineered features or other external resources like syntactic taggers or Gazetteers.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K5PFRAY4/Kuru, Can, Deniz - 2016 - CharNER Character-Level Named Entity Recognition(2).pdf}
}

@inproceedings{kushman2013UsingSemanticUnification,
  title = {Using {{Semantic Unification}} to {{Generate Regular Expressions}} from {{Natural Language}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Kushman, N. and Barzilay, R.},
  date = {2013},
  pages = {826--836},
  url = {http://www.aclweb.org/anthology/N13-1103},
  abstract = {We consider the problem of translating natural language text queries into regular expres- sions which represent their meaning. The mis- match in the level of abstraction between the natural language representation and the regu- lar expression representation make this a novel and challenging problem. However, a given regular expression can be written in many se- mantically equivalent forms, and we exploit this flexibility to facilitate translation by find- ing a form which more directly corresponds to the natural language. We evaluate our tech- nique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a state- of-the-art semantic parsing baseline, yielding a 29\% absolute improvement in accuracy.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MPPSRLKA/Kushman, Barzilay - 2013 - Using Semantic Unification to Generate Regular Expressions from Natural Language(2).pdf},
  isbn = {978-1-937284-47-3},
  issue = {June},
  keywords = {u}
}

@inproceedings{kwiatkowski2010InducingProbabilisticCCG,
  title = {Inducing {{Probabilistic CCG Grammars}} from {{Logical Form}} with {{Higher}}-{{Order Unification}}},
  booktitle = {{{EMNLP}}},
  author = {Kwiatkowski, T. and Zettlemoyer, L. S. and Goldwater, S. and Steedman, M.},
  date = {2010},
  pages = {1223--1233},
  abstract = {This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously es- timating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VEETF5NQ/Kwiatkowski et al. - 2010 - Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification(2).pdf},
  isbn = {1-932432-86-8},
  issue = {October},
  keywords = {u}
}

@inproceedings{kwiatkowski2013ScalingSemanticParsers,
  title = {Scaling {{Semantic Parsers}} with {{On}}-the-Fly {{Ontology Matching}}},
  booktitle = {{{EMNLP}}},
  author = {Kwiatkowski, T. and Choi, E. and Artzi, Y. and Zettlemoyer, L. S.},
  date = {2013},
  pages = {1545--1556},
  url = {http://www.aclweb.org/anthology/D13-1161},
  abstract = {We consider the challenge of learning seman- tic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to rep- resent in a fixed target ontology. For ex- ample, even simple phrases such as ‘daugh- ter’ and ‘number of people living in’ can- not be directly represented in Freebase, whose ontology instead encodes facts about gen- der, parenthood, and population. In this pa- per, we introduce a new semantic parsing ap- proach that learns to resolve such ontologi- cal mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logical- form meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art per- formance on two benchmark semantic parsing datasets, including a nine point accuracy im- provement on a recent Freebase QA corpus.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UG2KD72R/Kwiatkowski et al. - 2013 - Scaling Semantic Parsers with On-the-fly Ontology Matching(2).pdf},
  isbn = {978-1-937284-97-8},
  issue = {October}
}

@inproceedings{labeau2019ExperimentingPowerDivergences,
  title = {Experimenting with {{Power Divergences}} for {{Language Modeling}}},
  booktitle = {{{EMNLP}}},
  author = {Labeau, M. and Cohen, S. B.},
  date = {2019},
  pages = {4104--4114},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WTQ64CPZ/Labeau, Cohen - 2019 - Experimenting with Power Divergences for Language Modeling(2).pdf},
  keywords = {u},
  number = {2018}
}

@inproceedings{lai2017RACELargescaleReAding,
  title = {{{RACE}}: {{Large}}-Scale {{ReAding Comprehension Dataset From Examinations}}},
  booktitle = {{{EMNLP}}},
  author = {Lai, G. and Xie, Q. and Liu, H. and Yang, Y. and Hovy, E.},
  date = {2017},
  pages = {785--794},
  doi = {10.18653/v1/d17-1082},
  abstract = {We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43\%) and the ceiling human performance (95\%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/\textasciitilde glai1/data/race/ and the code is available at https://github.com/qizhex/RACE\_AR\_baselines.},
  annotation = {394 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.04683},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B5MIA5BK/Lai et al. - 2017 - RACE Large-scale ReAding Comprehension Dataset From Examinations(2).pdf},
  isbn = {978-1-945626-83-8}
}

@article{lake2015HumanlevelConceptLearning,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, B. M. and Salakhutdinov, R. and Tenenbaum, J. B.},
  date = {2015},
  journaltitle = {Science},
  volume = {350},
  pages = {1332--1338},
  issn = {10959203},
  doi = {10.1126/science.aab3050},
  abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation.We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
  annotation = {1512 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {26659050},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9L7Y6SRK/Lake, Salakhutdinov, Tenenbaum - 2015 - Human-level concept learning through probabilistic program induction(2).pdf},
  isbn = {0036-8075},
  number = {6266}
}

@inproceedings{lakkaraju2016InterpretableDecisionSets,
  title = {Interpretable {{Decision Sets}}: {{A Joint Framework}} for {{Description}} and {{Prediction}}},
  booktitle = {{{KDD}}},
  author = {Lakkaraju, H. and Bach, S. H. and Jure, L.},
  date = {2016},
  volume = {1},
  pages = {1675--1684},
  issn = {2154-817X},
  doi = {10.1145/2939672.2939874},
  abstract = {One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.},
  annotation = {353 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {27853627},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DDFC5ELP/Lakkaraju, Bach, Jure - 2016 - Interpretable Decision Sets A Joint Framework for Description and Prediction(2).pdf},
  isbn = {2154817X (Linking)}
}

@inproceedings{lample2016NeuralArchitecturesNamed,
  title = {Neural {{Architectures}} for {{Named Entity Recognition}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
  date = {2016},
  pages = {260--270},
  publisher = {{Association for Computational Linguistics}},
  location = {{San Diego, California}},
  doi = {10.18653/v1/N16-1030},
  url = {http://aclweb.org/anthology/N16-1030},
  urldate = {2020-11-21},
  annotation = {2224 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LCFPWBDY/Lample et al. - 2016 - Neural Architectures for Named Entity Recognition.pdf},
  langid = {english}
}

@inproceedings{lample2018PhraseBasedNeuralUnsupervised,
  title = {Phrase-{{Based}} \& {{Neural Unsupervised Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Lample, G. and Ott, M. and Conneau, A. and Denoyer, L. and Ranzato, M.},
  date = {2018},
  issn = {1532-8422},
  doi = {10.1053/j.jvca.2010.06.032},
  url = {http://arxiv.org/abs/1804.07755},
  abstract = {Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT'14 English-French and WMT'16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.},
  annotation = {42 citations (Semantic Scholar/DOI) [2021-03-26] 383 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {20829068},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GM49DQY7/Lample et al. - 2018 - Phrase-Based & Neural Unsupervised Machine Translation(2).pdf},
  isbn = {0892-0915 (Print)\textbackslash r0892-0915 (Linking)},
  keywords = {u}
}

@inproceedings{lample2018UnsupervisedMachineTranslation,
  title = {Unsupervised {{Machine Translation Using Monolingual Corpora Only}}},
  booktitle = {{{ICLR}}},
  author = {Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  date = {2018-04-13},
  url = {http://arxiv.org/abs/1711.00043},
  urldate = {2020-12-01},
  abstract = {Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.},
  annotation = {582 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1711.00043},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7898VADW/Lample et al. - 2018 - Unsupervised Machine Translation Using Monolingual.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{lapata2003ProbabilisticTextStructuring,
  title = {Probabilistic Text Structuring: Experiments with Sentence Ordering},
  booktitle = {{{ACL}}},
  author = {Lapata, M.},
  date = {2003},
  pages = {545--552},
  doi = {10.3115/1075096.1075165},
  url = {http://portal.acm.org/citation.cfm?id=1075165},
  abstract = {Ordering information is a critical task for natural language generation applications. In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation. We describe a model that learns constraints on sentence order from a corpus of domain-specific texts and an algorithm that yields the most likely order among several alternatives. We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model's task. We also assess the appropriateness of such a model for multidocument summarization.},
  annotation = {292 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZI93RWDN/Lapata - 2003 - Probabilistic text structuring experiments with sentence ordering(2).pdf}
}

@inproceedings{law2019LorentzianDistanceLearning,
  title = {Lorentzian {{Distance Learning}} for {{Hyperbolic Representations}}},
  booktitle = {{{ICML}}},
  author = {Law, M. and Liao, R. and Snell, J. and Zemel, R.},
  date = {2019},
  volume = {97},
  pages = {3672--3681},
  url = {http://proceedings.mlr.press/v97/law19a.html},
  abstract = {We introduce an approach to learn representations based on the Lorentzian distance in hyperbolic geometry. Hyperbolic geometry is especially suited to hierarchically-structured datasets, which are prevalent in the real world. Current hyperbolic representation learning methods compare examples with the Poincaré distance. They try to minimize the distance of each node in a hierarchy with its descendants while maximizing its distance with other nodes. This formulation produces node representations close to the centroid of their descendants. To obtain efficient and interpretable algorithms, we exploit the fact that the centroid w.r.t the squared Lorentzian distance can be written in closed-form. We show that the Euclidean norm of such a centroid decreases as the curvature of the hyperbolic space decreases. This property makes it appropriate to represent hierarchies where parent nodes minimize the distances to their descendants and have smaller Euclidean norm than their children. Our approach obtains state-of-the-art results in retrieval and classification tasks on different datasets.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/56JVA7E8/Law et al. - 2019 - Lorentzian Distance Learning for Hyperbolic Representations(2).pdf},
  keywords = {u}
}

@article{lawrence2020ArgumentMiningSurvey,
  title = {Argument {{Mining}}: {{A Survey}}},
  shorttitle = {Argument {{Mining}}},
  author = {Lawrence, John and Reed, Chris},
  date = {2020},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {45},
  pages = {765--818},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00364},
  abstract = {Argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. Understanding argumentative structure makes it possible to determine not only what positions people are adopting, but also why they hold the opinions they do, providing valuable insights in domains as diverse as financial market prediction and public relations. This survey explores the techniques that establish the foundations for argument mining, provides a review of recent advances in argument mining techniques, and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.},
  annotation = {43 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MS5XDNX2/Lawrence and Reed - 2020 - Argument Mining A Survey.pdf},
  langid = {english},
  number = {4}
}

@article{lazer2009ComputationalSocialScience,
  title = {Computational {{Social Science}}},
  author = {Lazer, D. and Pentland, A. and Adamic, L. and Aral, S. and Barabási, A. and Brewer, D. and Christakis, N. and Contractor, N. and Fowler, J. and Gutmann, M. and Jebara, T. and King, G. and Macy, M. and Roy, D. and Van Alstyne, M.},
  date = {2009},
  journaltitle = {Science},
  volume = {323},
  issn = {10015728},
  doi = {10.13618/j.issn.1001-5728.2014.06.012},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2I2FFK32/Lazer et al. - 2009 - Computational Social Science(2).pdf},
  isbn = {0265-1491},
  issue = {February},
  keywords = {u}
}

@inproceedings{lebret2016NeuralTextGeneration,
  title = {Neural {{Text Generation}} from {{Structured Data}} with {{Application}} to the {{Biography Domain}}},
  booktitle = {{{EMNLP}}},
  author = {Lebret, R. and Grangier, D. and Auli, M.},
  date = {2016},
  doi = {10.18653/v1/D16-1128},
  url = {http://arxiv.org/abs/1603.07771},
  abstract = {This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.},
  annotation = {220 citations (Semantic Scholar/DOI) [2021-03-26] 24 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1603.07771},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3I943P6R/Lebret, Grangier, Auli - 2016 - Neural Text Generation from Structured Data with Application to the Biography Domain(2).pdf},
  keywords = {u}
}

@inproceedings{lee2004SupervisedWordSense,
  title = {Supervised Word Sense Disambiguation with Support Vector Machines and Multiple Knowledge Sources},
  booktitle = {{{ACL}}},
  author = {Lee, Y. K. and Ng, H. T. and Chia, T. K.},
  date = {2004},
  pages = {137--140},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2PXJYXR9/Lee, Ng, Chia - 2004 - Supervised word sense disambiguation with support vector machines and multiple knowledge sources(2).pdf},
  issue = {July}
}

@inproceedings{lee2016GlobalNeuralCCG,
  title = {Global {{Neural CCG Parsing}} with {{Optimality Guarantees}}},
  booktitle = {{{EMNLP}}},
  author = {Lee, K. and Lewis, M. and Zettlemoyer, L. S.},
  date = {2016},
  pages = {2366--2376},
  doi = {10.18653/v1/d16-1262},
  abstract = {We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9\% of held-out sentences, exploring on average only 190 subtrees.},
  annotation = {29 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1607.01432},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BTSJYGXI/Lee, Lewis, Zettlemoyer - 2016 - Global Neural CCG Parsing with Optimality Guarantees(2).pdf}
}

@inproceedings{lee2018DeterministicNonAutoregressiveNeural,
  title = {Deterministic {{Non}}-{{Autoregressive Neural Sequence Modeling}} by {{Iterative Refinement}}},
  booktitle = {{{EMNLP}}},
  author = {Lee, J. and Mansimov, E. and Cho, K.},
  date = {2018},
  pages = {1173--1182},
  doi = {10.18653/v1/d18-1149},
  abstract = {We propose a conditional non-autoregressive neural sequence model based on iterative refinement. The proposed model is designed based on the principles of latent variable models and denoising autoencoders, and is generally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En-De and En-Ro) and image caption generation, and observe that it significantly speeds up decoding while maintaining the generation quality comparable to the autoregressive counterpart.},
  annotation = {172 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1802.06901},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PXNQXW3N/Lee, Mansimov, Cho - 2018 - Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement(2).pdf},
  keywords = {u}
}

@inproceedings{lee2018HigherorderCoreferenceResolution,
  title = {Higher-Order {{Coreference Resolution}} with {{Coarse}}-to-Fine {{Inference}}},
  booktitle = {{{NAACL}}},
  author = {Lee, Kenton and He, Luheng and Zettlemoyer, Luke},
  date = {2018-04-15},
  url = {http://arxiv.org/abs/1804.05392},
  urldate = {2021-03-10},
  abstract = {We introduce a fully differentiable approximation to higher-order inference for coreference resolution. Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations. This enables the model to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting accuracy. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient.},
  annotation = {161 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.05392},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KLLG2F4U/Lee et al. - 2018 - Higher-order Coreference Resolution with Coarse-to.pdf;/home/hiaoxui/.local/share/zotero_files/storage/EWFMF4JC/1804.html}
}

@inproceedings{lee2019LatentRetrievalWeakly,
  title = {Latent {{Retrieval}} for {{Weakly Supervised Open Domain Question Answering}}},
  booktitle = {{{ACL}}},
  author = {Lee, K. and Chang, M. and Toutanova, K.},
  date = {2019},
  pages = {6086--6096},
  doi = {10.18653/v1/p19-1612},
  abstract = {Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.},
  annotation = {148 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1906.00300},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DF5HZNP3/Lee, Chang, Toutanova - 2019 - Latent Retrieval for Weakly Supervised Open Domain Question Answering(2).pdf},
  keywords = {u}
}

@inproceedings{lee2019SetTransformerFramework,
  title = {Set {{Transformer}}: {{A Framework}} for {{Attention}}-Based {{Permutation}}-{{Invariant Neural Networks}}},
  booktitle = {{{ICML}}},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R and Choi, Seungjin and Teh, Yee Whye},
  date = {2019},
  pages = {10},
  abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition and fewshot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D8KRXRVA/Lee et al. - Set Transformer A Framework for Attention-based P.pdf},
  langid = {english}
}

@report{lee2019WideNeuralNetworks,
  title = {Wide {{Neural Networks}} of {{Any Depth Evolve}} as {{Linear Models Under Gradient Descent}}},
  author = {Lee, J. and Xiao, L. and Schoenholz, S. S. and Bahri, Y. and Sohl-Dickstein, J. and Pennington, J.},
  date = {2019},
  url = {http://arxiv.org/abs/1902.06720},
  abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
  annotation = {294 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1902.06720},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SQ4CA5UM/Lee et al. - 2019 - Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent(2).pdf},
  keywords = {u}
}

@inproceedings{lei2015MoldingCNNsText,
  title = {Molding {{CNNs}} for Text: Non-Linear, Non-Consecutive Convolutions},
  booktitle = {{{EMNLP}}},
  author = {Lei, T. and Barzilay, R. and Jaakkola, T. S.},
  date = {2015},
  pages = {1565--1575},
  url = {http://arxiv.org/abs/1508.04112},
  abstract = {The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2\% accuracy on the fine-grained sentiment classification task.},
  annotation = {123 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1508.04112},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R8Q5AVMZ/Lei, Barzilay, Jaakkola - 2015 - Molding CNNs for text non-linear, non-consecutive convolutions(2).pdf},
  isbn = {978-1-941643-32-7},
  issue = {September}
}

@thesis{lei2017InterpretableNeuralModels,
  title = {Interpretable {{Neural Models}} for {{Natural Language Processing}}},
  author = {Lei, T.},
  date = {2017},
  journaltitle = {Massachusetts Institute of Technology},
  abstract = {Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2017.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TD6AKJ95/Lei - 2017 - Interpretable Neural Models for Natural Language Processing(2).pdf}
}

@inproceedings{levesque2013OurBestBehaviour,
  title = {On Our Best Behaviour.},
  booktitle = {{{IJCAI}}},
  author = {Levesque, H. J.},
  date = {2013},
  issn = {0035-8797},
  eprint = {966205},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q2QPY5CS/Levesque - 2013 - On our best behaviour(2).pdf}
}

@inproceedings{levy2014DependencyBasedWordEmbeddings,
  title = {Dependency-{{Based Word Embeddings}}},
  booktitle = {{{ACL}}},
  author = {Levy, O. and Goldberg, Y.},
  date = {2014},
  pages = {302--308},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QPPWW98I/Levy, Goldberg - 2014 - Dependency-Based Word Embeddings(2).pdf},
  keywords = {u}
}

@inproceedings{levy2018LongShortTermMemory,
  title = {Long {{Short}}-{{Term Memory As}} a {{Dynamically Computed Element}}-{{Wise Weighted Sum}}},
  booktitle = {{{ACL}}},
  author = {Levy, O. and Lee, K. and FitzGerald, N. and Zettlemoyer, L. S.},
  date = {2018},
  pages = {1--9},
  abstract = {We present an alternate view to explain the success of LSTMs: the gates themselves are powerful recurrent models that provide more representational power than previ-ously appreciated. We do this by showing that much of the LSTM's architecture can be removed, producing a restricted class of RNNs where the main recurrence computes an element-wise weighted sum of context-independent functions of the inputs. Experiments on a range of challenging NLP problems demonstrate that the simplified models work as well as the original LSTMs, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients.},
  archiveprefix = {arXiv},
  eprint = {1805.03716},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CFVIXPH2/Levy et al. - 2018 - Long Short-Term Memory As a Dynamically Computed Element-Wise Weighted Sum(2).pdf}
}

@inproceedings{lewis2015JointCCGParsing,
  title = {Joint {{A}}∗{{CCG}} Parsing and Semantic Role Labeling},
  booktitle = {{{EMNLP}}},
  author = {Lewis, M. and He, L. and Zettlemoyer, L. S.},
  date = {2015},
  pages = {1444--1454},
  abstract = {Joint models of syntactic and semantic parsing have the potential to improve performance on both tasks-but to date, the best results have been achieved with pipelines. We introduce a joint model using CCG, which is motivated by the close link between CCG syntax and semantics. Semantic roles are recovered by labelling the deep dependency structures produced by the grammar. Furthermore, because CCG is lexicalized, we show it is possible to factor the parsing model over words and introduce a new A∗parsing algorithm-which we demonstrate is faster and more accurate than adaptive supertagging. Our joint model is the first to substantially improve both syntactic and semantic accuracy over a comparable pipeline, and also achieves state-of-the-art results for a nonensemble semantic role labelling model.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L57HNLJU/Lewis, He, Zettlemoyer - 2015 - Joint A∗CCG parsing and semantic role labeling(2).pdf}
}

@inproceedings{lewis2020BARTDenoisingSequencetoSequence,
  title = {{{BART}}: {{Denoising Sequence}}-to-{{Sequence Pre}}-Training for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  booktitle = {{{ACL}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  date = {2020},
  url = {http://arxiv.org/abs/1910.13461},
  urldate = {2020-09-09},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  annotation = {506 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1910.13461},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3L2TJL9Y/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf},
  langid = {english}
}

@article{li2006SketchAlgorithmEstimating,
  title = {A Sketch Algorithm for Estimating Two-Way and Multi-Way Associations},
  author = {Li, P. and Church, K. W.},
  date = {2006},
  journaltitle = {Computational Linguistics},
  volume = {33},
  pages = {305--354},
  issn = {08912017},
  doi = {10.1162/coli.2007.33.3.305},
  abstract = {We should not have to look at the entire corpus (e.g., the Web) to know if two (or more) words are strongly associated or not. One can often obtain estimates of associations from a small sample. We develop a sketch-based algorithm that constructs a contingency table for a sample. One can estimate the contingency table for the entire population using straightforward scaling. However, one can do better by taking advantage of the margins (also known as document frequencies). The proposed method cuts the errors roughly in half over Broder's sketches.},
  annotation = {53 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y6B4YSF4/Li, Church - 2006 - A sketch algorithm for estimating two-way and multi-way associations(2).pdf},
  keywords = {u},
  number = {3}
}

@inproceedings{li2009IntervalEventStream,
  title = {Interval Event Stream Processing},
  booktitle = {{{DEBS}}},
  author = {Li, M. and Mani, M. and Rundensteiner, E. A. and Wang, D. and Lin, T.},
  date = {2009},
  doi = {10.1145/1619258.1619302},
  abstract = {Event stream processing (ESP) has become increasingly important in modern applications, ranging from supply chain management to real-time intrusion detection. Existing ESP engines have focused on detecting temporal patterns from instantaneous events, that is, events with no duration. Under such a model, an event instance can only be happening "before", "after" or "at the same time as" another event instance. However, such sequential patterns are inadequate to express the complex temporal relationships in domains such as medical, finance and meteorology, where the events' durations could play an important role.},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GQ4VMFLH/Li et al. - 2009 - Interval event stream processing(2).pdf},
  isbn = {978-1-60558-665-6}
}

@inproceedings{li2014ScalingDistributedMachine,
  title = {Scaling {{Distributed Machine Learning}} with the {{Parameter Server}}},
  booktitle = {Symposium on {{Operating Systems Design}} and {{Implementation}}},
  author = {Li, M. and Andersen, D. G. and Park, J. W. and Ahmed, A. and Josifovski, V. and Long, J. and Shekita, E. J. and Su, B.},
  date = {2014},
  pages = {583--598},
  url = {http://www.mzi.gov.si/si/medijsko_sredisce/novica/article/799/8867/},
  abstract = {We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters, represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance. To demonstrate the scalability of the proposed frame-work, we show experimental results on petabytes of real data with billions of examples and parameters on prob-lems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/73JZUD3G/Li et al. - 2014 - Scaling Distributed Machine Learning with the Parameter Server(2).pdf},
  isbn = {978-1-931971-16-4}
}

@inproceedings{li2015FastAccuratePrediction,
  title = {Fast and {{Accurate Prediction}} of {{Sentence Specificity}}},
  booktitle = {{{AAAI}}},
  author = {Li, J. J. and Nenkova, A.},
  date = {2015},
  pages = {2281--2287},
  issn = {1895104X},
  doi = {10.2478/s11535-006-0039-x},
  abstract = {Soybean kernels of cultivars Bosa and ZPS 015 were used in the experiment. The contents of available lysine as well as water and salt soluble proteins, were analysed in fresh soybean kernels, soybean products made after the processes of dry extrusion, micronisation, microwave toasting and autoclaving. Utilizing a technological procedure of processing, kernels were exposed to temperatures from 57 to 150°C. The duration of exposure of the soybean kernels to the increased temperatures, ranged from 25-30 seconds in dry extrusion to 30 minutes in autoclaving. All treatments were subjected to different sources of heat, causing different thermodynamic processes to take place in kernels and change their chemical composition; i.e. nutritive quality. The content of water and salt soluble proteins decreased under the influence of higher temperatures in the course of all treatments of processing. The drop of solubility already was drastically effected by temperatures of 100°C in dry extrusion, while there was a gradual decrease in other treatments. The content of available lysine was determined by the modified Carpenter methods with DNFB. The processes of micronisation and microwave toasting showed the greatest effect on the reduction of lysine availability. Dry extrusion and autoclaving, performed within closed systems — in which the increased moisture content has a special effect — resulted in significantly smaller changes of the available lysine content.},
  annotation = {26 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YBCDTYZ3/Li, Nenkova - 2015 - Fast and Accurate Prediction of Sentence Specificity(2).pdf},
  isbn = {978-1-57735-701-8},
  keywords = {u}
}

@inproceedings{li2015HierarchicalNeuralAutoencoder,
  title = {A {{Hierarchical Neural Autoencoder}} for {{Paragraphs}} and {{Documents}}},
  booktitle = {{{ACL}}},
  author = {Li, J. and Luong, M. and Jurafsky, D.},
  date = {2015},
  url = {http://arxiv.org/abs/1506.01057},
  abstract = {Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization\textbackslash footnote\{Code for the three models described in this paper can be found at www.stanford.edu/\textasciitilde jiweil/ .\vphantom\}},
  annotation = {482 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1506.01057},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/24IT72DF/Li, Luong, Jurafsky - 2015 - A Hierarchical Neural Autoencoder for Paragraphs and Documents(2).pdf},
  keywords = {u}
}

@inproceedings{li2016DeepReinforcementLearning,
  title = {Deep {{Reinforcement Learning}} for {{Dialogue Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Li, J. and Monroe, W. and Ritter, A. and Galley, M. and Gao, J. and Jurafsky, D.},
  date = {2016},
  url = {http://arxiv.org/abs/1606.01541},
  abstract = {Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.},
  annotation = {808 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1606.01541},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y9J3KZST/Li et al. - 2016 - Deep Reinforcement Learning for Dialogue Generation(2).pdf},
  number = {4}
}

@inproceedings{li2016PersonaBasedNeuralConversation,
  title = {A {{Persona}}-{{Based Neural Conversation Model}}},
  booktitle = {{{ACL}}},
  author = {Li, J. and Galley, M. and Brockett, C. and Spithourakis, G. P. and Gao, J. and Dolan, B.},
  date = {2016},
  url = {http://arxiv.org/abs/1603.06155},
  abstract = {We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.},
  annotation = {646 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1603.06155},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RQALIXNZ/Li et al. - 2016 - A Persona-Based Neural Conversation Model(2).pdf},
  isbn = {978-1-5108-2758-5},
  keywords = {u}
}

@inproceedings{li2018LearningIncompleteLabels,
  title = {Learning with Incomplete Labels},
  booktitle = {{{AAAI}}},
  author = {Li, Y. and Xu, Z. and Zhang, Z.},
  date = {2018},
  pages = {3588--3595},
  abstract = {For many real-world tagging problems, training labels are usually obtained through social tagging and are notoriously incomplete. Consequently, handling data with incomplete labels has become a difficult challenge, which usually leads to a degenerated performance on label prediction. To improve the generalization performance, in this paper, we first propose the Improved Cross-View learning (referred as ICVL) model, which considers both global and local patterns of label relationship to enrich the original label set. Further, by extending the ICVL model with an outlier detection mechanism, we introduce the Improved Cross-View learning with Outlier Detection (referred as ICVL-OD) model to remove the abnormal tags resulting from label enrichment. Extensive evaluations on three benchmark datasets demonstrate that ICVL and ICVL-OD outstand with superior performances in comparison with the competing methods.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GDK3AP5A/Li, Xu, Zhang - 2018 - Learning with incomplete labels(2).pdf},
  isbn = {978-1-57735-800-8}
}

@inproceedings{li2018VisualizingLossLandscape,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  booktitle = {{{NeurIPS}}},
  author = {Li, H. and Xu, Z. and Taylor, G. and Studer, C. and Goldstein, T.},
  date = {2018},
  url = {http://arxiv.org/abs/1712.09913},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comp arisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  annotation = {479 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1712.09913},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KVE3A82C/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets(2).pdf}
}

@inproceedings{li2019CNMInterpretableComplexvalued,
  title = {{{CNM}}: {{An Interpretable Complex}}-Valued {{Network}} for {{Matching}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Li, Q. and Wang, B. and Melucci, M.},
  date = {2019},
  url = {http://arxiv.org/abs/1904.05298},
  abstract = {This paper seeks to model human language by the mathematical framework of quantum physics. With the well-designed mathematical formulations in quantum physics, this framework unifies different linguistic units in a single complex-valued vector space, e.g. words as particles in quantum states and sentences as mixed systems. A complex-valued network is built to implement this framework for semantic matching. With well-constrained complex-valued components, the network admits interpretations to explicit physical meanings. The proposed complex-valued network for matching (CNM) achieves comparable performances to strong CNN and RNN baselines on two benchmarking question answering (QA) datasets.},
  annotation = {17 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.05298},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/364Y2KZ4/Li, Wang, Melucci - 2019 - CNM An Interpretable Complex-valued Network for Matching(2).pdf}
}

@inproceedings{li2019DependencySpanEndtoEnd,
  title = {Dependency or {{Span}}, {{End}}-to-{{End Uniform Semantic Role Labeling}}},
  booktitle = {{{AAAI}}},
  author = {Li, Zuchao and He, Shexia and Zhao, Hai and Zhang, Yiqing and Zhang, Zhuosheng and Zhou, Xi and Zhou, Xiang},
  date = {2019-07-17},
  volume = {33},
  pages = {6730--6737},
  doi = {10.1609/aaai.v33i01.33016730},
  abstract = {Semantic role labeling (SRL) aims to discover the predicateargument structure of a sentence. End-to-end SRL without syntactic input has received great attention. However, most of them focus on either span-based or dependency-based semantic representation form and only show specific model optimization respectively. Meanwhile, handling these two SRL tasks uniformly was less successful. This paper presents an end-to-end model for both dependency and span SRL with a unified argument representation to deal with two different types of argument annotations in a uniform fashion. Furthermore, we jointly predict all predicates and arguments, especially including long-term ignored predicate identification subtask. Our single model achieves new state-of-the-art results on both span (CoNLL 2005, 2012) and dependency (CoNLL 2008, 2009) SRL benchmarks.},
  annotation = {49 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KFDXNWTE/Li et al. - 2019 - Dependency or Span, End-to-End Uniform Semantic Ro.pdf},
  langid = {english}
}

@article{li2019GenerativeModelPunctuation,
  title = {A {{Generative Model}} for {{Punctuation}} in {{Dependency Trees}}},
  author = {Li, X. L. and Wang, D. and Eisner, J. M.},
  date = {2019},
  journaltitle = {TACL},
  volume = {7},
  pages = {357--373},
  doi = {10.1162/tacl_a_00273},
  abstract = {Treebanks traditionally treat punctuation marks as ordinary words, but linguists have suggested that a tree’s “true” punctuation marks are not observed (Nunberg, 1990). These latent “underlying” marks serve to delimit or separate constituents in the syntax tree. When the tree’s yield is rendered as a written sentence, a string rewriting mechanism transduces the underlying marks into “surface” marks, which are part of the observed (surface) string but should not be regarded as part of the tree. We formalize this idea in a generative model of punctuation that admits efficient dynamic programming. We train it without observing the underlying marks, by locally maximizing the incomplete data likelihood (similarly to the EM algorithm). When we use the trained model to reconstruct the tree’s underlying punctuation, the results appear plausible across 5 languages, and in particular are consistent with Nunberg’s analysis of English. We show that our generative model can be used to beat baselines on punctuation restoration. Also, our reconstruction of a sentence’s underlying punctuation lets us appropriately render the surface punctuation (via our trained underlying-to-surface mechanism) when we syntactically transform the sentence.},
  annotation = {2 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1906.11298v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KWHWFMXF/Li, Wang, Eisner - 2019 - A Generative Model for Punctuation in Dependency Trees(2).pdf}
}

@report{li2019PosteriorControlBlackbox,
  title = {Posterior {{Control}} of {{Blackbox Generation}}},
  author = {Li, L. X. and Rush, A. M.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HVLXVLFT/Li, Rush - 2019 - Posterior Control of Blackbox Generation(2).pdf},
  keywords = {u}
}

@inproceedings{li2019SpecializingWordEmbeddings,
  title = {Specializing {{Word Embeddings}} ( for {{Parsing}} ) by {{Information Bottleneck}}},
  booktitle = {{{EMNLP}}-{{IJCNLP}}},
  author = {Li, X. L. and Eisner, J. M.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QUJDBMKM/Li, Eisner - 2019 - Specializing Word Embeddings ( for Parsing ) by Information Bottleneck(2).pdf}
}

@article{li2019TutorialDirichletProcess,
  title = {A Tutorial on {{Dirichlet}} Process Mixture Modeling},
  author = {Li, Y. and Schofield, E. and Gönen, M.},
  date = {2019},
  journaltitle = {Journal of Mathematical Psychology},
  volume = {91},
  pages = {128--144},
  publisher = {{Elsevier Inc.}},
  issn = {10960880},
  doi = {10.1016/j.jmp.2019.04.004},
  abstract = {Bayesian nonparametric (BNP) models are becoming increasingly important in psychology, both as theoretical models of cognition and as analytic tools. However, existing tutorials tend to be at a level of abstraction largely impenetrable by non-technicians. This tutorial aims to help beginners understand key concepts by working through important but often omitted derivations carefully and explicitly, with a focus on linking the mathematics with a practical computation solution for a Dirichlet Process Mixture Model (DPMM)—one of the most widely used BNP methods. Abstract concepts are made explicit and concrete to non-technical readers by working through the theory that gives rise to them. A publicly accessible computer program written in the statistical language R is explained line-by-line to help readers understand the computation algorithm. The algorithm is also linked to a construction method called the Chinese Restaurant Process in an accessible tutorial in this journal (Gershman and Blei, 2012). The overall goals are to help readers understand more fully the theory and application so that they may apply BNP methods in their own work and leverage the technical details in this tutorial to develop novel methods.},
  annotation = {5 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A58ACDX6/Li, Schofield, Gönen - 2019 - A tutorial on Dirichlet process mixture modeling(2).pdf}
}

@inproceedings{li2020DiceLossDataimbalanced,
  title = {Dice {{Loss}} for {{Data}}-Imbalanced {{NLP Tasks}}},
  booktitle = {{{ACL}}},
  author = {Li, X. and Sun, X. and Meng, Y. and Liang, J. and Wu, F. and Li, J.},
  date = {2020},
  url = {http://arxiv.org/abs/1911.02855},
  abstract = {Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of background examples (or easy-negative examples) overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sorensen-Dice coefficient or Tversky index, which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples.Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.},
  annotation = {17 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1911.02855},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8YP4V27T/Li et al. - 2020 - Dice Loss for Data-imbalanced NLP Tasks(2).pdf},
  keywords = {u}
}

@inproceedings{li2020EfficientRuemannianOptimization,
  title = {Efficient {{Ruemannian Optimization}} on the {{Stiefel Manifold}} via the {{Cayley Transform}}},
  booktitle = {{{ICLR}}},
  author = {Li, J. and Fuxin, L. and Todorovic, S.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z565T75I/Li, Fuxin, Todorovic - 2020 - Efficient Ruemannian Optimization on the Stiefel Manifold via the Cayley Transform(2).pdf},
  keywords = {u}
}

@report{li2020EmpiricalAnalysisUnlabeled,
  title = {Empirical {{Analysis}} of {{Unlabeled Entity Problem}} in {{Named Entity Recognition}}},
  author = {Li, Yangming and Liu, Lemao and Shi, Shuming},
  date = {2020-12-13},
  url = {http://arxiv.org/abs/2012.05426},
  urldate = {2020-12-30},
  abstract = {In many scenarios, named entity recognition (NER) models severely suffer from unlabeled entity problem, where the entities of a sentence may not be fully annotated. Through empirical studies performed on synthetic datasets, we find two causes of the performance degradation. One is the reduction of annotated entities and the other is treating unlabeled entities as negative instances. The first cause has less impact than the second one and can be mitigated by adopting pretraining language models. The second cause seriously misguides a model in training and greatly affects its performances. Based on the above observations, we propose a general approach that is capable of eliminating the misguidance brought by unlabeled entities. The core idea is using negative sampling to keep the probability of training with unlabeled entities at a very low level. Experiments on synthetic datasets and real-world datasets show that our model is robust to unlabeled entity problem and surpasses prior baselines. On well-annotated datasets, our model is competitive with state-of-the-art method.},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2012.05426},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JBZ7YRPV/Li et al. - 2020 - Empirical Analysis of Unlabeled Entity Problem in .pdf;/home/hiaoxui/.local/share/zotero_files/storage/JCP5J6E8/2012.html}
}

@inproceedings{li2020UnsupervisedCrosslingualAdaptation,
  title = {Unsupervised {{Cross}}-Lingual {{Adaptation}} for {{Sequence Tagging}} and {{Beyond}}},
  booktitle = {{{EMNLP}}},
  author = {Li, Xin and Bing, Lidong and Zhang, Wenxuan and Li, Zheng and Lam, Wai},
  date = {2020-10-23},
  url = {http://arxiv.org/abs/2010.12405},
  urldate = {2020-11-09},
  abstract = {Cross-lingual adaptation with multilingual pre-trained language models (mPTLMs) mainly consists of two lines of works: zero-shot approach and translation-based approach, which have been studied extensively on the sequence-level tasks. We further verify the efficacy of these cross-lingual adaptation approaches by evaluating their performances on more fine-grained sequence tagging tasks. After re-examining their strengths and drawbacks, we propose a novel framework to consolidate the zero-shot approach and the translation-based approach for better adaptation performance. Instead of simply augmenting the source data with the machine-translated data, we tailor-make a warm-up mechanism to quickly update the mPTLMs with the gradients estimated on a few translated data. Then, the adaptation approach is applied to the refined parameters and the cross-lingual transfer is performed in a warm-start way. The experimental results on nine target languages demonstrate that our method is beneficial to the cross-lingual adaptation of various sequence tagging tasks.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2010.12405},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CU95FDQP/Li et al. - 2020 - Unsupervised Cross-lingual Adaptation for Sequence.pdf;/home/hiaoxui/.local/share/zotero_files/storage/25BRLM2L/2010.html},
  keywords = {u}
}

@report{li2021PrefixTuningOptimizingContinuous,
  title = {Prefix-{{Tuning}}: {{Optimizing Continuous Prompts}} for {{Generation}}},
  shorttitle = {Prefix-{{Tuning}}},
  author = {Li, Xiang Lisa and Liang, Percy},
  date = {2021-01-01},
  url = {http://arxiv.org/abs/2101.00190},
  urldate = {2021-01-07},
  abstract = {Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\textbackslash\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2101.00190},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8SQ2B42Q/Li and Liang - 2021 - Prefix-Tuning Optimizing Continuous Prompts for G.pdf;/home/hiaoxui/.local/share/zotero_files/storage/RSYWW5BH/2101.html}
}

@inproceedings{liang2006AlignmentAgreement,
  title = {Alignment by Agreement},
  booktitle = {{{NAACL}}},
  author = {Liang, P. and Taskar, B. and Klein, D.},
  date = {2006},
  pages = {104--111},
  doi = {10.3115/1220835.1220849},
  url = {http://portal.acm.org/citation.cfm?doid=1220835.1220849},
  abstract = {We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32\% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29\% reduction in AER over symmetrized IBM model 4 predictions.},
  annotation = {487 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T5ZPHX8R/Liang, Taskar, Klein - 2006 - Alignment by agreement(2).pdf},
  issue = {June},
  keywords = {u}
}

@inproceedings{liang2008StructureCompilationTrading,
  title = {Structure {{Compilation}} : {{Trading Structure}} for {{Features}}},
  booktitle = {{{ICML}}},
  author = {Liang, P. and Daumé III, H. and Klein, D.},
  date = {2008},
  pages = {592--599},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9RKVAL7J/Liang, Daumé III, Klein - 2008 - Structure Compilation Trading Structure for Features(2).pdf},
  keywords = {u}
}

@inproceedings{liang2009LearningSemanticCorrespondences,
  title = {Learning {{Semantic Correspondences}} with {{Less Supervision}}},
  booktitle = {{{ACL}}-{{IJCNLP}}},
  author = {Liang, P. and Jordan, M. I. and Klein, D.},
  date = {2009},
  pages = {91--99},
  doi = {10.3115/1687878.1687893},
  abstract = {A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.},
  annotation = {259 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DBTGSKUH/Liang, Jordan, Klein - 2009 - Learning Semantic Correspondences with Less Supervision(2).pdf},
  isbn = {978-1-932432-45-9},
  issue = {August}
}

@inproceedings{liang2011LearningDependencyBasedCompositional,
  title = {Learning {{Dependency}}-{{Based Compositional Semantics}}},
  booktitle = {{{ACL}}},
  author = {Liang, P. and Jordan, M. I. and Klein, D.},
  date = {2011},
  volume = {39},
  pages = {389--446},
  issn = {0891-2017},
  doi = {10.1162/COLI_a_00127},
  url = {http://www.mitpressjournals.org/doi/10.1162/COLI_a_00127},
  abstract = {Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.},
  annotation = {519 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ASINTK9T/Liang, Jordan, Klein - 2011 - Learning Dependency-Based Compositional Semantics(2).pdf},
  isbn = {978-1-932432-87-9},
  number = {2}
}

@report{liang2013DependencyBasedCompositionalSemantics,
  title = {Dependency-{{Based Compositional Semantics}}},
  author = {Liang, P.},
  date = {2013},
  volume = {39},
  pages = {389--446},
  issn = {0891-2017},
  doi = {10.1162/COLI_a_00127},
  url = {http://www.mitpressjournals.org/doi/10.1162/COLI_a_00127},
  abstract = {Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BEWMXZXM/Liang - 2013 - Lambda Dependency-Based Compositional Semantics(2).pdf},
  isbn = {9781932432879},
  number = {2}
}

@report{liang2013LambdaDependencyBasedCompositional,
  title = {Lambda {{Dependency}}-{{Based Compositional Semantics}}},
  author = {Liang, P.},
  date = {2013},
  pages = {1--7},
  issn = {04194217},
  doi = {10.1162/COLI},
  url = {http://arxiv.org/abs/1309.4408},
  abstract = {This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.},
  annotation = {93 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {22251136},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AB934KHV/Liang - 2013 - Lambda Dependency-Based Compositional Semantics(2).pdf},
  isbn = {9781608459858}
}

@article{liang2014TalkingComputersNatural,
  title = {Talking to Computers in Natural Language},
  author = {Liang, P.},
  date = {2014},
  journaltitle = {Crossroads},
  volume = {21},
  pages = {18--21},
  issn = {15284972},
  doi = {10.1145/2659831},
  url = {http://dl.acm.org/citation.cfm?doid=2677339.2659831},
  abstract = {Intended for a wide circle of specialists in automated systems. Above all, however, it is intended for those who work on systems for communicating with machines.},
  entrysubtype = {magazine},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RCJQAHTS/Liang - 2014 - Talking to computers in natural language(2).pdf},
  number = {1}
}

@article{liang2015BringingMachineLearning,
  title = {Bringing {{Machine Learning}} and {{Compositional Semantics Together}}},
  author = {Liang, P. and Potts, C.},
  date = {2015},
  journaltitle = {Annual Reviews of Linguistics},
  volume = {1},
  pages = {355--376},
  issn = {2333-9683},
  doi = {10.1146/annurev-linguist-030514-125312},
  url = {http://www.annualreviews.org/doi/10.1146/annurev-linguist-030514-125312},
  abstract = {Computational semantics has long been considered a field divided between logical and statistical approaches, but this divide is rapidly eroding with the development of statistical models that learn compositional semantic theories from corpora and databases. This review presents a simple discriminative learning framework for defining such models and relating them to logical theories. Within this framework, we discuss the task of learning to map utterances to logical forms (semantic parsing) and the task of learning from denotations with logical forms as latent variables. We also consider models that use distributed (e.g., vector) representations rather than logical ones, showing that these can be considered part of the same overall framework for understanding meaning and structural complexity.},
  annotation = {62 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZWNN8DAM/Liang, Potts - 2015 - Bringing Machine Learning and Compositional Semantics Together(2).pdf},
  isbn = {2333-9683},
  number = {1}
}

@article{liang2016LearningExecutableSemantic,
  title = {Learning {{Executable Semantic Parsers}} for {{Natural Language Understanding}}},
  author = {Liang, P.},
  date = {2016},
  journaltitle = {Communications of the ACM},
  issn = {00010782},
  doi = {10.1145/2866568},
  url = {http://arxiv.org/abs/1603.06677},
  abstract = {For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are interested in learning semantic parsers from data, which introduces a new layer of statistical and computational issues. This article lays out the components of a statistical semantic parser, highlighting the key challenges. We will see that semantic parsing is a rich fusion of the logical and the statistical world, and that this fusion will play an integral role in the future of natural language understanding systems.},
  annotation = {87 citations (Semantic Scholar/DOI) [2021-03-26] 87 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1603.06677},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YRRT2GP8/Liang - 2016 - Learning Executable Semantic Parsers for Natural Language Understanding(2).pdf}
}

@article{liang2019LearningKwayDdimensional,
  title = {Learning {{K}}-Way {{D}}-Dimensional Discrete Embedding for Hierarchical Data Visualization and Retrieval},
  author = {Liang, X. and Min, M. R. and Guo, H. and Wang, G.},
  date = {2019},
  journaltitle = {IJCAI},
  pages = {2966--2972},
  issn = {10450823},
  doi = {10.24963/ijcai.2019/411},
  abstract = {Traditional embedding approaches associate a real-valued embedding vector with each symbol or data point, which is equivalent to applying a linear transformation to “one-hot” encoding of discrete symbols or data objects. Despite simplicity, these methods generate storage-inefficient representations and fail to effectively encode the internal semantic structure of data, especially when the number of symbols or data points and the dimensionality of the real-valued embedding vectors are large. In this paper, we propose a regularized autoencoder framework to learn compact Hierarchical K-way D-dimensional (HKD) discrete embedding of symbols or data points, aiming at capturing essential semantic structures of data. Experimental results on synthetic and real-world datasets show that our proposed HKD embedding can effectively reveal the semantic structure of data via hierarchical data visualization and greatly reduce the search space of nearest neighbor retrieval while preserving high accuracy.},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S4FCQGH2/Liang et al. - 2019 - Learning K-way D-dimensional discrete embedding for hierarchical data visualization and retrieval(2).pdf},
  isbn = {9780999241141}
}

@report{liao2020EfficientGraphGeneration,
  title = {Efficient {{Graph Generation}} with {{Graph Recurrent Attention Networks}}},
  author = {Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Nash, Charlie and Hamilton, William L. and Duvenaud, David and Urtasun, Raquel and Zemel, Richard S.},
  date = {2020-07-17},
  url = {http://arxiv.org/abs/1910.00760},
  urldate = {2020-10-09},
  abstract = {We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. To the best of our knowledge, GRAN is the first deep graph generative model that can scale to this size. Our code is released at: https://github.com/lrjconan/GRAN.},
  annotation = {41 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1910.00760},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4FYYYCRA/Liao et al. - 2020 - Efficient Graph Generation with Graph Recurrent At.pdf;/home/hiaoxui/.local/share/zotero_files/storage/6VJJDPMK/1910.html},
  keywords = {u}
}

@report{lifu2017ZeroShotTransferLearning,
  title = {Zero-{{Shot Transfer Learning}} for {{Event Extraction}}},
  author = {Lifu, Huang and Heng, Ji and Cho, Kyunghyun and Voss, Clare R.},
  date = {2017-07-04},
  url = {http://arxiv.org/abs/1707.01066},
  urldate = {2021-02-02},
  abstract = {Most previous event extraction studies have relied heavily on features derived from annotated event mentions, thus cannot be applied to new event types without annotation effort. In this work, we take a fresh look at event extraction and model it as a grounding problem. We design a transferable neural architecture, mapping event mentions and types jointly into a shared semantic space using structural and compositional neural networks, where the type of each event mention can be determined by the closest of all candidate types . By leveraging (1)\textasciitilde available manual annotations for a small set of existing event types and (2)\textasciitilde existing event ontologies, our framework applies to new event types without requiring additional annotation. Experiments on both existing event types (e.g., ACE, ERE) and new event types (e.g., FrameNet) demonstrate the effectiveness of our approach. \textbackslash textit\{Without any manual annotations\} for 23 new event types, our zero-shot framework achieved performance comparable to a state-of-the-art supervised model which is trained from the annotations of 500 event mentions.},
  annotation = {53 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1707.01066},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PRDN33SG/Huang et al. - 2017 - Zero-Shot Transfer Learning for Event Extraction.pdf}
}

@article{lim2016NonparametricBayesianTopic,
  title = {Nonparametric {{Bayesian}} Topic Modelling with the Hierarchical {{Pitman}}–{{Yor}} Processes},
  author = {Lim, K. W. and Buntine, W. and Chen, C. and Du, L.},
  date = {2016},
  journaltitle = {International Journal of Approximate Reasoning},
  volume = {78},
  pages = {172--191},
  issn = {0888613X},
  doi = {10.1016/j.ijar.2016.07.007},
  abstract = {The Dirichlet process and its extension, the Pitman–Yor process, are stochastic processes that take probability distributions as a parameter. These processes can be stacked up to form a hierarchical nonparametric Bayesian model. In this article, we present efficient methods for the use of these processes in this hierarchical context, and apply them to latent variable models for text analytics. In particular, we propose a general framework for designing these Bayesian models, which are called topic models in the computer science community. We then propose a specific nonparametric Bayesian topic model for modelling text from social media. We focus on tweets (posts on Twitter) in this article due to their ease of access. We find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications.},
  annotation = {24 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1609.06783},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/23U97WAV/Lim et al. - 2016 - Nonparametric Bayesian topic modelling with the hierarchical Pitman–Yor processes(2).pdf}
}

@inproceedings{lin2003AutomaticEvaluationSummaries,
  title = {Automatic Evaluation of Summaries Using {{N}}-Gram Co-Occurrence Statistics},
  booktitle = {{{NAACL}}},
  author = {Lin, C. and Hovy, E.},
  date = {2003},
  volume = {2003},
  pages = {71--78},
  doi = {10.3115/1073445.1073465},
  url = {http://portal.acm.org/citation.cfm?doid=1073445.1073465},
  abstract = {Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.},
  annotation = {1518 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M3H2DB2A/Lin, Hovy - 2003 - Automatic evaluation of summaries using N-gram co-occurrence statistics(2).pdf},
  issue = {June},
  keywords = {u}
}

@inproceedings{lin2015UnsupervisedPOSInduction,
  title = {Unsupervised {{POS Induction}} with {{Word Embeddings}}},
  booktitle = {{{NAACL}}},
  author = {Lin, C. and Ammar, W. and Dyer, C. and Levin, L.},
  date = {2015},
  pages = {1311--1316},
  url = {http://arxiv.org/abs/1503.06760},
  abstract = {Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsupervised problems has been less thoroughly explored. In this paper, we show that embeddings can likewise add value to the problem of unsupervised POS induction. In two representative models of POS induction, we replace multinomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages. We also analyze the effect of various choices while inducing word embeddings on "downstream" POS induction results.},
  annotation = {59 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1503.06760},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XAC8BHFM/Lin et al. - 2015 - Unsupervised POS Induction with Word Embeddings(2).pdf},
  isbn = {978-1-941643-49-5},
  keywords = {u}
}

@inproceedings{lin2017BilinearCNNModels,
  title = {Bilinear {{CNN Models}} for {{Fine}}-Grained {{Visual Recognition}}},
  booktitle = {{{ICCV}}},
  author = {Lin, T. and RoyChowdhury, A. and Maji, S.},
  date = {2017},
  archiveprefix = {arXiv},
  eprint = {1504.07889v3},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4VJYRM6K/Lin, RoyChowdhury, Maji - 2017 - Bilinear CNN Models for Fine-grained Visual Recognition(2).pdf}
}

@inproceedings{lin2017ListonlyEntityLinking,
  title = {List-Only {{Entity Linking}}},
  booktitle = {{{ACL}}},
  author = {Lin, Y. and Lin, C. and Ji, H.},
  date = {2017},
  pages = {536--541},
  doi = {10.18653/v1/P17-2085},
  url = {http://aclweb.org/anthology/P17-2085},
  abstract = {Traditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the KB may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call it as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which can be used as seed mentions to bridge other mentions and the uninformative entities. In this work, we select most linkable mentions as seed mentions and disambiguate other mentions by comparing them with the seed mentions rather than directly with the entities. Our experiments on linking mentions to seven automatically mined lists show promising results and demonstrate the effectiveness of our approach.},
  annotation = {13 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KYVX3SBM/Lin, Lin, Ji - 2017 - List-only Entity Linking(2).pdf}
}

@inproceedings{lin2018NeuralParticleSmoothing,
  title = {Neural {{Particle Smoothing}} for {{Sampling}} from {{Conditional Sequence Models}}},
  booktitle = {{{NAACL}}},
  author = {Lin, C. and Eisner, J. M.},
  date = {2018},
  volume = {21218},
  url = {http://arxiv.org/abs/1804.10747},
  abstract = {We introduce neural particle smoothing, a sequential Monte Carlo method for sampling annotations of an input string from a given probability model. In contrast to conventional particle filtering algorithms, we train a proposal distribution that looks ahead to the end of the input string by means of a right-to-left LSTM. We demonstrate that this innovation can improve the quality of the sample. To motivate our formal choices, we explain how our neural model and neural sampler can be viewed as low-dimensional but nonlinear approximations to working with HMMs over very large state spaces.},
  annotation = {6 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.10747},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UWDCDNNU/Lin, Eisner - 2018 - Neural Particle Smoothing for Sampling from Conditional Sequence Models(2).pdf},
  number = {3}
}

@inproceedings{lin2019HierarchicalPointerNet,
  title = {Hierarchical {{Pointer Net Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Lin, X. and Joty, S. and Han, S. and Bing, L.},
  date = {2019},
  archiveprefix = {arXiv},
  eprint = {1908.11571v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y5Z6KQVP/Lin et al. - 2019 - Hierarchical Pointer Net Parsing(2).pdf},
  keywords = {u}
}

@inproceedings{lin2019KagNetKnowledgeAwareGraph,
  title = {{{KagNet}}: {{Knowledge}}-{{Aware Graph Networks}} for {{Commonsense Reasoning}}},
  booktitle = {{{EMNLP}}},
  author = {Lin, B. Y. and Chen, X. and Chen, J. and Ren, X.},
  date = {2019},
  pages = {2829--2839},
  doi = {10.18653/v1/d19-1282},
  abstract = {Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.},
  annotation = {103 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.02151},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4C2Q6JBD/Lin et al. - 2019 - KagNet Knowledge-Aware Graph Networks for Commonsense Reasoning(2).pdf},
  keywords = {u}
}

@inproceedings{lin2019NeuralFiniteStateTransducers,
  title = {Neural {{Finite}}-{{State Transducers}}: {{Beyond Rational Relations}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Lin, C. and Zhu, H. and Gormley, M. R. and Eisner, J. M.},
  date = {2019},
  pages = {272--283},
  doi = {10.18653/v1/n19-1024},
  abstract = {We introduce neural finite state transducers (NFSTs), a family of string transduction models defining joint and conditional probability distributions over pairs of strings. The probability of a string pair is obtained by marginalizing over all its accepting paths in a finite state transducer. In contrast to ordinary weighted FSTs, however, each path is scored using an arbitrary function such as a recurrent neural network, which breaks the usual conditional independence assumption (Markov property). NFSTs are more powerful than previous finite-state models with neural features (Rastogi et al., 2016.) We present training and inference algorithms for locally and globally normalized variants of NFSTs. In experiments on different transduction tasks, they compete favorably against seq2seq models while offering interpretable paths that correspond to hard monotonic alignments.},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T7F6CXLQ/Lin et al. - 2019 - Neural Finite-State Transducers Beyond Rational Relations(2).pdf},
  keywords = {u}
}

@report{lin2019SubnormalizedSequenceModels,
  title = {Subnormalized {{Sequence Models}}},
  author = {Lin, C. and Eisner, J. M.},
  date = {2019},
  pages = {1--9},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F3IHMCAK/Lin, Eisner - 2019 - Subnormalized Sequence Models(2).pdf}
}

@inproceedings{lin2020JointNeuralModel,
  title = {A {{Joint Neural Model}} for {{Information Extraction}} with {{Global Features}}},
  booktitle = {{{ACL}}},
  author = {Lin, Ying and Ji, Heng and Huang, Fei and Wu, Lingfei},
  date = {2020},
  pages = {7999--8009},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.713},
  url = {https://www.aclweb.org/anthology/2020.acl-main.713},
  urldate = {2021-01-27},
  abstract = {Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a VICTIM of a DIE event is likely to be a VICTIM of an ATTACK event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, ONEIE, that aims to extract the globally optimal IE result as a graph from an input sentence. ONEIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state-of-the-art on all subtasks. As ONEIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner. Our code and models for English, Spanish and Chinese are publicly available for research purpose 1.},
  annotation = {19 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HZRA54KR/Lin et al. - 2020 - A Joint Neural Model for Information Extraction wi.pdf},
  keywords = {u},
  langid = {english}
}

@article{lindsten2014ParticleGibbsAncestor,
  title = {Particle {{Gibbs}} with {{Ancestor Sampling}}},
  author = {Lindsten, F. and Jordan, M. I. and Schön, T. B.},
  date = {2014},
  journaltitle = {JMLR},
  volume = {15},
  pages = {2145--2184},
  issn = {15337928},
  archiveprefix = {arXiv},
  eprint = {1401.0604},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9NXQZS7A/Lindsten, Jordan, Schön - 2014 - Particle Gibbs with Ancestor Sampling(2).pdf}
}

@inproceedings{ling2015FindingFunctionForm,
  title = {Finding {{Function}} in {{Form}}: {{Compositional Character Models}} for {{Open Vocabulary Word Representation}}},
  booktitle = {{{EMNLP}}},
  author = {Ling, W. and Luís, T. and Marujo, L. and Astudillo, R. F. and Amir, S. and Dyer, C. and Black, A. W. and Trancoso, I.},
  date = {2015},
  url = {http://arxiv.org/abs/1508.02096},
  abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
  annotation = {521 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1508.02096},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F2NF4H42/Ling et al. - 2015 - Finding Function in Form Compositional Character Models for Open Vocabulary Word Representation(2).pdf}
}

@inproceedings{ling2016LatentPredictorNetworks,
  title = {Latent {{Predictor Networks}} for {{Code Generation}}},
  booktitle = {{{ACL}}},
  author = {Ling, W. and Grefenstette, E. and Hermann, K. M. and Kočiský, T. and Senior, A. and Wang, F. and Blunsom, P.},
  date = {2016},
  pages = {599--609},
  url = {http://arxiv.org/abs/1603.06744},
  abstract = {Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.},
  annotation = {220 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1603.06744},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4ZKDALZR/Ling et al. - 2016 - Latent Predictor Networks for Code Generation(2).pdf},
  isbn = {978-1-5108-2758-5},
  keywords = {u}
}

@inproceedings{lipton2018TroublingTrendsMachine,
  title = {Troubling {{Trends}} in {{Machine Learning Scholarship}}},
  booktitle = {{{ICML}}},
  author = {Lipton, Z. C. and Steinhardt, J.},
  date = {2018},
  pages = {1--15},
  archiveprefix = {arXiv},
  eprint = {1807.03341},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DKJKQFUR/Lipton, Steinhardt - 2018 - Troubling Trends in Machine Learning Scholarship(2).pdf}
}

@inproceedings{listgarten2005MultipleAlignmentContinuous,
  title = {Multiple {{Alignment}} of {{Continuous Time Series}}},
  booktitle = {{{NeurIPS}}},
  author = {Listgarten, J. and Neal, R. M. and Roweis, S. T. and Emili, A.},
  date = {2005},
  pages = {817--824},
  issn = {10495258},
  abstract = {Page 1. of Continuous Jennifer Listgarten † , Radford M. Neal † , Sam T. Roweis † and Andrew Emili ‡},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KCR9XBNG/Listgarten et al. - 2005 - Multiple Alignment of Continuous Time Series(2).pdf},
  isbn = {0-262-19534-8}
}

@inproceedings{liu2016HowNOTEvaluate,
  title = {How {{NOT To Evaluate Your Dialogue System}}: {{An Empirical Study}} of {{Unsupervised Evaluation Metrics}} for {{Dialogue Response Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Liu, C. and Lowe, R. and Serban, I. V. and Noseworthy, M. and Charlin, L. and Pineau, J.},
  date = {2016},
  url = {http://arxiv.org/abs/1603.08023},
  abstract = {We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.},
  annotation = {710 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1603.08023},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FBMY5R9G/Liu et al. - 2016 - How NOT To Evaluate Your Dialogue System An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Respo(2).pdf}
}

@inproceedings{liu2017DeepHypersphericalLearning,
  title = {Deep Hyperspherical Learning},
  booktitle = {{{NeurIPS}}},
  author = {Liu, W. and Zhang, Y. M. and Li, X. and Yu, Z. and Dai, B. and Zhao, T. and Song, L.},
  date = {2017},
  pages = {3951--3961},
  issn = {10495258},
  abstract = {Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.},
  archiveprefix = {arXiv},
  eprint = {1711.03189},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7FV5S3EP/Liu et al. - 2017 - Deep hyperspherical learning(2).pdf}
}

@inproceedings{liu2017SoftlabelMethodNoisetolerant,
  title = {A {{Soft}}-Label {{Method}} for {{Noise}}-Tolerant {{Distantly Supervised Relation Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Liu, T. and Wang, K. and Chang, B. and Sui, Z.},
  date = {2017},
  pages = {1790--1795},
  doi = {10.18653/v1/D17-1189},
  url = {http://aclweb.org/anthology/D17-1189},
  abstract = {Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels relational facts with knowledge bases. Previous sentence level denoise models don’t achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training. To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During training, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms other state-of-the-art systems. Author\{4\}\{Affiliation\}},
  annotation = {85 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6BWLJSRL/Liu et al. - 2017 - A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction(2).pdf}
}

@inproceedings{liu2018LearningMinimumHyperspherical,
  title = {Learning towards Minimum Hyperspherical Energy},
  booktitle = {{{NeurIPS}}},
  author = {Liu, W. and Lin, R. and Liu, Z. and Liu, L. and Yu, Z. and Dai, B. and Song, L.},
  date = {2018},
  pages = {6222--6233},
  issn = {10495258},
  abstract = {Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics - Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.},
  archiveprefix = {arXiv},
  eprint = {1805.09298},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KEG2B85R/Liu et al. - 2018 - Learning towards minimum hyperspherical energy(2).pdf},
  keywords = {u}
}

@inproceedings{liu2019IncorporatingContextualSyntactic,
  title = {Incorporating {{Contextual}} and {{Syntactic Structures Improves Semantic Similarity Modeling}}},
  booktitle = {{{EMNLP}}},
  author = {Liu, L. and Yang, W. and Rao, J. and Tang, R. and Lin, J.},
  date = {2019},
  pages = {1204--1209},
  url = {https://github.com/likicode/spwim},
  abstract = {Semantic similarity modeling is central to many NLP problems such as natural language inference and question answering. Syntactic structures interact closely with semantics in learning compositional representations and alleviating long-range dependency issues. However , such structure priors have not been well exploited in previous work for semantic mod-eling. To examine their effectiveness, we start with the Pairwise Word Interaction Model, one of the best models according to a recent reproducibility study, then introduce components for modeling context and structure using multi-layer BiLSTMs and TreeLSTMs. In addition, we introduce residual connections to the deep convolutional neural network component of the model. Extensive evaluations on eight benchmark datasets show that incorporating structural information contributes to consistent improvements over strong base-lines.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GJKB5GFQ/Liu et al. - 2019 - Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling(2).pdf},
  keywords = {u}
}

@inproceedings{liu2019InoculationFineTuningMethod,
  title = {Inoculation by {{Fine}}-{{Tuning}}: {{A Method}} for {{Analyzing Challenge Datasets}}},
  booktitle = {{{NAACL}}},
  author = {Liu, N. F. and Schwartz, R. and Smith, N. A.},
  date = {2019},
  pages = {2171--2179},
  doi = {10.18653/v1/n19-1225},
  abstract = {Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks. While model performance on these challenge datasets is significantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal. For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model's specific training set. We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI "stress tests" (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.},
  annotation = {52 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.02668},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4UJXYS8Y/Liu, Schwartz, Smith - 2019 - Inoculation by Fine-Tuning A Method for Analyzing Challenge Datasets(2).pdf}
}

@inproceedings{liu2019KnowledgeAugmentedLanguageModel,
  title = {Knowledge-{{Augmented Language Model}} and {{Its Application}} to {{Unsupervised Named}}-{{Entity Recognition}}},
  booktitle = {{{NAACL}}},
  author = {Liu, Angli and Du, Jingfei and Stoyanov, Veselin},
  date = {2019},
  pages = {1142--1150},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1117},
  url = {http://aclweb.org/anthology/N19-1117},
  urldate = {2020-10-16},
  abstract = {Traditional language models are unable to efficiently model entity names observed in text. All but the most popular named entities appear infrequently in text providing insufficient context. Recent efforts have recognized that context can be generalized between entity names that share the same type (e.g., person or location) and have equipped language models with access to an external knowledge base (KB). Our Knowledge-Augmented Language Model (KALM) continues this line of work by augmenting a traditional model with a KB. Unlike previous methods, however, we train with an end-to-end predictive objective optimizing the perplexity of text. We do not require any additional information such as named entity tags. In addition to improving language modeling performance, KALM learns to recognize named entities in an entirely unsupervised way by using entity type information latent in the model. On a Named Entity Recognition (NER) task, KALM achieves performance comparable with state-of-the-art supervised models. Our work demonstrates that named entities (and possibly other types of world knowledge) can be modeled successfully using predictive learning and training on large corpora of text without any additional information.},
  annotation = {16 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NDAJI3WX/Liu et al. - 2019 - Knowledge-Augmented Language Model and Its Applica.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{liu2019ReferentialReaderRecurrent,
  title = {The {{Referential Reader}}: {{A Recurrent Entity Network}} for {{Anaphora Resolution}}},
  shorttitle = {The {{Referential Reader}}},
  booktitle = {{{ACL}}},
  author = {Liu, Fei and Zettlemoyer, Luke and Eisenstein, Jacob},
  date = {2019-07-09},
  url = {http://arxiv.org/abs/1902.01541},
  urldate = {2021-03-29},
  abstract = {We present a new architecture for storing and accessing entity mentions during online text processing. While reading the text, entity references are identified, and may be stored by either updating or overwriting a cell in a fixed-length memory. The update operation implies coreference with the other mentions that are stored in the same cell; the overwrite operation causes these mentions to be forgotten. By encoding the memory operations as differentiable gates, it is possible to train the model end-to-end, using both a supervised anaphora resolution objective as well as a supplementary language modeling objective. Evaluation on a dataset of pronoun-name anaphora demonstrates strong performance with purely incremental text processing.},
  annotation = {8 citations (Semantic Scholar/arXiv) [2021-03-29]},
  archiveprefix = {arXiv},
  eprint = {1902.01541},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SLKQBJZ5/Liu et al. - 2019 - The Referential Reader A Recurrent Entity Network.pdf;/home/hiaoxui/.local/share/zotero_files/storage/EE9BG8JB/1902.html}
}

@report{liu2019RoBERTaRobustlyOptimized,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  author = {Liu, Y. and Ott, M. and Goyal, N. and Du, J. and Joshi, M. and Chen, D. and Levy, O. and Lewis, M. and Zettlemoyer, L. S. and Stoyanov, V.},
  date = {2019},
  url = {http://arxiv.org/abs/1907.11692},
  annotation = {2496 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UF9JZDUE/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach(2).pdf},
  keywords = {u}
}

@report{liu2020OrthogonalOverParameterizedTraining,
  title = {Orthogonal {{Over}}-{{Parameterized Training}}},
  author = {Liu, W. and Lin, R. and Liu, Z. and Rehg, J. M. and Xiong, L. and Weller, A. and Song, L.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TMLHZWLQ/Anonymous - 2020 - Orthogonal Over-Parameterized Training(6).pdf},
  keywords = {review}
}

@inproceedings{liu2021SwinTransformerHierarchical,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  booktitle = {{{CVPR}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-03-25},
  url = {http://arxiv.org/abs/2103.14030},
  urldate = {2021-03-26},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The code and models will be made publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arXiv},
  eprint = {2103.14030},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WYXTISB8/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf;/home/hiaoxui/.local/share/zotero_files/storage/W6SAFYU8/2103.html}
}

@inproceedings{locatello2019ChallengingCommonAssumptions,
  title = {Challenging {{Common Assumptions}} in the {{Unsupervised Learning}} of {{Disentangled Representations}}},
  booktitle = {{{ICML}}},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Rätsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier},
  date = {2019-06-18},
  url = {http://arxiv.org/abs/1811.12359},
  urldate = {2020-10-28},
  abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
  annotation = {390 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1811.12359},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R4BBWWW4/Locatello et al. - 2019 - Challenging Common Assumptions in the Unsupervised.pdf;/home/hiaoxui/.local/share/zotero_files/storage/CPPSS98E/1811.html},
  keywords = {u}
}

@inproceedings{logan2019BarackWifeHillary,
  title = {Barack’s {{Wife Hillary}}: {{Using Knowledge Graphs}} for {{Fact}}-{{Aware Language Modeling}}},
  shorttitle = {Barack’s {{Wife Hillary}}},
  booktitle = {{{ACL}}},
  author = {Logan, Robert and Liu, Nelson F. and Peters, Matthew E. and Gardner, Matt and Singh, Sameer},
  date = {2019},
  pages = {5962--5971},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1598},
  url = {https://www.aclweb.org/anthology/P19-1598},
  urldate = {2020-10-16},
  abstract = {Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText2 dataset,1 a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark (Merity et al., 2017). In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language models’ ability to complete sentences requiring factual knowledge, and show that the KGLM outperforms even very large language models in generating facts.},
  annotation = {68 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WFE8KTTX/Logan et al. - 2019 - Barack’s Wife Hillary Using Knowledge Graphs for .pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{louis2012CoherenceModelBased,
  title = {A {{Coherence Model Based}} on {{Syntactic Patterns}}},
  booktitle = {{{EMNLP}}},
  author = {Louis, A. and Nenkova, A.},
  date = {2012},
  pages = {1157--1168},
  abstract = {We introduce a model of coherence which captures the intentional discourse structure in text. Our work is based on the hypothesis that syntax provides a proxy for the communica- tive goal of a sentence and therefore the se- quence of sentences in a coherent discourse should exhibit detectable structural patterns. Results show that our method has high dis- criminating power for separating out coherent and incoherent news articles reaching accura- cies of up to 90\%. We also show that our syn- tactic patterns are correlated with manual an- notations of intentional structure for academic conference articles and can successfully pre- dict the coherence of abstract, introduction and related work sections of these articles. 1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A2T4MUKT/Louis, Nenkova - 2012 - A Coherence Model Based on Syntactic Patterns(2).pdf},
  isbn = {978-1-937284-43-5},
  issue = {July},
  keywords = {u}
}

@inproceedings{louizos2017BayesianCompressionDeep,
  title = {Bayesian {{Compression}} for {{Deep Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Louizos, C. and Ullrich, K. and Welling, M.},
  date = {2017},
  url = {http://arxiv.org/abs/1705.08665},
  abstract = {Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.},
  annotation = {286 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1705.08665},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VVXAL7N6/Louizos, Ullrich, Welling - 2017 - Bayesian Compression for Deep Learning(2).pdf},
  keywords = {u}
}

@report{loynd2019WorkingMemoryGraphs,
  title = {Working {{Memory Graphs}}},
  author = {Loynd, R. and Fernandez, R. and Celikyilmaz, A. and Swaminathan, A. and Hausknecht, M.},
  date = {2019},
  url = {http://arxiv.org/abs/1911.07141},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1911.07141},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/58TL8DIH/Loynd et al. - 2019 - Working Memory Graphs(2).pdf},
  keywords = {u}
}

@inproceedings{lu2008GenerativeModelParsing,
  title = {A Generative Model for Parsing Natural Language to Meaning Representations},
  booktitle = {{{EMNLP}}},
  author = {Lu, W. and Ng, H. T. and Lee, W. S. and Zettlemoyer, L. S.},
  date = {2008},
  pages = {783--792},
  doi = {10.3115/1613715.1613815},
  url = {http://dl.acm.org/citation.cfm?id=1613815},
  abstract = {In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. © 2008 Association for Computational Linguistics.},
  annotation = {146 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LR7EHXG3/Lu et al. - 2008 - A generative model for parsing natural language to meaning representations(2).pdf}
}

@inproceedings{lu2011ProbabilisticForesttoStringModel,
  title = {A {{Probabilistic Forest}}-to-{{String Model}} for {{Language Generation}} from {{Typed Lambda Calculus Expressions}}},
  booktitle = {{{EMNLP}}},
  author = {Lu, W. and Ng, H. T.},
  date = {2011},
  pages = {1611--1622},
  url = {http://www.aclweb.org/anthology/D11-1149%5Cnhttp://dl.acm.org/citation.cfm?id=2145605},
  abstract = {"This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation."},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DF7MWRHZ/Lu, Ng - 2011 - A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions(2).pdf},
  isbn = {1-937284-11-5}
}

@inproceedings{lu2019LookupAdaptOneshot,
  title = {Look-up and {{Adapt}}: {{A One}}-Shot {{Semantic Parser}}},
  booktitle = {{{EMNLP}}},
  author = {Lu, Z. and Arabshahi, F. and Labutov, I. and Mitchell, T.},
  date = {2019},
  pages = {1129--1139},
  doi = {10.18653/v1/d19-1104},
  abstract = {Computing devices have recently become capable of interacting with their end users via natural language. However, they can only operate within a limited "supported" domain of discourse and fail drastically when faced with an out-of-domain utterance, mainly due to the limitations of their semantic parser. In this paper, we propose a semantic parser that generalizes to out-of-domain examples by learning a general strategy for parsing an unseen utterance through adapting the logical forms of seen utterances, instead of learning to generate a logical form from scratch. Our parser maintains a memory consisting of a representative subset of the seen utterances paired with their logical forms. Given an unseen utterance, our parser works by looking up a similar utterance from the memory and adapting its logical form until it fits the unseen utterance. Moreover, we present a data generation strategy for constructing utterance-logical form pairs from different domains. Our results show an improvement of up to 68.8\% on one-shot parsing under two different evaluation settings compared to the baselines.},
  annotation = {2 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1910.12197},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HIN5PQWX/Lu et al. - 2019 - Look-up and Adapt A One-shot Semantic Parser(2).pdf}
}

@inproceedings{luong2015AddressingRareWord,
  title = {Addressing the {{Rare Word Problem}} in {{Neural Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Luong, M. and Sutskever, I. and Le, Q. V. and Vinyals, O. and Zaremba, W.},
  date = {2015},
  url = {http://arxiv.org/abs/1410.8206},
  abstract = {Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task.},
  annotation = {607 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1410.8206},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KWJ4WMTL/Luong et al. - 2015 - Addressing the Rare Word Problem in Neural Machine Translation(2).pdf},
  keywords = {u}
}

@inproceedings{luong2015EffectiveApproachesAttentionbased,
  title = {Effective {{Approaches}} to {{Attention}}-Based {{Neural Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Luong, M. and Pham, H. and Manning, C. D.},
  date = {2015},
  url = {http://arxiv.org/abs/1508.04025},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  annotation = {4589 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1508.04025},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3JZSKDEJ/Luong, Pham, Manning - 2015 - Effective Approaches to Attention-based Neural Machine Translation(2).pdf}
}

@inproceedings{ma2016EndtoendSequenceLabeling,
  title = {End-to-End {{Sequence Labeling}} via {{Bi}}-Directional {{LSTM}}-{{CNNs}}-{{CRF}}},
  booktitle = {{{ACL}}},
  author = {Ma, X. and Hovy, E.},
  date = {2016},
  issn = {1098-6596},
  doi = {10.18653/v1/P16-1101},
  url = {http://arxiv.org/abs/1603.01354},
  abstract = {State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\textbackslash\% accuracy for POS tagging and 91.21\textbackslash\% F1 for NER.},
  annotation = {1531 citations (Semantic Scholar/DOI) [2021-03-26] 1531 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XN5WMPW6/Ma, Hovy - 2016 - End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF(2).pdf},
  isbn = {978-1-5108-2758-5}
}

@inproceedings{ma2018BagofwordsTargetNeural,
  title = {Bag-of-Words as Target for Neural Machine Translation},
  booktitle = {{{ACL}}},
  author = {Ma, S. and Sun, X. and Wang, Y. and Lin, J.},
  date = {2018},
  pages = {332--338},
  doi = {10.18653/v1/p18-2053},
  abstract = {A sentence can be translated into more than one correct sentences. However, most of the existing neural machine translation models only use one of the correct translations as the targets, and the other correct sentences are punished as the incorrect sentences in the training stage. Since most of the correct translations for one sentence share the similar bag-of-words, it is possible to distinguish the correct translations from the incorrect ones by the bag-of-words. In this paper, we propose an approach that uses both the sentences and the bag-of-words as targets in the training stage, in order to encourage the model to generate the potentially correct sentences that are not appeared in the training set. We evaluate our model on a Chinese-English translation dataset, and experiments show our model outperforms the strong baselines by the BLEU score of 4.55.},
  annotation = {50 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1805.04871},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F2DSBVXT/Ma et al. - 2018 - Bag-of-words as target for neural machine translation(2).pdf},
  isbn = {978-1-948087-34-6}
}

@inproceedings{ma2019FlowSeqNonAutoregressiveConditional,
  title = {{{FlowSeq}}: {{Non}}-{{Autoregressive Conditional Sequence Generation}} with {{Generative Flow}}},
  booktitle = {{{IJCNLP}}},
  author = {Ma, X. and Zhou, C. and Li, X. and Neubig, G. and Hovy, E.},
  date = {2019},
  pages = {4273--4283},
  doi = {10.18653/v1/d19-1437},
  abstract = {Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.},
  annotation = {63 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.02480},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IID8AA9V/Ma et al. - 2019 - FlowSeq Non-Autoregressive Conditional Sequence Generation with Generative Flow(2).pdf}
}

@inproceedings{maddison2017ConcreteDistributionContinuous,
  title = {The {{Concrete Distribution}}: {{A Continuous Relaxation}} of {{Discrete Random Variables}}},
  booktitle = {{{ICLR}}},
  author = {Maddison, C. J. and Mnih, A. and Teh, Y. W.},
  date = {2017},
  pages = {1--20},
  archiveprefix = {arXiv},
  eprint = {1611.00712v3},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XHE9CTFI/Maddison, Mnih, Teh - 2017 - The Concrete Distribution A Continuous Relaxation of Discrete Random Variables(2).pdf},
  keywords = {u}
}

@report{majewska2020VerbKnowledgeInjection,
  title = {Verb {{Knowledge Injection}} for {{Multilingual Event Processing}}},
  author = {Majewska, Olga and Vulić, Ivan and Glavaš, Goran and Ponti, Edoardo M. and Korhonen, Anna},
  date = {2020-12-30},
  url = {http://arxiv.org/abs/2012.15421},
  urldate = {2021-01-20},
  abstract = {In parallel to their overwhelming success across NLP tasks, language ability of deep Transformer networks, pretrained via language modeling (LM) objectives has undergone extensive scrutiny. While probing revealed that these models encode a range of syntactic and semantic properties of a language, they are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks, rather than leverage deeper linguistic knowledge. In this paper, we target one such area of their deficiency, verbal reasoning. We investigate whether injecting explicit information on verbs' semantic-syntactic behaviour improves the performance of LM-pretrained Transformers in event extraction tasks -- downstream tasks for which accurate verb processing is paramount. Concretely, we impart the verb knowledge from curated lexical resources into dedicated adapter modules (dubbed verb adapters), allowing it to complement, in downstream tasks, the language knowledge obtained during LM-pretraining. We first demonstrate that injecting verb knowledge leads to performance gains in English event extraction. We then explore the utility of verb adapters for event extraction in other languages: we investigate (1) zero-shot language transfer with multilingual Transformers as well as (2) transfer via (noisy automatic) translation of English verb-based lexical constraints. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when verb adapters are trained on noisily translated constraints.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2012.15421},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JQ7MNQYR/Majewska et al. - 2020 - Verb Knowledge Injection for Multilingual Event Pr.pdf;/home/hiaoxui/.local/share/zotero_files/storage/3F6AJDGL/2012.html},
  keywords = {u}
}

@inproceedings{malinin2019ReverseKLDivergenceTraining,
  title = {Reverse {{KL}}-{{Divergence Training}} of {{Prior Networks}}: {{Improved Uncertainty}} and {{Adversarial Robustness}}},
  booktitle = {{{NeurIPS}}},
  author = {Malinin, A. and Gales, M.},
  date = {2019},
  issn = {1049-5258},
  url = {http://arxiv.org/abs/1905.13472},
  abstract = {Ensemble approaches for uncertainty estimation have recently been applied to the tasks of misclassification detection, out-of-distribution input detection and adversarial attack detection. Prior Networks have been proposed as an approach to efficiently \textbackslash emph\{emulate\} an ensemble of models for classification by parameterising a Dirichlet prior distribution over output distributions. These models have been shown to outperform alternative ensemble approaches, such as Monte-Carlo Dropout, on the task of out-of-distribution input detection. However, scaling Prior Networks to complex datasets with many classes is difficult using the training criteria originally proposed. This paper makes two contributions. First, we show that the appropriate training criterion for Prior Networks is the \textbackslash emph\{reverse\} KL-divergence between Dirichlet distributions. This addresses issues in the nature of the training data target distributions, enabling prior networks to be successfully trained on classification tasks with arbitrarily many classes, as well as improving out-of-distribution detection performance. Second, taking advantage of this new training criterion, this paper investigates using Prior Networks to detect adversarial attacks and proposes a generalized form of adversarial training. It is shown that the construction of successful \textbackslash emph\{adaptive\} whitebox attacks, which affect the prediction and evade detection, against Prior Networks trained on CIFAR-10 and CIFAR-100 using the proposed approach requires a greater amount of computational effort than against networks defended using standard adversarial training or MC-dropout.},
  annotation = {29 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1905.13472},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7A4KX2X9/Malinin, Gales - 2019 - Reverse KL-Divergence Training of Prior Networks Improved Uncertainty and Adversarial Robustness(2).pdf},
  keywords = {u}
}

@inproceedings{malinin2020EbsembleDistributionDistillation,
  title = {Ebsemble {{Distribution Distillation}}},
  booktitle = {{{ICLR}}},
  author = {Malinin, A. and Mlodozeniec, B and Gales, M.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9AJLGZPL/Malinin, Mlodozeniec, Gales - 2020 - Ebsemble Distribution Distillation(2).pdf},
  keywords = {u}
}

@report{malinin2020UncertaintyStructuredPrediction,
  title = {Uncertainty in {{Structured Prediction}}},
  author = {Malinin, A. and Gales, M.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BJWMTW6C/Malinin, Gales - 2020 - Uncertainty in Structured Prediction(2).pdf;/home/hiaoxui/.local/share/zotero_files/storage/QWDQKDJZ/Anonymous - 2017 - Uncertainty in Structured Prediction(3).pdf},
  keywords = {review}
}

@inproceedings{malisiewicz2011EnsembleExemplarSVMs,
  title = {Ensemble of {{Exemplar SVMs}} for {{Object Detection}} and {{Beyond}}},
  booktitle = {{{ICCV}}},
  author = {Malisiewicz, T. and Gupta, A. and Efros, A. A.},
  date = {2011},
  pages = {89--96},
  abstract = {This paper proposes a conceptually simple but surpris- ingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspon- dence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar- SVMs is thus defined by a single positive instance and mil- lions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generaliza- tion. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computa- tional cost increase. But the central benefit of our approach is that it creates an explicit association between each de- tection and a single training exemplar. Because most de- tections show good alignment to their associated exemplar, it is possible to transfer any available exemplar meta-data (segmentation, geometric structure, 3D model, etc.) directly onto the detections, which can then be used as part of over- all scene understanding.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E9UR7ZYM/Malisiewicz, Gupta, Efros - 2011 - Ensemble of Exemplar SVMs for Object Detection and Beyond(2).pdf},
  isbn = {978-1-4577-1102-2},
  keywords = {u}
}

@inproceedings{manhaeve2018DeepProbLogNeuralProbabilistic,
  title = {{{DeepProbLog}} : {{Neural Probabilistic Logic Programming}}},
  booktitle = {{{NeurIPS}}},
  author = {Manhaeve, R. and Kimmig, A. and Dumančić, S. and Demeester, T. and De Raedt, L.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1805.10872v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8BCTSEQU/Manhaeve et al. - 2018 - DeepProbLog Neural Probabilistic Logic Programming(2).pdf}
}

@inproceedings{mani2006MachineLearningTemporal,
  title = {Machine Learning of Temporal Relations},
  booktitle = {{{COLING}}-{{ACL}}},
  author = {Mani, I. and Verhagen, M. and Wellner, B. and Lee, C. M. and Pustejovsky, J.},
  date = {2006},
  pages = {753--760},
  abstract = {This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93\% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions. © 2006 Association for Computational Linguistics.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H4AN9ARC/Mani et al. - 2006 - Machine learning of temporal relations(2).pdf},
  isbn = {1-932432-65-5}
}

@report{manning2000IntroductionFormalComputational,
  title = {An {{Introduction}} to {{Formal Computational Semantics}}},
  author = {Manning, C. D.},
  date = {2000},
  pages = {1--15},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IRLZZBUR/Manning - 2000 - An Introduction to Formal Computational Semantics(2).pdf}
}

@inproceedings{manning2013ComputationalLinguisticsDeep,
  title = {Computational {{Linguistics}} and {{Deep Learning}}},
  booktitle = {{{COLING}}},
  author = {Manning, C. D.},
  date = {2013},
  volume = {38},
  pages = {41--51},
  issn = {01272713},
  doi = {10.1162/COLI},
  abstract = {This paper evaluates the investment performance of Malaysian-based international equity funds. The results on the overall fund performance using Jensen's (1968) model indicate that, on average, international funds have significant negative risk-adjusted returns over the study period from 2008-2010. Since the model ignores market timing activity, it implicitly attributes the overall negative return to manager's poor stock selection ability. However, the performance breakdown results on managerial expertise using the models of Treynor and Mazuy (1966) and Henriksson and Merton (1981) show evidence of positive selectivity and negative market timing returns. Taken together, the highly significant negative timing returns suggest that, on average, international fund managers have perverse market timing ability. The paper finds little evidence that Malaysian investors achieve diversification benefits from investing in overseas equity markets.},
  archiveprefix = {arXiv},
  eprint = {22251136},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2B24YJ48/Manning - 2013 - Computational Linguistics and Deep Learning(2).pdf},
  isbn = {978-1-60845-985-8},
  keywords = {u}
}

@inproceedings{manning2014StanfordCoreNLPNatural,
  title = {The {{Stanford CoreNLP Natural Language Processing Toolkit}}},
  booktitle = {{{ACL}}},
  author = {Manning, C. D. and Surdeanu, M. and Bauer, J. and Finkel, J. and Bethard, S. and McClosky, D.},
  date = {2014},
  pages = {55--60},
  issn = {1098-6596},
  doi = {10.3115/v1/P14-5010},
  url = {http://aclweb.org/anthology/P14-5010},
  abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
  annotation = {5124 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9NUII5HC/Manning et al. - 2014 - The Stanford CoreNLP Natural Language Processing Toolkit(2).pdf},
  isbn = {978-1-941643-00-6}
}

@inproceedings{marcheggiani2017EncodingSentencesGraph,
  title = {Encoding {{Sentences}} with {{Graph Convolutional Networks}} for {{Semantic Role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Marcheggiani, Diego and Titov, Ivan},
  date = {2017},
  pages = {1506--1515},
  location = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1159},
  abstract = {Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-theart LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.},
  annotation = {413 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GQFQS82H/Marcheggiani and Titov - 2017 - Encoding Sentences with Graph Convolutional Networ.pdf},
  langid = {english}
}

@report{marcheggiani2017SimpleAccurateSyntaxAgnostic,
  title = {A {{Simple}} and {{Accurate Syntax}}-{{Agnostic Neural Model}} for {{Dependency}}-Based {{Semantic Role Labeling}}},
  author = {Marcheggiani, D. and Frolov, A. and Titov, I.},
  date = {2017},
  url = {http://arxiv.org/abs/1701.02593},
  abstract = {We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves competitive performance on English, even without any kind of syntactic information and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the English CoNLL-2009 dataset. We also consider Chinese, Czech and Spanish where our approach also achieves competitive results. Syntactic parsers are unreliable on out-of-domain data, so standard (i.e., syntactically-informed) SRL models are hindered when tested in this setting. Our syntax-agnostic model appears more robust, resulting in the best reported results on standard out-of-domain test sets.},
  annotation = {84 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1701.02593},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WAXQP8WN/Marcheggiani, Frolov, Titov - 2017 - A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling(2).pdf},
  keywords = {u}
}

@report{marchisio2020WhenDoesUnsupervised,
  title = {When {{Does Unsupervised Machine Translation Work}}?},
  author = {Marchisio, Kelly and Duh, Kevin and Koehn, Philipp},
  date = {2020-04-14},
  url = {http://arxiv.org/abs/2004.05516},
  urldate = {2020-10-23},
  abstract = {Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which these methods succeed, and where they fail. We conduct an extensive empirical evaluation of unsupervised MT using dissimilar language pairs, dissimilar domains, diverse datasets, and authentic low-resource languages. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that random word embedding initialization can dramatically affect downstream translation performance. We additionally find that unsupervised MT performance declines when source and target languages use different scripts, and observe very poor performance on authentic low-resource language pairs. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms.},
  annotation = {10 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2004.05516},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZZKPHUE7/Marchisio et al. - 2020 - When Does Unsupervised Machine Translation Work.pdf;/home/hiaoxui/.local/share/zotero_files/storage/FTBQTKSC/2004.html}
}

@article{marcus1993BuildingLargeAnnotated,
  title = {Building a {{Large Annotated Corpus}} of {{English}}: {{The Penn Treebank}}},
  author = {Marcus, M. and Santorini, B. and Marcinkiewicz, M. A.},
  date = {1993},
  journaltitle = {Computational Linguistics},
  volume = {19},
  pages = {313},
  issn = {0891-2017},
  abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/44R8PYCY/Marcus, Santorini, Marcinkiewicz - 1993 - Building a Large Annotated Corpus of English The Penn Treebank(2).pdf},
  keywords = {u},
  number = {2}
}

@inproceedings{martins2016SoftmaxSparsemaxSparse,
  title = {From {{Softmax}} to {{Sparsemax}}: {{A Sparse Model}} of {{Attention}} and {{Multi}}-{{Label Classification}}},
  booktitle = {{{ICML}}},
  author = {Martins, A. F. T. and Astudillo, R. F.},
  date = {2016},
  volume = {48},
  issn = {19410093},
  doi = {10.1109/72.279181},
  url = {http://arxiv.org/abs/1602.02068},
  abstract = {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.},
  annotation = {4997 citations (Semantic Scholar/DOI) [2021-03-26] 255 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {18267787},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MMPINAD9/Martins, Astudillo - 2016 - From Softmax to Sparsemax A Sparse Model of Attention and Multi-Label Classification(2).pdf},
  isbn = {978-1-5108-2900-8}
}

@inproceedings{matuszek2012JointModelLanguage,
  title = {A {{Joint Model}} of {{Language}} and {{Perception}} for {{Grounded Attribute Learning}}},
  booktitle = {{{ICML}}},
  author = {Matuszek, C. and FitzGerald, N. and Zettlemoyer, L. S. and Liefeng, B. and Fox, D.},
  date = {2012},
  pages = {1671--1678},
  url = {http://arxiv.org/abs/1206.6423},
  abstract = {As robots become more ubiquitous and ca- pable, it becomes ever more important for untrained users to easily interact with them. Recently, this has led to study of the lan- guage grounding problem, where the goal is to extract representations of the mean- ings of natural language tied to the physi- cal world. We present an approach for joint learning of language and perception models for grounded attribute induction. The per- ception model includes classifiers for phys- ical characteristics and a language model based on a probabilistic categorial grammar that enables the construction of composi- tional meaning representations. We evaluate on the task of interpreting sentences that de- scribe sets of objects in a physical workspace, and demonstrate accurate task performance and effective latent-variable concept induc- tion in physical grounded scenes. 1.},
  annotation = {274 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1206.6423},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A7EWQKD2/Matuszek et al. - 2012 - A Joint Model of Language and Perception for Grounded Attribute Learning(2).pdf},
  isbn = {978-1-4503-1285-1},
  keywords = {u}
}

@inproceedings{matuszek2013LearningParseNatural,
  title = {Learning to {{Parse Natural Language Commands}} to a {{Robot Control System}}},
  booktitle = {{{ISER}}},
  author = {Matuszek, C. and Herbst, E. and Zettlemoyer, L. S. and Fox, D.},
  date = {2013},
  pages = {403--415},
  issn = {21530858},
  doi = {10.1007/978-3-319-00065-7_28},
  url = {http://link.springer.com/10.1007/978-3-319-00065-7_28},
  abstract = {As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor envi- ronment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our for- mal representation allows a robot to interpret route instructions online while moving through a previously unknown environment.},
  annotation = {318 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {19886812},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FDTMSBT6/Matuszek et al. - 2013 - Learning to Parse Natural Language Commands to a Robot Control System(2).pdf},
  isbn = {978-3-319-00064-0},
  keywords = {u}
}

@inproceedings{may2014ParticleFilterRejuvenation,
  title = {Particle Filter Rejuvenation and Latent {{Dirichlet}} Allocation},
  booktitle = {{{ACL}}},
  author = {May, C. and Clemmer, A. and Van Durme, B.},
  date = {2014},
  pages = {446--451},
  doi = {10.3115/v1/p14-2073},
  abstract = {Previous research has established several methods of online learning for latent Dirichlet allocation (LDA). However, streaming learning for LDA - allowing only one pass over the data and constant storage complexity - is not as well explored. We use reservoir sampling to reduce the storage complexity of a previously-studied online algorithm, namely the particle filter, to constant. We then show that a simpler particle filter implementation performs just as well, and that the quality of the initialization dominates other factors of performance. ?? 2014 Association for Computational Linguistics.},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HJJ6SD3V/May, Clemmer, Van Durme - 2014 - Particle filter rejuvenation and latent Dirichlet allocation(2).pdf},
  isbn = {978-1-937284-73-2}
}

@inproceedings{may2019DownstreamPerformanceCompressed,
  title = {On the {{Downstream Performance}} of {{Compressed Word Embeddings}}},
  booktitle = {{{NeurIPS}}},
  author = {May, A. and Ré, C.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/77YKN4LB/May, Ré - 2019 - On the Downstream Performance of Compressed Word Embeddings(2).pdf},
  keywords = {u}
}

@report{mccarthy2020ImprovedVariationalNeural,
  title = {Improved {{Variational Neural Machine Translation}} by {{Promoting Mutual Information}}},
  author = {McCarthy, A. D. and Li, X. and Gu, J. and Dong, N.},
  date = {2020},
  pages = {1--10},
  url = {http://arxiv.org/abs/1909.09237},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.09237},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/82GSR7PK/McCarthy et al. - 2020 - Improved Variational Neural Machine Translation by Promoting Mutual Information(2).pdf},
  keywords = {u}
}

@inproceedings{mcdonald2013UniversalDependencyAnnotation,
  title = {Universal {{Dependency Annotation}} for {{Multilingual Parsing}}},
  booktitle = {{{ACL}}},
  author = {McDonald, R. and Nivre, J. and Quirmbach-Brundage, Y. and Goldberg, Y. and Das, D. and Ganchev, K. and Hall, K. and Petrov, S. and Zhang, H. and Täckström, O. and Bedini, C. and Castelló, N. B. and Lee, J.},
  date = {2013},
  volume = {82},
  pages = {92--97},
  issn = {00094846},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E7FTY5EM/McDonald et al. - 2013 - Universal Dependency Annotation for Multilingual Parsing(2).pdf},
  number = {4}
}

@inproceedings{meek2014LearningGraphicalCausal,
  title = {Toward Learning Graphical and Causal Process Models},
  booktitle = {Uncertainty in {{Artificial Intelligence Workshop}} on {{Causal Inference}}: {{Learning}} and {{Prediction}}},
  author = {Meek, C.},
  date = {2014},
  volume = {1274},
  pages = {43--48},
  issn = {16130073},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I2TASA77/Meek - 2014 - Toward learning graphical and causal process models(2).pdf}
}

@inproceedings{mei2016NeuralHawkesProcess,
  title = {The {{Neural Hawkes Process}}: {{A Neurally Self}}-{{Modulating Multivariate Point Process}}},
  booktitle = {{{NeurIPS}}},
  author = {Mei, H. and Eisner, J. M.},
  date = {2016},
  url = {http://arxiv.org/abs/1612.09328},
  abstract = {Many events occur in the world. Some event types are stochastically excited or inhibited---in the sense of having their probabilities elevated or decreased---by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.},
  annotation = {198 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1612.09328},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KYAEPZ48/Mei, Eisner - 2016 - The Neural Hawkes Process A Neurally Self-Modulating Multivariate Point Process(2).pdf}
}

@inproceedings{mei2016WhatTalkHow,
  title = {What to Talk about and How? {{Selective Generation}} Using {{LSTMs}} with {{Coarse}}-to-{{Fine Alignment}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Mei, H. and Bansal, M. and Walter, M. R.},
  date = {2016},
  pages = {720--730},
  doi = {10.18653/v1/N16-1086},
  url = {http://arxiv.org/abs/1509.00838},
  abstract = {We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59\% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.},
  annotation = {219 citations (Semantic Scholar/DOI) [2021-03-26] 219 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1509.00838},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9Y85SCIK/Mei, Bansal, Walter - 2016 - What to talk about and how Selective Generation using LSTMs with Coarse-to-Fine Alignment(2).pdf},
  isbn = {978-1-941643-91-4}
}

@inproceedings{mei2019ImputingMissingEvents,
  title = {Imputing {{Missing Events}} in {{Continuous}}-{{Time Event Streams}}},
  booktitle = {{{ICML}}},
  author = {Mei, H. and Qin, G. and Eisner, J. M.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WGSFL4G6/Mei, Qin, Eisner - 2019 - Imputing Missing Events in Continuous-Time Event Streams(2).pdf}
}

@inproceedings{mei2020NeuralDatalogTime,
  title = {Neural {{Datalog Through Time}}: {{Imformed Temporal Modeling}} via {{Logical Specification}}},
  booktitle = {{{ICML}}},
  author = {Mei, H. and Qin, G. and Xu, M. and Eisner, J. M.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5EX76939/Mei et al. - 2020 - Neural Datalog Through Time Imformed Temporal Modeling via Logical Specification(2).pdf}
}

@article{meister2020BestFirstBeamSearch,
  title = {Best-{{First Beam Search}}},
  author = {Meister, C. and Cotterell, R. and Vieira, T.},
  date = {2020},
  journaltitle = {TACL},
  abstract = {Decoding for many NLP tasks requires a heuristic algorithm for approximating exact search since the full search space is often intractable if not simply too large to traverse efficiently. The default algorithm for this job is beam search--a pruned version of breadth-first search--which in practice, returns better results than exact inference due to beneficial search bias. In this work, we show that standard beam search is a computationally inefficient choice for many decoding tasks; specifically, when the scoring function is a monotonic function in sequence length, other search algorithms can be used to reduce the number of calls to the scoring function (e.g., a neural network), which is often the bottleneck computation. We propose best-first beam search, an algorithm that provably returns the same set of results as standard beam search, albeit in the minimum number of scoring function calls to guarantee optimality (modulo beam size). We show that best-first beam search can be used with length normalization and mutual information decoding, among other rescoring functions. Lastly, we propose a memory-reduced variant of best-first beam search, which has a similar search bias in terms of downstream performance, but runs in a fraction of the time.},
  archiveprefix = {arXiv},
  eprint = {2007.03909},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7X7GWLG6/Meister, Cotterell, Vieira - 2020 - Best-First Beam Search(2).pdf}
}

@report{meister2020GeneralizedEntropyRegularization,
  title = {Generalized {{Entropy Regularization}} or: {{There}}'s {{Nothing Special}} about {{Label Smoothing}}},
  author = {Meister, C. and Salesky, E. and Cotterell, R.},
  date = {2020},
  url = {http://arxiv.org/abs/2005.00820},
  abstract = {Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. This class of techniques, of which label smoothing is one, has a connection to entropy regularization. Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored. We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks. We also find that variance in model performance can be explained largely by the resulting entropy of the model. Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place.},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2005.00820},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N3BGAZYY/Meister, Salesky, Cotterell - 2020 - Generalized Entropy Regularization or There's Nothing Special about Label Smoothing(2).pdf}
}

@report{meng2021COCOLMCorrectingContrasting,
  title = {{{COCO}}-{{LM}}: {{Correcting}} and {{Contrasting Text Sequences}} for {{Language Model Pretraining}}},
  shorttitle = {{{COCO}}-{{LM}}},
  author = {Meng, Yu and Xiong, Chenyan and Bajaj, Payal and Tiwary, Saurabh and Bennett, Paul and Han, Jiawei and Song, Xia},
  date = {2021-02-16},
  url = {http://arxiv.org/abs/2102.08473},
  urldate = {2021-02-27},
  abstract = {We present COCO-LM, a new self-supervised learning framework that pretrains Language Models by COrrecting challenging errors and COntrasting text sequences. COCO-LM employs an auxiliary language model to mask-and-predict tokens in original text sequences. It creates more challenging pretraining inputs, where noises are sampled based on their likelihood in the auxiliary language model. COCO-LM then pretrains with two tasks: The first task, corrective language modeling, learns to correct the auxiliary model's corruptions by recovering the original tokens. The second task, sequence contrastive learning, ensures that the language model generates sequence representations that are invariant to noises and transformations. In our experiments on the GLUE and SQuAD benchmarks, COCO-LM outperforms recent pretraining approaches in various pretraining settings and few-shot evaluations, with higher pretraining efficiency. Our analyses reveal that COCO-LM's advantages come from its challenging training signals, more contextualized token representations, and regularized sequence representations.},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2102.08473},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NKND72WJ/Meng et al. - 2021 - COCO-LM Correcting and Contrasting Text Sequences.pdf;/home/hiaoxui/.local/share/zotero_files/storage/GUBP4ZHI/2102.html}
}

@inproceedings{mesquita2019KnowledgeNetBenchmarkDataset,
  title = {{{KnowledgeNet}}: {{A Benchmark Dataset}} for {{Knowledge Base Population}}},
  booktitle = {{{EMNLP}}},
  author = {Mesquita, F. and Cannaviccio, M. and Schmidek, J. and Mirza, P. and Barbosa, D.},
  date = {2019},
  pages = {749--758},
  doi = {10.18653/v1/d19-1069},
  abstract = {KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., entity linking, relation extraction). We discuss five baseline approaches , where the best approach achieves an F1 score of 0.50, significantly outperforming a traditional approach by 79\% (0.28). However, our best baseline is far from reaching human performance (0.82), indicating our dataset is challenging. The KnowledgeNet dataset and baselines are available at https://github. com/diffbot/knowledge-net},
  annotation = {13 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/57EN3FBT/Mesquita et al. - 2019 - KnowledgeNet A Benchmark Dataset for Knowledge Base Population(2).pdf},
  keywords = {u}
}

@inproceedings{miao2015NeuralVariationalInference,
  title = {Neural {{Variational Inference}} for {{Text Processing}}},
  booktitle = {{{ICML}}},
  author = {Miao, Y. and Yu, L. and Blunsom, P.},
  date = {2015},
  volume = {48},
  url = {http://arxiv.org/abs/1511.06038},
  abstract = {Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.},
  annotation = {362 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1511.06038},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4VMRLJJ4/Miao, Blunsom - 2016 - Language as a Latent Variable Discrete Generative Models for Sentence Compression(2).pdf},
  isbn = {0-89871-600-4},
  issue = {Mcmc},
  keywords = {u}
}

@inproceedings{miao2016LanguageLatentVariable,
  title = {Language as a {{Latent Variable}}: {{Discrete Generative Models}} for {{Sentence Compression}}},
  booktitle = {{{EMNLP}}},
  author = {Miao, Y. and Blunsom, P.},
  date = {2016},
  pages = {319--328},
  issn = {978-3-319-10589-5},
  doi = {10.1007/978-3-319-10590-1_53},
  url = {http://arxiv.org/abs/1609.07317},
  abstract = {In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.},
  annotation = {9170 citations (Semantic Scholar/DOI) [2021-03-26] 166 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {26353135},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VSEN8C6Y/Miao, Blunsom - 2016 - Language as a Latent Variable Discrete Generative Models for Sentence Compression(2).pdf},
  isbn = {978-3-319-10589-5},
  keywords = {u}
}

@inproceedings{miculicich2018DocumentLevelNeuralMachine,
  title = {Document-{{Level Neural Machine Translation}} with {{Hierarchical Attention Networks}}},
  booktitle = {{{EMNLP}}},
  author = {Miculicich, Lesly and Ram, Dhananjay and Pappas, Nikolaos and Henderson, James},
  date = {2018-10-01},
  url = {http://arxiv.org/abs/1809.01576},
  urldate = {2021-03-08},
  abstract = {Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.},
  annotation = {111 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1809.01576},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9KI4EADN/Miculicich et al. - 2018 - Document-Level Neural Machine Translation with Hie.pdf;/home/hiaoxui/.local/share/zotero_files/storage/UJPFEJWN/1809.html}
}

@inproceedings{miculicich2019PartiallysupervisedMentionDetection,
  title = {Partially-Supervised {{Mention Detection}}},
  booktitle = {Workshop on {{Computational Models}} of {{Reference}}, {{Anaphora}} and {{Coreference}}},
  author = {Miculicich, Lesly and Henderson, James},
  date = {2019-08-26},
  url = {http://arxiv.org/abs/1908.09507},
  urldate = {2021-03-08},
  abstract = {Learning to detect entity mentions without using syntactic information can be useful for integration and joint optimization with other tasks. However, it is common to have partially annotated data for this problem. Here, we investigate two approaches to deal with partial annotation of mentions: weighted loss and soft-target classification. We also propose two neural mention detection approaches: a sequence tagging, and an exhaustive search. We evaluate our methods with coreference resolution as a downstream task, using multitask learning. The results show that the recall and F1 score improve for all methods.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1908.09507},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EDENMVZ5/Miculicich and Henderson - 2019 - Partially-supervised Mention Detection.pdf;/home/hiaoxui/.local/share/zotero_files/storage/CCJR465W/1908.html}
}

@inproceedings{mielke2018SpellOnceSummon,
  title = {Spell {{Once}}, {{Summon Anywhere}}: {{A Two}}-{{Level Open}}-{{Vocabulary Language Model}}},
  booktitle = {{{AAAI}}},
  author = {Mielke, S. J. and Eisner, J. M.},
  date = {2018},
  url = {http://arxiv.org/abs/1804.08205},
  abstract = {We show how the spellings of known words can help us deal with unknown words in open-vocabulary NLP tasks. The method we propose can be used to extend any closed-vocabulary generative model, but in this paper we specifically consider the case of neural language modeling. Our Bayesian generative story combines a standard RNN language model (generating the word tokens in each sentence) with an RNN-based spelling model (generating the letters in each word type). These two RNNs respectively capture sentence structure and word structure, and are kept separate as in linguistics. By invoking the second RNN to generate spellings for novel words in context, we obtain an open-vocabulary language model. For known words, embeddings are naturally inferred by combining evidence from type spelling and token context. Comparing to baselines (including a novel strong baseline), we beat previous work and establish state-of-the-art results on multiple datasets.},
  annotation = {20 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.08205},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UZV7ZV6P/Mielke, Eisner - 2018 - Spell Once, Summon Anywhere A Two-Level Open-Vocabulary Language Model(2).pdf},
  number = {3}
}

@inproceedings{mielke2019WhatKindLanguage,
  title = {What {{Kind}} of {{Language Is Hard}} to {{Language}}-{{Model}}?},
  booktitle = {{{ACL}}},
  author = {Mielke, S. J. and Cotterell, R. and Gorman, K. and Roark, B. and Eisner, J. M.},
  date = {2019},
  url = {http://arxiv.org/abs/1906.04726},
  abstract = {How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that "translationese" is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.},
  annotation = {20 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1906.04726},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YXWQ8DGD/Mielke et al. - 2019 - What Kind of Language Is Hard to Language-Model(2).pdf},
  keywords = {u}
}

@inproceedings{mihalcea2007CharacterizingHumourExploration,
  title = {Characterizing {{Humour}} : {{An Exploration}} of {{Features}} in {{Humorous Texts}}},
  booktitle = {{{CICLing}}},
  author = {Mihalcea, R. and Pulman, S.},
  date = {2007},
  pages = {337--347},
  issn = {03029743},
  abstract = {This paper investigates the problem of automatic humour recognition, and provides and in-depth analysis of two of the most frequently observed features of humorous text: human-centeredness and negative polarity. Through experiments performed on two collections of humorous texts, we show that these properties of verbal humour are consistent across different data sets. Springet-Verlag Berlin Heidelberg 2001.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CXRRIPNZ/Mihalcea, Pulman - 2007 - Characterizing Humour An Exploration of Features in Humorous Texts(2).pdf},
  isbn = {3-540-70938-X}
}

@inproceedings{mikolov2013DistributedRepresentationsWords,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and {{Their Compositionality}}},
  booktitle = {{{NeurIPS}}},
  author = {Mikolov, T. and Chen, K. and Corrado, G. and Dean, J.},
  date = {2013},
  pages = {1--9},
  issn = {10495258},
  doi = {10.1162/jmlr.2003.3.4-5.951},
  annotation = {405 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {903},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LBWF9CFX/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and Their Compositionality(2).pdf},
  isbn = {2150-8097}
}

@report{mikolov2013EfficientEstimationWord,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, T. and Corrado, G. and Chen, K. and Dean, J.},
  date = {2013},
  pages = {1--12},
  archiveprefix = {arXiv},
  eprint = {1301.3781v3},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JV2V3RDJ/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space(2).pdf}
}

@article{miller1995WordNetLexicalDatabase,
  title = {{{WordNet}}: {{A Lexical Database}} for {{English}}},
  author = {Miller, G. A.},
  date = {1995},
  journaltitle = {Communications of the ACM},
  volume = {38},
  pages = {39--41},
  issn = {15577317},
  doi = {10.1145/219717.219748},
  abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonims that are in turn link through semantic relations that determine word definitions.},
  annotation = {9994 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KQYXPW7V/Miller - 1995 - WordNet A Lexical Database for English(2).pdf},
  number = {11}
}

@inproceedings{miller1996FullyStatisticalApproach,
  title = {A {{Fully Statistical Approach}} to {{Natural Language Interfaces}}},
  booktitle = {{{ACL}}},
  author = {Miller, S. and Stallard, D. and Bobrow, R. and Schwartz, R.},
  date = {1996},
  pages = {55--61},
  doi = {10.3115/981863.981871},
  url = {http://www.aclweb.org/anthology/P96-1008},
  annotation = {152 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G7M6JGF6/Miller et al. - 1996 - A Fully Statistical Approach to Natural Language Interfaces(2).pdf},
  keywords = {u}
}

@inproceedings{minard2016MEANTIMENewsReaderMultilingual,
  title = {{{MEANTIME}}, the {{NewsReader}} Multilingual Event and {{TIME}} Corpus},
  booktitle = {{{LREC}}},
  author = {Minard, A. L. and Speranza, M. and Urizar, R. and Altuna, B. and Van Erp, M. and Schoen, A. and Van Son, C.},
  date = {2016},
  pages = {4417--4422},
  abstract = {In this paper, we present the NewsReaderMEANTIMEcorpus, a semantically annotated corpus ofWikinews articles. The corpus consists of 480 news articles, i.e. 120 English news articles and their translations in Spanish, Italian, and Dutch. MEANTIME contains anno- tations at different levels. The document-level annotation includes markables (e.g. entity mentions, event mentions, time expressions, and numerical expressions), relations between markables (modeling, for example, temporal information and semantic role labeling), and entity and event intra-document coreference. The corpus-level annotation includes entity and event cross-document coreference. Semantic annotation on the English section was performed manually; for the annotation in Italian, Spanish, and (partially) Dutch, a pro- cedure was devised to automatically project the annotations on the English texts onto the translated texts, based on the manual alignment of the annotated elements; this enabled us not only to speed up the annotation process but also provided cross-lingual coreference. The English section of the corpus was extended with timeline annotations for the SemEval 2015 TimeLine shared task. The First CLIN Dutch Shared Task at CLIN26 was based on the Dutch section, while the EVALITA 2016 FactA (Event Factuality Annotation) shared task, based on the Italian section, is currently being organized},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TA4FY3G2/Minard et al. - 2016 - MEANTIME, the NewsReader multilingual event and TIME corpus(2).pdf},
  isbn = {978-2-9517408-9-1}
}

@inproceedings{mirza2016CATENACAusalTemporal,
  title = {{{CATENA}}: {{CAusal}} and Temporal Relation Extraction from Natural Language Texts},
  booktitle = {{{COLING}}},
  author = {Mirza, P. and Tonelli, S.},
  date = {2016},
  pages = {64--75},
  abstract = {We present CATENA, a sieve-based system to perform temporal and causal relation extraction and classification from English texts, exploiting the interaction between the temporal and the causal model. We evaluate the performance of each sieve, showing that the rule-based, the machine-learned and the reasoning components all contribute to achieving state-of-the-art performance on TempEval-3 and TimeBank-Dense data. Although causal relations are much sparser than temporal ones, the architecture and the selected features are mostly suitable to serve both tasks. The effects of the interaction between the temporal and the causal components, although limited, yield promising results and confirm the tight connection between the temporal and the causal dimension of texts.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XJZEPZZI/Mirza, Tonelli - 2016 - CATENA CAusal and temporal relation extraction from natural language texts(2).pdf},
  isbn = {978-4-87974-702-0},
  keywords = {u}
}

@inproceedings{misra2017MappingInstructionsVisual,
  title = {Mapping {{Instructions}} and {{Visual Observations}} to {{Actions}} with {{Reinforcement Learning}}},
  booktitle = {{{EMNLP}}},
  author = {Misra, D. and Langford, J. and Artzi, Y.},
  date = {2017},
  url = {http://arxiv.org/abs/1704.08795},
  abstract = {We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent's exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.},
  annotation = {126 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.08795},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ANQ8ECIG/Misra, Langford, Artzi - 2017 - Mapping Instructions and Visual Observations to Actions with Reinforcement Learning(2).pdf},
  keywords = {u}
}

@article{miyahara2020QuantumExpectationmaximizationAlgorithm,
  title = {Quantum Expectation-Maximization Algorithm},
  author = {Miyahara, H. and Aihara, K. and Lechner, W.},
  date = {2020},
  journaltitle = {Physical Review A},
  volume = {101},
  issn = {24699934},
  doi = {10.1103/PhysRevA.101.012326},
  abstract = {Clustering algorithms are a cornerstone of machine learning applications. Recently, a quantum algorithm for clustering based on the k-means algorithm has been proposed by Kerenidis, Landman, Luongo, and Prakash. Based on their work, we propose a quantum expectation-maximization algorithm for Gaussian mixture models (GMMs). The robustness and quantum speedup of the algorithm are shown. We also show numerically the advantage of GMM over k-means algorithm for nontrivial cluster data.},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1908.06655},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PGU9S4FM/Miyahara, Aihara, Lechner - 2020 - Quantum expectation-maximization algorithm(2).pdf},
  number = {1}
}

@report{modi2017InScriptNarrativeTexts,
  title = {{{InScript}}: {{Narrative}} Texts Annotated with Script Information},
  shorttitle = {{{InScript}}},
  author = {Modi, Ashutosh and Anikina, Tatjana and Ostermann, Simon and Pinkal, Manfred},
  date = {2017-03-15},
  url = {http://arxiv.org/abs/1703.05260},
  urldate = {2020-10-21},
  abstract = {This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.},
  annotation = {28 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1703.05260},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PS9HHBN2/Modi et al. - 2017 - InScript Narrative texts annotated with script in.pdf},
  keywords = {u},
  langid = {english}
}

@article{mohri1997FiniteStateTransducersLanguage,
  title = {Finite-{{State Transducers}} in {{Language}} and {{Speech Processing}}},
  author = {Mohri, M.},
  date = {1997},
  journaltitle = {Computational Linguistics},
  volume = {23},
  pages = {268--311},
  issn = {08912017},
  abstract = {Finite-state machines have been used in various domains of natural language processing. We consider here the use of a type of transducer that supports very efficient programs: sequential transducers. We recall classical theorems and give new ones characterizing sequential string-to-string transducers. Transducers that output weights also play an important role in language and speech processing. We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms. Some applications of these algorithms in speech recognition are described and illustrated.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/88BD8CPT/Mohri - 1997 - Finite-State Transducers in Language and Speech Processing(2).pdf},
  number = {2}
}

@inproceedings{moore2004ImprovingIBMWordalignment,
  title = {Improving {{IBM}} Word-Alignment Model 1},
  booktitle = {{{ACL}}},
  author = {Moore, R. C.},
  date = {2004},
  pages = {518-es},
  doi = {10.3115/1218955.1219021},
  url = {http://portal.acm.org/citation.cfm?doid=1218955.1219021},
  abstract = {We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1. We demonstrate reduction in alignment error rate of approximately 30\% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.},
  annotation = {115 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QCPA3ZMM/Moore - 2004 - Improving IBM word-alignment model 1(2).pdf},
  keywords = {u}
}

@report{moradshahi2020LocalizingOpenOntologyQA,
  title = {Localizing {{Open}}-{{Ontology QA Semantic Parsers}} in a {{Day Using Machine Translation}}},
  author = {Moradshahi, Mehrad and Campagna, Giovanni and Semnani, Sina J. and Xu, Silei and Lam, Monica S.},
  date = {2020-10-10},
  url = {http://arxiv.org/abs/2010.05106},
  urldate = {2020-10-28},
  abstract = {We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators. We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains. Our models achieve an overall test accuracy ranging between 61\% and 69\% for the hotels domain and between 64\% and 78\% for restaurants domain, which compares favorably to 69\% and 80\% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-the-art methodology by more than 30\% for hotels and 40\% for restaurants with localized ontologies for the subset of languages tested. Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours.},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2010.05106},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JTE238V8/Moradshahi et al. - 2020 - Localizing Open-Ontology QA Semantic Parsers in a .pdf;/home/hiaoxui/.local/share/zotero_files/storage/3SF48TQL/2010.html},
  keywords = {u}
}

@article{moro2014EntityLinkingMeets,
  title = {Entity {{Linking}} Meets {{Word Sense Disambiguation}}: A {{Unified Approach}}},
  author = {Moro, A. and Raganato, A. and Navigli, R.},
  date = {2014},
  journaltitle = {TACL},
  volume = {2},
  pages = {231--244},
  issn = {2307-387X},
  abstract = {Entity Linking (EL) and Word Sense Disambiguation (WSD) both address the lexical ambiguity of language. But while the two tasks are pretty similar, they differ in a fundamental respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while in WSD there is a perfect match between the word form (better, its lemma) and a suitable word sense. In this paper we present Babelfy, a unified graph-based approach to EL and WSD based on a loose identification of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence semantic interpretations. Our experiments show state-of- the-art performances on both tasks on 6 different datasets, including a multilingual setting. Babelfy is online at http://babelfy.org},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/47P7V447/Moro, Raganato, Navigli - 2014 - Entity Linking meets Word Sense Disambiguation a Unified Approach(2).pdf},
  number = {0}
}

@inproceedings{mukherjee2018ARMDNAssociativeRecurrent,
  title = {{{AR}}-{{MDN}}: {{Associative}} and {{Recurrent Mixture Density Networks}} for {{eRetail Demand Forecasting}}},
  booktitle = {Very {{Large Data Bases}}},
  author = {Mukherjee, S. and Shankar, D. and Ghosh, A. and Tathawadekar, N. and Kompalli, P. and Sarawagi, S. and Chaudhury, K.},
  date = {2018},
  url = {http://arxiv.org/abs/1803.03800},
  abstract = {Accurate demand forecasts can help on-line retail organizations better plan their supply-chain processes. The challenge, however, is the large number of associative factors that result in large, non-stationary shifts in demand, which traditional time series and regression approaches fail to model. In this paper, we propose a Neural Network architecture called AR-MDN, that simultaneously models associative factors, time-series trends and the variance in the demand. We first identify several causal features and use a combination of feature embeddings, MLP and LSTM to represent them. We then model the output density as a learned mixture of Gaussian distributions. The AR-MDN can be trained end-to-end without the need for additional supervision. We experiment on a dataset of an year's worth of data over tens-of-thousands of products from Flipkart. The proposed architecture yields a significant improvement in forecasting accuracy when compared with existing alternatives.},
  annotation = {13 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1803.03800},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RN59VVI8/Mukherjee et al. - 2018 - AR-MDN Associative and Recurrent Mixture Density Networks for eRetail Demand Forecasting(2).pdf}
}

@inproceedings{murakami2017LearningGenerateMarket,
  title = {Learning to {{Generate Market Comments}} from {{Stock Prices}}},
  booktitle = {{{ACL}}},
  author = {Murakami, S. and Watanabe, A. and Miyazawa, A. and Goshima, K. and Yanase, T. and Takamura, H. and Miyao, Y.},
  date = {2017},
  pages = {1374--1384},
  doi = {10.18653/v1/P17-1126},
  url = {https://doi.org/10.18653/v1/P17-1126},
  abstract = {This paper presents a novel encoder-decoder model for automatically generat-ing market comments from stock prices. The model first encodes both short-and long-term series of stock prices so that it can mention short-and long-term changes in stock prices. In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic op-eration such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best model generates market comments at the fluency and the informativeness approach-ing human-generated reference texts.},
  annotation = {32 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T533VHJW/Murakami et al. - 2017 - Learning to Generate Market Comments from Stock Prices(2).pdf},
  isbn = {978-1-945626-75-3}
}

@inproceedings{murdoch2017AutomaticRuleExtraction,
  title = {Automatic Rule Extraction from Long Short Term Memory Networks},
  booktitle = {{{ICLR}}},
  author = {Murdoch, J. and Szlam, A.},
  date = {2017},
  volume = {4},
  pages = {221--226},
  issn = {23208430},
  doi = {10.5121/ijci.2015.4221},
  abstract = {Although deep learning models have proven effective at solving problems in natu-ral language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of repre-sentative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.},
  archiveprefix = {arXiv},
  eprint = {1702.02540},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WTRYTYHV/Murdoch, Szlam - 2017 - Automatic rule extraction from long short term memory networks(2).pdf},
  number = {2}
}

@inproceedings{naik2018StressTestEvaluation,
  title = {Stress {{Test Evaluation}} for {{Natural Language Inference}}},
  booktitle = {{{COLING}}},
  author = {Naik, A. and Ravichander, A. and Sadeh, N. and Rose, C. and Neubig, G.},
  date = {2018},
  url = {http://arxiv.org/abs/1806.00692},
  abstract = {Natural language inference (NLI) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. NLI was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for NLI, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed "stress tests" that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area.},
  annotation = {112 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1806.00692},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3APKKDAR/Naik et al. - 2018 - Stress Test Evaluation for Natural Language Inference(2).pdf},
  keywords = {u}
}

@inproceedings{namysl2020NATNoiseAwareTraining,
  title = {{{NAT}}: {{Noise}}-{{Aware Training}} for {{Robust Neural Sequence Labeling}}},
  shorttitle = {{{NAT}}},
  booktitle = {{{ACL}}},
  author = {Namysl, Marcin and Behnke, Sven and Köhler, Joachim},
  date = {2020-05-14},
  url = {http://arxiv.org/abs/2005.07162},
  urldate = {2021-03-13},
  abstract = {Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs - as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. We employ a vanilla noise model at training time. For evaluation, we use both the original data and its variants perturbed with real OCR errors and misspellings. Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input. We make our code and data publicly available for the research community.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2005.07162},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M28DJHKU/Namysl et al. - 2020 - NAT Noise-Aware Training for Robust Neural Sequen.pdf;/home/hiaoxui/.local/share/zotero_files/storage/LXMX3FVQ/2005.html}
}

@inproceedings{napoles2012AnnotatedGigaword,
  title = {Annotated {{Gigaword}}},
  booktitle = {Joint {{Workshop}} on {{Automatic Knowledge Base Construction}} and {{Web}}-Scale {{Knowledge Extraction}}},
  author = {Napoles, C. and Gormley, M. and Van Durme, B.},
  date = {2012},
  pages = {95--100},
  url = {http://dl.acm.org/citation.cfm?id=2391218},
  abstract = {We have created layers of annotation on the English Gigaword v.5 corpus to render it useful as a standardized corpus for knowledge extraction and distributional semantics. Most existing large-scale work is based on inconsistent corpora which often have needed to be re-annotated by research teams independently, each time introducing biases that manifest as results that are only comparable at a high level. We provide to the community a public reference set based on current state-of-the-art syntactic analysis and coreference resolution, along with an interface for programmatic access. Our goal is to enable broader involvement in large-scale knowledge-acquisition efforts by researchers that otherwise may not have had the ability to produce such a resource on their own.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SVS5LU4K/Napoles, Gormley, Van Durme - 2012 - Annotated Gigaword(2).pdf},
  keywords = {u}
}

@inproceedings{narisawa2013204CmMan,
  title = {Is a 204 Cm {{Man Tall}} or {{Small}}? {{Acquisition}} of {{Numerical Common Sense}} from the {{Web}}.},
  booktitle = {{{ACL}}},
  author = {Narisawa, K. and Watanabe, Y. and Mizuno, J. and Okazaki, N. and Inui, K.},
  date = {2013},
  pages = {382--391},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZHV2TKHQ/Narisawa et al. - 2013 - Is a 204 cm Man Tall or Small Acquisition of Numerical Common Sense from the Web(2).pdf},
  isbn = {978-1-937284-50-3}
}

@inproceedings{natarajan2008LogicalHierarchicalHidden,
  title = {Logical {{Hierarchical Hidden Markov Models For Modeling User Activities}}},
  booktitle = {Conference on {{Inductive Logic Programming}}},
  author = {Natarajan, S. and Bui, H. H. and Tadepalli, P. and Kersting, K.},
  date = {2008},
  pages = {192--209},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZV5P84FK/Natarajan et al. - 2008 - Logical Hierarchical Hidden Markov Models For Modeling User Activities(2).pdf},
  keywords = {u}
}

@article{navigli2007WordSenseDisambiguation,
  title = {Word {{Sense Disambiguation}}: {{A Survey}}},
  author = {Navigli, R.},
  date = {2007},
  journaltitle = {ACM Computing Surveys},
  volume = {41},
  pages = {1725--1730},
  issn = {10450823},
  doi = {10.1145/1459352.1459355},
  abstract = {Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data and/or do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networksâ€™ links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.},
  annotation = {1726 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {18353985},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8UC9NVNA/Navigli - 2007 - Word Sense Disambiguation A Survey(2).pdf},
  isbn = {0360-0300},
  number = {2}
}

@inproceedings{navigli2019GameTheoryMeets,
  title = {Game {{Theory Meets Embeddings}} : A {{Unified Framework}} for {{Word Sense Disambiguation}}},
  booktitle = {{{EMNLP}}},
  author = {Navigli, R.},
  date = {2019},
  pages = {88--99},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z3U6MRN8/Navigli - 2019 - Game Theory Meets Embeddings a Unified Framework for Word Sense Disambiguation(2).pdf},
  keywords = {u}
}

@incollection{neal1998ViewEmAlgorithm,
  title = {A {{View}} of the {{Em Algorithm}} That {{Justifies Incremental}}, {{Sparse}}, and Other {{Variants}}},
  booktitle = {Learning in {{Graphical Models}}},
  author = {Neal, R. M. and Hinton, G. E.},
  date = {1998},
  pages = {355--368},
  issn = {978-1-932432-41-1},
  doi = {10.1007/978-94-011-5014-9_12},
  url = {http://link.springer.com/10.1007/978-94-011-5014-9_12},
  abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
  archiveprefix = {arXiv},
  eprint = {15991970},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2ZHVE692/Neal, Hinton - 1998 - A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants(2).pdf},
  isbn = {0-262-60032-3},
  keywords = {u}
}

@inproceedings{neelakantan2014EfficientNonparametricEstimation,
  title = {Efficient {{Non}}-Parametric {{Estimation}} of {{Multiple Embeddings}} per {{Word}} in {{Vector Space}}},
  booktitle = {{{EMNLP}}},
  author = {Neelakantan, A. and Shankar, J. and Passos, A. and McCallum, A.},
  date = {2014},
  url = {http://arxiv.org/abs/1504.06654},
  abstract = {There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.},
  annotation = {379 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1504.06654},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2IQ3CHI2/Neelakantan et al. - 2014 - Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space(2).pdf}
}

@inproceedings{nenkova2004EvaluatingContentSelection,
  title = {Evaluating Content Selection in Summarization: {{The}} Pyramid Method},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Nenkova, A. and Passonneau, R.},
  date = {2004},
  pages = {145--152},
  url = {papers2://publication/uuid/DC675E84-0A45-48B7-A26C-F08B4B9398D3},
  abstract = {We present an empirically grounded method for evaluating content selection in summariza- tion. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative im- portance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus im- proves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VC7W2AZM/Nenkova, Passonneau - 2004 - Evaluating content selection in summarization The pyramid method(2).pdf},
  keywords = {u}
}

@article{ng1997CorpusBasedApproachesSemantic,
  title = {Corpus-{{Based Approaches}} to {{Semantic Interpretation}} in {{Natural Language Processing}}},
  author = {Ng, H. T. and Zelle, J.},
  date = {1997},
  journaltitle = {AI Magazine},
  volume = {18},
  pages = {45--64},
  issn = {0738-4602},
  doi = {10.1609/aimag.v18i4.1321},
  abstract = {In recent years, there has been a flurry of research into empirical, corpus-based learning approaches to natural language processing (NLP). Most empir- ical NLP work to date has focused on relatively low-level language processing such as part-of- speech tagging, text segmentation, and syntactic parsing. The success of these approaches has stim- ulated research in using empirical learning tech- niques in other facets of NLP, including semantic analysis—uncovering the meaning of an utter- ance. This article is an introduction to some of the emerging research in the application of corpus- based learning techniques to problems in semantic interpretation. In particular, we focus on two im- portant problems in semantic interpretation, namely, word-sense disambiguation and semantic parsing.},
  annotation = {39 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ABE6IZYF/Ng, Zelle - 1997 - Corpus-Based Approaches to Semantic Interpretation in Natural Language Processing(2).pdf},
  keywords = {u},
  number = {4}
}

@article{nickel2017PoincareEmbeddingsLearning,
  title = {Poincaré Embeddings for Learning Hierarchical Representations},
  author = {Nickel, M. and Kiela, D.},
  date = {2017},
  journaltitle = {NeurIPS},
  pages = {6339--6348},
  issn = {10495258},
  abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space - or more precisely into an n-dimensional Poincaré ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincaré embeddings can outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
  archiveprefix = {arXiv},
  eprint = {1705.08039},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ELITD2KL/Nickel, Kiela - 2017 - Poincaré embeddings for learning hierarchical representations(2).pdf},
  keywords = {u}
}

@inproceedings{niculae2018SparseMAPDifferentiableSparse,
  title = {{{SparseMAP}}: {{Differentiable Sparse Structured Inference}}},
  booktitle = {{{ICML}}},
  author = {Niculae, V. and Martins, A. F. T. and Blondel, M. and Cardie, C.},
  date = {2018},
  url = {http://arxiv.org/abs/1802.04223},
  abstract = {Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function. SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns probability mass to all structures, including implausible ones. Importantly, SparseMAP can be computed using only calls to a MAP oracle, making it applicable to problems with intractable marginal inference, e.g., linear assignment. Sparsity makes gradient backpropagation efficient regardless of the structure, enabling us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.},
  annotation = {57 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1802.04223},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2DF96G92/Niculae et al. - 2018 - SparseMAP Differentiable Sparse Structured Inference(2).pdf}
}

@report{nie2018IncorporatingConsistencyVerification,
  title = {Incorporating {{Consistency Verification}} into {{Neural Data}}-to-{{Document Generation}}},
  author = {Nie, F. and Chen, H. and Wang, J. and Yao, J. and Lin, C. and Pan, R.},
  date = {2018},
  url = {http://arxiv.org/abs/1808.05306},
  abstract = {Recent neural models for data-to-document generation have achieved remarkable progress in producing fluent and informative texts. However, large proportions of generated texts do not actually conform to the input data. To address this issue, we propose a new training framework which attempts to verify the consistency between the generated texts and the input data to guide the training process. To measure the consistency, a relation extraction model is applied to check information overlaps between the input data and the generated texts. The non-differentiable consistency signal is optimized via reinforcement learning. Experimental results on a recently released challenging dataset ROTOWIRE show improvements from our framework in various metrics.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1808.05306},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B9ZPP77T/Nie et al. - 2018 - Incorporating Consistency Verification into Neural Data-to-Document Generation(2).pdf},
  keywords = {u}
}

@inproceedings{nie2018OperationsGuidedNeuralNetworks,
  title = {Operations-{{Guided Neural Networks}} for {{High Fidelity Data}}-{{To}}-{{Text Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Nie, F. and Wang, J. and Yao, J. and Pan, R. and Lin, C.},
  date = {2018},
  url = {http://arxiv.org/abs/1809.02735},
  abstract = {Recent neural models for data-to-text generation are mostly based on data-driven end-to-end training over encoder-decoder networks. Even though the generated texts are mostly fluent and informative, they often generate descriptions that are not consistent with the input structured data. This is a critical issue especially in domains that require inference or calculations over raw data. In this paper, we attempt to improve the fidelity of neural data-to-text generation by utilizing pre-executed symbolic operations. We propose a framework called Operation-guided Attention-based sequence-to-sequence network (OpAtt), with a specifically designed gating mechanism as well as a quantization module for operation results to utilize information from pre-executed operations. Experiments on two sports datasets show our proposed method clearly improves the fidelity of the generated texts to the input structured data.},
  annotation = {27 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1809.02735},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L5LZBANA/Nie et al. - 2018 - Operations-Guided Neural Networks for High Fidelity Data-To-Text Generation(2).pdf},
  keywords = {u}
}

@inproceedings{nigam2000AnalyzingEffectivenessApplicability,
  title = {Analyzing the Effectiveness and Applicability of Co-Training},
  booktitle = {{{CIKM}}},
  author = {Nigam, K. and Ghani, R.},
  date = {2000},
  pages = {86--93},
  doi = {10.1145/354756.354805},
  abstract = {Recen tly there has been signi?can tin terest in supervised learning algorithms that com bine labeled and unlabeled data for text learning tasks? The co?training setting ??? applies to datasets that ha e a natural separation of their features in v to t o disjoin w t sets? We demonstrate that when learning from labeled and unlabeled data? algorithms explicitly lev eraging a natural independen t split of the features outperform al? gorithms that do not? When a natural split does not exist? co?training algorithms that man ufacture a feature split ma y out?perform algorithms not using a split? These results help explain wh y co?training algorithms are both discriminativ e in nature and robust to the assumptions of their em bedded classi?ers?},
  annotation = {1001 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/98PPJAY8/Nigam, Ghani - 2000 - Analyzing the effectiveness and applicability of co-training(2).pdf}
}

@inproceedings{ning2018JointReasoningTemporal,
  title = {Joint Reasoning for Temporal and Causal Relations},
  booktitle = {{{ACL}}},
  author = {Ning, Q. and Feng, Z. and Wu, H. and Roth, D.},
  date = {2018},
  pages = {2278--2288},
  abstract = {Understanding temporal and causal relations between events is a fundamental natural language understanding task. Because a cause must be before its effect in time, temporal and causal relations are closely related and one relation even dictates the other one in many cases. However, limited attention has been paid to studying these two relations jointly. This paper presents a joint inference framework for them using constrained conditional models (CCMs). Specifically, we formulate the joint problem as an integer linear programming (ILP) problem, enforcing constraints inherently in the nature of time and causality. We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KDRATEGP/Ning et al. - 2018 - Joint reasoning for temporal and causal relations(2).pdf},
  isbn = {978-1-948087-32-2}
}

@inproceedings{nokland2019TrainingNeuralNetworks,
  title = {Training {{Neural Networks}} with {{Local Error Signals}}},
  booktitle = {{{ICML}}},
  author = {Nøkland, A. and Eidnes, L. H.},
  date = {2019},
  url = {http://arxiv.org/abs/1901.06656},
  abstract = {Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back-propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss functions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility. Code is available https://github.com/anokland/local-loss},
  annotation = {54 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1901.06656},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UFMJFI2R/Nøkland, Eidnes - 2019 - Training Neural Networks with Local Error Signals(2).pdf},
  keywords = {u}
}

@inproceedings{nowozin2016FGANTrainingGenerative,
  title = {F-{{GAN}}: {{Training Generative Neural Samplers}} Using {{Variational Divergence Minimization}}},
  shorttitle = {F-{{GAN}}},
  booktitle = {{{NeurIPS}}},
  author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  date = {2016-06-02},
  url = {http://arxiv.org/abs/1606.00709},
  urldate = {2021-03-16},
  abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
  annotation = {890 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1606.00709},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XF6KJK5H/Nowozin et al. - 2016 - f-GAN Training Generative Neural Samplers using V.pdf;/home/hiaoxui/.local/share/zotero_files/storage/V5YCVH9H/1606.html}
}

@inproceedings{och2003MinimumErrorRate,
  title = {Minimum {{Error Rate Training}} in {{Statistical Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Och, F. J.},
  date = {2003},
  pages = {160--167},
  doi = {10.3115/1075096.1075117},
  annotation = {3219 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AUWRCB93/Och - 2003 - Minimum Error Rate Training in Statistical Machine Translation(2).pdf}
}

@article{och2003SystematicComparisonVarious,
  title = {A {{Systematic Comparison}} of {{Various Statistical Alignment Models}}},
  author = {Och, F. J. and Ney, H.},
  date = {2003},
  journaltitle = {Computational Linguistics},
  volume = {29},
  pages = {19--51},
  issn = {0891-2017},
  doi = {10.1162/089120103321337421},
  url = {http://www.mitpressjournals.org/doi/10.1162/089120103321337421},
  abstract = {We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.},
  annotation = {4259 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2GXISXFF/Och, Ney - 2003 - A Systematic Comparison of Various Statistical Alignment Models(2).pdf},
  isbn = {0891-2017},
  number = {1}
}

@inproceedings{ogorman2016RicherEventDescription,
  title = {Richer {{Event Description}}: {{Integrating}} Event Coreference with Temporal, Causal and Bridging Annotation},
  booktitle = {Workshop on {{Computing News Storylines}}},
  author = {O'Gorman, T. and Wright-Bettner, K. and Palmer, M.},
  date = {2016},
  pages = {47--56},
  doi = {10.18653/v1/w16-5706},
  abstract = {We examine the effect of industry life-cycle stages on within-industry acquisitions and capital expenditures by conglomerates and single-segment firms controlling for endogeneity of organizational form.We find greater differences in acquisitions than in capital expenditures, which are similar across organizational types. In particular, 36\% of the growth recorded by conglomerate segments in growth industries comes from acquisitions, versus 9\% for single-segment firms. In growth industries, the effect of fi- nancial dependence on acquisitions and plant openings is mitigated for conglomerate firms. Plants acquired by conglomerate firms increase in productivity. The results suggest that organizational forms’ comparative advantages differ across industry conditions.},
  annotation = {62 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T8MQCVLR/O'Gorman, Wright-Bettner, Palmer - 2016 - Richer Event Description Integrating event coreference with temporal, causal and bridging a(2).pdf},
  keywords = {u}
}

@inproceedings{oren2019DistributionallyRobustLanguage,
  title = {Distributionally {{Robust Language Modeling}}},
  booktitle = {{{EMNLP}}},
  author = {Oren, Y. and Sagawa, S. and Hashimoto, T. B. and Liang, P.},
  date = {2019},
  volume = {2},
  url = {http://arxiv.org/abs/1909.02060},
  abstract = {Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.},
  annotation = {21 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.02060},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CBYHWSKA/Oren et al. - 2019 - Distributionally Robust Language Modeling(2).pdf}
}

@report{oroojlooyjadid2017DeepQNetworkBeer,
  title = {A {{Deep Q}}-{{Network}} for the {{Beer Game}}: {{A Reinforcement Learning}} Algorithm to {{Solve Inventory Optimization Problems}}},
  author = {Oroojlooyjadid, A. and Nazari, M. and Snyder, L. and Takáč, M.},
  date = {2017},
  pages = {1--38},
  url = {http://arxiv.org/abs/1708.05924},
  abstract = {The beer game is a widely used in-class game that is played in supply chain management classes to demonstrate the bullwhip effect. The game is a decentralized, multi-agent, cooperative problem that can be modeled as a serial supply chain network in which agents cooperatively attempt to minimize the total cost of the network even though each agent can only observe its own local information. Each agent chooses order quantities to replenish its stock. Under some conditions, a base-stock replenishment policy is known to be optimal. However, in a decentralized supply chain in which some agents (stages) may act irrationally (as they do in the beer game), there is no known optimal policy for an agent wishing to act optimally. We propose a machine learning algorithm, based on deep Q-networks, to optimize the replenishment decisions at a given stage. When playing alongside agents who follow a base-stock policy, our algorithm obtains near-optimal order quantities. It performs much better than a base-stock policy when the other agents use a more realistic model of human ordering behavior. Unlike most other algorithms in the literature, our algorithm does not have any limits on the beer game parameter values. Like any deep learning algorithm, training the algorithm can be computationally intensive, but this can be performed ahead of time; the algorithm executes in real time when the game is played. Moreover, we propose a transfer learning approach so that the training performed for one agent and one set of cost coefficients can be adapted quickly for other agents and costs. Our algorithm can be extended to other decentralized multi-agent cooperative games with partially observed information, which is a common type of situation in real-world supply chain problems.},
  annotation = {19 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1708.05924},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ST7EP3PW/Oroojlooyjadid et al. - 2017 - A Deep Q-Network for the Beer Game A Reinforcement Learning algorithm to Solve Inventory Optimization (2).pdf}
}

@inproceedings{ouchi2018SpanSelectionModel,
  title = {A {{Span Selection Model}} for {{Semantic Role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Ouchi, Hiroki and Shindo, Hiroyuki and Matsumoto, Yuji},
  date = {2018-10-04},
  url = {http://arxiv.org/abs/1810.02245},
  urldate = {2020-08-21},
  abstract = {We present a simple and accurate span-based model for semantic role labeling (SRL). Our model directly takes into account all possible argument spans and scores them for each label. At decoding time, we greedily select higher scoring labeled spans. One advantage of our model is to allow us to design and use spanlevel features, that are difficult to use in tokenbased BIO tagging approaches. Experimental results demonstrate that our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively.},
  annotation = {43 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1810.02245},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/849763GQ/Ouchi et al. - 2018 - A Span Selection Model for Semantic Role Labeling.pdf},
  keywords = {u},
  langid = {english}
}

@report{paige2014AsynchronousAnytimeSequential,
  title = {Asynchronous {{Anytime Sequential Monte Carlo}}},
  author = {Paige, B. and Wood, F. and Doucet, A. and Teh, Y. W.},
  date = {2014},
  pages = {1--11},
  issn = {10495258},
  url = {http://arxiv.org/abs/1407.2864},
  abstract = {We introduce a new sequential Monte Carlo algorithm we call the particle cascade. The particle cascade is an asynchronous, anytime alternative to traditional particle filtering algorithms. It uses no barrier synchronizations which leads to improved particle throughput and memory efficiency. It is an anytime algorithm in the sense that it can be run forever to emit an unbounded number of particles while keeping within a fixed memory budget. We prove that the particle cascade is an unbiased marginal likelihood estimator which means that it can be straightforwardly plugged into existing pseudomarginal methods.},
  annotation = {46 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1407.2864},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HV3HWSJZ/Paige et al. - 2014 - Asynchronous Anytime Sequential Monte Carlo(2).pdf},
  keywords = {u}
}

@article{palmer2004PropositionBankAnnotated,
  title = {The {{Proposition Bank}}: {{An Annotated Corpus}} of {{Semantic Roles}}},
  author = {Palmer, M. and Gildea, D. and Kingsbury, P.},
  date = {2004},
  journaltitle = {Computational Linguistics},
  volume = {31},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4UDBP4H6/Palmer, Gildea, Kingsbury - 2004 - The Proposition Bank An Annotated Corpus of Semantic Roles(2).pdf}
}

@misc{palmer2009SemLinkLinkingPropBank,
  title = {{{SemLink}} - {{Linking PropBank}} , {{VerbNet}}, {{FrameNet}}},
  author = {Palmer, M.},
  date = {2009},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R5QLPUDH/Palmer - 2009 - SemLink - Linking PropBank , VerbNet, FrameNet(2).pdf}
}

@inproceedings{pan2020LearningConstraintsStructured,
  title = {Learning {{Constraints}} for {{Structured Prediction Using Rectifier Networks}}},
  booktitle = {{{ACL}}},
  author = {Pan, X. and Mehta, M. and Srikumar, V.},
  date = {2020},
  pages = {4843--4858},
  url = {http://arxiv.org/abs/2006.01209},
  abstract = {Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2006.01209},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJRCKNZZ/Pan, Mehta, Srikumar - 2020 - Learning Constraints for Structured Prediction Using Rectifier Networks(2).pdf},
  keywords = {u}
}

@inproceedings{pang2002ThumbsSentimentClassification,
  title = {Thumbs up? {{Sentiment Classification}} Using {{Machine Learning Techniques}}},
  booktitle = {{{EMNLP}}},
  author = {Pang, B. and Lee, L. and Vaithyanathan, S.},
  date = {2002},
  issn = {0003-5696},
  doi = {10.1515/9783110239171.151},
  abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G82SEUXI/Pang, Lee, Vaithyanathan - 2002 - Thumbs up Sentiment Classification using Machine Learning Techniques(2).pdf}
}

@inproceedings{papai2012SliceNormalizedDynamic,
  title = {Slice {{Normalized Dynamic Markov Logic Networks}}},
  booktitle = {{{NeurIPS}}},
  author = {Papai, T. and Kautz, H. and Stefankovic, D.},
  date = {2012},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ABRZYL77/Papai, Kautz, Stefankovic - 2012 - Slice Normalized Dynamic Markov Logic Networks(2).pdf},
  keywords = {u}
}

@inproceedings{papineni2002BLEUMethodAutomatic,
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  booktitle = {{{ACL}}},
  author = {Papineni, K. and Roukos, S. and Ward, T. and Zhu, W.},
  date = {2002},
  pages = {311--318},
  issn = {00134686},
  doi = {10.3115/1073083.1073135},
  url = {http://dl.acm.org/citation.cfm?id=1073135},
  abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
  annotation = {9993 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1702.00764},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X62S64IZ/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine translation(2).pdf},
  isbn = {1-55860-883-4},
  issue = {July}
}

@inproceedings{paranjape2017MotifsTemporalNetworks,
  title = {Motifs in Temporal Networks},
  booktitle = {{{WSDM}}},
  author = {Paranjape, A. and Benson, A. R. and Leskovec, J.},
  date = {2017},
  pages = {601--610},
  doi = {10.1145/3018661.3018731},
  abstract = {Networks are a fundamental tool for modeling complex systems in a variety of domains including social and communication networks as well as biology and neuroscience. Small subgraph patterns in networks, called network motifs, are crucial to understanding the structure and function of these systems. However, the role of network motifs in temporal networks, which contain many timestamped links between the nodes, is not yet well understood. Here we develop a notion of a temporal network motif as an elementary unit of temporal networks and provide a general methodology for counting such motifs. We define temporal network motifs as induced subgraphs on sequences of temporal edges, design fast algorithms for counting temporal motifs, and prove their runtime complexity. Our fast algorithms achieve up to 56.5x speedup compared to a baseline method. Furthermore, we use our algorithms to count temporal motifs in a variety of networks. Results show that networks from different domains have significantly different motif counts, whereas networks from the same domain tend to have similar motif counts. We also find that different motifs occur at different time scales, which provides further insights into structure and function of temporal networks.},
  annotation = {262 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NNE24E73/Paranjape, Benson, Leskovec - 2017 - Motifs in temporal networks(2).pdf},
  isbn = {978-1-4503-4675-7}
}

@inproceedings{parikh2016DecomposableAttentionModel,
  title = {A {{Decomposable Attention Model}} for {{Natural Language Inference}}},
  booktitle = {{{EMNLP}}},
  author = {Parikh, A. P. and Täckström, O. and Das, D. and Uszkoreit, J.},
  date = {2016},
  url = {http://arxiv.org/abs/1606.01933},
  abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
  annotation = {823 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1606.01933},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZLVLBCDH/Parikh et al. - 2016 - A Decomposable Attention Model for Natural Language Inference(2).pdf}
}

@inproceedings{pasupat2014ZeroshotEntityExtraction,
  title = {Zero-Shot {{Entity Extraction}} from {{Web Pages}}},
  booktitle = {{{ACL}}},
  author = {Pasupat, P. and Liang, P.},
  date = {2014},
  pages = {391--401},
  abstract = {In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction sys-tems rely on seed examples or redundancy across multiple web pages. In this paper, we consider a new zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page. Our approach de-fines a log-linear model over latent extrac-tion predicates, which select lists of enti-ties from the web page. The main chal-lenge is to define features on widely vary-ing candidate entity lists. We tackle this by abstracting list elements and using aggre-gate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HXLJJKRX/Pasupat, Liang - 2014 - Zero-shot Entity Extraction from Web Pages(2).pdf},
  isbn = {978-1-937284-72-5}
}

@inproceedings{pasupat2015CompositionalSemanticParsing,
  title = {Compositional Semantic Parsing on Semi-Structured Tables},
  booktitle = {{{ACL}}-{{IJCNLP}}},
  author = {Pasupat, P. and Liang, P.},
  date = {2015},
  pages = {1470--1480},
  doi = {10.3115/v1/p15-1142},
  url = {http://arxiv.org/abs/1508.00305},
  abstract = {Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: Answering complex questions on semi-structured tables using question-Answer pairs as supervision. The central challenge arises from two compounding factors: The broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.},
  annotation = {277 citations (Semantic Scholar/DOI) [2021-03-26] 277 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1508.00305},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WKRLGAKB/Pasupat, Liang - 2015 - Compositional semantic parsing on semi-structured tables(2).pdf},
  isbn = {978-1-941643-72-3}
}

@inproceedings{pasupat2018MappingNaturalLanguage,
  title = {Mapping Natural Language Commands to Web Elements},
  booktitle = {{{EMNLP}}},
  author = {Pasupat, P. and Jiang, T. and Liu, E. and Guu, K. and Liang, P.},
  date = {2018},
  pages = {4970--4976},
  doi = {10.18653/v1/d18-1540},
  abstract = {The web provides a rich, open-domain environment with textual, structural, and spatial properties. We propose a new task for grounding language in this environment: given a natural language command (e.g., "click on the second article"), choose the correct element on the web page (e.g., a hyperlink or text box). We collected a dataset of over 50,000 commands that capture various phenomena such as functional references (e.g. "find who made this site"), relational reasoning (e.g. "article by john"), and visual reasoning (e.g. "top-most article"). We also implemented and analyzed three baseline models that capture different phenomena present in the dataset.},
  annotation = {10 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1808.09132v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G5NAWKAU/Pasupat et al. - 2018 - Mapping natural language commands to web elements(2).pdf}
}

@inproceedings{pauls2009ConsensusTrainingConsensus,
  title = {Consensus {{Training}} for {{Consensus Decoding}} in {{Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Pauls, A. and DeNero, J. and Klein, D.},
  date = {2009},
  doi = {10.3115/1699648.1699688},
  annotation = {31 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C88KRD9B/Pauls, DeNero, Klein - 2009 - Consensus Training for Consensus Decoding in Machine Translation(2).pdf},
  isbn = {978-1-932432-63-3},
  keywords = {u}
}

@inproceedings{peng2017AddressingDataSparsity,
  title = {Addressing the {{Data Sparsity Issue}} in {{Neural AMR Parsing}}},
  booktitle = {{{EACL}}},
  author = {Peng, X. and Wang, C. and Gildea, D. and Xue, N.},
  date = {2017},
  volume = {1},
  pages = {366--375},
  url = {http://arxiv.org/abs/1702.05053},
  abstract = {Neural attention models have achieved great success in different NLP tasks. How- ever, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we de- scribe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural atten- tion model and our results are also compet- itive against state-of-the-art systems that do not use extra linguistic resources.},
  annotation = {52 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1702.05053},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AKX8FRGV/Peng et al. - 2017 - Addressing the Data Sparsity Issue in Neural AMR Parsing(2).pdf},
  isbn = {978-1-5108-3860-4},
  keywords = {u}
}

@article{peng2017CrossSentenceNaryRelation,
  title = {Cross-{{Sentence N}}-Ary {{Relation Extraction}} with {{Graph LSTMs}}},
  author = {Peng, N. and Poon, H. and Quirk, C. and Toutanova, K.},
  date = {2017},
  journaltitle = {TACL},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WYI69789/Peng et al. - 2017 - Cross-Sentence N-ary Relation Extraction with Graph LSTMs(2).pdf},
  keywords = {u}
}

@inproceedings{peng2017DeepMultitaskLearning,
  title = {Deep Multitask Learning for Semantic Dependency Parsing},
  booktitle = {{{ACL}}},
  author = {Peng, H. and Thomson, S. and Smith, N. A.},
  date = {2017},
  pages = {2037--2048},
  doi = {10.18653/v1/P17-1186},
  abstract = {We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches-one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at https://github.com/Noahs-ARK/NeurboParser.},
  annotation = {112 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.06855},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RKBA4KZP/Peng, Thomson, Smith - 2017 - Deep multitask learning for semantic dependency parsing(2).pdf},
  isbn = {978-1-945626-75-3}
}

@inproceedings{peng2018BackpropagatingStructuredArgmax,
  title = {Backpropagating through {{Structured Argmax}} Using a {{SPIGOT}}},
  booktitle = {{{ACL}}},
  author = {Peng, H. and Thomson, S. and Smith, N. A.},
  date = {2018},
  pages = {1--11},
  url = {http://arxiv.org/abs/1805.04658},
  abstract = {We introduce the structured projection of intermediate gradients optimization technique (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks (Kim et al., 2017) and some reinforcement learning-inspired solutions (Yogatama et al., 2017). Like so-called straight-through estimators (Hinton, 2012), SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT's proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.},
  annotation = {27 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1805.04658},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4YH68767/Peng, Thomson, Smith - 2018 - Backpropagating through Structured Argmax using a SPIGOT(2).pdf}
}

@inproceedings{peng2018ControllableStoryGeneration,
  title = {Towards {{Controllable Story Generation}}},
  booktitle = {{{NAACL}}},
  author = {Peng, N. and Ghazvininejad, M. and May, J. and Knight, K.},
  date = {2018},
  pages = {43--49},
  abstract = {We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence 1 (Egidi and Gerrig, 2009) and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower per-plexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DKM24JBW/Peng et al. - 2018 - Towards Controllable Story Generation(2).pdf}
}

@inproceedings{peng2018LearningJointSemantic,
  title = {Learning {{Joint Semantic Parsers}} from {{Disjoint Data}}},
  booktitle = {{{NAACL}}},
  author = {Peng, H. and Thomson, S. and Swayamdipta, S. and Smith, N. A.},
  date = {2018},
  pages = {1492--1502},
  doi = {10.18653/v1/n18-1135},
  abstract = {We present a new approach to learning semantic parsers from multiple datasets, even when the target semantic formalisms are drastically different, and the underlying corpora do not overlap. We handle such "disjoint" data by treating annotations for unobserved formalisms as latent structured variables. Building on state-of-the-art baselines, we show improvements both in frame-semantic parsing and semantic dependency parsing by modeling them jointly.},
  annotation = {39 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.05990},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2ISSUKKG/Peng et al. - 2018 - Learning Joint Semantic Parsers from Disjoint Data(2).pdf}
}

@inproceedings{peng2019PaLMHybridParser,
  title = {{{PaLM}}: {{A Hybrid Parser}} and {{Language Model}}},
  booktitle = {{{EMNLP}}},
  author = {Peng, H. and Schwartz, R. and Smith, N. A.},
  date = {2019},
  url = {http://arxiv.org/abs/1909.02134},
  abstract = {We present PaLM, a hybrid parser and neural language model. Building on an RNN language model, PaLM adds an attention layer over text spans in the left context. An unsupervised constituency parser can be derived from its attention weights, using a greedy decoding algorithm. We evaluate PaLM on language modeling, and empirically show that it outperforms strong baselines. If syntactic annotations are available, the attention component can be trained in a supervised manner, providing syntactically-informed representations of the context, and further improving language modeling performance.},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.02134},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VEVDRUXH/Peng, Schwartz, Smith - 2019 - PaLM A Hybrid Parser and Language Model(2).pdf}
}

@inproceedings{peng2020RationalRecurrences,
  title = {Rational Recurrences},
  booktitle = {{{EMNLP}}},
  author = {Peng, H. and Schwartz, R. and Thomson, S. and Smith, N. A.},
  date = {2020},
  pages = {1203--1214},
  doi = {10.18653/v1/d18-1152},
  abstract = {Despite the tremendous empirical success of neural models in natural language processing, many of them lack the strong intuitions that accompany classical machine learning approaches. Recently, connections have been shown between convolutional neural networks (CNNs) and weighted finite state automata (WFSAs), leading to new interpretations and insights. In this work, we show that some recurrent neural networks also share this connection to WFSAs. We characterize this connection formally, defining rational recurrences to be recurrent hidden state update functions that can be written as the Forward calculation of a finite set of WFSAs. We show that several recent neural models use rational recurrences. Our analysis provides a fresh view of these models and facilitates devising new neural architectures that draw inspiration from WFSAs. We present one such model, which performs better than two recent baselines on language modeling and text classification. Our results demonstrate that transferring intuitions from classical models like WFSAs can be an effective approach to designing and understanding neural models.},
  annotation = {22 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1808.09357},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z4ADHFF4/Peng et al. - 2020 - Rational recurrences(2).pdf}
}

@inproceedings{peng2021RandomFeatureAttention,
  title = {Random {{Feature Attention}}},
  booktitle = {{{ICLR}}},
  author = {Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A. and Kong, Lingpeng},
  date = {2021-03-19},
  url = {http://arxiv.org/abs/2103.02143},
  urldate = {2021-03-26},
  abstract = {Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2103.02143},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JBY7GGYU/Peng et al. - 2021 - Random Feature Attention.pdf;/home/hiaoxui/.local/share/zotero_files/storage/PEMLB8XB/2103.html}
}

@inproceedings{pennington2014GloVeGlobalVector,
  title = {{{GloVe}}: {{Global Vector}} for {{Word Representation}}},
  booktitle = {{{EMNLP}}},
  author = {Pennington, J. and Socher, R. and Manning, C. D.},
  date = {2014},
  issn = {00047554},
  doi = {10.3115/v1/D14-1162},
  url = {http://nlp.},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  annotation = {9993 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1710995},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CJE4K685/Pennington, Socher, Manning - 2014 - GloVe Global Vector for Word Representation(2).pdf},
  isbn = {978-1-937284-96-1}
}

@inproceedings{perez-beltrachini2018BootstrappingGeneratorsNoisy,
  title = {Bootstrapping {{Generators}} from {{Noisy Data}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Perez-Beltrachini, L. and Lapata, M.},
  date = {2018},
  pages = {1516--1527},
  url = {http://arxiv.org/abs/1804.06385},
  abstract = {A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and associated texts. In this paper we aim to bootstrap generators from large scale datasets where the data (e.g., DBPedia facts) and related texts (e.g., Wikipedia abstracts) are loosely aligned. We tackle this challenging task by introducing a special-purpose content selection mechanism. We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention.},
  annotation = {24 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.06385},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XPAGMY8Q/Perez-Beltrachini, Lapata - 2018 - Bootstrapping Generators from Noisy Data(2).pdf},
  keywords = {u}
}

@inproceedings{peters2018DeepContextualizedWord,
  title = {Deep Contextualized Word Representations},
  booktitle = {{{NAACL}}},
  author = {Peters, M. E. and Neumann, M. and Iyyer, M. and Gardner, M. and Clark, C. and Lee, K. and Zettlemoyer, L. S.},
  date = {2018},
  url = {http://arxiv.org/abs/1802.05365},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  annotation = {5314 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L3AN34WN/Peters et al. - 2018 - Deep contextualized word representations(2).pdf}
}

@inproceedings{peters2018DissectingContextualWord,
  title = {Dissecting {{Contextual Word Embeddings}}: {{Architecture}} and {{Representation}}},
  shorttitle = {Dissecting {{Contextual Word Embeddings}}},
  booktitle = {{{EMNLP}}},
  author = {Peters, Matthew E. and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  date = {2018-09-27},
  url = {http://arxiv.org/abs/1808.08949},
  urldate = {2020-11-19},
  abstract = {Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.},
  annotation = {183 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1808.08949},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NMAF59QS/Peters et al. - 2018 - Dissecting Contextual Word Embeddings Architectur.pdf;/home/hiaoxui/.local/share/zotero_files/storage/7I5I2N99/1808.html},
  keywords = {u}
}

@inproceedings{peters2019KnowledgeEnhancedContextual,
  title = {Knowledge {{Enhanced Contextual Word Representations}}},
  booktitle = {{{EMNLP}}},
  author = {Peters, M. E. and Neumann, M. and Logan, R. and Schwartz, R. and Joshi, V. and Singh, S. and Smith, N. A.},
  date = {2019},
  pages = {43--54},
  doi = {10.18653/v1/d19-1005},
  abstract = {Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert's runtime is comparable to BERT's and it scales to large KBs.},
  annotation = {142 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.04164},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/98ZDBFLI/Peters et al. - 2019 - Knowledge Enhanced Contextual Word Representations(2).pdf}
}

@inproceedings{petroni2019LanguageModelsKnowledge,
  title = {Language {{Models}} as {{Knowledge Bases}}?},
  booktitle = {{{EMNLP}}},
  author = {Petroni, F. and Rocktäschel, T. and Lewis, P. and Bakhtin, A. and Wu, Y. and Miller, A. H. and Riedel, S.},
  date = {2019},
  abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
  archiveprefix = {arXiv},
  eprint = {1909.01066},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I9N7SNE6/Petroni et al. - 2019 - Language Models as Knowledge Bases(2).pdf}
}

@inproceedings{petrov2006LearningAccurateCompact,
  title = {Learning Accurate, Compact, and Interpretable Tree Annotation},
  booktitle = {{{ACL}}},
  author = {Petrov, S. and Barrett, L. and Thibaux, R. and Klein, D.},
  date = {2006},
  pages = {433--440},
  doi = {10.3115/1220175.1220230},
  url = {http://portal.acm.org/citation.cfm?doid=1220175.1220230},
  abstract = {We present an automatic approach to tree annota- tion in which basic nonterminal symbols are alter- nately split and merged to maximize the likelihood of a training treebank. Starting with a simple X- bar grammar, we learn a new grammar whose non- terminals are subsymbols of the original nontermi- nals. In contrast with previous work, we are able to split various terminals to different degrees, as ap- propriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more ac- curate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2\% on the Penn Treebank, higher than fully lexicalized systems.},
  annotation = {943 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PTZWAUA5/Petrov et al. - 2006 - Learning accurate, compact, and interpretable tree annotation(2).pdf},
  isbn = {1-932432-65-5},
  issue = {July}
}

@inproceedings{petrov2007ImprovedInferenceUnlexicalized,
  title = {Improved {{Inference}} for {{Unlexicalized Parsing}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Petrov, S. and Klein, D.},
  date = {2007},
  pages = {404--411},
  doi = {10.3115/1614164},
  abstract = {We present several improvements to unlexicalized parsing with hierarchically\textbackslash nstate-split PCFGs. First, we present a novel coarse-to-fine method\textbackslash nin which a grammar’s own hierarchical projections are used for incremental\textbackslash npruning, including a method for efficiently computing projections\textbackslash nof a grammar without a treebank. In our experiments, hierarchical\textbackslash npruning greatly accelerates parsing with no loss in empirical accuracy.\textbackslash nSecond, we compare various inference procedures for state-split PCFGs\textbackslash nfrom the standpoint of risk minimization, paying particular attention\textbackslash nto their practical tradeoffs. Finally, we present multilingual experiments\textbackslash nwhich show that parsing with hierarchical state-splitting is fast\textbackslash nand accurate in multiple languages and domains, even without any\textbackslash nlanguage-specific tuning.},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AI35RRBP/Petrov, Klein - 2007 - Improved Inference for Unlexicalized Parsing(2).pdf},
  issue = {April}
}

@inproceedings{piech2015LearningProgramEmbeddings,
  title = {Learning {{Program Embeddings}} to {{Propagate Feedback}} on {{Student Code}}},
  booktitle = {{{ICML}}},
  author = {Piech, C. and Huang, J. and Nguyen, A. and Phulsuksombati, M. and Sahami, M. and Guibas, L.},
  date = {2015},
  volume = {37},
  url = {http://arxiv.org/abs/1505.05969},
  abstract = {Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University's CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.},
  annotation = {112 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1505.05969},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6D6J4ZHQ/Piech et al. - 2015 - Learning Program Embeddings to Propagate Feedback on Student Code(2).pdf},
  isbn = {9781510810587 (ISBN)}
}

@incollection{pierrehumberl1990MeaningIntonationalContours,
  title = {The {{Meaning}} of {{Intonational Contours}} in the {{Interpretation}} of {{Discourse}}},
  booktitle = {Intentions in {{Communication}}},
  author = {Pierrehumberl, J. and Hirschberg, J.},
  date = {1990},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LZALXJRC/Pierrehumberl, Hirschberg - 1990 - The Meaning of Intonational Contours in the Interpretation of Discourse(2).pdf},
  keywords = {u}
}

@inproceedings{pimentel2020InformationTheoreticProbingLinguistic,
  title = {Information-{{Theoretic Probing}} for {{Linguistic Structure}}},
  booktitle = {{{ACL}}},
  author = {Pimentel, T. and Valvoda, J. and Maudslay, R. H. and Zmigrod, R. and Williams, A. and Cotterell, R.},
  date = {2020},
  url = {http://arxiv.org/abs/2004.03061},
  abstract = {The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually ``know'' about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network's learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research---plus English---totalling eleven languages.},
  annotation = {36 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2004.03061},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9S3N2XS4/Pimentel et al. - 2020 - Information-Theoretic Probing for Linguistic Structure(2).pdf},
  keywords = {u}
}

@inproceedings{pitler2008EasilyIdentifiableDiscourse,
  title = {Easily {{Identifiable Discourse Relations}}},
  booktitle = {{{COLING}}},
  author = {Pitler, E. and Raghupathy, M. and Mehta, H. and Nenkova, A. and Lee, A. and Joshi, A.},
  date = {2008},
  pages = {87--90},
  url = {http://www.aclweb.org/anthology/C08-2022},
  abstract = {Knott (1996) provides an extensive of connectives and their properties13 It is important to select a word with some syntactic mo- tivation to an argument span, but due to the lack of consistent alignment between syntax and , we must},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MQYGF2JM/Pitler et al. - 2008 - Easily Identifiable Discourse Relations(2).pdf},
  isbn = {978-1-905593-44-6},
  issue = {June},
  keywords = {u}
}

@inproceedings{poliak2018CollectingDiverseNatural,
  title = {Collecting {{Diverse Natural Language Inference Problems}} for {{Sentence Representation Evaluation}}},
  booktitle = {{{EMNLP}}},
  author = {Poliak, A. and Haldar, A. and Rudinger, R. and Hu, J. E. and Pavlick, E. and White, A. S. and Van Durme, B.},
  date = {2018},
  pages = {67--81},
  doi = {10.18653/v1/D18-1007},
  url = {http://aclweb.org/anthology/D18-1007},
  abstract = {We present a large-scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation captures distinct types of reasoning. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. We refer to our collection as the DNC: Diverse Natural Language Inference Collection. The DNC is available online at https://www.decomp.net, and will grow over time as additional resources are recast and added from novel sources.},
  annotation = {71 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.08207v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/488X6QRK/Poliak et al. - 2018 - Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation(2).pdf}
}

@inproceedings{poliak2018EvaluationSemanticPhenomena,
  title = {On the {{Evaluation}} of {{Semantic Phenomena}} in {{Neural Machine Translation Using Natural Language Inference}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Poliak, A. and Belinkov, Y. and Glass, J. and Van Durme, B.},
  date = {2018},
  pages = {513--523},
  url = {http://arxiv.org/abs/1804.09779},
  abstract = {We propose a process for investigating the extent to which sentence representations arising from neural machine translation (NMT) systems encode distinct semantic phenomena. We use these representations as features to train a natural language inference (NLI) classifier based on datasets recast from existing semantic annotations. In applying this process to a representative NMT system, we find its encoder appears most suited to supporting inferences at the syntax-semantics interface, as compared to anaphora resolution requiring world-knowledge. We conclude with a discussion on the merits and potential deficiencies of the existing process, and how it may be improved and extended as a broader framework for evaluating semantic coverage.},
  annotation = {26 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.09779},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4G2DHLC2/Poliak et al. - 2018 - On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference(2).pdf},
  keywords = {u}
}

@inproceedings{poliak2018HypothesisOnlyBaselines,
  title = {Hypothesis {{Only Baselines}} in {{Natural Language Inference}}},
  booktitle = {Joint {{Conference}} on {{Lexical}} and {{Computational Semantics}}},
  author = {Poliak, A. and Naradowsky, J. and Haldar, A. and Rudinger, R. and Van Durme, B.},
  date = {2018},
  pages = {180--191},
  doi = {10.18653/v1/s18-2023},
  annotation = {211 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1805.01042v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R2NHZSUD/Poliak et al. - 2018 - Hypothesis Only Baselines in Natural Language Inference(2).pdf}
}

@report{poliak2020SurveyRecognizingTextual,
  title = {A {{Survey}} on {{Recognizing Textual Entailment}} as an {{NLP Evaluation}}},
  author = {Poliak, Adam},
  date = {2020},
  pages = {18},
  abstract = {Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the reasoning capabilities of NLP systems. We then focus our discussion on RTE by highlighting prominent RTE datasets as well as advances in RTE dataset that focus on specific linguistic phenomena that can be used to evaluate NLP systems on a fine-grained level. We conclude by arguing that when evaluating NLP systems, the community should utilize newly introduced RTE datasets that focus on specific linguistic phenomena.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WT5YJ6PE/Poliak - A Survey on Recognizing Textual Entailment as an N.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{ponti2019CrosslingualSemanticSpecialization,
  title = {Cross-Lingual {{Semantic Specialization}} via {{Lexical Relation Induction}}},
  booktitle = {{{EMNLP}}},
  author = {Ponti, E. M. and Vuli, I. and Glavas, G. and Reichart, R. and Korhonen, A.},
  date = {2019},
  pages = {2206--2217},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YWQR6FID/Ponti et al. - 2019 - Cross-lingual Semantic Specialization via Lexical Relation Induction(2).pdf},
  keywords = {u}
}

@inproceedings{poon2009UnsupervisedSemanticParsing,
  title = {Unsupervised Semantic Parsing},
  booktitle = {{{EMNLP}}},
  author = {Poon, H. and Domingos, P.},
  date = {2009},
  volume = {1},
  pages = {1--10},
  doi = {10.3115/1699510.1699512},
  abstract = {We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.},
  annotation = {281 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CRIUFK2J/Poon, Domingos - 2009 - Unsupervised semantic parsing(2).pdf},
  issue = {August}
}

@inproceedings{poon2013GroundedUnsupervisedSemantic,
  title = {Grounded {{Unsupervised Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Poon, H.},
  date = {2013},
  pages = {933--943},
  doi = {10.3115/1699510.1699512},
  abstract = {We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM. To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84\%, effectively tying with the best published results by supervised approaches.},
  annotation = {281 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QJA4KFXR/Poon - 2013 - Grounded Unsupervised Semantic Parsing(2).pdf},
  number = {2010}
}

@inproceedings{poria2015DeepConvolutionalNeural,
  title = {Deep {{Convolutional Neural Network Textual Features}} and {{Multiple Kernel Learning}} for {{Utterance}}-{{Level Multimodal Sentiment Analysis}}},
  booktitle = {{{EMNLP}}},
  author = {Poria, S. and Cambria, E. and Gelbukh, A.},
  date = {2015},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QHZ6UU9B/Poria, Cambria, Gelbukh - 2015 - Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-Level (2).pdf}
}

@article{povey2011KaldiSpeechRecognition,
  title = {The {{Kaldi}} Speech Recognition Toolkit},
  author = {Povey, D. and Ghoshal, A. and Boulianne, G. and Burget, L. and Glembek, O. and Goel, N. and Hannemann, M. and Motlicek, P. and Qian, Y. and Schwarz, P. and Silovsky, J. and Stemmer, G. and Vesely, K.},
  date = {2011},
  journaltitle = {ASRU},
  pages = {1--4},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.},
  annotation = {1013 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KHMIEECW/Povey et al. - 2011 - The Kaldi speech recognition toolkit(2).pdf},
  isbn = {978-1-4673-0366-8},
  keywords = {u}
}

@article{power2012GeneratingNumericalApproximations,
  title = {Generating {{Numerical Approximations}}},
  author = {Power, R. and Williams, S.},
  date = {2012},
  journaltitle = {Computational Linguistics},
  volume = {38},
  pages = {113--134},
  issn = {0891-2017},
  doi = {10.1162/COLI_a_00086},
  url = {http://www.mitpressjournals.org/doi/10.1162/COLI_a_00086},
  abstract = {We describe a computational model for planning phrases like “more than a quarter” and “25.9 per cent” which describe proportions at different levels of precision. The model lays out the key choices in planning a numerical description, using formal definitions of mathematical form (e.g., the distinction between fractions and percentages) and roundness adapted from earlier studies. The task is modeled as a constraint satisfaction problem, with solutions subsequently ranked by preferences (e.g., for roundness). Detailed constraints are based on a corpus of numerical expressions collected in the NUMGEN project, and evaluated through empirical studies in which subjects were asked to produce (or complete) numerical expressions in specified contexts.},
  annotation = {22 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T75GZWYL/Power, Williams - 2012 - Generating Numerical Approximations(2).pdf},
  number = {1}
}

@inproceedings{pradhan2013RobustLinguisticAnalysis,
  title = {Towards {{Robust Linguistic Analysis}} Using {{OntoNotes}}},
  booktitle = {{{CoNLL}}},
  author = {Pradhan, Sameer and Moschitti, Alessandro and Xue, Nianwen and Ng, Hwee Tou and Bjorkelund, Anders and Uryupina, Olga and Zhang, Yuchen and Zhong, Zhi},
  date = {2013},
  pages = {10},
  abstract = {Large-scale linguistically annotated corpora have played a crucial role in advancing the state of the art of key natural language technologies such as syntactic, semantic and discourse analyzers, and they serve as training data as well as evaluation benchmarks. Up till now, however, most of the evaluation has been done on monolithic corpora such as the Penn Treebank, the Proposition Bank. As a result, it is still unclear how the state-of-the-art analyzers perform in general on data from a variety of genres or domains. The completion of the OntoNotes corpus, a large-scale, multi-genre, multilingual corpus manually annotated with syntactic, semantic and discourse information, makes it possible to perform such an evaluation. This paper presents an analysis of the performance of publicly available, state-of-the-art tools on all layers and languages in the OntoNotes v5.0 corpus. This should set the benchmark for future development of various NLP components in syntax and semantics, and possibly encourage research towards an integrated system that makes use of the various layers jointly to improve overall performance.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YZELAYYH/Pradhan et al. - Towards Robust Linguistic Analysis using OntoNotes.pdf},
  langid = {english}
}

@inproceedings{press2020ImprovingTransformerModels,
  title = {Improving {{Transformer Models}} by {{Reordering}} Their {{Sublayers}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Press, Ofir and Smith, Noah A. and Levy, Omer},
  date = {2020},
  pages = {2996--3005},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.270},
  url = {https://www.aclweb.org/anthology/2020.acl-main.270},
  urldate = {2020-09-29},
  annotation = {14 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {{{ACL}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7T4UB7AG/Press et al. - 2020 - Improving Transformer Models by Reordering their S.pdf},
  keywords = {u},
  langid = {english}
}

@article{pustejovsky2003TIMEBANKCorpus,
  title = {The {{TIMEBANK Corpus}}},
  author = {Pustejovsky, J. and Hanks, P. and Sauri, R. and See, A. and Gaizauskas, R. and Setzer, A. and Sundheim, B. and Radev, D. and Day, D. and Ferro, L. and Lazo, M.},
  date = {2003},
  journaltitle = {Corpus Linguistics},
  volume = {2003},
  pages = {647--656},
  issn = {0302-9743},
  doi = {10.1007/978-3-540-73351-5},
  url = {http://www.springerlink.com/content/c9313110264107m6},
  abstract = {The application of the multilingual knowledge encoded in Wikipedia to an open–domain Cross–Lingual Question Answering system\textbackslash n based on the Inter Lingual Index (ILI) module of EuroWordNet is proposed and evaluated. This strategy overcomes the problems\textbackslash n due to ILI’s low coverage on proper nouns (Named Entities). Moreover, as these are open class words (highly changing), using\textbackslash n a community–based up–to–date resource avoids the tedious maintenance of hand–coded bilingual dictionaries. A study reveals\textbackslash n the importance to translate Named Entities in CL–QA and the advantages of relying on Wikipedia over ILI for doing this. Tests\textbackslash n on questions from the Cross–Language Evaluation Forum (CLEF) justify our approach (20\% of these are correctly answered thanks\textbackslash n to Wikipedia’s Multilingual Knowledge).},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5BPCN5BD/Pustejovsky et al. - 2003 - The TIMEBANK Corpus(2).pdf},
  isbn = {978-3-540-73350-8}
}

@inproceedings{qi2019AnsweringComplexOpendomain,
  title = {Answering {{Complex Open}}-Domain {{Questions Through Iterative Query Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Qi, P. and Lin, X. and Mehr, L. and Wang, Z. and Manning, C. D.},
  date = {2019},
  pages = {2590--2602},
  doi = {10.18653/v1/d19-1261},
  abstract = {It is challenging for current one-step retrieve-and-read question answering (QA) systems to answer questions like "Which novel by the author of 'Armada' will be adapted as a feature film by Steven Spielberg?" because the question seldom contains retrievable clues about the missing entity (here, the author). Answering such a question requires multi-hop reasoning where one must gather information about the missing entity (or facts) to proceed with further reasoning. We present GoldEn (Gold Entity) Retriever, which iterates between reading context and retrieving more supporting documents to answer open-domain multi-hop questions. Instead of using opaque and computationally expensive neural retrieval models, GoldEn Retriever generates natural language search queries given the question and available context, and leverages off-the-shelf information retrieval systems to query for missing entities. This allows GoldEn Retriever to scale up efficiently for open-domain multi-hop reasoning while maintaining interpretability. We evaluate GoldEn Retriever on the recently proposed open-domain multi-hop QA dataset, HotpotQA, and demonstrate that it outperforms the best previously published model despite not using pretrained language models such as BERT.},
  annotation = {38 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1910.07000},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YAKEXRA7/Qi et al. - 2019 - Answering Complex Open-domain Questions Through Iterative Query Generation(2).pdf},
  keywords = {u}
}

@inproceedings{qin2018LearningLatentSemantic,
  title = {Learning {{Latent Semantic Annotations}} for {{Grounding Natural Language}} to {{Structured Data}}},
  booktitle = {{{EMNLP}}},
  author = {Qin, G. and Yao, J. and Wang, X. and Wang, J. and Lin, C.},
  date = {2018},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TY6VJAR7/Qin et al. - 2018 - Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data(2).pdf}
}

@report{qin2021KnowingHowAsk,
  title = {Knowing {{How}} to {{Ask}}: {{Querying LMs}} with {{Mixtures}} of {{Soft Prompts}}},
  author = {Qin, G. and Eisner, J. M.},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/774VFWEA/paper.pdf}
}

@inproceedings{qiu2020BlockwiseSelfAttentionLong,
  title = {Blockwise {{Self}}-{{Attention}} for {{Long Document Understanding}}},
  booktitle = {{{EMNLP}}},
  author = {Qiu, Jiezhong and Ma, Hao and Levy, Omer and Yih, Scott Wen-tau and Wang, Sinong and Tang, Jie},
  date = {2020-11-01},
  url = {http://arxiv.org/abs/1911.02972},
  urldate = {2021-03-27},
  abstract = {We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1\% less memory and 12.0-25.1\% less time to learn the model. During testing, BlockBERT saves 27.8\% inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.},
  annotation = {20 citations (Semantic Scholar/arXiv) [2021-03-27]},
  archiveprefix = {arXiv},
  eprint = {1911.02972},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M2VWP3NT/Qiu et al. - 2020 - Blockwise Self-Attention for Long Document Underst.pdf;/home/hiaoxui/.local/share/zotero_files/storage/4NK757M2/1911.html}
}

@inproceedings{quirk2015LanguageCodeLearning,
  title = {Language to {{Code}}: {{Learning Semantic Parsers}} for {{If}}-{{This}}-{{Then}}-{{That Recipes}}},
  booktitle = {{{ACL}}-{{IJCNLP}}},
  author = {Quirk, C. and Mooney, R. and Galley, M.},
  date = {2015},
  pages = {878--888},
  doi = {10.3115/v1/P15-1085},
  abstract = {Using natural language to write programs is a touchstone problem for computational linguistics. We present an approach that learns to map natural-language descrip-tions of simple " if-then " rules to executable code. By training and testing on a large cor-pus of naturally-occurring programs (called " recipes ") and their natural language de-scriptions, we demonstrate the ability to effectively map language to code. We compare a number of semantic parsing ap-proaches on the highly noisy training data collected from ordinary users, and find that loosely synchronous systems perform best.},
  annotation = {113 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CRNK63Z6/Quirk, Mooney, Galley - 2015 - Language to Code Learning Semantic Parsers for If-This-Then-That Recipes(2).pdf},
  isbn = {978-1-941643-72-3},
  keywords = {u}
}

@inproceedings{quirk2017DistantSupervisionRelation,
  title = {Distant {{Supervision}} for {{Relation Extraction}} beyond the {{Sentence Boundary}}},
  booktitle = {{{EACL}}},
  author = {Quirk, Chris and Poon, Hoifung},
  date = {2017-08-14},
  url = {http://arxiv.org/abs/1609.04873},
  urldate = {2021-03-03},
  abstract = {The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross- sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.},
  annotation = {108 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1609.04873},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YTYFPEUJ/Quirk and Poon - 2017 - Distant Supervision for Relation Extraction beyond.pdf;/home/hiaoxui/.local/share/zotero_files/storage/FAL28947/1609.html}
}

@article{r.b.2003KnowledgeActionFrame,
  title = {Knowledge, {{Action}}, and the {{Frame Problem}}},
  author = {R. B., Scherl and H. J., Levesque},
  date = {2003},
  journaltitle = {Artificial Intelligence},
  volume = {144},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NDC5HJCK/aij-frame.pdf},
  keywords = {u}
}

@report{rabanser2017IntroductionTensorDecompositions,
  title = {Introduction to {{Tensor Decompositions}} and Their {{Applications}} in {{Machine Learning}}},
  author = {Rabanser, S. and Shchur, O. and Günnemann, S.},
  date = {2017},
  pages = {1--13},
  url = {http://arxiv.org/abs/1711.10781},
  abstract = {Tensors are multidimensional arrays of numerical values and therefore generalize matrices to multiple dimensions. While tensors first emerged in the psychometrics community in the \$20\^\{\textbackslash text\{th\}\}\$ century, they have since then spread to numerous other disciplines, including machine learning. Tensors and their decompositions are especially beneficial in unsupervised learning settings, but are gaining popularity in other sub-disciplines like temporal and multi-relational data analysis, too. The scope of this paper is to give a broad overview of tensors, their decompositions, and how they are used in machine learning. As part of this, we are going to introduce basic tensor concepts, discuss why tensors can be considered more rigid than matrices with respect to the uniqueness of their decomposition, explain the most important factorization algorithms and their properties, provide concrete examples of tensor decomposition applications in machine learning, conduct a case study on tensor-based estimation of mixture models, talk about the current state of research, and provide references to available software libraries.},
  annotation = {90 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1711.10781},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NBZRAQDM/Rabanser, Shchur, Günnemann - 2017 - Introduction to Tensor Decompositions and their Applications in Machine Learning(2).pdf}
}

@article{rabiner2019TutorialHiddenMarkov,
  title = {A {{Tutorial}} on {{Hidden Markov Models}} and {{Selected Applications}} in {{Speech Recognition}}},
  author = {Rabiner, L. R.},
  date = {2019},
  journaltitle = {IEEE},
  volume = {77},
  pages = {257--286},
  issn = {00189219},
  doi = {10.1109/5.18626},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
  annotation = {9995 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {18626},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F9SQVHNF/Rabiner - 2019 - A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition(2).pdf},
  isbn = {0018-9219},
  number = {2}
}

@inproceedings{rabinovich2017AbstractSyntaxNetworks,
  title = {Abstract Syntax Networks for Code Generation and Semantic Parsing},
  booktitle = {{{ACL}}},
  author = {Rabinovich, M. and Stern, M. and Klein, D.},
  date = {2017},
  pages = {1139--1149},
  doi = {10.18653/v1/P17-1105},
  abstract = {Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark HEARTHSTONE dataset for code generation, our model obtains 79.2 BLEU and 22.7\% exact match accuracy, compared to previous state-ofthe-art values of 67.1 and 6.1\%. Furthermore, we perform competitively on the ATIS, JOBS, and GEO semantic parsing datasets with no task-specific engineering.},
  annotation = {192 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.07535},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/53RDRHE9/Rabinovich, Stern, Klein - 2017 - Abstract syntax networks for code generation and semantic parsing(2).pdf}
}

@report{radford2019LanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  date = {2019},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q5VVEBN7/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf},
  keywords = {u},
  langid = {english}
}

@report{raffel2019ExploringLimitsTransfer,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text}}-to-{{Text Transformer}}},
  author = {Raffel, C. and Shazeer, N. and Roberts, A. and Lee, K. and Narang, S. and Matena, M. and Zhou, Y. and Li, W. and Liu, P. J.},
  date = {2019},
  pages = {1--53},
  url = {http://arxiv.org/abs/1910.10683},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.},
  annotation = {965 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1910.10683},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DE3A5GJM/Raffel et al. - 2019 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(2).pdf}
}

@inproceedings{raghunathan2018CertifiedDefensesAdversarial,
  title = {Certified {{Defenses}} against {{Adversarial Examples}}},
  booktitle = {{{ICLR}}},
  author = {Raghunathan, A. and Steinhardt, J. and Liang, P.},
  date = {2018},
  pages = {1--15},
  url = {http://arxiv.org/abs/1801.09344},
  abstract = {While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \textbackslash epsilon = 0.1 can cause more than 35\% test error.},
  annotation = {496 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1801.09344},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PPQKWPBW/Raghunathan, Steinhardt, Liang - 2018 - Certified Defenses against Adversarial Examples(2).pdf}
}

@inproceedings{rahimi2007RegularizationMethodImmune,
  title = {Regularization Method and Immune Genetic Algorithm for Inverse Problems of Ship Maneuvering},
  booktitle = {{{NeurIPS}}},
  author = {Rahimi, A. and Recht, B.},
  date = {2007},
  issn = {10071172},
  doi = {10.1007/s12204-009-0467-7},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WZ4CH8QS/Rahimi, Recht - 2007 - Regularization method and immune genetic algorithm for inverse problems of ship maneuvering(2).pdf}
}

@inproceedings{rajani2019ExplainYourselfLeveraging,
  title = {Explain {{Yourself}}! {{Leveraging Language Models}} for {{Commonsense Reasoning}}},
  booktitle = {{{ACL}}},
  author = {Rajani, N. F. and McCann, B. and Xiong, C. and Socher, R.},
  date = {2019},
  pages = {4932--4942},
  doi = {10.18653/v1/p19-1487},
  abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10\% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
  annotation = {94 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1906.02361},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJY89XUK/Rajani et al. - 2019 - Explain Yourself! Leveraging Language Models for Commonsense Reasoning(2).pdf},
  keywords = {u}
}

@inproceedings{rajpurkar2016SQuAD100000,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  booktitle = {{{EMNLP}}},
  author = {Rajpurkar, P. and Zhang, J. and Lopyrev, K. and Liang, P.},
  date = {2016},
  issn = {9781941643327},
  doi = {10.18653/v1/D16-1264},
  url = {http://arxiv.org/abs/1606.05250},
  abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
  annotation = {2600 citations (Semantic Scholar/DOI) [2021-03-26] 2600 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {299497},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZMRKV3LA/Rajpurkar et al. - 2016 - SQuAD 100,000 Questions for Machine Comprehension of Text(2).pdf},
  isbn = {978-1-941643-32-7},
  issue = {ii}
}

@inproceedings{rajpurkar2018KnowWhatYou,
  title = {Know {{What You Don}}'t {{Know}}: {{Unanswerable Questions}} for {{SQuAD}}},
  booktitle = {{{ACL}}},
  author = {Rajpurkar, P. and Jia, R. and Liang, P.},
  date = {2018},
  pages = {1--6},
  url = {http://arxiv.org/abs/1806.03822},
  abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
  annotation = {773 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1806.03822},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4FTUHXID/Rajpurkar, Jia, Liang - 2018 - Know What You Don't Know Unanswerable Questions for SQuAD(2).pdf}
}

@article{ramos-soto2015LinguisticDescriptionsAutomatic,
  title = {Linguistic Descriptions for Automatic Generation of Textual Short-Term Weather Forecasts on Real Prediction Data},
  author = {Ramos-Soto, A. and Bugarín, A. and Barro, S. and Taboada, J.},
  date = {2015},
  journaltitle = {TFS},
  volume = {23},
  pages = {44--57},
  issn = {10636706},
  doi = {10.1109/TFUZZ.2014.2328011},
  abstract = {We present in this paper an application that automatically generates textual short-term weather forecasts for every municipality in Galicia (NW Spain), using the real data provided by the Galician Meteorology Agency (MeteoGalicia). This solution combines in an innovative way computing with perceptions techniques and strategies for linguistic description of data, together with a natural language generation (NLG) system. The application, which is named GALiWeather, extracts relevant information from weather forecast input data and encodes it into intermediate descriptions using linguistic variables and temporal references. These descriptions are later translated into natural language texts by the NLG system. The obtained forecast results have been thoroughly validated by an expert meteorologist from MeteoGalicia using a quality assessment methodology, which covers two key dimensions of a text: the accuracy of its content and the correctness of its form. Following this validation, GALiWeather will be released as a real service, offering custom forecasts for a wide public.},
  annotation = {56 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1411.4925v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C29RC8NE/Ramos-Soto et al. - 2015 - Linguistic descriptions for automatic generation of textual short-term weather forecasts on real predictio(2).pdf},
  isbn = {9783642407680},
  number = {1}
}

@thesis{ransom2001NewSearchTechniques,
  title = {New {{Search Techniques}} for {{Binary Pulsars}}},
  author = {Ransom, S. M.},
  date = {2001},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8WFB4RM2/Ransom - 2001 - New Search Techniques for Binary Pulsars(2).pdf},
  keywords = {u}
}

@article{ransom2002FourierTechniquesVery,
  title = {Fourier {{Techniques}} for {{Very Long Astrophysical Time Series Analysis}}},
  author = {Ransom, S. M. and Eikenberry, S. S. and Middleditch, J.},
  date = {2002},
  journaltitle = {The Astronomical Journal},
  volume = {124},
  pages = {38},
  issn = {1538-3881},
  doi = {10.1086/342285},
  url = {http://arxiv.org/abs/astro-ph/0204349},
  abstract = {We present an assortment of both standard and advanced Fourier techniques that are useful in the analysis of astrophysical time series of very long duration-where the observation time is much greater than the time resolution of the individual data points. We begin by reviewing the operational characteristics of Fourier transforms of time-series data, including power-spectral statistics, discussing some of the differences between analyses of binned data, sampled data, and event data, and we briefly discuss algorithms for calculating discrete Fourier transforms (DFTs) of very long time series. We then discuss the response of DFTs to periodic signals and present techniques to recover Fourier amplitude ``lost'' during simple traditional analyses if the periodicities change frequency during the observation. These techniques include Fourier interpolation, which allows us to correct the response for signals that occur between Fourier frequency bins. We then present techniques for estimating additional signal properties such as the signal's centroid and duration in time, the first and second derivatives of the frequency, the pulsed fraction, and an overall estimate of the significance of a detection. Finally, we present a recipe for a basic but thorough Fourier analysis of a time series for well-behaved pulsations.},
  annotation = {209 citations (Semantic Scholar/DOI) [2021-03-26] 209 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {astro-ph/0204349},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5BE5X3V2/Ransom, Eikenberry, Middleditch - 2002 - Fourier Techniques for Very Long Astrophysical Time Series Analysis(2).pdf},
  keywords = {u},
  number = {3}
}

@article{ransom2003NewSearchTechnique,
  title = {A {{New Search Technique}} for {{Short Orbital Period Binary Pulsars}}},
  author = {Ransom, S. M. and Cordes, J. M. and Eikenberry, S. S.},
  date = {2003},
  journaltitle = {The Astrophysical Journal},
  volume = {589},
  pages = {911--920},
  issn = {0004-637X},
  doi = {10.1086/374806},
  url = {http://stacks.iop.org/0004-637X/589/i=2/a=911},
  abstract = {We describe a new and efficient technique, which we call sideband or phase-modulation searching, that allows one to detect short-period binary pulsars in observations longer than the orbital period. The orbital motion of the pulsar during long observations effectively modulates the phase of the pulsar signal, causing sidebands to appear around the pulsar spin frequency and its harmonics in the Fourier transform. For the majority of binary radio pulsars or low-mass X-ray binaries (LMXBs), large numbers of sidebands are present, allowing efficient searches using Fourier transforms of short portions of the original power spectrum. Analysis of the complex amplitudes and phases of the sidebands can provide enough information to solve for the Keplerian orbital parameters. This technique is particularly applicable to radio pulsar searches in globular clusters and searches for coherent X-ray pulsations from LMXBs and is complementary to more standard "acceleration" searches.},
  annotation = {66 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {astro-ph/0210010},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8W979E46/Ransom, Cordes, Eikenberry - 2003 - A New Search Technique for Short Orbital Period Binary Pulsars(2).pdf},
  keywords = {u},
  number = {2}
}

@inproceedings{ranzato2016SequenceLevelTraining,
  title = {Sequence Level Training with Recurrent Neural Networks},
  booktitle = {{{ICLR}}},
  author = {Ranzato, M. and Chopra, S. and Auli, M. and Zaremba, W.},
  date = {2016},
  abstract = {Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.},
  archiveprefix = {arXiv},
  eprint = {1511.06732},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CMNMFYIA/Ranzato et al. - 2016 - Sequence level training with recurrent neural networks(4).pdf}
}

@inproceedings{rao2015ParserAbstractMeaning,
  title = {Parser for {{Abstract Meaning Representation}} Using {{Learning}} to {{Search}}},
  booktitle = {{{SemEval}}},
  author = {Rao, S. and Vyas, Y. and Daumé III, H. and Resnik, P.},
  date = {2015},
  abstract = {We develop a novel technique to parse English sentences into Abstract Meaning Representation (AMR) using SEARN, a Learning to Search approach, by modeling the concept and the relation learning in a unified framework. We evaluate our parser on multiple datasets from varied domains and show an absolute improvement of 2\% to 6\% over the state-of-the-art. Additionally we show that using the most frequent concept gives us a baseline that is stronger than the state-of-the-art for concept prediction. We plan to release our parser for public use.},
  archiveprefix = {arXiv},
  eprint = {1510.07586},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/INW4694Y/Rao et al. - 2015 - Parser for Abstract Meaning Representation using Learning to Search(2).pdf}
}

@inproceedings{rao2018LearningAskGood,
  title = {Learning to {{Ask Good Questions}}: {{Ranking Clarification Questions}} Using {{Neural Expected Value}} of {{Perfect Information}}},
  booktitle = {{{ACL}}},
  author = {Rao, S. and Daumé III, H.},
  date = {2018},
  url = {http://arxiv.org/abs/1805.04655},
  abstract = {Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. In this work, we build a neural network model for the task of ranking clarification questions. Our model is inspired by the idea of expected value of perfect information: a good question is one whose expected answer will be useful. We study this problem using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We create a dataset of clarification questions consisting of \textasciitilde 77K posts paired with a clarification question (and answer) from three domains of StackExchange: askubuntu, unix and superuser. We evaluate our model on 500 samples of this dataset against expert human judgments and demonstrate significant improvements over controlled baselines.},
  annotation = {54 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1805.04655},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KR5KJF83/Rao, Daumé III - 2018 - Learning to Ask Good Questions Ranking Clarification Questions using Neural Expected Value of Perfect Informa(2).pdf},
  keywords = {u}
}

@inproceedings{ratner2017SnorkelRapidTraining,
  title = {Snorkel: {{Rapid}} Training Data Creation with Weak Supervision},
  booktitle = {Very {{Large Data Bases}}},
  author = {Ratner, A. and Bach, S. H. and Ehrenberg, H. and Fries, J. and Wu, S. and Re, C.},
  date = {2017},
  volume = {11},
  pages = {269--282},
  issn = {21508097},
  doi = {10.14778/3157794.3157797},
  abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8x faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8x speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
  annotation = {322 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9HIRDRPZ/Ratner et al. - 2017 - Snorkel Rapid training data creation with weak supervision(2).pdf},
  keywords = {u},
  number = {3}
}

@incollection{rausch2011PracticalMultipleSequence,
  title = {Practical {{Multiple Sequence Alignment}}},
  booktitle = {Problem {{Solving Handbook}} in {{Computational Biology}} and {{Bioinformatics}}},
  author = {Rausch, T. and Reinert, K.},
  date = {2011},
  doi = {10.1007/978-0-387-09760-2},
  url = {http://link.springer.com/10.1007/978-0-387-09760-2},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IMFLL93W/Rausch, Reinert - 2011 - Practical Multiple Sequence Alignment(2).pdf},
  isbn = {978-0-387-09759-6}
}

@report{ravenscroft2021CD2CRCoreferenceResolution,
  title = {{{CD2CR}}: {{Co}}-Reference {{Resolution Across Documents}} and {{Domains}}},
  shorttitle = {{{CD2CR}}},
  author = {Ravenscroft, James and Cattan, Arie and Clare, Amanda and Dagan, Ido and Liakata, Maria},
  date = {2021-01-29},
  url = {http://arxiv.org/abs/2101.12637},
  urldate = {2021-02-27},
  abstract = {Cross-document co-reference resolution (CDCR) is the task of identifying and linking mentions to entities and concepts across many text documents. Current state-of-the-art models for this task assume that all documents are of the same type (e.g. news articles) or fall under the same theme. However, it is also desirable to perform CDCR across different domains (type or theme). A particular use case we focus on in this paper is the resolution of entities mentioned across scientific work and newspaper articles that discuss them. Identifying the same entities and corresponding concepts in both scientific articles and news can help scientists understand how their work is represented in mainstream media. We propose a new task and English language dataset for cross-document cross-domain co-reference resolution (CD\$\^2\$CR). The task aims to identify links between entities across heterogeneous document types. We show that in this cross-domain, cross-document setting, existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CD\$\^2\$CR. Our data set, annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2101.12637},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FXEDUQX9/Ravenscroft et al. - 2021 - CD2CR Co-reference Resolution Across Documents an.pdf;/home/hiaoxui/.local/share/zotero_files/storage/75YMELK9/2101.html}
}

@inproceedings{reddi2018ConvergenceAdam,
  title = {On the Convergence of Adam and Beyond},
  booktitle = {{{ICLR}}},
  author = {Reddi, S. J. and Kale, S. and Kumar, S.},
  date = {2018},
  pages = {1--23},
  issn = {08695652},
  doi = {10.1134/S0001434607010294},
  abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of ex-ponential moving averages of squared past gradients. It has been empirically ob-served that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous anal-ysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with " long-term memory " of past gradi-ents, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  annotation = {19 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1709.01507},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P76DAWIH/Reddi, Kale, Kumar - 2018 - On the convergence of adam and beyond(2).pdf}
}

@article{reddy2014LargescaleSemanticParsing,
  title = {Large-Scale {{Semantic Parsing}} without {{Question}}-{{Answer Pairs}}},
  author = {Reddy, S. and Lapata, M. and Steedman, M.},
  date = {2014},
  journaltitle = {TACL},
  volume = {2},
  pages = {377--392},
  issn = {2307-387X},
  doi = {10.1017/CBO9781107415324.004},
  url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/398},
  abstract = {In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the FREE 917 and WEBQUESTIONS benchmark datasets show our semantic parser improves over the state of the art.},
  annotation = {1013 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {25810777},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FSK5KAY8/Reddy, Lapata, Steedman - 2014 - Large-scale Semantic Parsing without Question-Answer Pairs(2).pdf},
  isbn = {9782951740877},
  keywords = {u}
}

@inproceedings{reddy2017UniversalSemanticParsing,
  title = {Universal {{Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Reddy, S. and Täckström, O. and Petrov, S. and Steedman, M. and Lapata, M.},
  date = {2017},
  pages = {89--101},
  url = {http://arxiv.org/abs/1702.03196},
  abstract = {Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions. Our code and data can be downloaded at https://github.com/sivareddyg/udeplambda.},
  annotation = {61 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1702.03196},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I3QPVD6Q/Reddy et al. - 2017 - Universal Semantic Parsing(2).pdf},
  keywords = {u}
}

@article{reisinger2015SemanticProtoRoles,
  title = {Semantic {{Proto}}-{{Roles}}},
  author = {Reisinger, D. and Rudinger, R. and Ferraro, F. and Harman, C. and Rawlins, K. and Van Durme, B.},
  date = {2015},
  journaltitle = {TACL},
  volume = {3},
  pages = {475--488},
  doi = {10.1162/tacl_a_00152},
  abstract = {We present the first large-scale, corpus based verification of Dowty’s seminal theory of proto-roles. Our results demonstrate both the need for and the feasibility of a property-based annotation scheme of semantic relationships, as opposed to the currently dominant notion of categorical roles.},
  annotation = {55 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JUZT997G/Reisinger et al. - 2015 - Semantic Proto-Roles(2).pdf}
}

@article{reiter2005ChoosingWordsComputergenerated,
  title = {Choosing Words in Computer-Generated Weather Forecasts},
  author = {Reiter, E. and Sripada, S. and Hunter, J. and Yu, J. and Davy, I.},
  date = {2005},
  journaltitle = {AI},
  volume = {167},
  pages = {137--169},
  issn = {00043702},
  doi = {10.1016/j.artint.2005.06.006},
  abstract = {One of the main challenges in automatically generating textual weather forecasts is choosing appropriate English words to communicate numeric weather data. A corpus-based analysis of how humans write forecasts showed that there were major differences in how individual writers performed this task, that is, in how they translated data into words. These differences included both different preferences between potential near-synonyms that could be used to express information, and also differences in the meanings that individual writers associated with specific words. Because we thought these differences could confuse readers, we built our SumTime-Mousam weather-forecast generator to use consistent data-to-word rules, which avoided words which were only used by a few people, and words which were interpreted differently by different people. An evaluation by forecast users suggested that they preferred SumTime-Mousam's texts to human-generated texts, in part because of better word choice; this may be the first time that an evaluation has shown that nlg texts are better than human-authored texts. © 2005 Elsevier B.V. All rights reserved.},
  annotation = {241 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FIFNGXPM/Reiter et al. - 2005 - Choosing words in computer-generated weather forecasts(2).pdf},
  isbn = {0004-3702},
  number = {1-2}
}

@article{reiter2009InvestigationValidityMetrics,
  title = {An {{Investigation}} into the {{Validity}} of {{Some Metrics}} for {{Automatically Evaluating Natural Language Generation Systems}}},
  author = {Reiter, E. and Belz, A.},
  date = {2009},
  journaltitle = {Computational Linguistics},
  volume = {35},
  pages = {529--558},
  issn = {08912017},
  doi = {10.1162/coli.2009.35.4.35405},
  abstract = {There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies.},
  annotation = {144 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BRB2UT2R/Reiter, Belz - 2009 - An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Sys(2).pdf},
  keywords = {u},
  number = {4}
}

@article{reiter2018BLEUStructuredReview,
  title = {{{BLEU Structured Review}}: {{A Structured Review}} of the {{Validity}} of {{BLEU}}},
  author = {Reiter, E.},
  date = {2018},
  journaltitle = {Computational Linguistics},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UXWNF2WB/Reiter - 2018 - BLEU Structured Review A Structured Review of the Validity of BLEU(2).pdf},
  issue = {August 2017}
}

@inproceedings{rezende2014StochasticBackpropagationApproximate,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  booktitle = {{{ICML}}},
  author = {Rezende, D. J. and Mohamed, S. and Wierstra, D.},
  date = {2014},
  url = {http://arxiv.org/abs/1401.4082},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  annotation = {86 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1401.4082},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/98HW4VH6/Rezende, Mohamed, Wierstra - 2014 - Stochastic Backpropagation and Approximate Inference in Deep Generative Models(2).pdf}
}

@inproceedings{rezende2015VariationalInferenceNormalizing,
  title = {Variational Inference with Normalizing Flows},
  booktitle = {{{ICML}}},
  author = {Rezende, D. J. and Mohamed, S.},
  date = {2015},
  pages = {1530--1538},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archiveprefix = {arXiv},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SQPI36CN/Rezende, Mohamed - 2015 - Variational inference with normalizing flows(2).pdf},
  isbn = {978-1-5108-1058-7},
  keywords = {u}
}

@inproceedings{ribeiro2016WhyShouldTrust,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  booktitle = {{{KDD}}},
  author = {Ribeiro, M. T. and Singh, S. and Guestrin, C.},
  date = {2016},
  issn = {9781450321389},
  doi = {10.18653/v1/N16-3020},
  url = {http://arxiv.org/abs/1602.04938},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  annotation = {244 citations (Semantic Scholar/DOI) [2021-03-26] 4115 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {214160309},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SG8XU69I/Ribeiro, Singh, Guestrin - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier(2).pdf},
  isbn = {978-1-4503-2138-9}
}

@inproceedings{ribeiro2018SemanticallyEquivalentAdversarial,
  title = {Semantically Equivalent Adversarial Rules for Debugging {{NLP}} Models},
  booktitle = {{{ACL}}},
  author = {Ribeiro, M. T. and Singh, S. and Guestrin, C.},
  date = {2018},
  pages = {856--865},
  doi = {10.18653/v1/p18-1079},
  abstract = {Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) - semantic-preserving perturbations that induce changes in the model's predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) - simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.},
  annotation = {199 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CA3PEF5S/Ribeiro, Singh, Guestrin - 2018 - Semantically equivalent adversarial rules for debugging NLP models(4).pdf}
}

@inproceedings{ribeiro2019EnhancingAMRtoTextGeneration,
  title = {Enhancing {{AMR}}-to-{{Text Generation}} with {{Dual Graph Representations}}},
  booktitle = {{{EMNLP}}},
  author = {Ribeiro, L. F. R. and Gardent, C. and Gurevych, I.},
  date = {2019},
  pages = {3181--3192},
  doi = {10.18653/v1/d19-1314},
  abstract = {Generating text from graph-based data, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a graph with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The model learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets.},
  annotation = {25 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.00352},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N9CDQY73/Ribeiro, Gardent, Gurevych - 2019 - Enhancing AMR-to-Text Generation with Dual Graph Representations(2).pdf},
  keywords = {u}
}

@inproceedings{ribeiro2020AccuracyBehavioralTesting,
  title = {Beyond {{Accuracy}}: {{Behavioral Testing}} of {{NLP Models}} with {{C heckList}}},
  booktitle = {{{ACL}}},
  author = {Ribeiro, M. T. and Wu, T. and Guestrin, C. and Singh, S.},
  date = {2020},
  archiveprefix = {arXiv},
  eprint = {2005.04118v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LYXS9KWU/Ribeiro et al. - 2020 - Beyond Accuracy Behavioral Testing of NLP Models with C heckList(2).pdf;/home/hiaoxui/.local/share/zotero_files/storage/U7ZJI3Q3/D18-1390.pdf}
}

@inproceedings{richardson2013MCTestChallengeDataset,
  title = {{{MCTest}}: {{A Challenge Dataset}} for the {{Open}}-{{Domain Machine Comprehension}} of {{Text}}},
  booktitle = {{{EMNLP}}},
  author = {Richardson, M. and Burges, C. J. C. and Renshaw, E.},
  date = {2013},
  abstract = {We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MH4BYURJ/Richardson, Burges, Renshaw - 2013 - MCTest A Challenge Dataset for the Open-Domain Machine Comprehension of Text(2).pdf}
}

@inproceedings{richardson2017LearningSemanticCorrespondences,
  title = {Learning {{Semantic Correspondences}} in {{Technical Documentation}}},
  booktitle = {{{ACL}}},
  author = {Richardson, K. and Kuhn, J.},
  date = {2017},
  pages = {1612--1622},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6IXXLNZP/Richardson, Kuhn - 2017 - Learning Semantic Correspondences in Technical Documentation(2).pdf}
}

@inproceedings{richardson2018PolyglotSemanticParsing,
  title = {Polyglot {{Semantic Parsing}} in {{APIs}}},
  booktitle = {{{NAACL}}},
  author = {Richardson, K. and Berant, J. and Kuhn, J.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1803.06966v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G7BPHEE7/Richardson, Berant, Kuhn - 2018 - Polyglot Semantic Parsing in APIs(2).pdf}
}

@inproceedings{rillof1996AutomaticallyGeneratingExtraction,
  title = {Automatically {{Generating Extraction Patterns}} from {{Untagged Text}}},
  booktitle = {{{AAAI}}},
  author = {Rillof, E.},
  date = {1996},
  pages = {1044--1049},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DHV8MDVF/Rillof - 1996 - Automatically Generating Extraction Patterns from Untagged Text(2).pdf}
}

@inproceedings{riloff1999LearningDictionariesInformation,
  title = {Learning {{Dictionaries}} for {{Information Extraction}} by {{Multi}}-{{Level Bootstrapping}}},
  booktitle = {{{AAAI}}},
  author = {Riloff, E. and Jones, R.},
  date = {1999},
  pages = {474--479},
  url = {http://dl.acm.org/citation.cfm?id=315364},
  abstract = {Information extraction systems usually require two dictionaries: a semantic lexicon and a dictionary of extraction patterns for the domain. We present a multilevel bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. As input, our technique requires only unannotated training texts and a handful of seed words for a category. We use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap its extractions into the semantic lexicon, which is the basis for selecting the next extraction pattern. To make this approach more robust, we add a second level of bootstrapping (metabootstrapping) that retains only the most reliable lexicon entries produced by mutual bootstrapping and then restarts the process. We evaluated this multilevel bootstrapping technique on a collection of corporate web pages and a corpus of terrorism news articles. The algorithm produced high-quality dictionaries for several semantic categories.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2D4AYBJZ/Riloff, Jones - 1999 - Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping(2).pdf},
  keywords = {u}
}

@inproceedings{riloff2013SarcasmContrastPositive,
  title = {Sarcasm as {{Contrast}} between a {{Positive Sentiment}} and {{Negative Situation}}},
  booktitle = {{{EMNLP}}},
  author = {Riloff, E. and Qadir, A. and Surve, P. and Silva, L. D. and Gilbert, N. and Huang, R.},
  date = {2013},
  url = {papers3://publication/uuid/9E81B37F-37A5-421F-A49F-5F1A107CC460},
  abstract = {A common form of sarcasm on Twitter con- sists of a positive sentiment contrasted with a negative situation. For example, many sarcas- tic tweets include a positive sentiment, such as “love” or “enjoy”, followed by an expression that describes an undesirable activity or state (e.g., “taking exams” or “being ignored”).We have developed a sarcasm recognizer to iden- tify this type of sarcasm in tweets. We present a novel bootstrapping algorithmthat automati- cally learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. We show that identifying contrast- ing contexts using the phrases learned through bootstrapping yields improved recall for sar- casm recognition. 1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JXGUXNGI/Riloff et al. - 2013 - Sarcasm as Contrast between a Positive Sentiment and Negative Situation(2).pdf},
  isbn = {978-1-937284-97-8}
}

@article{riloff2018RetrospectiveMutualBootstrapping,
  title = {A {{Retrospective}} on {{Mutual Bootstrapping}}},
  author = {Riloff, E. and Jones, R.},
  date = {2018},
  journaltitle = {AI Magazine},
  volume = {51},
  pages = {51--61},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TT6XXQVN/Riloff, Jones - 2018 - A Retrospective on Mutual Bootstrapping(2).pdf}
}

@report{ringner2009LawUnconsciousStatistician,
  title = {The Law of the Unconscious Statistician},
  author = {Ringner, B.},
  date = {2009},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/A2W5LL46/Ringner - 2009 - The law of the unconscious statistician(2).pdf}
}

@inproceedings{ritter2010DataDrivenResponseGeneration,
  title = {Data-{{Driven Response Generation}} in {{Social Media}}},
  booktitle = {{{EMNLP}}},
  author = {Ritter, A. and Cherry, C. and Dolan, W. B.},
  date = {2010},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C92RWRTL/Ritter, Cherry, Dolan - 2010 - Data-Driven Response Generation in Social Media(2).pdf},
  keywords = {u}
}

@inproceedings{rocktaschel2017EndtoendDifferentiableProving,
  title = {End-to-End Differentiable Proving},
  booktitle = {{{NeurIPS}}},
  author = {Rocktäschel, T. and Riedel, S.},
  date = {2017},
  pages = {3789--3801},
  issn = {10495258},
  abstract = {We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.},
  archiveprefix = {arXiv},
  eprint = {1705.11040},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PJGQCQ7S/Rocktäschel, Riedel - 2017 - End-to-end differentiable proving(2).pdf},
  keywords = {u}
}

@article{rogers2020PrimerBERTologyWhat,
  title = {A {{Primer}} in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  date = {2020-11-09},
  journaltitle = {TACL},
  url = {http://arxiv.org/abs/2002.12327},
  urldate = {2020-11-19},
  annotation = {142 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2002.12327},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D2W9YANK/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf;/home/hiaoxui/.local/share/zotero_files/storage/ICJXID3W/2002.html}
}

@report{rojas1997TutorialIntroductionLambda,
  title = {A {{Tutorial Introduction}} to the {{Lambda Calculus}}},
  author = {Rojas, R.},
  date = {1997},
  volume = {58},
  pages = {1--9},
  issn = {00033472},
  doi = {10.1006/anbe.1999.1219},
  abstract = {This paper is a short and painless introduction to the λ calculus. Originally developed in order to study some mathematical properties of effectively com- putable functions, this formalism has provided a strong theoretical foundation for the family of functional programming languages. We show how to perform some arithmetical computations using the λ calculus and how to define recur- sive functions, even though functions in λ calculus are not given names and thus cannot refer explicitly to themselves.},
  archiveprefix = {arXiv},
  eprint = {10512656},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NN5KTP3D/Rojas - 1997 - A Tutorial Introduction to the Lambda Calculus(2).pdf}
}

@inproceedings{romanov2019WhatNameReducing,
  title = {What's in a {{Name}}? {{Reducing Bias}} in {{Bios}} without {{Access}} to {{Protected Attributes}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Romanov, A. and De-Arteaga, M. and Wallach, H. and Chayes, J. and Borgs, C. and Chouldechova, A. and Geyik, S. and Kenthapadi, K. and Rumshisky, A. and Kalai, A. T.},
  date = {2019},
  url = {http://arxiv.org/abs/1904.05233},
  abstract = {There is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual's true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals' names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale dataset of online biographies. We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier's overall true positive rate.},
  annotation = {23 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.05233},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QAYSC6BT/Romanov et al. - 2019 - What's in a Name Reducing Bias in Bios without Access to Protected Attributes(2).pdf}
}

@report{rong2014Word2vecParameterLearning,
  title = {Word2vec {{Parameter Learning Explained}}},
  author = {Rong, X.},
  date = {2014},
  pages = {1--21},
  archiveprefix = {arXiv},
  eprint = {1411.2738v4},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RW6SB74X/Rong - 2014 - word2vec Parameter Learning Explained(2).pdf}
}

@inproceedings{ross2019HowWellNLI,
  title = {How Well Do {{NLI}} Models Capture Verb Veridicality ?},
  booktitle = {{{EMNLP}}},
  author = {Ross, A. and Pavlick, E.},
  date = {2019},
  pages = {2230--2240},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9PEQYJ4A/Ross, Pavlick - 2019 - How well do NLI models capture verb veridicality(2).pdf},
  keywords = {u}
}

@report{rosset2020KnowledgeAwareLanguageModel,
  title = {Knowledge-{{Aware Language Model Pretraining}}},
  author = {Rosset, Corby and Xiong, Chenyan and Phan, Minh and Song, Xia and Bennett, Paul and Tiwary, Saurabh},
  date = {2020-06-29},
  url = {http://arxiv.org/abs/2007.00655},
  urldate = {2020-10-16},
  abstract = {How much knowledge do pretrained language models hold? Recent research observed that pretrained transformers are adept at modeling semantics but it is unclear to what degree they grasp human knowledge, or how to ensure they do so. In this paper we incorporate knowledge-awareness in language model pretraining without changing the transformer architecture, inserting explicit knowledge layers, or adding external storage of semantic information. Rather, we simply signal the existence of entities to the input of the transformer in pretraining, with an entityextended tokenizer; and at the output, with an additional entity prediction task. Our experiments show that solely by adding these entity signals in pretraining, significantly more knowledge is packed into the transformer parameters: we observe improved language modeling accuracy, factual correctness in LAMA knowledge probing tasks, and semantics in the hidden representations through edge probing. We also show that our knowledge-aware language model (KALM) can serve as a drop-in replacement for GPT-2 models, significantly improving downstream tasks like zero-shot question-answering with no task-related training.},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2007.00655},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GSZQ7SEP/Rosset et al. - 2020 - Knowledge-Aware Language Model Pretraining.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{roth2004LinearProgrammingFormulation,
  title = {A {{Linear Programming Formulation}} for {{Global Inference}} in {{Natural Language Tasks}}},
  booktitle = {{{CoNLL}}},
  author = {Roth, D. and Yih, W.},
  date = {2004},
  pages = {3--10},
  issn = {00010782},
  doi = {10.3115/1690219.1690287},
  abstract = {In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.},
  annotation = {2063 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JLX2ZLHX/Roth, Yih - 2004 - A Linear Programming Formulation for Global Inference in Natural Language Tasks(2).pdf},
  isbn = {978-1-937284-43-5},
  issue = {July}
}

@inproceedings{roth2005IntegerLinearProgramming,
  title = {Integer {{Linear Programming Inference}} for {{Conditional Random Fields}}},
  booktitle = {{{ICML}}},
  author = {Roth, D. and Yih, W.},
  date = {2005},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YL4PC2MQ/Roth, Yih - 2005 - Integer Linear Programming Inference for Conditional Random Fields(2).pdf},
  keywords = {u}
}

@article{roth2015ContextawareFrameSemanticRole,
  title = {Context-Aware {{Frame}}-{{Semantic Role Labeling}}},
  author = {Roth, M. and Lapata, M.},
  date = {2015},
  journaltitle = {TACL},
  volume = {3},
  pages = {449--460},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00150},
  abstract = {Frame semantic representations have been useful in several applications ranging from text-to-scene generation, to question answering and social network analysis. Predicting such representations from raw text is, however, a challenging task and corresponding models are typically only trained on a small set of sentence-level annotations. In this paper, we present a semantic role labeling system that takes into account sentence and discourse context. We introduce several new features which we motivate based on linguistic insights and experimentally demonstrate that they lead to significant improvements over the current state-of-the-art in FrameNet-based semantic role labeling.},
  annotation = {28 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RKC3SBFA/Roth, Lapata - 2015 - Context-aware Frame-Semantic Role Labeling(2).pdf}
}

@inproceedings{rothCompositionWordRepresentations2014,
  title = {Composition of Word Representations Improves Semantic Role Labelling},
  booktitle = {{{EMNLP}}},
  author = {Roth, M. and Woodsend, K.},
  date = {2014},
  pages = {407--413},
  doi = {10.3115/v1/d14-1045},
  abstract = {State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance. Unfortunately, such corpora are expensive to produce and often do not generalize well across domains. Even in domain, errors are often made where syntactic information does not provide sufficient cues. In this paper, we mitigate both of these problems by employing distributional word representations gathered from unlabelled data. While straight-forward word representations of predicates and arguments improve performance, we show that further gains are achieved by composing representations that model the interaction between predicate and argument, and capture full argument spans.},
  annotation = {53 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2ZT7RK7K/Roth, Woodsend - 2014 - Composition of word representations improves semantic role labelling(2).pdf}
}

@inproceedings{rothNeuralSemanticRole2016,
  title = {Neural Semantic Role Labeling with Dependency Path Embeddings},
  booktitle = {{{ACL}}},
  author = {Roth, M. and Lapata, M.},
  date = {2016},
  volume = {2},
  pages = {1192--1202},
  doi = {10.18653/v1/p16-1113},
  abstract = {This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as subsequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method.},
  annotation = {145 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1605.07515},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G66RWLTB/Roth, Lapata - 2016 - Neural semantic role labeling with dependency path embeddings(2).pdf}
}

@report{rubin2020SmBoPSemiautoregressiveBottomup,
  title = {{{SmBoP}}: {{Semi}}-Autoregressive {{Bottom}}-up {{Semantic Parsing}}},
  shorttitle = {{{SmBoP}}},
  author = {Rubin, Ohad and Berant, Jonathan},
  date = {2020-10-23},
  url = {http://arxiv.org/abs/2010.12412},
  urldate = {2020-12-06},
  abstract = {The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step \$t\$ the top-\$K\$ sub-trees of height \$\textbackslash leq t\$. Our parser enjoys several benefits compared to top-down autoregressive parsing. First, since sub-trees in each decoding step are generated in parallel, the theoretical runtime is logarithmic rather than linear. Second, our bottom-up approach learns representations with meaningful semantic sub-programs at each step, rather than semantically vague partial trees. Last, SmBoP includes Transformer-based layers that contextualize sub-trees with one another, allowing us, unlike traditional beam-search, to score trees conditioned on other trees that have been previously explored. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP is competitive with top-down autoregressive parsing. On the test set, SmBoP obtains an EM score of \$60.5\textbackslash\%\$, similar to the best published score for a model that does not use database content, which is at \$60.6\textbackslash\%\$.},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2010.12412},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SLLQSNP3/Rubin and Berant - 2020 - SmBoP Semi-autoregressive Bottom-up Semantic Pars.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KIKPMRKL/2010.html}
}

@inproceedings{ruderStrongBaselinesNeural2018,
  title = {Strong Baselines for Neural Semi-Supervised Learning under Domain Shift},
  booktitle = {{{ACL}}},
  author = {Ruder, S. and Plank, B.},
  date = {2018},
  pages = {1044--1054},
  doi = {10.18653/v1/p18-1096},
  abstract = {Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline.},
  annotation = {72 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.09530},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K9EXEFKD/Ruder, Plank - 2018 - Strong baselines for neural semi-supervised learning under domain shift(2).pdf},
  isbn = {978-1-948087-32-2},
  keywords = {u}
}

@inproceedings{rudingerNeuralDavidsonianSemanticProtorole2018,
  title = {Neural-{{Davidsonian Semantic Proto}}-Role {{Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Rudinger, R. and Teichert, A. and Culkin, R. and Zhang, S. and Van Durme, B.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1804.07976v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q6SXAJ87/Rudinger et al. - 2018 - Neural-Davidsonian Semantic Proto-role Labeling(2).pdf}
}

@inproceedings{rudingerNeuralModelsFactuality2018,
  title = {Neural {{Models}} of {{Factuality}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Rudinger, R. and White, A. S. and Van Durme, B.},
  date = {2018},
  pages = {731--744},
  doi = {10.18653/v1/n18-1067},
  abstract = {We present two neural models for event factuality prediction, which yield significant performance gains over previous models on three event factuality datasets: FactBank, UW, and MEANTIME. We also present a substantial expansion of the It Happened portion of the Universal Decompositional Semantics dataset, yielding the largest event factuality dataset to date. We report model results on this extended factuality dataset as well.},
  annotation = {36 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.02472v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DFAT7QMI/Rudinger, White, Van Durme - 2018 - Neural Models of Factuality(2).pdf}
}

@inproceedings{rushNeuralAttentionModel2015,
  title = {A {{Neural Attention Model}} for {{Abstractive Sentence Summarization}}},
  booktitle = {{{EMNLP}}},
  author = {Rush, A. M. and Chopra, S. and Weston, J.},
  date = {2015},
  url = {http://arxiv.org/abs/1509.00685},
  abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
  annotation = {1690 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1509.00685},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JI4FUQKR/Rush, Chopra, Weston - 2015 - A Neural Attention Model for Abstractive Sentence Summarization(2).pdf}
}

@inproceedings{rushTorchStructDeepStructured2020,
  title = {Torch-{{Struct}}: {{Deep Structured Prediction Library}}},
  booktitle = {{{ACL}}},
  author = {Rush, Alexander},
  date = {2020},
  pages = {335--342},
  abstract = {The literature on structured prediction for NLP describes a rich collection of distributions and algorithms over sequences, segmentations, alignments, and trees; however, these algorithms are difficult to utilize in deep learning frameworks. We introduce Torch-Struct, a library for structured prediction designed to take advantage of and integrate with vectorized, auto-differentiation based frameworks. TorchStruct includes a broad collection of probabilistic structures accessed through a simple and flexible distribution-based API that connects to any deep learning model. The library utilizes batched, vectorized operations and exploits auto-differentiation to produce readable, fast, and testable code. Internally, we also include a number of general-purpose optimizations to provide cross-algorithm efficiency. Experiments show significant performance gains over fast baselines. Case studies demonstrate the benefits of the library. TorchStruct is available at https://github.com/ harvardnlp/pytorch-struct.},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M54JI2E6/Rush - 2020 - Torch-Struct Deep Structured Prediction Library.pdf},
  langid = {english}
}

@inproceedings{sabourDynamicRoutingCapsules2017,
  title = {Dynamic {{Routing Between Capsules}}},
  booktitle = {{{NeurIPS}}},
  author = {Sabour, S. and Frosst, N. and Hinton, G. E.},
  date = {2017},
  url = {http://arxiv.org/abs/1710.09829},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  annotation = {1915 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1710.09829},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5PYW24GG/Sabour, Frosst, Hinton - 2017 - Dynamic Routing Between Capsules(2).pdf},
  issue = {Nips},
  keywords = {u}
}

@report{saeed2009OverviewMultipleSequence,
  title = {An {{Overview}} of {{Multiple Sequence Alignment Systems}}},
  author = {Saeed, F. and Khokhar, A.},
  date = {2009},
  pages = {24},
  url = {http://arxiv.org/abs/0901.2747},
  abstract = {An overview of current multiple alignment systems to date are described.The useful algorithms, the procedures adopted and their limitations are presented.We also present the quality of the alignments obtained and in which cases(kind of alignments, kind of sequences etc) the particular systems are useful.},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {0901.2747},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GSPPWD4N/Saeed, Khokhar - 2009 - An Overview of Multiple Sequence Alignment Systems(2).pdf}
}

@report{sahlgren2021SingletonFallacyWhy,
  title = {The {{Singleton Fallacy}}: {{Why Current Critiques}} of {{Language Models Miss}} the {{Point}}},
  shorttitle = {The {{Singleton Fallacy}}},
  author = {Sahlgren, Magnus and Carlsson, Fredrik},
  date = {2021-02-08},
  url = {http://arxiv.org/abs/2102.04310},
  urldate = {2021-02-15},
  abstract = {This paper discusses the current critique against neural network-based Natural Language Understanding (NLU) solutions known as language models. We argue that much of the current debate rests on an argumentation error that we will refer to as the singleton fallacy: the assumption that language, meaning, and understanding are single and uniform phenomena that are unobtainable by (current) language models. By contrast, we will argue that there are many different types of language use, meaning and understanding, and that (current) language models are build with the explicit purpose of acquiring and representing one type of structural understanding of language. We will argue that such structural understanding may cover several different modalities, and as such can handle several different types of meaning. Our position is that we currently see no theoretical reason why such structural knowledge would be insufficient to count as "real" understanding.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2102.04310},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DR996EDZ/Sahlgren and Carlsson - 2021 - The Singleton Fallacy Why Current Critiques of La.pdf;/home/hiaoxui/.local/share/zotero_files/storage/582E5EGQ/2102.html}
}

@inproceedings{sakaguchiErrorrepairDependencyParsing2017,
  title = {Error-Repair {{Dependency Parsing}} for {{Ungrammatical Texts}}},
  booktitle = {{{ACL}}},
  author = {Sakaguchi, K. and Post, M. and Van Durme, B.},
  date = {2017},
  pages = {189--195},
  doi = {10.18653/v1/p17-2030},
  abstract = {We propose a new dependency pars-ing scheme which jointly parses a sen-tence and repairs grammatical errors by extending the non-directional transition-based formalism of Goldberg and El-hadad (2010) with three additional ac-tions: SUBSTITUTE, DELETE, INSERT. Be-cause these actions may cause an infinite loop in derivation, we also introduce sim-ple constraints that ensure the parser ter-mination. We evaluate our model with re-spect to dependency accuracy and gram-maticality improvements for ungrammat-ical sentences, demonstrating the robust-ness and applicability of our scheme.},
  annotation = {5 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UPDZUFXI/Sakaguchi, Post, Van Durme - 2017 - Error-repair Dependency Parsing for Ungrammatical Texts(2).pdf},
  keywords = {u}
}

@inproceedings{sakaguchiWinoGrandeAdversarialWinograd2019,
  title = {{{WinoGrande}}: {{An Adversarial Winograd Schema Challenge}} at {{Scale}}},
  booktitle = {{{AAAI}}},
  author = {Sakaguchi, K. and Bras, R. L. and Bhagavatula, C. and Choi, Y.},
  date = {2019},
  url = {http://arxiv.org/abs/1907.10641},
  abstract = {The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90\% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on WinoGrande achieve 59.4-79.1\%, which are 15-35\% below human performance of 94.0\%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - WSC (90.1\%), DPR (93.1\%), COPA (90.6\%), KnowRef (85.6\%), and Winogender (97.1\%). These results have dual implications: on one hand, they demonstrate the effectiveness of WinoGrande when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.},
  annotation = {92 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1907.10641},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3T4Y48CN/Sakaguchi et al. - 2019 - WinoGrande An Adversarial Winograd Schema Challenge at Scale(2).pdf},
  keywords = {u}
}

@article{salakhutdinov2007LearningNonlinearEmbedding,
  title = {Learning a {{Nonlinear Embedding}} by {{Preserving Class Neighbourhood Structure}}},
  author = {Salakhutdinov, R. and Hinton, G. E.},
  date = {2007},
  journaltitle = {JMLR},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9YCWIVBA/Salakhutdinov, Hinton - 2007 - Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure(2).pdf},
  keywords = {u},
  number = {1}
}

@inproceedings{salakhutdinovDeepBoltzmannMachines2009,
  title = {Deep {{Boltzmann Machines}}},
  booktitle = {{{AISTATS}}},
  author = {Salakhutdinov, R. and Hinton, G. E.},
  date = {2009},
  pages = {448--455},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N3QHQSC2/Salakhutdinov, Hinton - 2009 - Deep Boltzmann Machines(2).pdf},
  number = {3}
}

@inproceedings{salakhutdinovOptimizationEMExpectationconjugategradient2003,
  title = {Optimization with {{EM}} and Expectation-Conjugate-Gradient},
  booktitle = {{{ICML}}},
  author = {Salakhutdinov, R. and Roweis, S. T. and Ghahramani, Z.},
  date = {2003},
  volume = {20},
  pages = {672},
  doi = {10.1145/ 1273496.1273497},
  abstract = {We show a close relationship between the Expectation- Maximization (EM) algorithm and direct optimization algorithms such as gradientbased methods for parameter learning. We identify analytic conditions under which EM exhibits Newton-like behavior, and conditions under which it possesses poor, first-order convergence. Based on this analysis, we propose two novel algorithms for maximum likelihood estimation of latent variable models, and report empirical results showing that, as predicted by theory, the proposed new algorithms can substantially outperform standard EM in terms of speed of convergence in certain cases. 1.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XSDTFB4A/Salakhutdinov, Roweis, Ghahramani - 2003 - Optimization with EM and expectation-conjugate-gradient(2).pdf},
  isbn = {1-57735-189-4},
  number = {2}
}

@inproceedings{salimansImprovedTechniquesTraining2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  booktitle = {{{NeurIPS}}},
  author = {Salimans, T. and Goodfellow, I. and Zaremba, W. and Cheung, V. and Radford, A. and Chen, X.},
  date = {2016},
  pages = {1--10},
  url = {http://arxiv.org/abs/1606.03498},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  annotation = {4106 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1606.03498},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6Y38Z7VR/Salimans et al. - 2016 - Improved Techniques for Training GANs(2).pdf}
}

@inproceedings{salimansMarkovChainMonte2015,
  title = {Markov {{Chain Monte Carlo}} and {{Variational Inference}}: {{Bridging}} the {{Gap}}},
  booktitle = {{{ICML}}},
  author = {Salimans, T. and Kingma, D. P. and Welling, M.},
  date = {2015},
  url = {http://arxiv.org/abs/1410.6460},
  abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
  annotation = {330 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1410.6460},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JDYG27I6/Salimans, Kingma, Welling - 2015 - Markov Chain Monte Carlo and Variational Inference Bridging the Gap(2).pdf},
  isbn = {1410.6460},
  keywords = {u}
}

@report{salinas2017DeepARProbabilisticForecasting,
  title = {{{DeepAR}}: {{Probabilistic Forecasting}} with {{Autoregressive Recurrent Networks}}},
  author = {Salinas, D. and Flunkert, V. and Gasthaus, J.},
  date = {2017},
  url = {http://arxiv.org/abs/1704.04110},
  abstract = {Probabilistic forecasting, i.e. estimating the probability distribution of a time series' future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an auto regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15\% compared to state-of-the-art methods.},
  annotation = {195 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.04110},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VQRCFCPT/Salinas, Flunkert, Gasthaus - 2017 - DeepAR Probabilistic Forecasting with Autoregressive Recurrent Networks(2).pdf}
}

@inproceedings{sanhHierarchicalMultiTaskApproach2019,
  title = {A {{Hierarchical Multi}}-{{Task Approach}} for {{Learning Embeddings}} from {{Semantic Tasks}}},
  booktitle = {{{AAAI}}},
  author = {Sanh, V. and Wolf, T. and Ruder, S.},
  date = {2019},
  pages = {6949--6956},
  issn = {2159-5399},
  doi = {10.1609/aaai.v33i01.33016949},
  abstract = {Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.},
  annotation = {87 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1811.06031},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CKCF8GLC/Sanh, Wolf, Ruder - 2019 - A Hierarchical Multi-Task Approach for Learning Embeddings from Semantic Tasks(2).pdf},
  keywords = {u}
}

@inproceedings{sapATOMICAtlasMachine2019,
  title = {{{ATOMIC}}: {{An Atlas}} of {{Machine Commonsense}} for {{If}}-{{Then Reasoning}}},
  booktitle = {{{AAAI}}},
  author = {Sap, M. and Bras, R. L. and Allaway, E. and Bhagavatula, C. and Lourie, N. and Rashkin, H. and Roof, B. and Smith, N. A. and Choi, Y.},
  date = {2019},
  pages = {3027--3035},
  issn = {2159-5399},
  doi = {10.1609/aaai.v33i01.33013027},
  abstract = {We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., “if X pays Y a compliment, then Y will likely return the compliment”). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.},
  annotation = {184 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1811.00146},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L9JJEG33/Sap et al. - 2019 - ATOMIC An Atlas of Machine Commonsense for If-Then Reasoning(2).pdf}
}

@inproceedings{sapSOCIALIQACommonsense2019,
  title = {{{SOCIAL IQA}} : {{Commonsense Reasoning}} about {{Social Interactions Maarten Sap}}},
  booktitle = {{{EMNLP}}},
  author = {Sap, M. and Rashkin, H. and Chen, D. and Bras, R. L. and Choi, Y.},
  date = {2019},
  pages = {4453--4463},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/URXX24XS/Sap et al. - 2019 - SOCIAL IQA Commonsense Reasoning about Social Interactions Maarten Sap(2).pdf},
  keywords = {u}
}

@inproceedings{sarawagiSemiMarkovConditionalRandom2005,
  title = {Semi-{{Markov Conditional Random Fields}} for {{Information Extraction}}},
  booktitle = {{{NeurIPS}}},
  author = {Sarawagi, S. and Cohen, W. W.},
  date = {2005},
  pages = {1185--1192},
  issn = {10495258},
  doi = {10.1.1.128.3524},
  url = {http://papers.nips.cc/paper/2648-semi-markov-conditional-random-fields-for-information-extraction},
  abstract = {We describe semi-Markov conditional random fields (semi-CRFs), a con-ditionally trained version of semi-Markov chains. Intuitively, a semi-CRF on an input sequence x outputs a " segmentation " of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements x i of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time—often only a small constant factor slower than conventional CRFs. In experiments on five named entity recognition problems, semi-CRFs generally outper-form conventional CRFs.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/F2GXR4S8/Sarawagi, Cohen - 2005 - Semi-Markov Conditional Random Fields for Information Extraction(2).pdf},
  isbn = {0-262-19534-8},
  keywords = {u}
}

@inproceedings{schlangenGeneralAbstractModel2009,
  title = {A {{General}}, {{Abstract Model}} of {{Incremental Dialogue Processing}}},
  booktitle = {{{EACL}}},
  author = {Schlangen, D. and Skantze, G.},
  date = {2009},
  doi = {10.5087/dad.2011.105},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3GCNEIG8/Schlangen, Skantze - 2009 - A General, Abstract Model of Incremental Dialogue Processing(2).pdf},
  keywords = {u}
}

@inproceedings{schnabelEvaluationMethodsUnsupervised2015,
  title = {Evaluation Methods for Unsupervised Word Embeddings},
  booktitle = {{{EMNLP}}},
  author = {Schnabel, T. and Labutov, I. and Mimno, D.},
  date = {2015},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E5GK4ED5/Schnabel, Labutov, Mimno - 2015 - Evaluation methods for unsupervised word embeddings(2).pdf}
}

@inproceedings{schneiderNLTKFrameNetAPI2017,
  title = {The {{NLTK FrameNet API}}: {{Designing}} for Discoverability with a Rich Linguistic Resource},
  booktitle = {{{EMNLP}}},
  author = {Schneider, N. and Wooters, C.},
  date = {2017},
  doi = {10.18653/v1/d17-2001},
  abstract = {A new Python API, integrated within the NLTK suite, offers access to the FrameNet 1.7 lexical database. The lexicon (structured in terms of frames) as well as annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt.},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1703.07438},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DYD73RBX/Schneider, Wooters - 2017 - The NLTK FrameNet API Designing for discoverability with a rich linguistic resource(2).pdf},
  isbn = {978-1-945626-97-5}
}

@article{schroedl2005ImprovedSearchAlgorithm,
  title = {An Improved Search Algorithm for Optimal Multiple-Sequence Alignment},
  author = {Schroedl, S.},
  date = {2005},
  journaltitle = {JAIR},
  volume = {23},
  pages = {587--623},
  issn = {10769757},
  doi = {10.1613/jair.1534},
  abstract = {Multiple sequence alignment (MSA) is a ubiquitous problem in computational biology. Although it is NP-hard to find an optimal solution for an arbitrary number of sequences, due to the importance of this problem researchers are trying to push the limits of exact algorithms further. Since MSA can be cast as a classical path finding problem, it is attracting a growing number of AI researchers interested in heuristic search algorithms as a challenge with actual practical relevance. In this paper, we first review two previous, complementary lines of research. Based on Hirschberg's algorithm, Dynamic Programming needs O(kN\^(k-1)) space to store both the search frontier and the nodes needed to reconstruct the solution path, for k sequences of length N. Best first search, on the other hand, has the advantage of bounding the search space that has to be explored using a heuristic. However, it is necessary to maintain all explored nodes up to the final solution in order to prevent the search from re-expanding them at higher cost. Earlier approaches to reduce the Closed list are either incompatible with pruning methods for the Open list, or must retain at least the boundary of the Closed list. In this article, we present an algorithm that attempts at combining the respective advantages; like A* it uses a heuristic for pruning the search space, but reduces both the maximum Open and Closed size to O(kN\^(k-1)), as in Dynamic Programming. The underlying idea is to conduct a series of searches with successively increasing upper bounds, but using the DP ordering as the key for the Open priority queue. With a suitable choice of thresholds, in practice, a running time below four times that of A* can be expected. In our experiments we show that our algorithm outperforms one of the currently most successful algorithms for optimal multiple sequence alignments, Partial Expansion A*, both in time and memory. Moreover, we apply a refined heuristic based on optimal alignments not only of pairs of sequences, but of larger subsets. This idea is not new; however, to make it practically relevant we show that it is equally important to bound the heuristic computation appropriately, or the overhead can obliterate any possible gain. Furthermore, we discuss a number of improvements in time and space efficiency with regard to practical implementations. Our algorithm, used in conjunction with higher-dimensional heuristics, is able to calculate for the first time the optimal alignment for almost all of the problems in Reference 1 of the benchmark database BAliBASE.},
  annotation = {31 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/934DDEGS/Schroedl - 2005 - An improved search algorithm for optimal multiple-sequence alignment(2).pdf}
}

@inproceedings{sebastianDecisionTreeFields2011,
  title = {Decision {{Tree Fields}}},
  booktitle = {{{ICCV}}},
  author = {Sebastian, N. and Rother, C. and Bagon, S. and Sharp, T. and Yao, B. and Kohli, P.},
  date = {2011},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KRUUP5XB/Sebastian et al. - 2011 - Decision Tree Fields(2).pdf},
  keywords = {u}
}

@inproceedings{seegerBayesianIntermittentDemand2016,
  title = {Bayesian Intermittent Demand Forecasting for Large Inventories},
  booktitle = {{{NeurIPS}}},
  author = {Seeger, M. and Salinas, D. and Flunkert, V.},
  date = {2016},
  pages = {4646--4654},
  issn = {10495258},
  abstract = {We present a scalable and robust Bayesian method for demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics. Inference is approximated by the Newton-Raphson algorithm, reduced to linear-time Kalman smoothing, which allows us to operate on several orders of magnitude larger problems than previous related work. In a study on large real-world sales datasets, our method outperforms competing approaches on fast and medium moving items.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TAQ77PR3/Seeger, Salinas, Flunkert - 2016 - Bayesian intermittent demand forecasting for large inventories(2).pdf}
}

@inproceedings{seeGetPointSummarization2017,
  title = {Get to the Point: {{Summarization}} with Pointer-Generator Networks},
  booktitle = {{{ACL}}},
  author = {See, A. and Liu, P. J. and Manning, C. D.},
  date = {2017},
  pages = {1073--1083},
  doi = {10.18653/v1/P17-1099},
  abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
  annotation = {1619 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.04368},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WU8YKEDA/See, Liu, Manning - 2017 - Get to the point Summarization with pointer-generator networks(2).pdf},
  isbn = {978-1-945626-75-3}
}

@inproceedings{selmanPELCNFProbabilisticEvent2011,
  title = {{{PEL}}-{{CNF}} : {{Probabilistic}} Event Logic Conjunctive Normal Form for Video Interpretation},
  booktitle = {{{ICCV}}},
  author = {Selman, J. and Amer, M. and Fern, A. and Todorovic, S.},
  date = {2011},
  doi = {10.1109/ICCVW.2011.6130308},
  annotation = {12 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V3EKQTST/Selman et al. - 2011 - PEL-CNF Probabilistic event logic conjunctive normal form for video interpretation(2).pdf},
  keywords = {u}
}

@inproceedings{selsamLearningSATSolver2018,
  title = {Learning a {{SAT Solver}} from {{Single}}-{{Bit Supervision}}},
  booktitle = {{{ICLR}}},
  author = {Selsam, D. and Lamm, M. and Bünz, B. and Liang, P. and de Moura, L. and Dill, D. L.},
  date = {2018},
  pages = {1--11},
  url = {http://arxiv.org/abs/1802.03685},
  abstract = {We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.},
  annotation = {173 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1802.03685},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7J4NDBJX/Selsam et al. - 2018 - Learning a SAT Solver from Single-Bit Supervision(2).pdf},
  options = {useprefix=true}
}

@inproceedings{sennrich2016NeuralMachineTranslation,
  title = {Neural {{Machine Translation}} of {{Rare Words}} with {{Subword Units}}},
  booktitle = {{{ACL}}},
  author = {Sennrich, R. and Haddow, B. and Birch, A.},
  date = {2016},
  url = {http://arxiv.org/abs/1508.07909},
  abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
  annotation = {3268 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1508.07909},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z3FLMTPQ/Sennrich, Haddow, Birch - 2016 - Neural Machine Translation of Rare Words with Subword Units(2).pdf},
  keywords = {u}
}

@inproceedings{sennrich2017HowGrammaticalCharacterlevel,
  title = {How Grammatical Is Character-Level Neural Machine Translation? {{Assessing}} Mt Quality with Contrastive Translation Pairs},
  booktitle = {{{EACL}}},
  author = {Sennrich, R.},
  date = {2017},
  pages = {376--382},
  doi = {10.18653/v1/e17-2060},
  abstract = {Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of error. We present LingEval971, a large-scale data set of 97 000 contrastive translation pairs based on the WMT English!German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.},
  annotation = {99 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1612.04629},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZGLBDP85/Sennrich - 2017 - How grammatical is character-level neural machine translation Assessing mt quality with contrastive translation pai(2).pdf},
  isbn = {978-1-5108-3860-4},
  keywords = {u}
}

@inproceedings{seo2017BidirectionalAttentionFlow,
  title = {Bidirectional {{Attention Flow}} for {{Machine Comprehension}}},
  booktitle = {{{ICLR}}},
  author = {Seo, M. and Kembhavi, A. and Farhadi, A. and Hajishirzi, H.},
  date = {2017},
  url = {http://arxiv.org/abs/1611.01603},
  abstract = {Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.},
  annotation = {1295 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1611.01603},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9Y9T5UUA/Seo et al. - 2017 - Bidirectional Attention Flow for Machine Comprehension(2).pdf},
  keywords = {u}
}

@inproceedings{serrano2019AttentionInterpretable,
  title = {Is {{Attention Interpretable}}?},
  booktitle = {{{ACL}}},
  author = {Serrano, S. and Smith, N. A.},
  date = {2019},
  pages = {2931--2951},
  doi = {10.18653/v1/p19-1282},
  abstract = {Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.},
  annotation = {115 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1906.03731},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J9ZITHCM/Serrano, Smith - 2019 - Is Attention Interpretable(2).pdf}
}

@inproceedings{shang2015NeuralRespondingMachine,
  title = {Neural {{Responding Machine}} for {{Short}}-{{Text Conversation}}},
  booktitle = {{{ACL}}},
  author = {Shang, L. and Lu, Z. and Li, H.},
  date = {2015},
  url = {http://arxiv.org/abs/1503.02364},
  abstract = {We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75\% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.},
  annotation = {822 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1503.02364},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XU4VZ2YD/Shang, Lu, Li - 2015 - Neural Responding Machine for Short-Text Conversation(2).pdf},
  keywords = {u}
}

@inproceedings{sharan2017LearningOvercompleteHMMs,
  title = {Learning {{Overcomplete HMMs}}},
  booktitle = {{{NeurIPS}}},
  author = {Sharan, V. and Kakade, S. and Liang, P. and Valiant, G.},
  date = {2017},
  pages = {1--10},
  url = {http://arxiv.org/abs/1711.02309},
  abstract = {We study the problem of learning overcomplete HMMs---those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results---both positive and negative---which help define the boundaries between the tractable and intractable settings. Specifically, we show positive results for a large subclass of HMMs whose transition matrices are sparse, well-conditioned, and have small probability mass on short cycles. On the other hand, we show that learning is impossible given only a polynomial number of samples for HMMs with a small output alphabet and whose transition matrices are random regular graphs with large degree. We also discuss these results in the context of learning HMMs which can capture long-term dependencies.},
  annotation = {10 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1711.02309},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RZXHZB7S/Sharan et al. - 2017 - Learning Overcomplete HMMs(2).pdf}
}

@article{shelton2014TutorialStructuredContinuoustime,
  title = {Tutorial on Structured Continuous-Time {{Markov}} Processes},
  author = {Shelton, C. R. and Ciardo, G.},
  date = {2014},
  journaltitle = {JAIR},
  volume = {51},
  pages = {725--778},
  issn = {10769757},
  abstract = {A continuous-time Markov process (CTMP) is a collection of variables indexed by a continuous quantity, time. It obeys the Markov property that the distribution over a future variable is independent of past variables given the state at the present time. We introduce continuous-time Markov process representations and algorithms for filtering, smoothing, expected sufficient statistics calculations, and model estimation, assuming no prior knowledge of continuous-time processes but some basic knowledge of probability and statistics. We begin by describing "flat" or unstructured Markov processes and then move to structured Markov processes (those arising from state spaces consisting of assignments to variables) including Kronecker, decision-diagram, and continuous-time Bayesian network representations. We provide the first connection between decision-diagrams and continuous-time Bayesian networks.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6DJWC4Z3/Shelton, Ciardo - 2014 - Tutorial on structured continuous-time Markov processes(2).pdf}
}

@inproceedings{shen2016MinimumRiskTraining,
  title = {Minimum {{Risk Training}} for {{Neural Machine Translation}}},
  booktitle = {{{ACL}}},
  author = {Shen, S. and Cheng, Y. and He, Z. and He, W. and Wu, H. and Sun, M. and Liu, Y.},
  date = {2016},
  pages = {1--9},
  url = {http://arxiv.org/abs/1512.02433},
  abstract = {We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. Experiments on Chinese-English and English-French translation show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system.},
  annotation = {314 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1512.02433},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CPTI39QY/Shen et al. - 2016 - Minimum Risk Training for Neural Machine Translation(2).pdf}
}

@inproceedings{shen2018NASHEndtoEndNeural,
  title = {{{NASH}}: {{Toward End}}-to-{{End Neural Architecture}} for {{Generative Semantic Hashing}}},
  booktitle = {{{ACL}}},
  author = {Shen, D. and Su, Q. and Chapfuwa, P. and Wang, W. and Wang, G. and Carin, L. and Henao, R.},
  date = {2018},
  pages = {1--10},
  abstract = {Semantic hashing has become a power-ful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous tech-niques generally require two-stage train-ing, and the binary constraints are han-dled ad-hoc. In this paper, we present an end-to-end Neural Architecture for Se-mantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli la-tent variables. A neural variational in-ference framework is proposed for train-ing, where gradients are directly back-propagated through the discrete latent variable to optimize the hash function. We also draw connections between pro-posed method and rate-distortion the-ory, which provides a theoretical foun-dation for the effectiveness of the pro-posed framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsuper-vised and supervised scenarios.},
  archiveprefix = {arXiv},
  eprint = {1805.05361},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C6GM5457/Shen et al. - 2018 - NASH Toward End-to-End Neural Architecture for Generative Semantic Hashing(2).pdf},
  keywords = {u}
}

@inproceedings{shen2019OrderedNeuronsIntegrating,
  title = {Ordered {{Neurons}}: {{Integrating Tree Structures}} into {{Recurrent Neural Networks}}},
  booktitle = {{{ICLR}}},
  author = {Shen, Y. and Tan, S. and Sordoni, A. and Courville, A.},
  date = {2019},
  url = {http://arxiv.org/abs/1810.09536},
  abstract = {Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents. This paper proposes to add such an inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated. Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.},
  annotation = {143 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1810.09536},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5FSXHJH6/Shen et al. - 2019 - Ordered Neurons Integrating Tree Structures into Recurrent Neural Networks(2).pdf},
  keywords = {u}
}

@inproceedings{shi2018StructuredWordEmbedding,
  title = {Structured {{Word Embedding}} for {{Low Memory Neural Network Language Model}}},
  booktitle = {Interspeech},
  author = {Shi, K. and Yu, K.},
  date = {2018},
  pages = {1254--1258},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YNKUY64N/Shi, Yu - 2018 - Structured Word Embedding for Low Memory Neural Network Language Model(2).pdf},
  keywords = {u}
}

@inproceedings{shi2019FastDirectSearch,
  title = {Fast {{Direct Search}} in an {{Optimally Compressed Continuous Target Space}} for {{Efficient Multi}}-{{Label Active Learning}}},
  booktitle = {{{ICML}}},
  author = {Shi, W. and Yu, Q.},
  date = {2019},
  volume = {97},
  pages = {5769--5778},
  url = {http://proceedings.mlr.press/v97/shi19b.html},
  abstract = {Active learning for multi-label classification poses fundamental challenges given the complex label correlations and a potentially large and sparse label space. We propose a novel CS-BPCA process that integrates compressed sensing and Bayesian principal component analysis to perform a two-level label transformation, resulting in an optimally compressed continuous target space. Besides leveraging correlation and sparsity of a large label space for effective compression, an optimal compressing rate and the relative importance of the resultant targets are automatically determined through Bayesian inference. Furthermore, the orthogonality of the transformed space completely decouples the correlations among targets, which significantly simplifies multi-label sampling in the target space. We define a novel sampling function that leverages a multi-output Gaussian Process (MOGP). Gradient-free optimization strategies are developed to achieve fast online hyper-parameter learning and model retraining for active learning. Experimental results over multiple real-world datasets and comparison with competitive multi-label active learning models demonstrate the effectiveness of the proposed framework.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S8A9XFIC/Shi, Yu - 2019 - Fast Direct Search in an Optimally Compressed Continuous Target Space for Efficient Multi-Label Active Learning(2).pdf},
  keywords = {u}
}

@inproceedings{shi2019RetrofittingContextualizedWord,
  title = {Retrofitting {{Contextualized Word Embeddings}} with {{Paraphrases}}},
  booktitle = {{{EMNLP}}},
  author = {Shi, W. and Chen, M. and Zhou, P. and Chang, K.},
  date = {2019},
  pages = {1198--1203},
  url = {http://arxiv.org/abs/1909.09700},
  abstract = {Contextualized word embedding models, such as ELMo, generate meaningful representations of words and their context. These models have been shown to have a great impact on downstream applications. However, in many cases, the contextualized embedding of a word changes drastically when the context is paraphrased. As a result, the downstream model is not robust to paraphrasing and other linguistic variations. To enhance the stability of contextualized word embedding models, we propose an approach to retrofitting contextualized embedding models with paraphrase contexts. Our method learns an orthogonal transformation on the input space, which seeks to minimize the variance of word representations on paraphrased contexts. Experiments show that the retrofitted model significantly outperforms the original ELMo on various sentence classification and language inference tasks.},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.09700},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7NLTBFGR/Shi et al. - 2019 - Retrofitting Contextualized Word Embeddings with Paraphrases(2).pdf},
  keywords = {u}
}

@report{shi2019SimpleBERTModels,
  title = {Simple {{BERT Models}} for {{Relation Extraction}} and {{Semantic Role Labeling}}},
  author = {Shi, Peng and Lin, Jimmy},
  date = {2019-04-10},
  url = {http://arxiv.org/abs/1904.05255},
  urldate = {2020-10-21},
  abstract = {We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.},
  annotation = {86 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.05255},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9BENHYKV/Shi and Lin - 2019 - Simple BERT Models for Relation Extraction and Sem.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QCJ8HGRG/1904.html},
  keywords = {u}
}

@inproceedings{shi2019VisuallyGroundedNeural,
  title = {Visually {{Grounded Neural Syntax Acquisition}}},
  booktitle = {{{ACL}}},
  author = {Shi, H. and Mao, J. and Gimpel, K. and Livescu, K.},
  date = {2019},
  pages = {1842--1861},
  doi = {10.18653/v1/p19-1180},
  abstract = {We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without any explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees. We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data. We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches.},
  annotation = {24 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1906.02890},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TZXYXT5B/Shi et al. - 2019 - Visually Grounded Neural Syntax Acquisition(2).pdf},
  keywords = {u}
}

@article{shibuya2020NestedNamedEntity,
  title = {Nested {{Named Entity Recognition}} via {{Second}}-Best {{Sequence Learning}} and {{Decoding}}},
  author = {Shibuya, Takashi and Hovy, Eduard},
  date = {2020-09},
  journaltitle = {TACL},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {605--620},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00334},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00334},
  urldate = {2021-01-16},
  abstract = {When an entity name contains other names within it, the identification of all combinations of names can become difficult and expensive. We propose a new method to recognize not only outermost named entities but also inner nested ones. We design an objective function for training a neural model that treats the tag sequence for nested entities as the second best path within the span of their parent entity. In addition, we provide the decoding method for inference that extracts entities iteratively from outermost ones to inner ones in an outsideto-inside way. Our method has no additional hyperparameters to the conditional random field based model widely used for flat named entity recognition tasks. Experiments demonstrate that our method performs better than or at least as well as existing methods capable of handling nested entities, achieving F1-scores of 85.82\%, 84.34\%, and 77.36\% on ACE2004, ACE-2005, and GENIA datasets, respectively.},
  annotation = {11 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4MP4RESG/Shibuya and Hovy - 2020 - Nested Named Entity Recognition via Second-best Se.pdf},
  langid = {english}
}

@report{shih2019XLEditorPosteditingSentences,
  title = {{{XL}}-{{Editor}}: {{Post}}-Editing {{Sentences}} with {{XLNet}}},
  author = {Shih, Y. and Chang, W. and Yang, Y.},
  date = {2019},
  url = {http://arxiv.org/abs/1910.10479},
  abstract = {While neural sequence generation models achieve initial success for many NLP applications, the canonical decoding procedure with left-to-right generation order (i.e., autoregressive) in one-pass can not reflect the true nature of human revising a sentence to obtain a refined result. In this work, we propose XL-Editor, a novel training framework that enables state-of-the-art generalized autoregressive pretraining methods, XLNet specifically, to revise a given sentence by the variable-length insertion probability. Concretely, XL-Editor can (1) estimate the probability of inserting a variable-length sequence into a specific position of a given sentence; (2) execute post-editing operations such as insertion, deletion, and replacement based on the estimated variable-length insertion probability; (3) complement existing sequence-to-sequence models to refine the generated sequences. Empirically, we first demonstrate better post-editing capabilities of XL-Editor over XLNet on the text insertion and deletion tasks, which validates the effectiveness of our proposed framework. Furthermore, we extend XL-Editor to the unpaired text style transfer task, where transferring the target style onto a given sentence can be naturally viewed as post-editing the sentence into the target style. XL-Editor achieves significant improvement in style transfer accuracy and also maintains coherent semantic of the original sentence, showing the broad applicability of our method.},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1910.10479},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ME59ZJ87/Shih, Chang, Yang - 2019 - XL-Editor Post-editing Sentences with XLNet(2).pdf}
}

@inproceedings{shin2020AutoPromptElicitingKnowledge,
  title = {{{AutoPrompt}}: {{Eliciting Knowledge}} from {{Language Models}} with {{Automatically Generated Prompts}}},
  shorttitle = {{{AutoPrompt}}},
  booktitle = {{{EMNLP}}},
  author = {Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L. and Wallace, Eric and Singh, Sameer},
  date = {2020-11-07},
  url = {http://arxiv.org/abs/2010.15980},
  urldate = {2020-11-10},
  abstract = {The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.},
  annotation = {9 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2010.15980},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U8I2BJ8W/Shin et al. - 2020 - AutoPrompt Eliciting Knowledge from Language Mode.pdf;/home/hiaoxui/.local/share/zotero_files/storage/WTNYEMLN/2010.html}
}

@inproceedings{shiv2019NovelPositionalEncodings,
  title = {Novel {{Positional Encodings}} to {{Enable}} Tree-{{Structured}} Transformers},
  booktitle = {{{NeurIPS}}},
  author = {Shiv, V. L. and Quirk, C.},
  date = {2019},
  abstract = {With interest in program synthesis and similarly flavored problems rapidly increasing , neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6RUCJK3H/Shiv, Quirk - 2019 - Novel Positional Encodings to Enable tree-Structured transformers(2).pdf}
}

@inproceedings{shu2018CompressingWordEmbeddings,
  title = {Compressing {{Word Embeddings}} via {{Deep Computational Code Learning}}},
  booktitle = {{{ICLR}}},
  author = {Shu, R. and Nakayama, H.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1711.01068v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GNNLMCRY/Shu, Nakayama - 2018 - Compressing Word Embeddings via Deep Computational Code Learning(2).pdf}
}

@inproceedings{shwartz2019StillPainNeck,
  title = {Still a {{Pain}} in the {{Neck}}: {{Evaluating Text Representations}} on {{Lexical Composition}}},
  booktitle = {{{EMNLP}}},
  author = {Shwartz, V. and Dagan, I.},
  date = {2019},
  volume = {7},
  pages = {403--419},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00277},
  abstract = {Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans. Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations.},
  annotation = {20 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1902.10618},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HTBQPDDS/Shwartz, Dagan - 2019 - Still a Pain in the Neck Evaluating Text Representations on Lexical Composition(2).pdf},
  keywords = {u}
}

@report{shwartz2020YouAreGrounded,
  title = {"{{You}} Are Grounded!": {{Latent Name Artifacts}} in {{Pre}}-Trained {{Language Models}}},
  author = {Shwartz, V. and Rudinger, R. and Tafjord, O.},
  date = {2020},
  url = {http://arxiv.org/abs/2004.03012},
  abstract = {Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for "Donald is a" substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.},
  annotation = {4 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2004.03012},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HWLCH3LZ/Shwartz, Rudinger, Tafjord - 2020 - You are grounded! Latent Name Artifacts in Pre-trained Language Models(2).pdf},
  keywords = {u}
}

@article{silver2016MasteringGameGo,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, D. and Huang, A. and Maddison, C. J. and Guez, A. and Sifre, L. and Van Den Driessche, G. and Schrittwieser, J. and Antonoglou, I. and Panneershelvam, V. and Lanctot, M. and Dieleman, S. and Grewe, D. and Nham, J. and Kalchbrenner, N. and Sutskever, I. and Lillicrap, T. and Leach, M. and Kavukcuoglu, K. and Graepel, T. and Hassabis, D.},
  date = {2016},
  journaltitle = {Nature},
  volume = {529},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  issn = {14764687},
  doi = {10.1038/nature16961},
  url = {http://dx.doi.org/10.1038/nature16961},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  annotation = {8357 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {26819042},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/BTYRW9LA/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search(2).pdf},
  number = {7587}
}

@incollection{singh2002OpenMindCommon,
  title = {Open {{Mind Common Sense}}: {{Knowledge Acquisition}} from the {{General Public}}},
  shorttitle = {Open {{Mind Common Sense}}},
  booktitle = {On the {{Move}} to {{Meaningful Internet Systems}} 2002: {{CoopIS}}, {{DOA}}, and {{ODBASE}}},
  author = {Singh, Push and Lin, Thomas and Mueller, Erik T. and Lim, Grace and Perkins, Travell and Li Zhu, Wan},
  editor = {Meersman, Robert and Tari, Zahir},
  date = {2002},
  volume = {2519},
  pages = {1223--1237},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36124-3_77},
  url = {http://link.springer.com/10.1007/3-540-36124-3_77},
  urldate = {2020-11-14},
  abstract = {Open Mind Common Sense is a knowledge acquisition system designed to acquire commonsense knowledge from the general public over the web. We describe and evaluate our first fielded system, which enabled the construction of a 450,000 assertion commonsense knowledge base. We then discuss how our second-generation system addresses weaknesses discovered in the first. The new system acquires facts, descriptions, and stories by allowing participants to construct and fill in natural language templates. It employs word-sense disambiguation and methods of clarifying entered knowledge, analogical inference to provide feedback, and allows participants to validate knowledge and in turn each other.},
  editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
  editorbtype = {redactor},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6KDCELET/Singh et al. - 2002 - Open Mind Common Sense Knowledge Acquisition from.pdf},
  isbn = {978-3-540-00106-5 978-3-540-36124-4},
  langid = {english},
  options = {useprefix=true},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@report{singh2018NaturalLanguageProcessing,
  title = {Natural {{Language Processing}} for {{Information Extraction}}},
  author = {Singh, S.},
  date = {2018},
  pages = {1--24},
  archiveprefix = {arXiv},
  eprint = {1807.02383v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Y4AC6ZLZ/Singh - 2018 - Natural Language Processing for Information Extraction(2).pdf}
}

@report{singh2020CopyNextExplicitSpan,
  title = {{{CopyNext}} : {{Explicit Span Copying}} and {{Alignment}} in {{Sequence}} to {{Sequence Models}}},
  author = {Singh, A. and Xia, P. and Qin, G. and Yarmohammadi, M. and Durme, B. Van},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9PDUI9S3/Singh et al. - 2020 - CopyNext Explicit Span Copying and Alignment in Sequence to Sequence Models(2).pdf}
}

@article{siskind1996ComputationalStudyCrosssituational,
  title = {A Computational Study of Cross-Situational Techniques for Learning Word-to-Meaning Mappings},
  author = {Siskind, J. M.},
  date = {1996},
  journaltitle = {Cognition},
  volume = {61},
  pages = {39--91},
  issn = {00100277},
  doi = {10.1016/S0010-0277(96)00728-7},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027796007287},
  abstract = {This paper presents a computational study of part of the lexical-acquisition task faced by children, namely the acquisition of word-to-meaning mappings. It first approximates this task as a formal mathematical problem. It then presents an implemented algorithm for solving this problem, illustrating its operation on a small example. This algorithm offers one precise interpretation of the intuitive notions of cross-situational learning and the principle of contrast applied between words in an utterance. It robustly learns a homonymous lexicon despite noisy multi-word input, in the presence of referential uncertainty, with no prior knowledge that is specific to the language being learned. Computational simulations demonstrate the robustness of this algorithm and illustrate how algorithms based on cross-situational learning and the principle of contrast might be able to solve lexical-acquisition problems of the size faced by children, under weak, worst-case assumptions about the type and quantity of data available.},
  annotation = {529 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {8990968},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U964HHSK/Siskind - 1996 - A computational study of cross-situational techniques for learning word-to-meaning mappings(2).pdf},
  isbn = {0010-0277},
  keywords = {u},
  number = {1-2}
}

@inproceedings{smiley2016WhenPlummetWhen,
  title = {When to {{Plummet}} and {{When}} to {{Soar}}: {{Corpus Based Verb Selection}} for {{Natural Language Generation}}},
  booktitle = {{{INLG}}},
  author = {Smiley, C. and Plachouras, V. and Schilder, F. and Bretz, H. and Leidner, J. L. and Song, D.},
  date = {2016},
  pages = {36--39},
  abstract = {For data-to-text tasks in Natural Language Generation (NLG), researchers are often faced with choices about the right words to express phenomena seen in the data. One common phenomenon centers around the description of trends between two data points and selecting the appropriate verb to express both the di-rection and intensity of movement. Our re-search shows that rather than simply select-ing the same verbs again and again, variation and naturalness can be achieved by quantify-ing writers' patterns of usage around verbs.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YRX6RT6H/Smiley et al. - 2016 - When to Plummet and When to Soar Corpus Based Verb Selection for Natural Language Generation(2).pdf}
}

@inproceedings{smith2005ContrastiveEstimationTraining,
  title = {Contrastive Estimation: {{Training Log}}-{{Linear Models}} on {{Unlabeled Data}}},
  booktitle = {{{ACL}}},
  author = {Smith, N. A. and Eisner, J. M.},
  date = {2005},
  pages = {354--362},
  doi = {10.3115/1219840.1219884},
  abstract = {Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.},
  annotation = {351 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SJP6IN36/Smith, Eisner - 2005 - Contrastive estimation Training Log-Linear Models on Unlabeled Data(2).pdf},
  isbn = {1-932432-51-5},
  issue = {June},
  keywords = {u}
}

@inproceedings{smith2012UnsupervisedLearningApproximate,
  title = {Unsupervised Learning on an Approximate Corpus},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Smith, J. and Eisner, J. M.},
  date = {2012},
  pages = {131--141},
  abstract = {Unsupervised learning techniques can take advantage of large amounts of unannotated text, but the largest text corpus (the Web) is not easy to use in its full form. Instead, we have statistics about this corpus in the form of n-gram counts (Brants and Franz, 2006). While n-gram counts do not directly provide sentences, a distribution over sentences can be estimated from them in the same way that ngram language models are estimated. We treat this distribution over sentences as an approximate corpus and show how unsupervised learning can be performed on such a corpus using variational inference. We compare hidden Markov model (HMM) training on exact and approximate corpora of various sizes, measuring speed and accuracy on unsupervised part-of-speech tagging.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DJI7SG9U/Smith, Eisner - 2012 - Unsupervised learning on an approximate corpus(2).pdf},
  isbn = {1-937284-20-4}
}

@inproceedings{snow2008CheapFastIt,
  title = {Cheap and {{Fast}} - {{But}} Is It {{Good}} ? {{Evaluating Non}}-{{Expert Annotations}} for {{Natural Language Tasks}}},
  booktitle = {{{EMNLP}}},
  author = {Snow, R. and O'Connor, B. and Jurafsky, D. and Ng, A. Y.},
  date = {2008},
  pages = {254--263},
  issn = {09246495},
  doi = {10.1.1.142.8286},
  abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We ex-plore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: af-fect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechani-cal Turk non-expert annotations and existing gold standard labels provided by expert label-ers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effec-tive as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annota-tion quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
  archiveprefix = {arXiv},
  eprint = {23259955},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GFFFUXFG/Snow et al. - 2008 - Cheap and Fast - But is it Good Evaluating Non-Expert Annotations for Natural Language Tasks(2).pdf},
  isbn = {978-1-4503-2922-4},
  keywords = {u}
}

@inproceedings{snyder2007DatabasetextAlignmentStructured,
  title = {Database-Text Alignment via Structured Multilabel Classification},
  booktitle = {{{IJCAI}}},
  author = {Snyder, B. and Barzilay, R.},
  date = {2007},
  pages = {1713--1718},
  issn = {10450823},
  abstract = {This paper addresses the task of aligning a database with a corresponding text. The goal is to link individual database entries with sentences that verbalize the same information. By providing explicit semantics-to-text links, these alignments can aid the training of natural language generation and information extraction systems. Beyond these pragmatic benefits, the alignment problem is appealing from a modeling perspective: the mappings between database entries and text sentences exhibit rich structural dependencies, unique to this task. Thus, the key challenge is to make use of as many global dependencies as possible without sacrificing tractability. To this end, we cast text-database alignment as a structured multilabel classification task where each sentence is labeled with a subset of matching database entries. In contrast to existing multilabel classifiers, our approach operates over arbitrary global features of inputs and proposed labels. We compare our model with a baseline classifier that makes locally optimal decisions. Our results show that the proposed model yields a 15\% relative reduction in error, and compares favorably with human performance.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/STH6QN2L/Snyder, Barzilay - 2007 - Database-text alignment via structured multilabel classification(2).pdf}
}

@inproceedings{socher2013ParsingCompositionalVector,
  title = {Parsing with {{Compositional Vector Grammars}}},
  booktitle = {{{ACL}}},
  author = {Socher, R. and Bauer, J. and Manning, C. D. and Ng, A. Y.},
  date = {2013},
  volume = {80},
  pages = {5080--5083},
  issn = {15205126},
  doi = {10.1021/ja01552a021},
  abstract = {Natural language parsing has typically been done with small sets of discrete categories such as \{NP\} and \{VP\}, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (\{CVG)\}, which combines \{PCFGs\} with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The \{CVG\} improves the \{PCFG\} of the Stanford Parser by 3.8 \% to obtain an F1 score of 90.4\%. It is fast to train and implemented approximately as an efficient reranker it is about 20 \% faster than the current Stanford factored parser. The \{CVG\} learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as \{PP\} attachments. 1},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2JQ5FQPY/Socher et al. - 2013 - Parsing with Compositional Vector Grammars(2).pdf},
  keywords = {u},
  number = {19}
}

@inproceedings{socher2013RecursiveDeepModels,
  title = {Recursive {{Deep Models}} for {{Semantic Compositionality Over}} a {{Sentiment Treebank}}},
  booktitle = {{{EMNLP}}},
  author = {Socher, R. and Perelygin, A. and Wu, J. Y. and Chuang, J. and Manning, C. D. and Ng, A. Y. and Potts, C.},
  date = {2013},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0073791},
  abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80\% up to 85.4\%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7\%, an improvement of 9.7\% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
  annotation = {1068 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {24086296},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HP9LIUFT/Socher et al. - 2013 - Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank(2).pdf},
  isbn = {978-1-937284-97-8}
}

@inproceedings{song2013GeneralFrameworkRecognizing,
  title = {A {{General Framework}} for {{Recognizing Complex Events}} in {{Markov Logic}}},
  booktitle = {{{AAAI}}},
  author = {Song, Y. C. and Kautz, H. and Li, Y. and Luo, J.},
  date = {2013},
  pages = {68--73},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LPGE54LT/Song et al. - 2013 - A General Framework for Recognizing Complex Events in Markov Logic(2).pdf},
  keywords = {u}
}

@inproceedings{song2019GenerativeModelingEstimating,
  title = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
  booktitle = {{{NeurIPS}}},
  author = {Song, Y. and Ermon, S.},
  date = {2019},
  url = {http://arxiv.org/abs/1907.05600},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  annotation = {99 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1907.05600},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PBMAHZZW/Song, Ermon - 2019 - Generative Modeling by Estimating Gradients of the Data Distribution(2).pdf},
  issue = {NeurIPS},
  keywords = {u}
}

@inproceedings{soricut2003SentenceLevelDiscourse,
  title = {Sentence Level Discourse Parsing Using Syntactic and Lexical Information},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Soricut, R. and Marcu, D.},
  date = {2003},
  doi = {10.3115/1073445.1073475},
  abstract = {We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8\% over a state-ofthe- art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.},
  annotation = {457 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IK3J3CJA/Soricut, Marcu - 2003 - Sentence level discourse parsing using syntactic and lexical information(2).pdf}
}

@inproceedings{sorokin2017ContextAwareRepresentationsKnowledge,
  title = {Context-{{Aware Representations}} for {{Knowledge Base Relation Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Sorokin, Daniil and Gurevych, Iryna},
  date = {2017},
  pages = {1784--1789},
  publisher = {{Association for Computational Linguistics}},
  location = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1188},
  url = {http://aclweb.org/anthology/D17-1188},
  urldate = {2020-11-21},
  abstract = {We demonstrate that for sentence-level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation. Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence. We combine the context representations with an attention mechanism to make the final prediction.},
  annotation = {55 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YV9BL5I5/Sorokin and Gurevych - 2017 - Context-Aware Representations for Knowledge Base R.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{speer2017ConceptNetOpenMultilingual,
  title = {{{ConceptNet}} 5.5: {{An Open Multilingual Graph}} of {{General Knowledge}}},
  booktitle = {{{AAAI}}},
  author = {Speer, R. and Chin, J. and Havasi, C.},
  date = {2017},
  url = {http://arxiv.org/abs/1612.03975},
  abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
  annotation = {690 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1612.03975},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XFHYKJFR/Speer, Chin, Havasi - 2017 - ConceptNet 5.5 An Open Multilingual Graph of General Knowledge(2).pdf}
}

@report{sripada2003SumTimeMeteoParallelCorpus,
  title = {{{SumTime}}-{{Meteo}}: {{Parallel Corpus}} of {{Naturally Occurring Forecast Texts}} and {{Weather Data}}},
  author = {Sripada, S. G. and Reiter, E. and Hunter, J. and Yu, J.},
  date = {2003},
  pages = {1--13},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/54Q64VGQ/Sripada et al. - 2003 - SumTime-Meteo Parallel Corpus of Naturally Occurring Forecast Texts and Weather Data(2).pdf}
}

@inproceedings{srivastava2017JointConceptLearning,
  title = {Joint {{Concept Learning}} and {{Semantic Parsing}} from {{Natural Language Explanations}}},
  booktitle = {{{EMNLP}}},
  author = {Srivastava, S. and Labutov, S. and Mitchell, T. M.},
  date = {2017},
  pages = {1527--1536},
  url = {http://aclweb.org/anthology/D17-1161},
  abstract = {Natural language constitutes a predomi-nant medium for much of human learn-ing and pedagogy. We consider the prob-lem of concept learning from natural lan-guage explanations, and a small number of labeled examples of the concept. For example, in learning the concept of a phish-ing email, one might say 'this is a phishing email because it asks for your bank account number'. Solving this problem involves both learning to interpret open-ended nat-ural language statements, as well as learn-ing the concept itself. We present a joint model for (1) language interpretation (se-mantic parsing) and (2) concept learning (classification) that does not require label-ing statements with logical forms. Instead, the model prefers discriminative interpre-tations of statements in context of observ-able features of the data as a weak signal for parsing. On a dataset of email-related concepts, this approach yields across-the-board improvements in classification per-formance, with a 30\% relative improve-ment in F1 score over competitive classifi-cation methods in the low data regime.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5JJ2ZUB9/Srivastava, Labutov, Mitchell - 2017 - Joint Concept Learning and Semantic Parsing from Natural Language Explanations(2).pdf},
  keywords = {u}
}

@report{srivastava2019HighwayNetworks,
  title = {Highway {{Networks}}},
  author = {Srivastava, R. K. and Greff, K. and Schmidhuber, J.},
  date = {2019},
  url = {http://arxiv.org/abs/1505.00387},
  abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
  annotation = {938 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1505.00387},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4Y7252PS/Srivastava, Greff, Schmidhuber - 2019 - Highway Networks(2).pdf}
}

@inproceedings{stede2000HyperonymProblemRevisited,
  title = {The Hyperonym Problem Revisited: {{Conceptual}} and Lexical Hierarchies in Language},
  booktitle = {{{INLG}}},
  author = {Stede, M.},
  date = {2000},
  pages = {93--99},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C6CFUXRH/Stede - 2000 - The hyperonym problem revisited Conceptual and lexical hierarchies in language(2).pdf}
}

@inproceedings{steinhardt2017CertifiedDefensesData,
  title = {Certified {{Defenses}} for {{Data Poisoning Attacks}}},
  booktitle = {{{NeurIPS}}},
  author = {Steinhardt, J. and Koh, P. W. and Liang, P.},
  date = {2017},
  url = {http://arxiv.org/abs/1706.03691},
  abstract = {Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12\% to 23\% test error by adding only 3\% poisoned data.},
  annotation = {253 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1706.03691},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UEAPUL8K/Steinhardt, Koh, Liang - 2017 - Certified Defenses for Data Poisoning Attacks(2).pdf},
  number = {i}
}

@inproceedings{stengel-eskin2020UniversalDecompositionalSemantic,
  title = {Universal {{Decompositional Semantic Parsing}}},
  booktitle = {{{ACL}}},
  author = {Stengel-Eskin, Elias and White, Aaron Steven and Zhang, Sheng and Van Durme, Benjamin},
  date = {2020-05-02},
  url = {http://arxiv.org/abs/1910.10138},
  urldate = {2021-02-17},
  abstract = {We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores. We also introduce a strong pipeline model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction. By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups.},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1910.10138},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FUANW7XI/Stengel-Eskin et al. - 2020 - Universal Decompositional Semantic Parsing.pdf;/home/hiaoxui/.local/share/zotero_files/storage/736XBABT/1910.html}
}

@report{stengel-eskin2021JointUniversalSyntactic,
  title = {Joint {{Universal Syntactic}} and {{Semantic Parsing}}},
  author = {Stengel-Eskin, E. and Van Durme, B.},
  date = {2021},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZDJ7S64J/syntax_semantics_2021.pdf}
}

@inproceedings{stent2004TrainableSentencePlanning,
  title = {Trainable {{Sentence Planning}} for {{Complex Information Presentation}} in {{Spoken Dialog Systems}}},
  booktitle = {{{ACL}}},
  author = {Stent, A. and Prasad, R. and Walker, M.},
  date = {2004},
  pages = {79-es},
  doi = {10.3115/1218955.1218966},
  abstract = {A challenging problem for spoken dialog systems is the design of utterance generation modules that are fast, flexible and general, yet produce high quality output in particular domains. A promising approach is trainable generation, which uses general-purpose linguistic knowledge automatically adapted to the application domain. This paper presents a trainable sentence planner for the MATCH dialog system. We show that trainable sentence planning can produce output comparable to that of MATCH’s template-based generator even for quite complex information presentations.},
  annotation = {148 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PEA3NBDC/Stent, Prasad, Walker - 2004 - Trainable Sentence Planning for Complex Information Presentation in Spoken Dialog Systems(2).pdf},
  keywords = {u}
}

@inproceedings{stern2017MinimalSpanBasedNeural,
  title = {A {{Minimal Span}}-{{Based Neural Constituency Parser}}},
  booktitle = {{{ACL}}},
  author = {Stern, Mitchell and Andreas, Jacob and Klein, Dan},
  date = {2017},
  pages = {818--827},
  publisher = {{Association for Computational Linguistics}},
  location = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1076},
  url = {http://aclweb.org/anthology/P17-1076},
  urldate = {2020-08-07},
  abstract = {In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).},
  annotation = {108 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/I77CR6G9/Stern et al. - 2017 - A Minimal Span-Based Neural Constituency Parser.pdf},
  langid = {english}
}

@report{storks2019CommonsenseReasoningNatural,
  title = {Commonsense {{Reasoning}} for {{Natural Language Understanding}}: {{A Survey}} of {{Benchmarks}}, {{Resources}}, and {{Approaches}}},
  author = {Storks, S. and Gao, Q. and Chai, J. Y.},
  date = {2019},
  pages = {1--60},
  url = {http://arxiv.org/abs/1904.01172},
  abstract = {Commonsense knowledge and commonsense reasoning are some of the main bottlenecks in machine intelligence. In the NLP community, many benchmark datasets and tasks have been created to address commonsense reasoning for language understanding. These tasks are designed to assess machines' ability to acquire and learn commonsense knowledge in order to reason and understand natural language text. As these tasks become instrumental and a driving force for commonsense research, this paper aims to provide an overview of existing tasks and benchmarks, knowledge resources, and learning and inference approaches toward commonsense reasoning for natural language understanding. Through this, our goal is to support a better understanding of the state of the art, its limitations, and future challenges.},
  annotation = {30 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1904.01172},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VZ7W7BMG/Storks, Gao, Chai - 2019 - Commonsense Reasoning for Natural Language Understanding A Survey of Benchmarks, Resources, and Approaches(2).pdf},
  keywords = {u}
}

@inproceedings{strakova2019NeuralArchitecturesNested,
  title = {Neural {{Architectures}} for {{Nested NER}} through {{Linearization}}},
  booktitle = {{{ACL}}},
  author = {Straková, Jana and Straka, Milan and Hajič, Jan},
  date = {2019-08-19},
  archiveprefix = {arXiv},
  eprint = {1908.06926},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C4PQ933J/Straková et al. - 2019 - Neural Architectures for Nested NER through Linear.pdf},
  langid = {english}
}

@inproceedings{strapparava2005MakingComputersLaugh,
  title = {Making {{Computers Laugh}} : {{Investigations}} in {{Automatic Humor Recognition}}},
  booktitle = {{{EMNLP}}-{{HLT}}},
  author = {Strapparava, C.},
  date = {2005},
  pages = {531--538},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EJEX8E7M/Strapparava - 2005 - Making Computers Laugh Investigations in Automatic Humor Recognition(2).pdf},
  issue = {October}
}

@inproceedings{strubell2018LinguisticallyInformedSelfAttentionSemantic,
  title = {Linguistically-{{Informed Self}}-{{Attention}} for {{Semantic Role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Strubell, E. and Verga, P. and Andor, D. and Weiss, D. and McCallum, A.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1804.08199v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QRX4KNN3/Strubell et al. - 2018 - Linguistically-Informed Self-Attention for Semantic Role Labeling(2).pdf},
  keywords = {u}
}

@inproceedings{strubell2018SyntaxHelpsELMo,
  title = {Syntax {{Helps ELMo Understand Semantics}}: {{Is Syntax Still Relevant}} in a {{Deep Neural Architecture}} for {{SRL}}?},
  booktitle = {Workshop on the {{Relevance}} of {{Linguistic Structure}} in {{Neural Architectures}} for {{NLP}}},
  author = {Strubell, E. and Mccallum, A.},
  date = {2018},
  pages = {19--27},
  url = {https://nlp.stanford.edu/projects/},
  abstract = {Do unsupervised methods for learning rich, contextualized token representations obviate the need for explicit modeling of linguistic structure in neural network models for semantic role labeling (SRL)? We address this question by incorporating the massively successful ELMo em-beddings (Peters et al., 2018) into LISA (Strubell and McCallum, 2018), a strong, linguistically-informed neural network architecture for SRL. In experiments on the CoNLL-2005 shared task we find that though ELMo out-performs typical word embeddings, beginning to close the gap in F1 between LISA with predicted and gold syntactic parses, syntactically-informed models still out-perform syntax-free models when both use ELMo, especially on out-of-domain data. Our results suggest that linguistic structures are indeed still relevant in this golden age of deep learning for NLP.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/PUSNUJVV/Strubell, Mccallum - 2018 - Syntax Helps ELMo Understand Semantics Is Syntax Still Relevant in a Deep Neural Architecture for SRL(2).pdf},
  keywords = {u}
}

@article{styler2014TemporalAnnotationClinical,
  title = {Temporal {{Annotation}} in the {{Clinical Domain}}},
  author = {Styler, W. F. and Bethard, S. and Finan, S. and Palmer, M. and Pradhan, S. and de Groen, P. C and Erickson, B. and Miller, T. and Lin, C. and Savova, G. and Pustejovsky, J.},
  date = {2014},
  journaltitle = {TACL},
  volume = {2},
  pages = {143--154},
  doi = {10.1162/tacl_a_00172},
  abstract = {This article discusses the requirements of a formal specification for the annotation of temporal information in clinical narratives. We discuss the implementation and extension of ISO-TimeML for annotating a corpus of clinical notes, known as the THYME corpus. To reflect the information task and the heavily inference-based reasoning demands in the domain, a new annotation guideline has been developed, “the THYME Guidelines to ISO-TimeML (THYME-TimeML)”. To clarify what relations merit annotation, we distinguish between linguistically-derived and inferentially-derived temporal orderings in the text. We also apply a top performing TempEval 2013 system against this new resource to measure the difficulty of adapting systems to the clinical domain. The corpus is available to the community and has been proposed for use in a SemEval 2015 task.},
  annotation = {162 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UE9RFU6K/Styler et al. - 2014 - Temporal Annotation in the Clinical Domain(2).pdf},
  keywords = {u},
  number = {1},
  options = {useprefix=true}
}

@article{subhashini2011SurveyOntologyConstruction,
  title = {A {{Survey}} on {{Ontology Construction Methodologies}}},
  author = {Subhashini, R. and Akilandeswari, J.},
  date = {2011},
  journaltitle = {International Journal of Enterprise Computing and Business Systems},
  volume = {1},
  url = {http://www.ijecbs.com},
  abstract = {Ontology is defined as partial specification of conceptual vocabulary used for formulating knowledge-level theories about a domain of discourse. Ontology is applied in domains like natural disaster management system, medicine, military intelligence, cooking, enterprise, jobs, agriculture, wikipedia, automobiles and so on. This paper presents a review on various ontology construction methodologies for different domains. This paper also presents the merits and drawbacks in those methodologies.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZL3VEXEX/Subhashini, Akilandeswari - 2011 - A Survey on Ontology Construction Methodologies(2).pdf},
  keywords = {u},
  number = {1}
}

@inproceedings{subramanian2019DeepOrdinalRegression,
  title = {Deep {{Ordinal Regression}} for {{Pledge Specificity Prediction}}},
  booktitle = {{{EMNLP}}},
  author = {Subramanian, S. and Cohn, T. and Baldwin, T.},
  date = {2019},
  pages = {1729--1740},
  doi = {10.18653/v1/d19-1182},
  abstract = {Many pledges are made in the course of an election campaign, forming important corpora for political analysis of campaign strategy and governmental accountability. At present, there are no publicly available annotated datasets of pledges, and most political analyses rely on manual analysis. In this paper we collate a novel dataset of manifestos from eleven Australian federal election cycles, with over 12,000 sentences annotated with specificity (e.g., rhetorical vs.\textbackslash{} detailed pledge) on a fine-grained scale. We propose deep ordinal regression approaches for specificity prediction, under both supervised and semi-supervised settings, and provide empirical results demonstrating the effectiveness of the proposed techniques over several baseline approaches. We analyze the utility of pledge specificity modeling across a spectrum of policy issues in performing ideology prediction, and further provide qualitative analysis in terms of capturing party-specific issue salience across election cycles.},
  annotation = {0 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.00187},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QKA9Q9Z7/Subramanian, Cohn, Baldwin - 2019 - Deep Ordinal Regression for Pledge Specificity Prediction(2).pdf},
  keywords = {u}
}

@inproceedings{suhr2018LearningMapContextDependent,
  title = {Learning to {{Map Context}}-{{Dependent Sentences}} to {{Executable Formal Queries}}},
  booktitle = {{{NAACL}}},
  author = {Suhr, A. and Iyer, S. and Artzi, Y.},
  date = {2018},
  pages = {2238--2249},
  doi = {10.18653/v1/n18-1203},
  abstract = {We propose a context-dependent model to map utterances within an interaction to executable formal queries. To incorporate interaction his-tory, the model maintains an interaction-level encoder that updates after each turn, and can copy sub-sequences of previously predicted queries during generation. Our approach com-bines implicit and explicit modeling of refer-ences between utterances. We evaluate our model on the ATIS flight planning interac-tions, and demonstrate the benefits of model-ing context and explicit references.},
  annotation = {60 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/T6KEMBCT/Suhr, Iyer, Artzi - 2018 - Learning to Map Context-Dependent Sentences to Executable Formal Queries(2).pdf},
  keywords = {u}
}

@inproceedings{sukhbaatar2019AdaptiveAttentionSpan,
  title = {Adaptive {{Attention Span}} in {{Transformers}}},
  booktitle = {{{ACL}}},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  date = {2019-08-08},
  url = {http://arxiv.org/abs/1905.07799},
  urldate = {2021-03-28},
  abstract = {We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.},
  annotation = {95 citations (Semantic Scholar/arXiv) [2021-03-28]},
  archiveprefix = {arXiv},
  eprint = {1905.07799},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KH9IHCBY/Sukhbaatar et al. - 2019 - Adaptive Attention Span in Transformers.pdf;/home/hiaoxui/.local/share/zotero_files/storage/BCX78SAI/1905.html}
}

@report{sukhbaatar2019AugmentingSelfattentionPersistent,
  title = {Augmenting {{Self}}-Attention with {{Persistent Memory}}},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
  date = {2019-07-02},
  url = {http://arxiv.org/abs/1907.01470},
  urldate = {2021-03-10},
  abstract = {Transformer networks have lead to important progress in language modeling and machine translation. These models include two consecutive modules, a feed-forward layer and a self-attention layer. The latter allows the network to capture long term dependencies and are often regarded as the key ingredient in the success of Transformers. Building upon this intuition, we propose a new model that solely consists of attention layers. More precisely, we augment the self-attention layers with persistent memory vectors that play a similar role as the feed-forward layer. Thanks to these vectors, we can remove the feed-forward layer without degrading the performance of a transformer. Our evaluation shows the benefits brought by our model on standard character and word level language modeling benchmarks.},
  annotation = {28 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1907.01470},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7XP7VBMG/Sukhbaatar et al. - 2019 - Augmenting Self-attention with Persistent Memory.pdf;/home/hiaoxui/.local/share/zotero_files/storage/Q965MJYH/1907.html}
}

@article{sukthanker2020AnaphoraCoreferenceResolution,
  title = {Anaphora and Coreference Resolution: {{A}} Review},
  shorttitle = {Anaphora and Coreference Resolution},
  author = {Sukthanker, Rhea and Poria, Soujanya and Cambria, Erik and Thirunavukarasu, Ramkumar},
  date = {2020-07},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {59},
  pages = {139--162},
  issn = {15662535},
  doi = {10.1016/j.inffus.2020.01.010},
  annotation = {37 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/W4RGY4LW/Sukthanker et al. - 2020 - Anaphora and coreference resolution A review.pdf},
  langid = {english}
}

@report{sun2017NeuralNetworkPushdown,
  title = {The {{Neural Network Pushdown Automaton}}: {{Model}}, {{Stack}} and {{Learning Simulations}}},
  shorttitle = {The {{Neural Network Pushdown Automaton}}},
  author = {Sun, G. Z. and Giles, C. L. and Chen, H. H. and Lee, Y. C.},
  date = {2017-11-15},
  url = {http://arxiv.org/abs/1711.05738},
  urldate = {2020-10-09},
  abstract = {In order for neural networks to learn complex languages or grammars, they must have sufficient computational power or resources to recognize or generate such languages. Though many approaches have been discussed, one ob- vious approach to enhancing the processing power of a recurrent neural network is to couple it with an external stack memory - in effect creating a neural network pushdown automata (NNPDA). This paper discusses in detail this NNPDA - its construction, how it can be trained and how useful symbolic information can be extracted from the trained network. In order to couple the external stack to the neural network, an optimization method is developed which uses an error function that connects the learning of the state automaton of the neural network to the learning of the operation of the external stack. To minimize the error function using gradient descent learning, an analog stack is designed such that the action and storage of information in the stack are continuous. One interpretation of a continuous stack is the probabilistic storage of and action on data. After training on sample strings of an unknown source grammar, a quantization procedure extracts from the analog stack and neural network a discrete pushdown automata (PDA). Simulations show that in learning deterministic context-free grammars - the balanced parenthesis language, 1*n0*n, and the deterministic Palindrome - the extracted PDA is correct in the sense that it can correctly recognize unseen strings of arbitrary length. In addition, the extracted PDAs can be shown to be identical or equivalent to the PDAs of the source grammars which were used to generate the training strings.},
  annotation = {25 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1711.05738},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3WNBCX7M/Sun et al. - 2017 - The Neural Network Pushdown Automaton Model, Stac.pdf;/home/hiaoxui/.local/share/zotero_files/storage/H5JCMBNV/1711.html},
  keywords = {u}
}

@report{sun2020GuessingWhatPlausible,
  title = {Guessing {{What}}'s {{Plausible But Remembering What}}'s {{True}}: {{Accurate Neural Reasoning}} for {{Question}}-{{Answering}}},
  author = {Sun, H. and Arnold, A. O. and Bedrax-Weiss, T. and Pereira, F. and Cohen, W. W.},
  date = {2020},
  url = {http://arxiv.org/abs/2004.03658},
  abstract = {Neural approaches to natural language processing (NLP) often fail at the logical reasoning needed for deeper language understanding. In particular, neural approaches to reasoning that rely on embedded \textbackslash emph\{generalizations\} of a knowledge base (KB) implicitly model which facts that are \textbackslash emph\{plausible\}, but may not model which facts are \textbackslash emph\{true\}, according to the KB. While generalizing the facts in a KB is useful for KB completion, the inability to distinguish between plausible inferences and logically entailed conclusions can be problematic in settings like as KB question answering (KBQA). We propose here a novel KB embedding scheme that supports generalization, but also allows accurate logical reasoning with a KB. Our approach introduces two new mechanisms for KB reasoning: neural retrieval over a set of embedded triples, and "memorization" of highly specific information with a compact sketch structure. Experimentally, this leads to substantial improvements over the state-of-the-art on two KBQA benchmarks.},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2004.03658},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DEWKWQ6R/Sun et al. - 2020 - Guessing What's Plausible But Remembering What's True Accurate Neural Reasoning for Question-Answering(2).pdf},
  keywords = {u}
}

@inproceedings{surdeanu2012MultiinstanceMultilabelLearning,
  title = {Multi-Instance {{Multi}}-Label {{Learning}} for {{Relation Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Surdeanu, M. and Tibshirani, J. and Nallapati, R. and Manning, C. D.},
  date = {2012},
  pages = {455--465},
  doi = {10.3115/v1/D14-1200},
  url = {http://dl.acm.org/citation.cfm?id=2390948.2391003},
  abstract = {Distant supervision for relation extraction (RE) -- gathering training data by aligning a database of facts with text -- is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains.},
  annotation = {141 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {91150},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VBTH8AU2/Surdeanu et al. - 2012 - Multi-instance Multi-label Learning for Relation Extraction(2).pdf},
  isbn = {978-1-937284-43-5},
  issue = {July}
}

@report{surdeanu2014OverviewEnglishSlot,
  title = {Overview of the {{English Slot Filling Track}} at the {{TAC2014 Knowledge Base Population Evaluation}}},
  author = {Surdeanu, Mihai and Ji, Heng},
  date = {2014},
  pages = {15},
  abstract = {We overview the English Slot Filling (SF) track of the TAC2014 Knowledge Base Population (KBP) evaluation. The goal of this KBP track is to promote research in the extraction of binary relations between named and numeric entities from free text. The main changes this year include: (a) the inclusion of ambiguous queries, i.e., queries that point to multiple real-life entities with the same name; (b) accepting outputs created through inference; and (c) a simplification of the task and of the input format by removing references to the knowledge base for the entities included in queries. The SF track attracted 31 registered teams, out of which 18 teams submitted at least one run. The highest score this year was 36.72 F1, with a median of 19.80 F1.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P6VNZZYN/Surdeanu and Ji - Overview of the English Slot Filling Track at the .pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{sutskever2014SequenceSequenceLearning,
  title = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {{{NeurIPS}}},
  author = {Sutskever, I. and Vinyals, O. and Le, Q. V.},
  date = {2014},
  pages = {3104--3112},
  issn = {09205691},
  doi = {10.1007/s10107-014-0839-0},
  url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  annotation = {397 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2079951},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2HU2QGGD/Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks(2).pdf},
  isbn = {1409.3215}
}

@report{swayamdipta2017FrameSemanticParsingSoftmaxMargin,
  title = {Frame-{{Semantic Parsing}} with {{Softmax}}-{{Margin Segmental RNNs}} and a {{Syntactic Scaffold}}},
  author = {Swayamdipta, S. and Thomson, S. and Dyer, C. and Smith, N. A.},
  date = {2017},
  abstract = {We present a new, efficient frame-semantic parser that labels semantic arguments to FrameNet predicates. Built using an extension to the segmental RNN that emphasizes recall, our basic system achieves competitive performance without any calls to a syntactic parser. We then introduce a method that uses phrase-syntactic annotations from the Penn Treebank during training only, through a multitask objective; no parsing is required at training or test time. This "syntactic scaffold" offers a cheaper alternative to traditional syntactic pipelining, and achieves state-of-the-art performance.},
  archiveprefix = {arXiv},
  eprint = {1706.09528},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V8IMWSFH/Swayamdipta et al. - 2017 - Frame-Semantic Parsing with Softmax-Margin Segmental RNNs and a Syntactic Scaffold(2).pdf}
}

@inproceedings{swayamdipta2018SyntacticScaffoldsSemantic,
  title = {Syntactic {{Scaffolds}} for {{Semantic Structures}}},
  booktitle = {{{EMNLP}}},
  author = {Swayamdipta, S. and Thomson, S. and Lee, K. and Zettlemoyer, L. S. and Dyer, C. and Smith, N. A.},
  date = {2018},
  pages = {3772--3782},
  doi = {10.18653/v1/d18-1412},
  abstract = {We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.},
  annotation = {57 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1808.10485},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7B43XIAV/Swayamdipta et al. - 2018 - Syntactic Scaffolds for Semantic Structures(2).pdf}
}

@report{szegedy2013IntriguingPropertiesNeural,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, C. and Zaremba, W. and Sutskever, I. and Bruna, J. and Erhan, D. and Goodfellow, I. and Fergus, R.},
  date = {2013},
  url = {http://arxiv.org/abs/1312.6199},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  annotation = {6175 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1312.6199},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LC32KVQN/Szegedy et al. - 2013 - Intriguing properties of neural networks(2).pdf}
}

@inproceedings{szegedy2016RethinkingInceptionArchitecture,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  booktitle = {{{CVPR}}},
  author = {Szegedy, C. and Vanhoucke, V. and Ioffe, S. and Shlens, J. and Wojna, Z.},
  date = {2016},
  pages = {2818--2826},
  issn = {10636919},
  doi = {10.1109/CVPR.2016.308},
  abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2\% top-1 and 5:6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5\% top-5 error and 17:3\% top-1 error on the validation set and 3:6\% top-5 error on the official test set.},
  annotation = {9993 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1512.00567},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/G7ADUFIG/Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer Vision(2).pdf},
  isbn = {978-1-4673-8850-4},
  keywords = {u}
}

@inproceedings{taghipour2015SemiSupervisedWordSense,
  title = {Semi-{{Supervised Word Sense Disambiguation Using Word Embeddings}} in {{General}} and {{Specific Domains}}},
  booktitle = {{{HLT}}},
  author = {Taghipour, K.},
  date = {2015},
  pages = {314--323},
  doi = {10.3115/v1/N15-1035},
  abstract = {One of the weaknesses of current supervised word sense disambiguation (WSD) systems is that they only treat a word as a discrete en-tity. However, a continuous-space represen-tation of words (word embeddings) can pro-vide valuable information and thus improve generalization accuracy. Since word embed-dings are typically obtained from unlabeled data using unsupervised methods, this method can be seen as a semi-supervised word sense disambiguation approach. This paper investi-gates two ways of incorporating word embed-dings in a word sense disambiguation setting and evaluates these two methods on some Sen-sEval/SemEval lexical sample and all-words tasks and also a domain-specific lexical sam-ple task. The obtained results show that such representations consistently improve the ac-curacy of the selected supervised WSD sys-tem. Moreover, our experiments on a domain-specific dataset show that our supervised base-line system beats the best knowledge-based systems by a large margin.},
  annotation = {96 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HCT3L229/Taghipour - 2015 - Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains(2).pdf}
}

@inproceedings{tai2015ImprovedSemanticRepresentations,
  title = {Improved {{Semantic Representations From Tree}}-{{Structured Long Short}}-{{Term Memory Networks}}},
  booktitle = {{{ACL}}},
  author = {Tai, K. S. and Socher, R. and Manning, C. D.},
  date = {2015},
  url = {http://arxiv.org/abs/1503.00075},
  abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
  annotation = {2038 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1503.00075},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8RFYLWJ2/Tai, Socher, Manning - 2015 - Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks(2).pdf},
  keywords = {u}
}

@inproceedings{taketa2004TextRankBriningOrder,
  title = {{{TextRank}}: {{Brining Order}} into {{Texts}}},
  booktitle = {{{EMNLP}}},
  author = {Taketa, F.},
  date = {2004},
  issn = {03050491},
  doi = {10.1016/0305-0491(73)90144-2},
  abstract = {1. 1. The hemoglobins found in various members of the Felidae have been separated and compared with respect to structure in relation to 2,3-diphosphoglycerate (2,3-DPG) sensitivity. 2. 2. Multiple hemoglobin components are found in the blood of all Felidae, and they are characterized by the presence of one of two types of β-chains that are common to the members of the family. 3. 3. The two types, A-β and B-β, are distinguished from one another by differences in positions of the βT-1 and βT-XIV peptides in fingerprints of tryptic digests. 4. 4. Components that contain the A-β type are invariably 2,3-DPG-sensitive whereas those that contain the B-β type are insensitive. 5. 5. The A-β and B-β chains are apparently products of nonallelic genes but are found in widely variable proportions in mixtures of hemoglobins in the blood of different members of the Felidae. © 1973.},
  annotation = {6 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ND8VCP8T/Taketa - 2004 - TextRank Brining Order into Texts(2).pdf}
}

@report{tallec2017UnbiasingTruncatedBackpropagation,
  title = {Unbiasing {{Truncated Backpropagation Through Time}}},
  author = {Tallec, C. and Ollivier, Y.},
  date = {2017},
  pages = {1--13},
  url = {http://arxiv.org/abs/1705.08209},
  abstract = {Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step. However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce Anticipated Reweighted Truncated Backpropagation (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling, ARTBP slightly outperforms truncated BPTT.},
  annotation = {32 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1705.08209},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NTFB7NBN/Tallec, Ollivier - 2017 - Unbiasing Truncated Backpropagation Through Time(2).pdf},
  keywords = {u}
}

@inproceedings{talmor2018WebKnowledgebaseAnswering,
  title = {The {{Web}} as a {{Knowledge}}-Base for {{Answering Complex Questions}}},
  booktitle = {{{NAACL}}},
  author = {Talmor, A. and Berant, J.},
  date = {2018},
  pages = {1--10},
  archiveprefix = {arXiv},
  eprint = {1803.06643},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CHDQW99W/Talmor, Berant - 2018 - The Web as a Knowledge-base for Answering Complex Questions(2).pdf}
}

@inproceedings{talmor2019CommonsenseQAQuestionAnswering,
  title = {{{CommonsenseQA}}: {{A Question Answering Challenge Targeting Commonsense Knowledge}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Talmor, A. and Herzig, J. and Lourie, N. and Berant, J.},
  date = {2019},
  url = {http://arxiv.org/abs/1811.00937},
  abstract = {When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56\% accuracy, well below human performance, which is 89\%.},
  annotation = {174 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1811.00937},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FP5V3T9K/Talmor et al. - 2019 - CommonsenseQA A Question Answering Challenge Targeting Commonsense Knowledge(2).pdf}
}

@report{talmor2019OLMpicsWhatLanguage,
  title = {{{oLMpics}} -- {{On}} What {{Language Model Pre}}-Training {{Captures}}},
  author = {Talmor, A. and Elazar, Y. and Goldberg, Y. and Berant, J.},
  date = {2019},
  url = {http://arxiv.org/abs/1912.13283},
  abstract = {Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.},
  annotation = {66 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1912.13283},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UU47V2DQ/Talmor et al. - 2019 - oLMpics -- On what Language Model Pre-training Captures(2).pdf}
}

@inproceedings{tamborrino2020PretrainingAlmostAll,
  title = {Pre-Training {{Is}} ({{Almost}}) {{All You Need}}: {{An Application}} to {{Commonsense Reasoning}}},
  booktitle = {{{ACL}}},
  author = {Tamborrino, A. and Pellicano, N. and Pannier, B. and Voitot, P. and Naudin, L.},
  date = {2020},
  url = {http://arxiv.org/abs/2004.14074},
  abstract = {Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets. By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80\% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g \$\textbackslash times 10\$ standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.},
  annotation = {11 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2004.14074},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HEK2DRAQ/Tamborrino et al. - 2020 - Pre-training Is (Almost) All You Need An Application to Commonsense Reasoning(2).pdf}
}

@inproceedings{tan2018DeepSemanticRole,
  title = {Deep {{Semantic Role Labeling}} with {{Self}}-{{Attention}}},
  booktitle = {{{AAAI}}},
  author = {Tan, Zhixing and Wang, Mingxuan and Xie, Jun and Chen, Yidong and Shi, Xiaodong},
  date = {2018},
  pages = {8},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YXEU5HXB/Tan et al. - Deep Semantic Role Labeling with Self-Attention.pdf},
  langid = {english}
}

@inproceedings{tang2014LearningSentimentSpecificWord,
  title = {Learning {{Sentiment}}-{{Specific Word Embedding}} for {{Twitter Sentiment Classification}}},
  booktitle = {{{ACL}}},
  author = {Tang, D. and Wei, F. and Yang, N. and Zhou, M. and Liu, T. and Qin, B.},
  date = {2014},
  volume = {2},
  pages = {1--54},
  issn = {03029743},
  doi = {10.1561/2200000006},
  annotation = {6482 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {18487783},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XRR5DNS7/Tang et al. - 2014 - Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification(2).pdf},
  isbn = {978-1-937284-72-5},
  keywords = {u}
}

@inproceedings{tang2015DocumentModelingGated,
  title = {Document {{Modeling}} with {{Gated Recurrent Neural Network}} for {{Sentiment Classification}}},
  booktitle = {{{EMNLP}}},
  author = {Tang, D. and Qin, B. and Liu, T.},
  date = {2015},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QXNZ9VWJ/Tang, Qin, Liu - 2015 - Document Modeling with Gated Recurrent Neural Network for Sentiment Classification(2).pdf}
}

@inproceedings{tang2015LINELargescaleInformation,
  title = {{{LINE}}: {{Large}}-Scale {{Information Network Embedding}}},
  booktitle = {{{WWW}}},
  author = {Tang, J. and Qu, M. and Wang, M. and Zhang, M. and Yan, J. and Mei, Q.},
  date = {2015},
  doi = {10.1145/2736277.2741093},
  url = {http://arxiv.org/abs/1503.03578%0Ahttp://dx.doi.org/10.1145/2736277.2741093},
  abstract = {This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the "LINE," which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.},
  annotation = {2615 citations (Semantic Scholar/DOI) [2021-03-26] 2615 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1503.03578},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XFSEFNUG/Tang et al. - 2015 - LINE Large-scale Information Network Embedding(2).pdf},
  isbn = {978-1-4503-3469-3}
}

@report{tay2020EfficientTransformersSurvey,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  shorttitle = {Efficient {{Transformers}}},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  date = {2020-09-16},
  url = {http://arxiv.org/abs/2009.06732},
  urldate = {2021-02-09},
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  annotation = {35 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2009.06732},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6DYVIM72/Tay et al. - 2020 - Efficient Transformers A Survey.pdf;/home/hiaoxui/.local/share/zotero_files/storage/YGGZZD62/2009.html}
}

@incollection{taylor2003PennTreebankOverview,
  title = {The {{Penn Treebank}}: {{An Overview}}},
  shorttitle = {The {{Penn Treebank}}},
  booktitle = {Treebanks},
  author = {Taylor, Ann and Marcus, Mitchell and Santorini, Beatrice},
  editor = {Abeillé, Anne},
  date = {2003},
  volume = {20},
  pages = {5--22},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-010-0201-1_1},
  url = {http://link.springer.com/10.1007/978-94-010-0201-1_1},
  urldate = {2021-02-16},
  editorb = {Ide, Nancy and Véronis, Jean},
  editorbtype = {redactor},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ILIMHIJE/Taylor et al. - 2003 - The Penn Treebank An Overview.pdf},
  isbn = {978-1-4020-1335-5 978-94-010-0201-1},
  langid = {english},
  series = {Text, {{Speech}} and {{Language Technology}}}
}

@inproceedings{teh2006HierarchicalBayesianLanguage,
  title = {A Hierarchical {{Bayesian}} Language Model Based on {{Pitman}}-{{Yor}} Processes},
  booktitle = {{{ACL}}},
  author = {Teh, Y. W.},
  date = {2006},
  pages = {985--992},
  doi = {10.3115/1220175.1220299},
  abstract = {We propose a new hierarchical Bayesian n-gram model of natural languages. Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages. We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models. Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney. © 2006 Association for Computational Linguistics.},
  annotation = {524 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KKLQSXNL/Teh - 2006 - A hierarchical Bayesian language model based on Pitman-Yor processes(2).pdf},
  isbn = {1-932432-65-5}
}

@inproceedings{teichert2017SemanticProtoRoleLabeling,
  title = {Semantic {{Proto}}-{{Role Labeling}}},
  booktitle = {{{AAAI}}},
  author = {Teichert, A. and Poliak, A. and Van Durme, B. and Gormley, M. R.},
  date = {2017},
  pages = {4459--4465},
  abstract = {The semantic function tags of Bonial, Stowe, and Palmer (2013) and the ordinal, multi-property annotations of Reisinger et al. (2015) draw inspiration from Dowty's seman-tic proto-role theory. We approach proto-role labeling as a multi-label classification problem and establish strong results for the task by adapting a successful model of traditional se-mantic role labeling. We achieve a proto-role micro-averaged F1 of 81.7 using gold syntax and explore joint and condi-tional models of proto-roles and categorical roles. In compar-ing the effect of Bonial, Stowe, and Palmer's tags to Prop-Bank ArgN-style role labels, we are surprised that neither an-notations greatly improve proto-role prediction; however, we observe that ArgN models benefit much from observed syntax and from observed or modeled proto-roles while our models of the semantic function tags do not.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MWRXZW46/Teichert et al. - 2017 - Semantic Proto-Role Labeling(2).pdf},
  keywords = {u}
}

@inproceedings{tellex2011UnderstandingNaturalLanguage,
  title = {Understanding {{Natural Language Commands}} for {{Robotic Navigation}} and {{Mobile Manipulation}}.},
  booktitle = {{{AAAI}}},
  author = {Tellex, S. and Kollar, T. and Dickerson, S.},
  date = {2011},
  pages = {1507--1514},
  url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewFile/3623/4113},
  abstract = {This paper describes a new model for understanding natural language commands given to autonomous systems that perform navigation and mobile manipulation in semi-structured environments. Previous approaches have used models with fixed structure to infer the likelihood of a sequence of actions given the environment and the command. In contrast, our framework, called Generalized Grounding Graphs (G3), dynamically instantiates a probabilistic graphical model for a particular natural language command according to the command’s hierarchical and compositional semantic structure. Our system performs inference in the model to successfully find and execute plans corresponding to natural language commands such as “Put the tire pallet on the truck.” The model is trained using a corpus of commands collected using crowdsourcing. We pair each command with robot actions and use the corpus to learn the parameters of the model. We evaluate the robot’s performance by inferring plans from natural language commands, executing each plan in a realistic robot simulator, and asking users to evaluate the system’s performance. We demonstrate that our system can successfully follow many natural language commands from the corpus.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NQSQ3RIX/Tellex, Kollar, Dickerson - 2011 - Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation(2).pdf},
  keywords = {u}
}

@inproceedings{tenney2019WhatYouLearn,
  title = {What {{Do You Learn}} from {{Context}}? {{Probing}} for {{Sentence Structure}} in {{Contextualized Word Representations}}},
  booktitle = {{{ICLR}}},
  author = {Tenney, I. and Xia, P. and Chen, B. and Wang, A. and Poliak, A. and McCoy, R. T. and Kim, N. and Van Durme, B. and Bowman, S. R. and Das, D. and Pavlick, E.},
  date = {2019},
  pages = {1--17},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9E66WV76/Tenney et al. - 2019 - What Do You Learn from Context Probing for Sentence Structure in Contextualized Word Representations(2).pdf}
}

@article{theune2001DataSpeechGeneral,
  title = {From {{Data}} to {{Speech}}: {{A General Approach}}},
  author = {Theune, M. and Klabbers, E. and Odijk, J. and De Pijper, J. R. and Krahmer, E.},
  date = {2001},
  journaltitle = {Natural Language Engineering},
  volume = {7},
  pages = {47--86},
  issn = {1351-3249, 1351-3249},
  url = {http://findit.lib.cuhk.edu.hk/852cuhk/?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Allbashell&atitle=From+Data+to+Speech%3A+A+General+Approach&title=Natural+Language+Engineering&issn=13513249&date=2001-03-},
  abstract = {We present a data-to-speech system called D2S, which can be used for the creation of data-to-speech systems in different languages \& domains. The most important characteristic of a data-to-speech system is that it combines language \& speech generation: language generation is used to produce a natural language text expressing the system's input data, \& speech generation is used to make this text audible. In D2S, this combination is exploited by using linguistic information available in the language generation module for the computation of prosody. This allows us to achieve a better prosodic output quality than can be achieved in a plain text-to-speech system. For language generation in D2S, the use of syntactically enriched templates is guided by knowledge of the discourse context, while for speech generation pre-recorded phrases are combined in a prosodically sophisticated manner. This combination of techniques makes it possible to create linguistically sound but efficient systems with a high quality language \& speech output. 1 Table, 16 Figures, 69 References. Adapted from the source document},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LG2BIEZR/Theune et al. - 2001 - From Data to Speech A General Approach(2).pdf},
  isbn = {1469-8110},
  number = {1}
}

@inproceedings{toshniwal2020LearningIgnoreLong,
  title = {Learning to {{Ignore}}: {{Long Document Coreference}} with {{Bounded Memory Neural Networks}}},
  shorttitle = {Learning to {{Ignore}}},
  booktitle = {{{EMNLP}}},
  author = {Toshniwal, Shubham and Wiseman, Sam and Ettinger, Allyson and Livescu, Karen and Gimpel, Kevin},
  date = {2020-11-16},
  url = {http://arxiv.org/abs/2010.02807},
  urldate = {2021-03-28},
  abstract = {Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy.},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-28]},
  archiveprefix = {arXiv},
  eprint = {2010.02807},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LMBP7IC9/Toshniwal et al. - 2020 - Learning to Ignore Long Document Coreference with.pdf;/home/hiaoxui/.local/share/zotero_files/storage/SU2S269X/2010.html}
}

@inproceedings{toshniwal2020PeTraSparselySupervised,
  title = {{{PeTra}}: {{A Sparsely Supervised Memory Model}} for {{People Tracking}}},
  shorttitle = {{{PeTra}}},
  booktitle = {{{ACL}}},
  author = {Toshniwal, Shubham and Ettinger, Allyson and Gimpel, Kevin and Livescu, Karen},
  date = {2020-05-06},
  url = {http://arxiv.org/abs/2005.02990},
  urldate = {2021-03-28},
  abstract = {We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots. PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture. We empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance. To measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach. PeTra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation.},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-28]},
  archiveprefix = {arXiv},
  eprint = {2005.02990},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IQ95KKPC/Toshniwal et al. - 2020 - PeTra A Sparsely Supervised Memory Model for Peop.pdf;/home/hiaoxui/.local/share/zotero_files/storage/QBIDKIB5/2005.html}
}

@inproceedings{tran2016UnsupervisedNeuralHidden,
  title = {Unsupervised {{Neural Hidden Markov Models}}},
  booktitle = {{{EMNLP}}},
  author = {Tran, K. and Bisk, Y. and Vaswani, A. and Marcu, D. and Knight, K.},
  date = {2016},
  pages = {63--71},
  doi = {10.18653/v1/W16-5907},
  url = {http://arxiv.org/abs/1609.09007},
  abstract = {In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.},
  annotation = {34 citations (Semantic Scholar/DOI) [2021-03-26] 34 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1609.09007},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TTRMJYYD/Tran et al. - 2016 - Unsupervised Neural Hidden Markov Models(2).pdf}
}

@inproceedings{tran2017NamedEntityRecognition,
  title = {Named {{Entity Recognition}} with Stack Residual {{LSTM}} and Trainable Bias Decoding},
  booktitle = {{{IJCNLP}}},
  author = {Tran, Q. and MacKinlay, A. and Yepes, A.},
  date = {2017-07-11},
  url = {http://arxiv.org/abs/1706.07598},
  urldate = {2020-07-23},
  abstract = {Recurrent Neural Network models are the state-of-the-art for Named Entity Recognition (NER). We present two innovations to improve the performance of these models. The first innovation is the introduction of residual connections between the Stacked Recurrent Neural Network model to address the degradation problem of deep neural networks. The second innovation is a bias decoding mechanism that allows the trained system to adapt to non-differentiable and externally computed objectives, such as the entity-based F-measure. Our work improves the state-of-the-art results for both Spanish and English languages on the standard train/development/test split of the CoNLL 2003 Shared Task NER dataset.},
  annotation = {35 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1706.07598},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XBBJI4Y5/Tran et al. - 2017 - Named Entity Recognition with stack residual LSTM .pdf},
  keywords = {u}
}

@report{trask2018NeuralArithmeticLogic,
  title = {Neural {{Arithmetic Logic Units}}},
  author = {Trask, A. and Hill, F. and Reed, S. and Rae, J. and Dyer, C. and Blunsom, P.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1808.00508v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P3HIEFWY/Trask et al. - 2018 - Neural Arithmetic Logic Units(2).pdf}
}

@inproceedings{trivedi2017KnowevolveDeepTemporal,
  title = {Know-Evolve: {{Deep}} Temporal Reasoning for Dynamic Knowledge Graphs},
  booktitle = {{{ICML}}},
  author = {Trivedi, R. and Dai, H. and Wang, Y. and Song, L.},
  date = {2017},
  volume = {7},
  pages = {5313--5327},
  abstract = {The availability of large scale event data with time stamps has given rise to dynamically evolving knowledge graphs that contain temporal information for each edge. Reasoning over time in such dynamic knowledge graphs is not yet well understood. To this end, we present Know-Evolve, a novel deep evolutionary knowledge network that learns non-linearly evolving entity representations over time. The occurrence of a fact (edge) is modeled as a multivariate point process whose intensity function is modulated by the score for that fact computed based on the learned entity embeddings. We demonstrate significantly improved performance over various relational learning approaches on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to prior reasoning approaches in multi-relational setting.},
  archiveprefix = {arXiv},
  eprint = {1705.05742v3},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HHDEKV5W/Trivedi et al. - 2017 - Know-evolve Deep temporal reasoning for dynamic knowledge graphs(2).pdf},
  isbn = {978-1-5108-5514-4}
}

@inproceedings{trivedi2019DyREPLearningRepresentations,
  title = {{{DyREP}}: {{Learning Representations}} over {{Dynamic Graphs}}},
  booktitle = {{{ICLR}}},
  author = {Trivedi, R. and Farajtabar, M. and Biswal, P. and Zha, H.},
  date = {2019},
  abstract = {Representations of sets are challenging to learn because operations on sets should be permutation-invariant. To this end, we propose a Permutation-Optimisation module that learns how to permute a set end-to-end. The permuted set can be further processed to learn a permutation-invariant representation of that set, avoiding a bottleneck in traditional set models. We demonstrate our model's ability to learn permutations and set representations with either explicit or implicit supervision on four datasets, on which we achieve state-of-the-art results: number sorting, image mosaics, classification from image mosaics, and visual question answering.},
  archiveprefix = {arXiv},
  eprint = {1812.03928},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U8QYRYIB/Trivedi et al. - 2019 - DyREP Learning Representations over Dynamic Graphs(2).pdf}
}

@inproceedings{tromble2008LatticeMinimumBayesRisk,
  title = {Lattice {{Minimum Bayes}}-{{Risk Decoding}} for {{Statistical Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Tromble, R. W. and Kumar, S. and Och, F. and Macherey, W.},
  date = {2008},
  volume = {17},
  pages = {64--69},
  doi = {10.3115/1613715.1613792},
  abstract = {I address the commentators' calls for clarification of theoretical terms, discussion of similarities to other proposals, and extension of the ideas. In doing so, I keep the focus on the purpose of memory: enabling the organism to make sense of its environment so that it can take action appropriate to constraints resulting from the physical, personal, social, and cultural situations.},
  annotation = {135 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FUG9NKWM/Tromble et al. - 2008 - Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation(2).pdf},
  isbn = {0-00-140110-6},
  keywords = {u},
  number = {1}
}

@inproceedings{tsatsaronis2007WordSenseDisambiguation,
  title = {Word Sense Disambiguation with Spreading Activation Networks Generated from Thesauri},
  booktitle = {{{IJCAI}}},
  author = {Tsatsaronis, G. and Vazirgiannis, M. and Androutsopoulos, I.},
  date = {2007},
  pages = {1725--1730},
  issn = {10450823},
  doi = {10.1145/1459352.1459355},
  abstract = {Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data and/or do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networksâ€™ links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.},
  annotation = {1726 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {18353985},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P3CF78VG/Tsatsaronis, Vazirgiannis, Androutsopoulos - 2007 - Word sense disambiguation with spreading activation networks generated from thesa(2).pdf},
  isbn = {0360-0300}
}

@inproceedings{tsuboi2008TrainingConditionalRandom,
  title = {Training Conditional Random Fields Using Incomplete Annotations},
  booktitle = {{{COLING}}},
  author = {Tsuboi, Yuta and Kashima, Hisashi and Oda, Hiroki and Mori, Shinsuke and Matsumoto, Yuji},
  date = {2008},
  volume = {1},
  pages = {897--904},
  publisher = {{Association for Computational Linguistics}},
  location = {{Manchester, United Kingdom}},
  doi = {10.3115/1599081.1599194},
  url = {http://portal.acm.org/citation.cfm?doid=1599081.1599194},
  urldate = {2021-02-08},
  abstract = {We address corpus building situations, where complete annotations to the whole corpus is time consuming and unrealistic. Thus, annotation is done only on crucial part of sentences, or contains unresolved label ambiguities. We propose a parameter estimation method for Conditional Random Fields (CRFs), which enables us to use such incomplete annotations. We show promising results of our method as applied to two types of NLP tasks: a domain adaptation task of a Japanese word segmentation using partial annotations, and a partof-speech tagging task using ambiguous tags in the Penn treebank corpus.},
  annotation = {76 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {The 22nd {{International Conference}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WWL4SP7B/Tsuboi et al. - 2008 - Training conditional random fields using incomplet.pdf},
  isbn = {978-1-905593-44-6},
  langid = {english}
}

@inproceedings{tu2017CANEContextAwareNetwork,
  title = {{{CANE}}: {{Context}}-{{Aware Network Embedding}} for {{Relation Modeling}}},
  booktitle = {{{ACL}}},
  author = {Tu, C. and Liu, H. and Liu, Z. and Sun, M.},
  date = {2017},
  pages = {1722--1731},
  doi = {10.18653/v1/p17-1158},
  abstract = {OBJECTIVE To evaluate the effect of an intensivist-model of critical care delivery on the risk of death following injury. SUMMARY BACKGROUND DATA An intensivist-model of ICU care is associated with improved outcomes and less resource utilization in mixed medical and surgical ICUs. The process of trauma center verification assures a relatively high standard of care and quality assurance; thus, it is unclear what the effect of a specific model of ICU care delivery might have on trauma-related mortality. METHODS Using data from a large multicenter (68 centers) prospective cohort study, we evaluated the relationship between the model of ICU care (open vs. intensivist-model) and in-hospital mortality following severe injury. An intensivist-model was defined as an ICU where critically ill trauma patients were either on a distinct ICU service (led by an intensivist) or were comanaged with an intensivist (a physician board-certified in critical care). RESULTS After adjusting for differences in baseline characteristics, the relative risk of death in intensivist-model ICUs was 0.78 (0.58-1.04) compared with an open ICU model. The effect was greatest in the elderly [RR, 0.55 (0.39-0.77)], in units led by surgical intensivists [RR, 0.67 (0.50-0.90)], and in designated trauma centers 0.64 (0.46-0.88). CONCLUSIONS Care in an intensivist-model ICU is associated with a large reduction in in-hospital mortality following trauma, particularly in elderly patients who might have limited physiologic reserve and extensive comorbidity. That the effect is greatest in trauma centers and in units led by surgical intensivists suggests the importance of content expertise in the care of the critically injured. Injured patients are best cared for using an intensivist-model of dedicated critical care delivery, a criterion that should be considered in the verification of trauma centers.},
  annotation = {172 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EFQZD22J/Tu et al. - 2017 - CANE Context-Aware Network Embedding for Relation Modeling(2).pdf},
  keywords = {u}
}

@inproceedings{urbizu2020SequenceSequenceCoreference,
  title = {Sequence to {{Sequence Coreference Resolution}}},
  booktitle = {{{COLING}}},
  author = {Urbizu, Gorka and Soraluze, Ander and Arregi, Olatz},
  date = {2020},
  pages = {8},
  abstract = {Until recently, coreference resolution has been a critical task on the pipeline of any NLP task involving deep language understanding, such as machine translation, chatbots, summarization or sentiment analysis. However, nowadays, those end tasks are learned end-to-end by deep neural networks without adding any explicit knowledge about coreference. Thus, coreference resolution is used less in the training of other NLP tasks or trending pretrained language models. In this paper we present a new approach to face coreference resolution as a sequence to sequence task based on the Transformer architecture. This approach is simple and universal, compatible with any language or dataset (regardless of singletons) and easier to integrate with current language models architectures. We test it on the ARRAU corpus, where we get 65.6 F1 CoNLL. We see this approach not as a final goal, but a means to pretrain sequence to sequence language models (T5) on coreference resolution.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZTDUZJ44/Urbizu et al. - Sequence to Sequence Coreference Resolution.pdf},
  langid = {english}
}

@inproceedings{valera2017AutomaticDiscoveryStatistical,
  title = {Automatic {{Discovery}} of the {{Statistical Types}} of {{Variables}} in a {{Dataset}}},
  booktitle = {{{ICML}}},
  author = {Valera, I. and Ghahramani, Z.},
  date = {2017},
  pages = {4--5},
  url = {http://arxiv.org/abs/1602.05003},
  abstract = {Circular variables arise in a multitude of data-modelling contexts ranging from robotics to the social sciences, but they have been largely overlooked by the machine learning community. This paper partially redresses this imbalance by extending some standard probabilistic modelling tools to the circular domain. First we introduce a new multivariate distribution over circular variables, called the multivariate Generalised von Mises (mGvM) distribution. This distribution can be constructed by restricting and renormalising a general multivariate Gaussian distribution to the unit hyper-torus. Previously proposed multivariate circular distributions are shown to be special cases of this construction. Second, we introduce a new probabilistic model for circular regression, that is inspired by Gaussian Processes, and a method for probabilistic principal component analysis with circular hidden variables. These models can leverage standard modelling tools (e.g. covariance functions and methods for automatic relevance determination). Third, we show that the posterior distribution in these models is a mGvM distribution which enables development of an efficient variational free-energy scheme for performing approximate inference and approximate maximum-likelihood learning.},
  annotation = {8 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1602.05003},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KNAHMLMX/Valera, Ghahramani - 2017 - Automatic Discovery of the Statistical Types of Variables in a Dataset(2).pdf}
}

@inproceedings{valiant1984TheoryLearnable,
  title = {A Theory of the Learnable},
  booktitle = {Annual {{ACM Symposium}} on {{Theory}} of {{Computing}}},
  author = {Valiant, L. G.},
  date = {1984},
  volume = {27},
  pages = {436--445},
  issn = {07378017},
  doi = {10.1145/800057.808710},
  abstract = {Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.},
  annotation = {3668 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6BGFBN3J/Valiant - 1984 - A theory of the learnable(2).pdf},
  isbn = {0-89791-133-4},
  number = {11}
}

@inproceedings{valmadre2017EndtoendRepresentationLearning,
  title = {End-to-End Representation Learning for {{Correlation Filter}} Based Tracking},
  booktitle = {{{CVPR}}},
  author = {Valmadre, J. and Bertinetto, L. and Henriques, J. F. and Vedaldi, A. and Torr, P. H. S.},
  date = {2017},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.531},
  url = {http://arxiv.org/abs/1704.06036},
  abstract = {The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.},
  annotation = {812 citations (Semantic Scholar/DOI) [2021-03-26] 812 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.06036},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2LDWFITW/Valmadre et al. - 2017 - End-to-end representation learning for Correlation Filter based tracking(2).pdf},
  isbn = {978-1-5386-0457-1}
}

@inproceedings{vanaken2017AutomaticDatabaseManagement,
  title = {Automatic Database Management System Tuning through Large-Scale Machine Learning},
  booktitle = {Special {{Interest Group}} on {{Management}} of {{Data}}},
  author = {Van Aken, D. and Pavlo, A. and Gordon, G. J.},
  date = {2017},
  volume = {Part F1277},
  pages = {1009--1024},
  issn = {07308078},
  doi = {10.1145/3035918.3064029},
  abstract = {© 2017 Copyright held by the owner/author(s). Database management system (DBMS) configuration tuning is an essential aspect of any data-intensive application effort. But this is historically a difficult task because DBMSs have hundreds of configuration "knobs" that control everything in the system, such as the amount of memory to use for caches and how often data is written to storage. The problem with these knobs is that they are not standardized (i.e., two DBMSs use a different name for the same knob), not independent (i.e., changing one knob can impact others), and not universal (i.e., what works for one application may be sub-optimal for another). Worse, information about the effects of the knobs typically comes only from (expensive) experience. To overcome these challenges, we present an automated approach that leverages past experience and collects new information to tune DBMS configurations: we use a combination of supervised and un-supervised machine learning methods to (1) select the most impactful knobs, (2) map unseen database workloads to previous workloads from which we can transfer experience, and (3) recommend knob settings. We implemented our techniques in a new tool called OtterTune and tested it on three DBMSs. Our evaluation shows that OtterTune recommends configurations that are as good as or better than ones generated by existing tools or a human expert.},
  annotation = {220 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/47XNEUUC/Van Aken, Pavlo, Gordon - 2017 - Automatic database management system tuning through large-scale machine learning(2).pdf},
  isbn = {978-1-4503-4197-4}
}

@article{vanderheijden2013LearningBayesianNetworks,
  title = {Learning {{Bayesian}} Networks for Clinical Time Series Analysis},
  author = {van der Heijden, M. and Velikova, M. and Lucas, P. J. F.},
  date = {2013},
  journaltitle = {Journal of Biomedical Infomatics},
  volume = {48},
  pages = {94--105},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4ND7GBC6/van der Heijden, Velikova, Lucas - 2013 - Learning Bayesian networks for clinical time series analysis(2).pdf},
  options = {useprefix=true}
}

@thesis{vandurme2009ExtractingImplicitKnowledge,
  title = {Extracting {{Implicit Knowledge}} from {{Text}}},
  author = {Van Durme, B.},
  date = {2009},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UCZXQBKI/Van Durme - 2009 - Extracting Implicit Knowledge from Text(2).pdf},
  keywords = {u}
}

@inproceedings{vandurme2009StreamingPointwiseMutual,
  title = {Streaming Pointwise Mutual Information},
  booktitle = {{{NeurIPS}}},
  author = {Van Durme, B. and Lall, A.},
  date = {2009},
  pages = {1892--1900},
  abstract = {Recent work has led to the ability to perform space efficient, approximate counting over large vocabularies in a streaming context. Motivated by the existence of data structures of this type, we explore the computation of associativity scores, other- wise known as pointwise mutual information (PMI), in a streaming context. We give theoretical bounds showing the impracticality of perfect online PMI compu- tation, and detail an algorithm with high expected accuracy. Experiments on news articles show our approach gives high accuracy on real world data.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7EEPV6MZ/Van Durme, Lall - 2009 - Streaming pointwise mutual information(2).pdf},
  isbn = {978-1-61567-911-9}
}

@inproceedings{vandurme2013PPDBParaphraseDatabase,
  title = {{{PPDB}}: {{The Paraphrase Database}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Van Durme, B. and Callison-burch, C.},
  date = {2013},
  pages = {758--764},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/729V6QIS/Van Durme, Callison-burch - 2013 - PPDB The Paraphrase Database(2).pdf}
}

@article{vanhoeve2006RevisitingSequenceConstraint,
  title = {Revisiting the Sequence Constraint},
  author = {Van Hoeve, W. J. and Pesant, G. and Rousseau, L. M. and Sabharwal, A.},
  date = {2006},
  journaltitle = {Lecture Notes in Computer Science},
  volume = {4204},
  pages = {620--634},
  issn = {16113349},
  doi = {10.1007/11889205_44},
  abstract = {Many combinatorial problems, such as car sequencing and fostering, feature sequence constraints, restricting the number of occurrences of certain values in every subsequence of a given width. To date, none of the filtering algorithms proposed guaranteed domain consistency. In this paper, we present three filtering algorithms for the sequence constraint, with complementary strengths. One borrows ideas from dynamic programming; another reformulates it as a regular constraint; the last is customized. The last two algorithms establish domain consistency. Our customized algorithm does so in polynomial time, and can even be applied to a generalized sequence constraint for subsequences of variable widths. Experimental results show the practical usefulness of each. © Springer-Verlag Berlin Heidelberg 2006.},
  annotation = {66 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IZ4UURIS/Van Hoeve et al. - 2006 - Revisiting the sequence constraint(2).pdf},
  isbn = {3540462678}
}

@inproceedings{vashishtha2019FineGrainedTemporalRelation,
  title = {Fine-{{Grained Temporal Relation Extraction}}},
  booktitle = {{{ACL}}},
  author = {Vashishtha, S. and Van Durme, B. and White, A. S.},
  date = {2019},
  archiveprefix = {arXiv},
  eprint = {1902.01390v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9AQNHYWF/Vashishtha, Van Durme, White - 2019 - Fine-Grained Temporal Relation Extraction(2).pdf}
}

@inproceedings{vaswani2013DecodingLargeScaleNeural,
  title = {Decoding with {{Large}}-{{Scale Neural Language Models Improves Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Vaswani, A. and Zhao, Y. and Fossum, V. and Chiang, D.},
  date = {2013},
  abstract = {We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K9ZU49VW/Vaswani et al. - 2013 - Decoding with Large-Scale Neural Language Models Improves Translation(2).pdf}
}

@inproceedings{vaswani2016SupertaggingLSTMs,
  title = {Supertagging {{With LSTMs}}},
  booktitle = {{{NAACL}}},
  author = {Vaswani, A. and Bisk, Y. and Sagae, K. and Musa, R.},
  date = {2016},
  pages = {232--237},
  url = {http://www.aclweb.org/anthology/N16-1027},
  abstract = {In this paper we present new state-of-the-art performance on CCG supertagging and pars-ing. Our model outperforms existing ap-proaches by an absolute gain of 1.5\%. We an-alyze the performance of several neural mod-els and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4P32JZBG/Vaswani et al. - 2016 - Supertagging With LSTMs(2).pdf},
  isbn = {978-1-941643-91-4}
}

@inproceedings{vaswani2017AttentionAllYou,
  title = {Attention {{Is All You Need}}},
  booktitle = {{{NeurIPS}}},
  author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. N. and Kaiser, L. and Polosukhin, I.},
  date = {2017},
  issn = {0140-525X},
  doi = {10.1017/S0140525X16001837},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  annotation = {1102 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1000303116},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/L6N3VRGP/Vaswani et al. - 2017 - Attention Is All You Need(2).pdf}
}

@inproceedings{vieira2017DynaSelfoptimizingDeclarative,
  title = {Dyna: {{Toward}} a Self-Optimizing Declarative Language for Machine Learning Applications},
  booktitle = {{{ACM SIGPLAN International Workshop}} on {{Machine Learning}} and {{Programming Languages}}},
  author = {Vieira, T. and Francis-Landau, M. and Filardo, N. W. and Khorasani, F. and Eisner, J. M.},
  date = {2017},
  pages = {8--17},
  doi = {10.1145/3088525.3088562},
  abstract = {Declarative programming is a paradigm that allows programmers to specify what they want to compute, leaving how to compute it to a solver. Our declarative programming language, Dyna, is designed to compactly specify computations like those that are frequently encountered in machine learning. As a declarative language, Dyna's solver has a large space of (correct) strategies available to it. We describe a reinforcement learning framework for adaptively choosing among these strategies to maximize efficiency for a given workload. Adaptivity in execution is especially important for software that will run under a variety of workloads, where no fixed policy works well. We hope that reinforcement learning will identify good policies reasonably quickly - offloading the burden of writing efficient code from human programmers.},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DA9IRFGS/Vieira et al. - 2017 - Dyna Toward a self-optimizing declarative language for machine learning applications(2).pdf},
  isbn = {978-1-4503-5071-6}
}

@article{vieira2017LearningPruneExploring,
  title = {Learning to {{Prune}}: {{Exploring}} the {{Frontier}} of {{Fast}} and {{Accurate Parsing}}},
  author = {Vieira, T. and Eisner, J. M.},
  date = {2017},
  journaltitle = {TACL},
  volume = {5},
  pages = {263--278},
  issn = {2307-387X},
  abstract = {Pruning hypotheses during dynamic program- ming is commonly used to speed up inference in settings such as parsing. Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy. This poses a difficult machine learning problem, which we tackle with the LOLS algorithm. LOLS training must continually compute the ef- fects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CF3QLBHR/Vieira, Eisner - 2017 - Learning to Prune Exploring the Frontier of Fast and Accurate Parsing(2).pdf},
  keywords = {u},
  number = {2011}
}

@inproceedings{vogel1996HMMbasedWordAlignment,
  title = {{{HMM}}-Based {{Word Alignment}} in {{Statistical Machine Translation}}},
  booktitle = {{{COLING}}},
  author = {Vogel, S. and Ney, H. and Tillmann, C.},
  date = {1996},
  volume = {96pp},
  pages = {836--841},
  doi = {10.3115/993268.993313},
  url = {http://portal.acm.org/citation.cfm?doid=993268.993313},
  abstract = {In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.},
  annotation = {910 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q88WGUFF/Vogel, Ney, Tillmann - 1996 - HMM-based Word Alignment in Statistical Machine Translation(2).pdf},
  keywords = {u}
}

@inproceedings{vogel2010LearningFollowNavigational,
  title = {Learning to {{Follow Navigational Directions}}},
  booktitle = {{{ACL}}},
  author = {Vogel, A. and Jurafsky, D.},
  date = {2010},
  pages = {806--814},
  abstract = {We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3BLYWLNT/Vogel, Jurafsky - 2010 - Learning to Follow Navigational Directions(2).pdf},
  isbn = {978-1-61738-808-8},
  issue = {July},
  keywords = {u}
}

@inproceedings{volpi2018GeneralizingUnseenDomains,
  title = {Generalizing to Unseen Domains via Adversarial Data Augmentation},
  booktitle = {{{NeurIPS}}},
  author = {Volpi, R. and Duchi, J. and Namkoong, H. and Murino, V. and Sener, O. and Savarese, S.},
  date = {2018},
  pages = {5334--5344},
  issn = {10495258},
  abstract = {We are concerned with learning models that generalize well to different unseen domains. We consider a worst-case formulation over data distributions that are near the source domain in the feature space. Only using training data from a single source distribution, we propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is "hard" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). On digit recognition and semantic segmentation tasks, our method learns models improve performance across a range of a priori unknown target domains.},
  archiveprefix = {arXiv},
  eprint = {1805.12018},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X2CXAYKB/Volpi et al. - 2018 - Generalizing to unseen domains via adversarial data augmentation(2).pdf}
}

@inproceedings{vulic2019WeReallyNeed,
  title = {Do {{We Really Need Fully Unsupervised Cross}}-{{Lingual Embeddings}}?},
  booktitle = {{{EMNLP}}},
  author = {Vulić, I. and Glavaš, G. and Reichart, R. and Korhonen, A.},
  date = {2019},
  pages = {4406--4417},
  doi = {10.18653/v1/d19-1449},
  abstract = {Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly focused on fully unsupervised approaches that project monolingual embeddings into a shared cross-lingual space without any cross-lingual signal. The lack of any supervision makes such approaches conceptually attractive. Yet, their only core difference from (weakly) supervised projection-based CLWE methods is in the way they obtain a seed dictionary used to initialize an iterative self-learning procedure. The fully unsupervised methods have arguably become more robust, and their primary use case is CLWE induction for pairs of resource-poor and distant languages. In this paper, we question the ability of even the most robust unsupervised CLWE approaches to induce meaningful CLWEs in these more challenging settings. A series of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods.},
  annotation = {31 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.01638},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7MC99SBG/Vulić et al. - 2019 - Do We Really Need Fully Unsupervised Cross-Lingual Embeddings(2).pdf},
  keywords = {u}
}

@inproceedings{wadden2019EntityRelationEvent,
  title = {Entity, {{Relation}}, and {{Event Extraction}} with {{Contextualized Span Representations}}},
  booktitle = {{{EMNLP}}},
  author = {Wadden, David and Wennberg, Ulme and Luan, Yi and Hajishirzi, Hannaneh},
  date = {2019-09-09},
  archiveprefix = {arXiv},
  eprint = {1909.03546},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/58SHQC2Q/Wadden et al. - 2019 - Entity, Relation, and Event Extraction with Contex.pdf;/home/hiaoxui/.local/share/zotero_files/storage/KIHJEE4G/1909.html}
}

@inproceedings{wallace2019NLPModelsKnow,
  title = {Do {{NLP Models Know Numbers}}? {{Probing Numeracy}} in {{Embeddings}}},
  booktitle = {{{EMNLP}}},
  author = {Wallace, E. and Wang, Y. and Li, S. and Singh, S. and Gardner, M.},
  date = {2019},
  pages = {5310--5318},
  doi = {10.18653/v1/d19-1534},
  abstract = {The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens---they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise---ELMo captures numeracy the best for all pre-trained methods---but BERT, which uses sub-word units, is less exact.},
  annotation = {52 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.07940},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N5ZFL2BS/Wallace et al. - 2019 - Do NLP Models Know Numbers Probing Numeracy in Embeddings(2).pdf}
}

@inproceedings{wallace2019UniversalAdversarialTriggers,
  title = {Universal {{Adversarial Triggers}} for {{Attacking}} and {{Analyzing NLP}}},
  booktitle = {{{EMNLP}}},
  author = {Wallace, E. and Feng, S. and Kandpal, N. and Gardner, M. and Singh, S.},
  date = {2019},
  pages = {2153--2162},
  url = {http://arxiv.org/abs/1908.07125},
  abstract = {Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94\% to 0.55\%, 72\% of "why" questions in SQuAD to be answered "to kill american people", and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.},
  annotation = {123 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1908.07125},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CDR7XJET/Wallace et al. - 2019 - Universal Adversarial Triggers for Attacking and Analyzing NLP(2).pdf;/home/hiaoxui/.local/share/zotero_files/storage/Y4A33VDZ/Wallace et al. - 2019 - Universal Adversarial Triggers for Attacking and A.pdf},
  issue = {Section 5},
  keywords = {u}
}

@inproceedings{walzer2007TemporalConstraintsRulebased,
  title = {Temporal Constraints for Rule-Based Event Processing},
  booktitle = {{{PIKM}}},
  author = {Walzer, K. and Schill, A. and Löser, A.},
  date = {2007},
  pages = {93--99},
  doi = {10.1145/1316874.1316890},
  abstract = {Complex event processing (CEP) is an important technology for event-driven systems with a broad application space ranging from supply chain management for RFID, systems monitoring, and stock market analysis to news services. The purpose of CEP is the identification of patterns of events with logical, temporal or causal relationships out of single occurring events. The Rete algorithm is commonly used in rule-based systems to trigger certain actions if a corresponding rule holds. It allows for a high number of rules and is therefore ideally suited for event processing systems. However, traditional Rete networks are limited to operations such as unification and the extraction of predicates from a knowledge base. There is no support for temporal operators. We propose an extension of the Rete algorithm for support of temporal operators. Thereby, we are using interval time semantics. We present the issues created by this extension as well as our pursued methodology to address them. A description language is used to specify the patterns of interest. This specification is also called subscription to a complex event. On the occurrence of the specified event pattern, an action such as the creation of a new event or the execution of certain function is performed. © 2007 ACM.},
  annotation = {11 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P34GJMZT/Walzer, Schill, Löser - 2007 - Temporal constraints for rule-based event processing(2).pdf},
  isbn = {978-1-59593-832-9}
}

@inproceedings{wang2013FastDropoutTraining,
  title = {Fast Dropout Training},
  booktitle = {{{ICML}}},
  author = {Wang, S. and Manning, C. D.},
  date = {2013},
  volume = {28},
  pages = {118--126},
  url = {http://machinelearning.wustl.edu/mlpapers/papers/wang13a},
  abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZIGE8WCM/Wang, Manning - 2013 - Fast dropout training(2).pdf},
  keywords = {u}
}

@inproceedings{wang2013FeatureNoisingLoglinear,
  title = {Feature Noising for Log-Linear Structured Prediction},
  booktitle = {{{EMNLP}}},
  author = {Wang, S. I. and Wang, M. and Wager, S. and Liang, P. and Manning, C. D.},
  date = {2013},
  pages = {1170--1179},
  abstract = {© 2013 Association for Computational Linguistics. NLP models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting. A recently re-popularized form of regularization is to generate fake training data by repeatedly adding noise to real data. We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data. We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs. We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently. The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transductive extension. Applied to text classification and NER, our method provides a {$>$}1\% absolute performance gain over use of standard L2 regularization.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2RBX5QLI/Wang et al. - 2013 - Feature noising for log-linear structured prediction(2).pdf},
  isbn = {978-1-937284-97-8},
  issue = {October}
}

@inproceedings{wang2014KnowledgeGraphText,
  title = {Knowledge {{Graph}} and {{Text Jointly Embedding}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, Z. and Zhang, J. and Feng, J. and Chen, Z.},
  date = {2014},
  doi = {10.3115/v1/d14-1167},
  abstract = {We examine the embedding approach to reason new relational facts from a large- scale knowledge graph and a text corpus. We propose a novel method of jointly em- bedding entities and words into the same continuous vector space. The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus. Entity names and Wikipedia an- chors are utilized to align the embeddings of entities and words in the same space. Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text. Particularly, jointly embedding enables the prediction of facts containing entities out of the knowledge graph, which cannot be han- dled by previous embedding methods. At the same time, concerning the quality of the word embeddings, experiments on the analogical reasoning task showthat jointly embedding is comparable to or slightly better than word2vec (Skip-Gram).},
  annotation = {285 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZVN7X6AF/Wang et al. - 2014 - Knowledge Graph and Text Jointly Embedding(2).pdf}
}

@inproceedings{wang2015BuildingSemanticParser,
  title = {Building a {{Semantic Parser Overnight}}},
  booktitle = {{{ACL}}},
  author = {Wang, Y. and Berant, J. and Liang, P.},
  date = {2015},
  pages = {1332--1342},
  doi = {10.3115/v1/P15-1129},
  url = {http://aclweb.org/anthology/P15-1129},
  abstract = {How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We fur- ther study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.},
  annotation = {212 citations (Semantic Scholar/DOI) [2021-03-26]},
  eprint = {1684229},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/8WXNT8Y4/Wang, Berant, Liang - 2015 - Building a Semantic Parser Overnight(2).pdf},
  isbn = {978-1-941643-72-3},
  keywords = {u}
}

@article{wang2016GalacticDependenciesTreebanks,
  title = {The {{Galactic Dependencies Treebanks}}: {{Getting More Data}} by {{Synthesizing New Languages}}},
  author = {Wang, D. and Eisner, J. M.},
  date = {2016},
  journaltitle = {TACL},
  volume = {4},
  pages = {491--505},
  issn = {2307-387X},
  url = {http://arxiv.org/abs/1710.03838},
  abstract = {We release Galactic Dependencies 1.0---a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format. This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages. Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages. We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages. As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a "nearby" source language. We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.},
  annotation = {34 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1710.03838},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FZHATEKS/Wang, Eisner - 2016 - The Galactic Dependencies Treebanks Getting More Data by Synthesizing New Languages(2).pdf}
}

@article{wang2017FineGrainedPredictionSyntactic,
  title = {Fine-{{Grained Prediction}} of {{Syntactic Typology}}: {{Discovering Latent Structure}} with {{Supervised Learning}}},
  author = {Wang, D. and Eisner, J. M.},
  date = {2017},
  journaltitle = {TACL},
  volume = {5},
  pages = {147--161},
  doi = {10.1162/tacl_a_00052},
  abstract = {We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction. While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data. The supervised learner must identify surface features of a language’s POS sequence (hand-engineered or neural features) that correlate with the language’s deeper structure (latent trees). In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar induction baseline by a large margin.},
  annotation = {14 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ETWBJLJ6/Wang, Eisner - 2017 - Fine-Grained Prediction of Syntactic Typology Discovering Latent Structure with Supervised Learning(2).pdf}
}

@article{wang2018DeepVisualDomain,
  title = {Deep {{Visual Domain Adaptation}}: {{A Survey}}},
  author = {Wang, M. and Deng, W.},
  date = {2018},
  journaltitle = {Neurocomputing},
  pages = {1--20},
  archiveprefix = {arXiv},
  eprint = {1802.03601v4},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9GKZEW9U/Wang, Deng - 2018 - Deep Visual Domain Adaptation A Survey(2).pdf},
  keywords = {u}
}

@inproceedings{wang2018SyntheticDataMade,
  title = {Synthetic {{Data Made}} to {{Order}}: {{The Case}} of {{Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, D. and Eisner, J. M.},
  date = {2018},
  doi = {10.1109/IEEESTD.1998.87897},
  abstract = {To approximately parse an unfamiliar language , it helps to have a treebank of a similar language. But what if the closest available treebank still has the wrong word order? We show how to (stochastically) permute the constituents of an existing dependency tree-bank so that its surface part-of-speech statistics approximately match those of the target language. The parameters of the permutation model can be evaluated for quality by dynamic programming and tuned by gradient descent (up to a local optimum). This optimization procedure yields trees for a new artificial language that resembles the target language. We show that delexicalized parsers for the target language can be successfully trained using such "made to order" artificial languages.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/WQQISFPY/Wang, Eisner - 2018 - Synthetic Data Made to Order The Case of Parsing(2).pdf},
  keywords = {u}
}

@inproceedings{wang2019DeepFactorsForecasting,
  title = {Deep {{Factors}} for {{Forecasting}}},
  booktitle = {{{ICML}}},
  author = {Wang, Y. and Smola, A. and Maddix, D. C. and Gasthaus, J. and Foster, D. and Januschowski, T.},
  date = {2019},
  url = {http://arxiv.org/abs/1905.12417},
  abstract = {Producing probabilistic forecasts for large collections of similar and/or dependent time series is a practically relevant and challenging task. Classical time series models fail to capture complex patterns in the data, and multivariate techniques struggle to scale to large problem sizes. Their reliance on strong structural assumptions makes them data-efficient, and allows them to provide uncertainty estimates. The converse is true for models based on deep neural networks, which can learn complex patterns and dependencies given enough data. In this paper, we propose a hybrid model that incorporates the benefits of both approaches. Our new method is data-driven and scalable via a latent, global, deep component. It also handles uncertainty through a local classical model. We provide both theoretical and empirical evidence for the soundness of our approach through a necessary and sufficient decomposition of exchangeable time series into a global and a local part. Our experiments demonstrate the advantages of our model both in term of data efficiency, accuracy and computational complexity.},
  annotation = {35 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1905.12417},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JKZCY53Q/Wang et al. - 2019 - Deep Factors for Forecasting(2).pdf}
}

@inproceedings{wang2019GLUEMultiTaskBenchmark,
  title = {{{GLUE}}: {{A Multi}}-{{Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  booktitle = {{{ICLR}}},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  date = {2019},
  url = {http://arxiv.org/abs/1804.07461},
  urldate = {2020-11-19},
  abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
  annotation = {1222 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.07461},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JDECJCNA/Wang et al. - 2019 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf;/home/hiaoxui/.local/share/zotero_files/storage/TJ3DNYCH/1804.html}
}

@inproceedings{wang2019HowBestUse,
  title = {How to Best Use {{Syntax}} in {{Semantic Role Labelling}}},
  booktitle = {{{ACL}}},
  author = {Wang, Yufei and Johnson, Mark and Wan, Stephen and Sun, Yifang and Wang, Wei},
  date = {2019-06-01},
  abstract = {There are many different ways in which external information might be used in an NLP task. This paper investigates how external syntactic information can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL'05 and CoNLL'12 benchmarks.},
  archiveprefix = {arXiv},
  eprint = {1906.00266},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FJBSH4T4/Wang et al. - 2019 - How to best use Syntax in Semantic Role Labelling.pdf},
  langid = {english}
}

@inproceedings{wang2019LearningSemanticParsers,
  title = {Learning {{Semantic Parsers}} from {{Denotations}} with {{Latent Structured Alignments}} and {{Abstract Programs}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, B. and Titov, I. and Lapata, M.},
  date = {2019},
  pages = {3765--3776},
  url = {http://arxiv.org/abs/1909.04165},
  abstract = {Semantic parsing aims to map natural language utterances onto machine interpretable meaning representations, aka programs whose execution against a real-world environment produces a denotation. Weakly-supervised semantic parsers are trained on utterance-denotation pairs treating programs as latent. The task is challenging due to the large search space and spuriousness of programs which may execute to the correct answer but do not generalize to unseen examples. Our goal is to instill an inductive bias in the parser to help it distinguish between spurious and correct programs. We capitalize on the intuition that correct programs would likely respect certain structural constraints were they to be aligned to the question (e.g., program fragments are unlikely to align to overlapping text spans) and propose to model alignments as structured latent variables. In order to make the latent-alignment framework tractable, we decompose the parsing task into (1) predicting a partial "abstract program" and (2) refining it while modeling structured alignments with differential dynamic programming. We obtain state-of-the-art performance on the WIKITABLEQUESTIONS and WIKISQL datasets. When compared to a standard attention baseline, we observe that the proposed structured-alignment mechanism is highly beneficial.},
  annotation = {7 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.04165},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/45DG9YL3/Wang, Titov, Lapata - 2019 - Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs(2).pdf},
  keywords = {u}
}

@inproceedings{wang2019NeuralGaussianCopula,
  title = {Neural {{Gaussian Copula}} for {{Variational Autoencoder}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, P. Z. and Wang, W. Y.},
  date = {2019},
  pages = {4332--4342},
  doi = {10.18653/v1/d19-1442},
  abstract = {Variational language models seek to estimate the posterior of latent variables with an approximated variational posterior. The model often assumes the variational posterior to be factorized even when the true posterior is not. The learned variational posterior under this assumption does not capture the dependency relationships over latent variables. We argue that this would cause a typical training problem called posterior collapse observed in all other variational language models. We propose Gaussian Copula Variational Autoencoder (VAE) to avert this problem. Copula is widely used to model correlation and dependencies of high-dimensional random variables, and therefore it is helpful to maintain the dependency relationships that are lost in VAE. The empirical results show that by modeling the correlation of latent variables explicitly using a neural parametric copula, we can avert this training difficulty while getting competitive results among all other VAE approaches.},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.03569},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ECCYF6HL/Wang, Wang - 2019 - Neural Gaussian Copula for Variational Autoencoder(2).pdf},
  keywords = {u}
}

@online{wang2019RATSQLRelationAwareSchema,
  title = {{{RAT}}-{{SQL}}: {{Relation}}-{{Aware Schema Encoding}} and {{Linking}} for {{Text}}-to-{{SQL Parsers}}},
  author = {Wang, B. and Shin, R. and Liu, X. and Polozov, O. and Richardson, M.},
  date = {2019},
  url = {http://arxiv.org/abs/1911.04942},
  abstract = {When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 53.7\%, compared to 47.4\% for the state-of-the-art model unaugmented with BERT embeddings. In addition, we observe qualitative improvements in the model's understanding of schema linking and alignment.},
  annotation = {47 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1911.04942},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LYE5DBNY/Wang et al. - 2019 - RAT-SQL Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers(2).pdf},
  keywords = {u}
}

@inproceedings{wang2019SATNetBridgingDeep,
  title = {{{SATNet}}: {{Bridging}} Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver},
  booktitle = {{{ICML}}},
  author = {Wang, P. and Donti, P. L. and Wilder, B. and Kolter, Z.},
  date = {2019},
  url = {http://arxiv.org/abs/1905.12149},
  abstract = {Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a "visual Sudok" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.},
  annotation = {49 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1905.12149},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H8PWCGNI/Wang et al. - 2019 - SATNet Bridging deep learning and logical reasoning using a differentiable satisfiability solver(2).pdf},
  keywords = {u}
}

@inproceedings{wang2019SuperGLUEStickierBenchmark,
  title = {{{SuperGLUE}}: {{A Stickier Benchmark}} for {{General}}-{{Purpose Language Understanding Systems}}},
  booktitle = {{{NeurIPS}}},
  author = {Wang, A. and Pruksachatkun, Y. and Nangia, N. and Singh, A. and Michael, J. and Hill, F. and Levy, O. and Bowman, S. R.},
  date = {2019},
  volume = {2019},
  pages = {1--30},
  url = {http://arxiv.org/abs/1905.00537},
  abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
  annotation = {295 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1905.00537},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q7DI8QYV/Wang et al. - 2019 - SuperGLUE A Stickier Benchmark for General-Purpose Language Understanding Systems(2).pdf},
  issue = {July},
  keywords = {u}
}

@report{wang2020LanguageModelsAre,
  title = {Language {{Models}} Are {{Open Knowledge Graphs}}},
  author = {Wang, C. and Liu, X. and Song, D.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3V6GAZE8/Language Models are Open Knowledge Graphs.pdf},
  keywords = {u}
}

@report{wang2020LinformerSelfAttentionLinear,
  title = {Linformer: {{Self}}-{{Attention}} with {{Linear Complexity}}},
  shorttitle = {Linformer},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  date = {2020-06-14},
  url = {http://arxiv.org/abs/2006.04768},
  urldate = {2021-03-28},
  abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n\^2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n\^2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the \textbackslash textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  annotation = {51 citations (Semantic Scholar/arXiv) [2021-03-27]},
  archiveprefix = {arXiv},
  eprint = {2006.04768},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IBTND84I/Wang et al. - 2020 - Linformer Self-Attention with Linear Complexity.pdf;/home/hiaoxui/.local/share/zotero_files/storage/BUUYMGYM/2006.html}
}

@inproceedings{wang2020PyramidLayeredModel,
  title = {Pyramid: {{A Layered Model}} for {{Nested Named Entity Recognition}}},
  shorttitle = {Pyramid},
  booktitle = {{{ACL}}},
  author = {Wang, Jue and Shou, Lidan and Chen, Ke and Chen, Gang},
  date = {2020},
  pages = {5918--5928},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.525},
  url = {https://www.aclweb.org/anthology/2020.acl-main.525},
  urldate = {2021-02-16},
  abstract = {This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER). In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape. Each time an embedding passes through a layer of the pyramid, its length is reduced by one. Its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention. We also design an inverse pyramid to allow bidirectional interaction between layers. The proposed method achieves state-of-the-art F1 scores in nested NER on ACE-2004, ACE2005, GENIA, and NNE, which are 80.27, 79.42, 77.78, and 93.70 with conventional embeddings, and 87.74, 86.34, 79.31, and 94.68 with pre-trained contextualized embeddings. In addition, our model can be used for the more general task of Overlapping Named Entity Recognition. A preliminary experiment confirms the effectiveness of our method in overlapping NER.},
  annotation = {4 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IRTM8THD/Wang et al. - 2020 - Pyramid A Layered Model for Nested Named Entity R.pdf},
  langid = {english}
}

@inproceedings{wang2020TwoAreBetter,
  title = {Two Are {{Better}} than {{One}}: {{Joint Entity}} and {{Relation Extraction}} with {{Table}}-{{Sequence Encoders}}},
  shorttitle = {Two Are {{Better}} than {{One}}},
  booktitle = {{{EMNLP}}},
  author = {Wang, Jue and Lu, Wei},
  date = {2020},
  pages = {1706--1721},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.133},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.133},
  urldate = {2021-03-16},
  annotation = {2 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/XEK6C59C/Wang and Lu - 2020 - Two are Better than One Joint Entity and Relation.pdf},
  langid = {english}
}

@thesis{wanzare2019ScriptAcquisitionCrowdsourcing,
  title = {Script {{Acquisition}} : {{A Crowdsourcing}} and {{Text}} Mining Approach},
  author = {Wanzare, L D. A.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DX4PRPRM/Wanzare - 2019 - Script Acquisition A Crowdsourcing and Text mining approach(2).pdf},
  keywords = {u}
}

@inproceedings{warstadt2019InvestigatingBERTKnowledge,
  title = {Investigating {{BERT}}'s {{Knowledge}} of {{Language}}: {{Five Analysis Method}} with {{NPIs}}},
  booktitle = {{{EMNLP}}},
  author = {Warstadt, A. and Cao, Y. and Grosu, I. and Peng, W. and Blix, H. and Nie, Y. and Alsop, A. and Bordia, S. and Liu, H. and Parrish, A. and Wang, S. and Phang, J. and Mohananey, A. and Htut, P. M. and Jeretiˇ, P. and Bowman, S. R.},
  date = {2019},
  archiveprefix = {arXiv},
  eprint = {1909.02597v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/448GGR5G/Warstadt et al. - 2019 - Investigating BERT's Knowledge of Language Five Analysis Method with NPIs(2).pdf}
}

@article{webberb.2003AnaphoraDiscourseStructure,
  title = {Anaphora and Discourse Structure},
  author = {{Webber B.} and {Stone M.} and Joshi, A. and {Knott A}},
  date = {2003},
  journaltitle = {Computational Linguistics},
  volume = {29},
  pages = {545--587},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/EKX2UKKM/Webber B. et al. - 2003 - Anaphora and discourse structure(2).pdf}
}

@inproceedings{weber2018FineLineLinguistic,
  title = {The {{Fine Line}} between {{Linguistic Generalization}} and {{Failure}} in {{Seq2Seq}}-{{Attention Models}}},
  booktitle = {Workshop on {{Generalization}} in the {{Age}} of {{Deep Learning}}},
  author = {Weber, N. and Shekhar, L. and Balasubramanian, N.},
  date = {2018},
  pages = {24--27},
  doi = {10.18653/v1/w18-1004},
  abstract = {Seq2Seq based neural architectures have become the go-to architecture to apply to sequence to sequence language tasks. Despite their excellent performance on these tasks, recent work has noted that these models usually do not fully capture the linguistic structure required to generalize beyond the dense sections of the data distribution \textbackslash cite\{ettinger2017towards\}, and as such, are likely to fail on samples from the tail end of the distribution (such as inputs that are noisy \textbackslash citep\{belkinovnmtbreak\} or of different lengths \textbackslash citep\{bentivoglinmtlength\}). In this paper, we look at a model's ability to generalize on a simple symbol rewriting task with a clearly defined structure. We find that the model's ability to generalize this structure beyond the training distribution depends greatly on the chosen random seed, even when performance on the standard test set remains the same. This suggests that a model's ability to capture generalizable structure is highly sensitive. Moreover, this sensitivity may not be apparent when evaluating it on standard test sets.},
  annotation = {22 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FI7TDL7V/Weber, Shekhar, Balasubramanian - 2018 - The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models(2).pdf},
  keywords = {u}
}

@inproceedings{weber2018HierarchicalQuantizedRepresentations,
  title = {Hierarchical {{Quantized Representations}} for {{Script Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Weber, N. and Shekhar, L. and Balasubramanian, N. and Chambers, N.},
  date = {2018},
  pages = {3783--3792},
  doi = {10.18653/v1/d18-1413},
  abstract = {Scripts define knowledge about how everyday scenarios (such as going to a restaurant) are expected to unfold. One of the challenges to learning scripts is the hierarchical nature of the knowledge. For example, a suspect arrested might plead innocent or guilty, and a very different track of events is then expected to happen. To capture this type of information, we propose an autoencoder model with a latent space defined by a hierarchy of categorical variables. We utilize a recently proposed vector quantization based approach, which allows continuous embeddings to be associated with each latent variable value. This permits the decoder to softly decide what portions of the latent hierarchy to condition on by attending over the value embeddings for a given setting. Our model effectively encodes and generates scripts, outperforming a recent language modeling-based method on several standard tasks, and allowing the autoencoder model to achieve substantially lower perplexity scores compared to the previous language modeling-based method.},
  annotation = {12 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X8ACWF6Z/Weber et al. - 2018 - Hierarchical Quantized Representations for Script Generation(2).pdf}
}

@inproceedings{webster2014LimitedMemoryIncremental,
  title = {Limited Memory Incremental Coreference Resolution},
  booktitle = {{{COLING}}},
  author = {Webster, Kellie and Curran, James R},
  date = {2014},
  pages = {11},
  abstract = {We propose an algorithm for coreference resolution based on analogy with shift-reduce parsing. By reconceptualising the task in this way, we unite ranking- and cluster-based approaches to coreference resolution, which have until now been largely orthogonal. Additionally, our framework naturally lends itself to rich discourse modelling, which we use to define a series of psycholinguistically motivated features. We achieve CoNLL scores of 63.33 and 62.91 on the CoNLL-2012 DEV and TEST splits of the OntoNotes 5 corpus, beating the publicly available state of the art systems. These results are also competitive with the best reported research systems despite our system having low memory requirements and a simpler model.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N6F98VN3/Webster and Curran - Limited memory incremental coreference resolution.pdf},
  langid = {english}
}

@inproceedings{webster2016UsingMentionAccessibility,
  title = {Using Mention Accessibility to Improve Coreference Resolution},
  booktitle = {{{ACL}}},
  author = {Webster, Kellie and Nothman, Joel},
  date = {2016},
  pages = {432--437},
  publisher = {{Association for Computational Linguistics}},
  location = {{Berlin, Germany}},
  doi = {10.18653/v1/P16-2070},
  url = {http://aclweb.org/anthology/P16-2070},
  urldate = {2021-03-28},
  abstract = {Modern coreference resolution systems require linguistic and general knowledge typically sourced from costly, manually curated resources. Despite their intuitive appeal, results have been mixed. In this work, we instead implement fine-grained surface-level features motivated by cognitive theory. Our novel fine-grained feature specialisation approach significantly improves the performance of a strong baseline, achieving state-of-the-art results of 65.29 and 61.13\% on CoNLL-2012 using gold and automatic preprocessing, with system extracted mentions.},
  annotation = {3 citations (Semantic Scholar/DOI) [2021-03-28]},
  eventtitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Z9KG3ZRM/Webster and Nothman - 2016 - Using mention accessibility to improve coreference.pdf},
  langid = {english}
}

@inproceedings{wei2018MoreAdaptiveAlgorithms,
  title = {More {{Adaptive Algorithms}} for {{Adversarial Bandits}}},
  booktitle = {{{COLT}}},
  author = {Wei, C. and Luo, H.},
  date = {2018},
  volume = {75},
  pages = {1--29},
  archiveprefix = {arXiv},
  eprint = {1801.03265v3},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4VY5JUHZ/Wei, Luo - 2018 - More Adaptive Algorithms for Adversarial Bandits(2).pdf},
  keywords = {u}
}

@report{weir2020ProbingNeuralLanguage,
  title = {Probing {{Neural Language Models}} for {{Human Tacit Assumptions}}},
  author = {Weir, N. and Poliak, A. and Van Durme, B.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/P46HAFQI/Weir, Poliak, Van Durme - 2020 - Probing Neural Language Models for Human Tacit Assumptions(2).pdf},
  keywords = {u}
}

@report{weischedel2010OntoNotesRelease,
  title = {{{OntoNotes Release}} 5.0},
  author = {Weischedel, R. and Pradhan, S. and Ramshaw, L. and Kaufman, J. and Franchini, M. and El-Bachouti, M. and Xue, N. and Palmer, M. and Marcus, M. and Taylor, A. and Greenberg, C. and Hovy, E. and Belvin, R. and Houston, A.},
  date = {2010},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MK8UQMVR/OntoNotes-Release-5.0.pdf}
}

@incollection{weischedel2011OntoNotesLargeTraining,
  title = {{{OntoNotes}}: {{A}} Large Training Corpus for Enhanced Processing},
  booktitle = {Handbook of {{Natural Language Processing}} and {{Machine Translation}}. {{Springer}}},
  author = {Weischedel, R. and Hovy, E. and Marcus, M. and Palmer, M. and Belvin, R. and Pradhan, S. and Ramshaw, L. and Xue, N.},
  date = {2011},
  abstract = {1 This paper describes a large multilingual richly annotated corpus which is being made available to the community. There is an emphasis on quality and consistency with interannotator agreement rates targeted at 90\%. The data covers multiple genres in English, Chinese, and Arabic, including a significant amount of parallel data. The annotation, intended to capture a skeletal representation of literal meaning, includes parse trees, predicate argument structures , word senses localized in an ontology, co-reference, and name types. The resource is delivered as an integrated database, supporting combined queries that access multiple annotation layers. Annual incremental releases are distributed via LDC.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NWGFWGLR/Weischedel et al. - 2011 - OntoNotes A large training corpus for enhanced processing(2).pdf}
}

@report{weiss2019ClinicalRiskWavelet,
  title = {Clinical {{Risk}}: {{Wavelet Reconstruction Networks}} for {{Marked Point Processes}}},
  author = {Weiss, J. C.},
  date = {2019},
  pages = {1--11},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/V2JWQW3P/Weiss - 2019 - Clinical Risk Wavelet Reconstruction Networks for Marked Point Processes(2).pdf},
  keywords = {review}
}

@inproceedings{welleck2019NeuralTextGeneration,
  title = {Neural {{Text Generation}} with {{Unlikelihood Training}}},
  booktitle = {{{ICLR}}},
  author = {Welleck, S. and Kulikov, I. and Roller, S. and Dinan, E. and Cho, K. and Weston, J.},
  date = {2019},
  archiveprefix = {arXiv},
  eprint = {1908.04319v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NELCDAD3/Welleck et al. - 2019 - Neural Text Generation with Unlikelihood Training(2).pdf}
}

@inproceedings{welleck2019NonMonotonicSequentialText,
  title = {Non-{{Monotonic Sequential Text Generation}}},
  booktitle = {{{ICML}}},
  author = {Welleck, Sean and Brantley, Kianté and Daumé III, Hal and Cho, Kyunghyun},
  date = {2019-10-23},
  url = {http://arxiv.org/abs/1902.02192},
  urldate = {2020-08-06},
  abstract = {Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy’s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation.},
  annotation = {61 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1902.02192},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6C96MJBJ/Welleck et al. - 2019 - Non-Monotonic Sequential Text Generation.pdf},
  langid = {english}
}

@inproceedings{welling2011BayesianLearningStochastic,
  title = {Bayesian {{Learning}} via {{Stochastic Gradient Langevin Dynamics}}},
  booktitle = {{{ICML}}},
  author = {Welling, M. and Teh, Y. W.},
  date = {2011},
  pages = {681--688},
  issn = {10495258},
  doi = {10.1515/jip-2012-0071},
  abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an in-built protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  annotation = {30 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1203.5753v5},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YA4YJLB2/Welling, Teh - 2011 - Bayesian Learning via Stochastic Gradient Langevin Dynamics(2).pdf},
  isbn = {978-1-4503-0619-5},
  keywords = {u}
}

@inproceedings{wen2015SemanticallyConditionedLSTMbased,
  title = {Semantically {{Conditioned LSTM}}-Based {{Natural Language Generation}} for {{Spoken Dialogue Systems}}},
  booktitle = {{{EMNLP}}},
  author = {Wen, T. and Gasic, M. and Mrksic, N. and Su, P. and Vandyke, D. and Young, S.},
  date = {2015},
  url = {http://arxiv.org/abs/1508.01745},
  abstract = {Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.},
  annotation = {623 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1508.01745},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2U9HSC84/Wen et al. - 2015 - Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems(2).pdf}
}

@inproceedings{wen2017MultiHorizonQuantileRecurrent,
  title = {A {{Multi}}-{{Horizon Quantile Recurrent Forecaster}}},
  booktitle = {{{NeurIPS}}},
  author = {Wen, R. and Torkkola, K. and Narayanaswamy, B. and Madeka, D.},
  date = {2017},
  url = {http://arxiv.org/abs/1711.11053},
  abstract = {We propose a framework for general probabilistic multi-step time series regression. Specifically, we exploit the expressiveness and temporal nature of Sequence-to-Sequence Neural Networks (e.g. recurrent and convolutional structures), the nonparametric nature of Quantile Regression and the efficiency of Direct Multi-Horizon Forecasting. A new training scheme, *forking-sequences*, is designed for sequential nets to boost stability and performance. We show that the approach accommodates both temporal and static covariates, learning across multiple related series, shifting seasonality, future planned event spikes and cold-starts in real life large-scale forecasting. The performance of the framework is demonstrated in an application to predict the future demand of items sold on Amazon.com, and in a public probabilistic forecasting competition to predict electricity price and load.},
  annotation = {80 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1711.11053},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GYS6CGTT/Wen et al. - 2017 - A Multi-Horizon Quantile Recurrent Forecaster(2).pdf}
}

@article{weston2013ConnectingLanguageKnowledge,
  title = {Connecting {{Language}} and {{Knowledge Bases}} with {{Embedding Models}} for {{Relation Extraction}}},
  author = {Weston, J. and Bordes, A. and Yakhnenko, O. and Usunier, N.},
  date = {2013},
  journaltitle = {Computational Linguistics},
  url = {http://arxiv.org/abs/1307.7973},
  abstract = {This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on two scoring functions that operate by learning low-dimensional embeddings of words and of entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over existing methods that rely on text features alone.},
  annotation = {185 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1307.7973},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/URVK2GN7/Weston et al. - 2013 - Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction(2).pdf},
  isbn = {9781937284978}
}

@inproceedings{weston2015MemoryNetworks,
  title = {Memory Networks},
  booktitle = {{{ICLR}}},
  author = {Weston, J. and Chopra, S. and Bordes, A.},
  date = {2015},
  pages = {1--15},
  issn = {1098-7576},
  doi = {v0},
  abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term mem- ory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chainingmultiple supporting sentences to an- swer questions that require understanding the intension of verbs.},
  archiveprefix = {arXiv},
  eprint = {9377276},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3EE782JP/Weston, Chopra, Bordes - 2015 - Memory networks(2).pdf},
  isbn = {978-1-4244-6917-8},
  keywords = {u}
}

@report{white2016ComputationalLinkingTheory,
  title = {Computational {{Linking Theory}}},
  author = {White, A. S. and Reisinger, D. and Rudinger, R. and Rawlins, K. and Van Durme, B.},
  date = {2016},
  url = {http://arxiv.org/abs/1610.02544},
  abstract = {A linking theory explains how verbs' semantic arguments are mapped to their syntactic arguments---the inverse of the Semantic Role Labeling task from the shallow semantic parsing literature. In this paper, we develop the Computational Linking Theory framework as a method for implementing and testing linking theories proposed in the theoretical literature. We deploy this framework to assess two cross-cutting types of linking theory: local v. global models and categorical v. featural models. To further investigate the behavior of these models, we develop a measurement model in the spirit of previous work in semantic role induction: the Semantic Proto-Role Linking Model. We use this model, which implements a generalization of Dowty's seminal Proto-Role Theory, to induce semantic proto-roles, which we compare to those Dowty proposes.},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1610.02544},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VD6ZKBXT/White et al. - 2016 - Computational Linking Theory(2).pdf},
  number = {1}
}

@inproceedings{white2016UniversalDecompositionalSemantics,
  title = {Universal {{Decompositional Semantics}} on {{Universal Dependencies}}},
  booktitle = {{{EMNLP}}},
  author = {White, A. S. and Reisinger, D. and Sakaguchi, K. and Vieira, T. and Zhang, S. and Rudinger, R. and Rawlins, K. and Van Durme, B.},
  date = {2016},
  pages = {1713--1723},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CVIJ4ZZ5/White et al. - 2016 - Universal Decompositional Semantics on Universal Dependencies(2).pdf}
}

@inproceedings{white2017InferenceEverythingRecasting,
  title = {Inference Is Everything: {{Recasting}} Semantic Resources into a Unified Evaluation Framework},
  booktitle = {{{IJCNLP}}},
  author = {White, A. S. and Rastogi, P. and Duh, K. and Van Durme, B.},
  date = {2017},
  pages = {996--1005},
  abstract = {We propose to unify a variety of existing se-mantic classification tasks, such as seman-tic role labeling, anaphora resolution, and paraphrase detection, under the heading of Recognizing Textual Entailment (RTE). We present a general strategy to automat-ically generate one or more sentential hy-potheses based on an input sentence and pre-existing manual semantic annotations. The resulting suite of datasets enables us to probe a statistical RTE model's perfor-mance on different aspects of semantics. We demonstrate the value of this approach by investigating the behavior of a popular neural network RTE model.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SGJX3JB2/White et al. - 2017 - Inference is everything Recasting semantic resources into a unified evaluation framework(2).pdf},
  keywords = {u}
}

@inproceedings{white2017SemanticProtoroleLinking,
  title = {The Semantic Proto-Role Linking Model},
  booktitle = {{{EACL}}},
  author = {White, A. S. and Rawlins, K. and Van Durme, B.},
  date = {2017},
  volume = {2},
  pages = {92--98},
  abstract = {We propose the semantic proto-role linking model, which jointly induces both predicate-specific semantic roles and predicate-general semantic proto-roles based on semantic proto-role property likelihood judgments. We use this model to empirically evaluate Dowty\{'\}s thematic proto-role linking theory.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/K3LJ239M/White, Rawlins, Van Durme - 2017 - The semantic proto-role linking model(2).pdf},
  isbn = {978-1-5108-3860-4}
}

@inproceedings{white2018LexicosyntacticInferenceNeural,
  title = {Lexicosyntactic {{Inference}} in {{Neural Models}}},
  booktitle = {{{EMNLP}}},
  author = {White, A. S. and Rudinger, R. and Rawlins, K. and Van Durme, B.},
  date = {2018},
  pages = {4717--4724},
  url = {http://arxiv.org/abs/1808.06232},
  abstract = {We investigate neural models' ability to capture lexicosyntactic inferences: inferences triggered by the interaction of lexical and syntactic information. We take the task of event factuality prediction as a case study and build a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts. We use this dataset, which we make publicly available, to probe the behavior of current state-of-the-art neural systems, showing that these systems make certain systematic errors that are clearly visible through the lens of factuality prediction.},
  annotation = {14 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1808.06232},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IFI4L5QN/White et al. - 2018 - Lexicosyntactic Inference in Neural Models(2).pdf},
  keywords = {u},
  number = {i}
}

@inproceedings{wiegreffe2019AttentionNotExplanation,
  title = {Attention Is Not {{Explanation}}},
  booktitle = {{{NAACL}}},
  author = {Wiegreffe, S. and Pinter, Y.},
  date = {2019},
  pages = {3543--3556},
  url = {http://arxiv.org/abs/1908.04626},
  abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
  annotation = {149 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1908.04626},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9PTLEFH5/Wiegreffe, Pinter - 2019 - Attention is not Explanation(2).pdf}
}

@inproceedings{wigren2019ParameterEliminationParticle,
  title = {Parameter Elimination in Particle {{Gibbs}} Sampling},
  booktitle = {{{NeurIPS}}},
  author = {Wigren, A. and Risuleo, R. S. and Murray, L. and Lindsten, F.},
  date = {2019},
  abstract = {Bayesian inference in state-space models is challenging due to high-dimensional state trajectories. A viable approach is particle Markov chain Monte Carlo, combining MCMC and sequential Monte Carlo to form "exact approximations" to otherwise intractable MCMC methods. The performance of the approximation is limited to that of the exact method. We focus on particle Gibbs and particle Gibbs with ancestor sampling, improving their performance beyond that of the underlying Gibbs sampler (which they approximate) by marginalizing out one or more parameters. This is possible when the parameter prior is conjugate to the complete data likelihood. Marginalization yields a non-Markovian model for inference, but we show that, in contrast to the general case, this method still scales linearly in time. While marginalization can be cumbersome to implement, recent advances in probabilistic programming have enabled its automation. We demonstrate how the marginalized methods are viable as efficient inference backends in probabilistic programming, and demonstrate with examples in ecology and epidemiology.},
  archiveprefix = {arXiv},
  eprint = {1910.14145v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U8JPVJY3/Wigren et al. - 2019 - Parameter elimination in particle Gibbs sampling(2).pdf},
  issue = {NeurIPS},
  keywords = {u}
}

@article{williams1999LatentTreeLearning,
  title = {Do Latent Tree Learning Models Identify Meaningful Structure in Sentences ?},
  author = {Williams, A. and Drozdov, A. and Bowman, S. R.},
  date = {1999},
  journaltitle = {TACL},
  archiveprefix = {arXiv},
  eprint = {1709.01121v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S5QTVNUB/Williams, Drozdov, Bowman - 1999 - Do latent tree learning models identify meaningful structure in sentences(2).pdf},
  keywords = {u}
}

@inproceedings{wilson2005RecognizingContextualPolarity,
  title = {Recognizing {{Contextual Polarity}} in {{Phrase}}-{{Level Sentiment Analysis}}},
  booktitle = {{{EMNLP}}-{{HLT}}},
  author = {Wilson, T. and Wiebe, J and Hoffmann, P.},
  date = {2005},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2BDRSNQL/Wilson, Wiebe, Hoffmann - 2005 - Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis(2).pdf}
}

@report{winata2019LowRankMatrixFactorization,
  title = {Low-{{Rank Matrix Factorization}} of {{LSTM}} as {{Effective Model Compression}}},
  author = {Winata, G. I. and Madotto, A. and Shin, J. and Barezi, E. J.},
  date = {2019},
  pages = {1--25},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MZ7X8BVF/Winata et al. - 2019 - Low-Rank Matrix Factorization of LSTM as Effective Model Compression(2).pdf},
  keywords = {review}
}

@inproceedings{winn2018LighterCanStill,
  title = {‘ {{Lighter}} ’ {{Can Still Be Dark}} : {{Modeling Comparative Color Descriptions}}},
  booktitle = {{{ACL}}},
  author = {Winn, O. and Muresan, S.},
  date = {2018},
  pages = {1--6},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3FDSK3PR/Winn, Muresan - 2018 - ‘ Lighter ’ Can Still Be Dark Modeling Comparative Color Descriptions(2).pdf},
  keywords = {u}
}

@inproceedings{wiseman2017ChallengesDatatoDocumentGeneration,
  title = {Challenges in {{Data}}-to-{{Document Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Wiseman, S. and Shieber, S. M. and Rush, A. M.},
  date = {2017},
  url = {http://arxiv.org/abs/1707.08052},
  abstract = {Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.},
  annotation = {233 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1707.08052},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SKAICTCQ/Wiseman, Shieber, Rush - 2017 - Challenges in Data-to-Document Generation(2).pdf}
}

@inproceedings{wiseman2018LearningNeuralTemplates,
  title = {Learning {{Neural Templates}} for {{Text Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Wiseman, S. and Shieber, S. M. and Rush, A. M.},
  date = {2018},
  pages = {1--11},
  doi = {arXiv:1808.10122v1},
  archiveprefix = {arXiv},
  eprint = {1808.10122},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/4C6WIFJM/Wiseman, Shieber, Rush - 2018 - Learning Neural Templates for Text Generation(2).pdf},
  keywords = {u}
}

@inproceedings{wong2006LearningSemanticParsing,
  title = {Learning for {{Semantic Parsing}} with {{Statistical Machine Translation}}},
  booktitle = {{{HLT}}},
  author = {Wong, Y. W. and Mooney, R. J.},
  date = {2006},
  pages = {439--446},
  doi = {10.3115/1220835.1220891},
  url = {http://www.aclweb.org/anthology/N/N06/N06-1056},
  abstract = {We present a novel statistical approach to semantic parsing, WASP, for construct-ing a complete, formal meaning represen-tation of a sentence. A semantic parser is learned given a set of sentences anno-tated with their correct meaning represen-tations. The main innovation of WASP is its use of state-of-the-art statistical ma-chine translation techniques. A word alignment model is used for lexical acqui-sition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods re-quiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.},
  annotation = {272 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/X9K4XB43/Wong, Mooney - 2006 - Learning for Semantic Parsing with Statistical Machine Translation(2).pdf},
  issue = {June}
}

@inproceedings{wong2007GenerationInvertingSemantic,
  title = {Generation by {{Inverting}} a {{Semantic Parser That Uses Statistical Machine Translation}}},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Wong, Y. W. and Mooney, R. J.},
  date = {2007},
  pages = {172--179},
  abstract = {This paper explores the use of statistical machine translation (SMT) methods for tactical natural language generation. We present results on using phrase-based SMT for learning to map meaning representations to natural language. Improved results are obtained by inverting a semantic parser that uses SMT methods to map sentences into meaning representations. Finally, we show that hybridizing these two approaches results in still more accurate generation systems. Automatic and human evaluation of generated sentences are presented across two domains and four languages. © 2007 Association for Computational Linguistics.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SDASNKT9/Wong, Mooney - 2007 - Generation by Inverting a Semantic Parser That Uses Statistical Machine Translation(2).pdf},
  issue = {April}
}

@inproceedings{wong2008LearningSynchronousGrammars,
  title = {Learning {{Synchronous Grammars}} for {{Semantic Parsing}} with {{Lambda Calculus}}},
  booktitle = {{{ICTAI}}},
  author = {Wong, Y. W. and Mooney, R. J.},
  date = {2008},
  volume = {2},
  pages = {135--142},
  issn = {10823409},
  doi = {10.1109/ICTAI.2008.96},
  abstract = {We formulate semantic parsing as a parsing problem on a synchronous context free grammar (SCFG) which is automatically built on the corpus of natural language sentences and the representation of semantic outputs. We then present an online learning fr...},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SB8CT6MD/Wong, Mooney - 2008 - Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus(2).pdf},
  isbn = {978-0-7695-3440-4},
  issue = {June}
}

@inproceedings{wu2017DynamicWindowNeural,
  title = {A {{Dynamic Window Neural Network}} for {{CCG Supertagging}}},
  booktitle = {{{AAAI}}},
  author = {Wu, H. and Zhang, J. and Zong, C.},
  date = {2017},
  pages = {3337--3343},
  abstract = {Combinatory Category Grammar (CCG) supertag-ging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. How-ever, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window ap-proach, which can be treated as an attention mech-anism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words di-rectly, which is superior to the regular dropout on word embeddings. We use this approach to demon-strate the state-of-the-art CCG supertagging perfor-mance on the standard test set.},
  archiveprefix = {arXiv},
  eprint = {1610.02749},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QEWFTGX5/Wu, Zhang, Zong - 2017 - A Dynamic Window Neural Network for CCG Supertagging(2).pdf}
}

@inproceedings{xia2019SyntaxAwareNeuralSemantic,
  title = {Syntax-{{Aware Neural Semantic Role Labeling}}},
  booktitle = {{{AAAI}}},
  author = {Xia, Qingrong and Li, Zhenghua and Zhang, Min and Zhang, Meishan and Fu, Guohong and Wang, Rui and Si, Luo},
  date = {2019},
  pages = {9},
  abstract = {Semantic role labeling (SRL), also known as shallow semantic parsing, is an important yet challenging task in NLP. Motivated by the close correlation between syntactic and semantic structures, traditional discrete-feature-based SRL approaches make heavy use of syntactic features. In contrast, deep-neural-network-based approaches usually encode the input sentence as a word sequence without considering the syntactic structures. In this work, we investigate several previous approaches for encoding syntactic trees, and make a thorough study on whether extra syntax-aware representations are beneficial for neural SRL models. Experiments on the benchmark CoNLL-2005 dataset show that syntax-aware SRL approaches can effectively improve performance over a strong baseline with external word representations from ELMo. With the extra syntax-aware representations, our approaches achieve new state-of-the-art 85.6 F1 (single model) and 86.6 F1 (ensemble) on the test data, outperforming the corresponding strong baselines with ELMo by 0.8 and 1.0, respectively. Detailed error analysis are conducted to gain more insights on the investigated approaches.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9YB2PSGP/Xia et al. - Syntax-Aware Neural Semantic Role Labeling.pdf},
  langid = {english}
}

@report{xia2020CGBERTConditionalText,
  title = {{{CG}}-{{BERT}}: {{Conditional Text Generation}} with {{BERT}} for {{Generalized Few}}-Shot {{Intent Detection}}},
  author = {Xia, C. and Zhang, C. and Nguyen, H. and Zhang, J. and Yu, P.},
  date = {2020},
  pages = {1--10},
  abstract = {In this paper, we formulate a more realistic and difficult problem setup for the intent detection task in natural language understanding, namely Generalized Few-Shot Intent Detection (GFSID). GFSID aims to discriminate a joint label space consisting of both existing intents which have enough labeled data and novel intents which only have a few examples for each class. To approach this problem, we propose a novel model, Conditional Text Generation with BERT (CG-BERT). CG-BERT effectively leverages a large pre-trained language model to generate text conditioned on the intent label. By modeling the utterance distribution with variational inference, CG-BERT can generate diverse utterances for the novel intents even with only a few utterances available. Experimental results show that CG-BERT achieves state-of-the-art performance on the GFSID task with 1-shot and 5-shot settings on two real-world datasets.},
  archiveprefix = {arXiv},
  eprint = {2004.01881},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9BAT4Z38/Xia et al. - 2020 - CG-BERT Conditional Text Generation with BERT for Generalized Few-shot Intent Detection(2).pdf},
  keywords = {review}
}

@inproceedings{xia2020IncrementalNeuralCoreference,
  title = {Incremental {{Neural Coreference Resolution}} in {{Constant Memory}}},
  booktitle = {{{EMNLP}}},
  author = {Xia, Patrick and Sedoc, João and Van Durme, Benjamin},
  date = {2020},
  pages = {8617--8624},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.695},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.695},
  urldate = {2021-02-09},
  abstract = {We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our endto-end algorithm proposes and scores each mention span against explicit entity representations created from the earlier document context (if any). These spans are then used to update the entity’s representations before being forgotten; we only retain a fixed set of salient entities throughout the document. In this work, we successfully convert a highperforming model (Joshi et al., 2020), asymptotically reducing its memory usage to constant space with only a 0.3\% relative loss in F1 on OntoNotes 5.0.},
  annotation = {1 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2JHB7IW6/Xia et al. - 2020 - Incremental Neural Coreference Resolution in Const.pdf},
  langid = {english}
}

@report{xia2020RecentIdeasContextualized,
  title = {Recent {{Ideas}} in {{Contextualized Encoders}} and {{Representations}}},
  author = {Xia, P. and Van Durme, B.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZMVPKNGC/Xia, Van Durme - 2020 - Recent Ideas in Contextualized Encoders and Representations(2).pdf},
  keywords = {u}
}

@report{xia2021LOMELargeOntology,
  title = {{{LOME}}: {{Large Ontology Multilingual Extraction}}},
  author = {Xia, Patrick and Qin, Guanghui and Vashishtha, Siddharth and Chen, Yunmo and Chen, Tongfei and May, Chandler and Harman, Craig and Rawlins, Kyle and White, Aaron Steven and Van Durme, Benjamin},
  date = {2021},
  pages = {9},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DPVPH6ZT/Xia et al. - LOME Large Ontology Multilingual Extraction.pdf},
  langid = {english}
}

@inproceedings{xiao2017LearningConditionalGenerative,
  title = {Learning {{Conditional Generative Models}} for {{Temporal Point Processes}}},
  booktitle = {{{AAAI}}},
  author = {Xiao, S. and Xu, H. and Yan, J. and Farajtabar, M. and Yang, X. and Song, L. and Zha, H.},
  date = {2017},
  pages = {6302--6309},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/69ZUT8LW/Xiao et al. - 2017 - Learning Conditional Generative Models for Temporal Point Processes(2).pdf},
  keywords = {u}
}

@inproceedings{xiao2017WassersteinLearningDeep,
  title = {Wasserstein {{Learning}} of {{Deep Generative Point Process Models}}},
  booktitle = {{{NeurIPS}}},
  author = {Xiao, S. and Farajtabar, M. and Ye, X. and Yan, J. and Song, L. and Zha, H.},
  date = {2017},
  issn = {10495258},
  url = {http://arxiv.org/abs/1705.08051},
  abstract = {Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.},
  annotation = {76 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1705.08051},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7DJVCKW4/Xiao et al. - 2017 - Wasserstein Learning of Deep Generative Point Process Models(2).pdf}
}

@inproceedings{xie2013SemanticFramesPredict,
  title = {Semantic {{Frames}} to {{Predict Stock Price Movement}}},
  booktitle = {{{ACL}}},
  author = {Xie, B. and Passonneau, R. J. and Wu, L.},
  date = {2013},
  pages = {873--883},
  abstract = {Semantic frames are a rich linguistic re-source. There has been much work on semantic frame parsers, but less that applies them to general NLP problems. We address a task to predict change in stock price from financial news. Seman-tic frames help to generalize from spe-cific sentences to scenarios, and to de-tect the (positive or negative) roles of spe-cific companies. We introduce a novel tree representation, and use it to train predic-tive models with tree kernels using sup-port vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that fea-tures derived from semantic frame pars-ing have significantly better performance across years on the polarity task.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/U8U4FVY7/Xie, Passonneau, Wu - 2013 - Semantic Frames to Predict Stock Price Movement(2).pdf},
  isbn = {978-1-937284-50-3}
}

@inproceedings{xie2017DiverseNeuralNetwork,
  title = {Diverse Neural Network Learns True Target Functions},
  booktitle = {{{AISTATS}}},
  author = {Xie, B. and Liang, Y. and Song, L.},
  date = {2017},
  abstract = {Neural networks are a powerful class of functions that can be trained with simple gradient descent to achieve state-of-the-art performance on a variety of applications. Despite their practical success, there is a paucity of results that provide theoretical guarantees on why they are so effective. Lying in the center of the problem is the difficulty of analyzing the non-convex loss function with potentially numerous local minima and saddle points. Can neural networks corresponding to the stationary points of the loss function learn the true target function? If yes, what are the key factors contributing to such nice optimization properties? In this paper, we answer these questions by analyzing one-hidden-layer neural networks with ReLU activation, and show that despite the non-convexity, neural networks with diverse units have no spurious local minima. We bypass the non-convexity issue by directly analyzing the first order optimality condition, and show that the loss can be made arbitrarily small if the minimum singular value of the “extended feature matrix” is large enough. We make novel use of techniques from kernel methods and geometric discrepancy, and identify a new relation linking the smallest singular value to the spectrum of a kernel function associated with the activation function and to the diversity of the units. Our results also suggest a novel regularization function to promote unit diversity for potentially better generalization.},
  archiveprefix = {arXiv},
  eprint = {1611.03131},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KD5F4WF2/Xie, Liang, Song - 2017 - Diverse neural network learns true target functions(2).pdf},
  keywords = {u}
}

@inproceedings{xie2020DifferentiableTopkOperator,
  title = {Differentiable {{Top}}-k {{Operator}} with {{Optimal Transport}}},
  booktitle = {{{NeurIPS}}},
  author = {Xie, Yujia and Dai, Hanjun and Chen, Minshuo and Dai, Bo and Zhao, Tuo and Zha, Hongyuan and Wei, Wei and Pfister, Tomas},
  date = {2020-02-18},
  url = {http://arxiv.org/abs/2002.06504},
  urldate = {2021-02-24},
  abstract = {The top-k operation, i.e., finding the k largest or smallest elements from a collection of scores, is an important model component, which is widely used in information retrieval, machine learning, and data mining. However, if the top-k operation is implemented in an algorithmic way, e.g., using bubble algorithm, the resulting model cannot be trained in an end-to-end way using prevalent gradient descent algorithms. This is because these implementations typically involve swapping indices, whose gradient cannot be computed. Moreover, the corresponding mapping from the input scores to the indicator vector of whether this element belongs to the top-k set is essentially discontinuous. To address the issue, we propose a smoothed approximation, namely the SOFT (Scalable Optimal transport-based diFferenTiable) top-k operator. Specifically, our SOFT top-k operator approximates the output of the top-k operation as the solution of an Entropic Optimal Transport (EOT) problem. The gradient of the SOFT operator can then be efficiently approximated based on the optimality conditions of EOT problem. We apply the proposed operator to the k-nearest neighbors and beam search algorithms, and demonstrate improved performance.},
  annotation = {5 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2002.06504},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C4EJYD9R/Xie et al. - 2020 - Differentiable Top-k Operator with Optimal Transpo.pdf;/home/hiaoxui/.local/share/zotero_files/storage/9QP3I95X/2002.html}
}

@inproceedings{xiong2017DynamicCoattentionNetworks,
  title = {Dynamic {{Coattention Networks For Question Answering}}},
  booktitle = {{{ICLR}}},
  author = {Xiong, C. and Zhong, V. and Socher, R.},
  date = {2017},
  url = {http://arxiv.org/abs/1611.01604},
  abstract = {Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0\% F1 to 75.9\%, while a DCN ensemble obtains 80.4\% F1.},
  annotation = {487 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1611.01604},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IF3TGKAC/Xiong, Zhong, Socher - 2017 - Dynamic Coattention Networks For Question Answering(2).pdf}
}

@inproceedings{xu2018StockMovementPrediction,
  title = {Stock {{Movement Prediction}} from {{Tweets}} and {{Historical Prices}}},
  booktitle = {{{ACL}}},
  author = {Xu, Y. and Cohen, S. B.},
  date = {2018},
  abstract = {Stock movement prediction is a challeng-ing problem: the market is highly stochas-tic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly ex-ploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with tempo-ral auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement predic-tion dataset which we collected. 1},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/II9K8V24/Xu, Cohen - 2018 - Stock Movement Prediction from Tweets and Historical Prices(2).pdf}
}

@inproceedings{xu2019HowPowerfulAre,
  title = {How Powerful Are Graph Neural Networks?},
  booktitle = {{{ICLR}}},
  author = {Xu, K. and Jegelka, S. and Hu, W. and Leskovec, J.},
  date = {2019},
  pages = {1--17},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archiveprefix = {arXiv},
  eprint = {1810.00826},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/H55SVB4V/Xu et al. - 2019 - How powerful are graph neural networks(2).pdf},
  keywords = {u}
}

@online{xu2019SphericalLatentSpaces,
  title = {Spherical {{Latent Spaces}} for {{Stable Variational Autoencoders}}},
  author = {Xu, J. and Durrett, G.},
  date = {2019},
  pages = {4503--4513},
  doi = {10.18653/v1/d18-1480},
  abstract = {A hallmark of variational autoencoders (VAEs) for text processing is their combination of powerful encoder-decoder models, such as LSTMs, with simple latent distributions, typically multivariate Gaussians. These models pose a difficult optimization problem: there is an especially bad local optimum where the variational posterior always equals the prior and the model does not use the latent variable at all, a kind of "collapse" which is encouraged by the KL divergence term of the objective. In this work, we experiment with another choice of latent distribution, namely the von Mises-Fisher (vMF) distribution, which places mass on the surface of the unit hypersphere. With this choice of prior and posterior, the KL divergence term now only depends on the variance of the vMF distribution, giving us the ability to treat it as a fixed hyperparameter. We show that doing so not only averts the KL collapse, but consistently gives better likelihoods than Gaussians across a range of modeling conditions, including recurrent language modeling and bag-of-words document modeling. An analysis of the properties of our vMF representations shows that they learn richer and more nuanced structures in their latent representations than their Gaussian counterparts.},
  annotation = {95 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1808.10805},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MMQMJXQC/Xu, Durrett - 2019 - Spherical Latent Spaces for Stable Variational Autoencoders(2).pdf},
  keywords = {u}
}

@article{xue2005PennChineseTreeBank,
  title = {The {{Penn Chinese TreeBank}}: {{Phrase}} Structure Annotation of a Large Corpus},
  author = {Xue, N. and Xia, F. and Chiou, F. D. and Palmer, M.},
  date = {2005},
  journaltitle = {Natural Language Engineering},
  volume = {11},
  pages = {207--238},
  issn = {13513249},
  doi = {10.1017/S135132490400364X},
  abstract = {With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with different segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are difficult. As a first step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The first two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking efforts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality.},
  annotation = {263 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E7CFTHG4/Xue et al. - 2005 - The Penn Chinese TreeBank Phrase structure annotation of a large corpus(2).pdf},
  isbn = {1469-8110},
  number = {2}
}

@inproceedings{xue2007TappingImplicitInformation,
  title = {Tapping the Implicit Information for the {{PS}} to {{DS}} Conversion of the {{Chinese Treebank}}},
  booktitle = {Treebanks and {{Linguistic Theories}}},
  author = {Xue, N.},
  date = {2007},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QA8KA6SU/Xue - 2007 - Tapping the implicit information for the PS to DS conversion of the Chinese Treebank(2).pdf}
}

@inproceedings{yadav2018SurveyRecentAdvances,
  title = {A {{Survey}} on {{Recent Advances}} in {{Named Entity Recognition}} from {{Deep Learning}} Models},
  booktitle = {{{COLING}}},
  author = {Yadav, Vikas and Bethard, Steven},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1910.11470},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3PLZZW67/Yadav and Bethard - 2019 - A Survey on Recent Advances in Named Entity Recogn.pdf},
  langid = {english}
}

@inproceedings{yang2015HumorRecognitionHumor,
  title = {Humor {{Recognition}} and {{Humor Anchor Extraction}}},
  booktitle = {{{EMNLP}}},
  author = {Yang, D. and Lavie, A. and Dyer, C. and Hovy, E.},
  date = {2015},
  pages = {2367--2376},
  abstract = {Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge. In this work, we first identify several semantic structures behind humor and design sets of features for each structure, and next employ a com- putational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/R6EB32AM/Yang et al. - 2015 - Humor Recognition and Humor Anchor Extraction(2).pdf},
  isbn = {978-1-941643-32-7},
  issue = {September}
}

@inproceedings{yang2015NetworkRepresentationLearning,
  title = {Network Representation Learning with Rich Text Information},
  booktitle = {{{IJCAI}}},
  author = {Yang, C. and Liu, Z. and Zhao, D. and Sun, M. and Chang, E. Y.},
  date = {2015},
  pages = {2111--2117},
  issn = {10450823},
  abstract = {Representation learning has shown its effectiveness in many tasks such as image classification and text mining. Network representation learning aims at learning distributed vector representation for each vertex in a network, which is also increasingly recognized as an important aspect for network analysis. Most network representation learning methods investigate network structures for learning. In reality, network vertices contain rich information (such as text), which cannot be well applied with algorithmic frameworks of typical representation learning methods. By proving that DeepWalk, a state-of-the-art network representation method, is actually equivalent to matrix factorization (MF), we propose text-associated DeepWalk (TADW). TADW incorporates text features of vertices into network representation learning under the framework of matrix factorization. We evaluate our method and various baseline methods by applying them to the task of multi-class classification of vertices. The experimental results show that, our method outperforms other baselines on all three datasets, especially when networks are noisy and training ratio is small. The source code of this paper can be obtained from https://github.com/ albertyang33/TADW},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/B8ZYRQJJ/Yang et al. - 2015 - Network representation learning with rich text information(2).pdf},
  isbn = {978-1-57735-738-4},
  keywords = {u}
}

@inproceedings{yang2015WikiQAChallengeDataset,
  title = {{{WikiQA}} : {{A Challenge Dataset}} for {{Open}}-{{Domain Question Answering}}},
  booktitle = {{{EMNLP}}},
  author = {Yang, Y. and Yih, W. and Meek, C.},
  date = {2015},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/C6S57CIM/Yang, Yih, Meek - 2015 - WikiQA A Challenge Dataset for Open-Domain Question Answering(2).pdf}
}

@inproceedings{yang2017JointSequentialRelational,
  title = {A Joint Sequential and Relational Model for Frame-Semantic Parsing},
  booktitle = {{{EMNLP}}},
  author = {Yang, B. and Mitchell, T. M.},
  date = {2017},
  pages = {1247--1256},
  abstract = {We introduce a new method for frame-semantic parsing that significantly improves the prior state of the art. Our model leverages the advantages of a deep bidirectional LSTM network which predicts semantic role labels word by word and a relational network which predicts semantic roles for individual text expressions in relation to a predicate. The two networks are integrated into a single model via knowledge distillation, and a unified graphical model is employed to jointly decode frames and semantic roles during inference. Experiments on the standard FrameNet data show that our model significantly outperforms existing neural and non-neural approaches, achieving a 5.7 F1 gain over the current state of the art, for full frame structure extraction.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VK88TAHC/Yang, Mitchell - 2017 - A joint sequential and relational model for frame-semantic parsing(2).pdf},
  isbn = {978-1-945626-83-8}
}

@inproceedings{yang2017LieAccessNeuralTuring,
  title = {Lie-{{Access Neural Turing Machines}}},
  booktitle = {{{ICLR}}},
  author = {Yang, G. and Rush, A. M.},
  date = {2017},
  url = {http://arxiv.org/abs/1611.02854},
  abstract = {External neural memory structures have recently become a popular tool for algorithmic deep learning (Graves et al. 2014, Weston et al. 2014). These models generally utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the storage necessary for computational tasks. In this work, we argue that these neural memory systems lack specific structure important for relative indexing, and propose an alternative model, Lie-access memory, that is explicitly designed for the neural setting. In this paradigm, memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and memory access is performed by linear smoothing in key space. We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintaining differentiability. To experiment with this approach, we implement a simplified Lie-access neural Turing machine (LANTM) with different Lie groups. We find that this approach is able to perform well on a range of algorithmic tasks.},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1611.02854},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/TSMLHHF6/Yang, Rush - 2017 - Lie-Access Neural Turing Machines(2).pdf},
  keywords = {u}
}

@inproceedings{yang2018BreakingSoftmaxBottleneck,
  title = {Breaking the {{Softmax Bottleneck}}: {{A High}}-{{Rank RNN Language Model}}},
  booktitle = {{{ICLR}}},
  author = {Yang, Z. and Dai, Z and Salakhutdinov, R. and Cohen, W. W.},
  date = {2018},
  pages = {1--13},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/CVUIWM3J/Yang et al. - 2018 - Breaking the Softmax Bottleneck A High-Rank RNN Language Model(2).pdf}
}

@inproceedings{yang2019DeepReinforcedSequencetoSet,
  title = {A {{Deep Reinforced Sequence}}-to-{{Set Model}} for {{Multi}}-{{Label Classification}}},
  booktitle = {{{ACL}}},
  author = {Yang, P. and Luo, F. and Ma, S. and Lin, J. and Sun, X.},
  date = {2019},
  pages = {5252--5258},
  doi = {10.18653/v1/p19-1518},
  abstract = {Multi-label text classification (MLTC) aims to assign multiple labels to each sample in the dataset. The labels usually have internal correlations. However, traditional methods tend to ignore the correlations between labels. In order to capture the correlations between labels, the sequence-to-sequence (Seq2Seq) model views the MLTC task as a sequence generation problem, which achieves excellent performance on this task. However, the Seq2Seq model is not suitable for the MLTC task in essence. The reason is that it requires humans to predefine the order of the output labels, while some of the output labels in the MLTC task are essentially an unordered set rather than an ordered sequence. This conflicts with the strict requirement of the Seq2Seq model for the label order. In this paper, we propose a novel sequence-to-set framework utilizing deep reinforcement learning, which not only captures the correlations between labels, but also reduces the dependence on the label order. Extensive experimental results show that our proposed method outperforms the competitive baselines by a large margin.},
  annotation = {9 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1809.03118},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HMRJVXVU/Yang et al. - 2019 - A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification(2).pdf}
}

@inproceedings{yang2019ReadAttendComment,
  title = {Read, {{Attend}} and {{Comment}}: {{A Deep Architecture}} for {{Automatic News Comment Generation}}},
  booktitle = {{{EMNLP}}},
  author = {Yang, Z. and Xu, C. and Wu, W. and Li, Z.},
  date = {2019},
  url = {http://arxiv.org/abs/1909.11974},
  abstract = {Automatic news comment generation is beneficial for real applications but has not attracted enough attention from the research community. In this paper, we propose a "read-attend-comment" procedure for news comment generation and formalize the procedure with a reading network and a generation network. The reading network comprehends a news article and distills some important points from it, then the generation network creates a comment by attending to the extracted discrete points and the news title. We optimize the model in an end-to-end manner by maximizing a variational lower bound of the true objective using the back-propagation algorithm. Experimental results on two public datasets indicate that our model can significantly outperform existing methods in terms of both automatic evaluation and human judgment.},
  annotation = {6 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1909.11974},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D2RY27EI/Yang et al. - 2019 - Read, Attend and Comment A Deep Architecture for Automatic News Comment Generation(2).pdf}
}

@inproceedings{yang2019XLNetGeneralizedAutoregressive,
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  booktitle = {{{NeurIPS}}},
  author = {Yang, Z. and Dai, Z. and Yang, Y. and Carbonell, J. and Salakhutdinov, R. and Le, Q. V.},
  date = {2019},
  archiveprefix = {arXiv},
  eprint = {1906.08237v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/QPQ72KBV/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for Language Understanding(2).pdf}
}

@inproceedings{yao2013AnswerExtractionSequence,
  title = {Answer Extraction as Sequence Tagging with Tree Edit Distance},
  booktitle = {{{NAACL}}-{{HLT}}},
  author = {Yao, X. and Van Durme, B. and Callison-Burch, C. and Clark, P.},
  date = {2013},
  pages = {858--867},
  abstract = {Our goal is to extract answers from pre- retrieved sentences for Question Answering (QA). We construct a linear-chain Conditional Random Field based on pairs of questions and their possible answer sentences, learning the association between questions and answer types. This casts answer extraction as an an- swer sequence tagging problem for the first time, where knowledge of shared structure be- tween question and source sentence is incor- porated through features based on Tree Edit Distance (TED). Our model is free of man- ually created question and answer templates, fast to run (processing 200 QA pairs per sec- ond excluding parsing time), and yields an F1 of 63.3\% on a new public dataset based on prior TREC QA evaluations. The developed system is open-source, and includes an imple- mentation of the TED model that is state of the art in the task of ranking QA pairs.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9NATBXAY/Yao et al. - Answer Extraction as Sequence Tagging with Tree Ed.pdf;/home/hiaoxui/.local/share/zotero_files/storage/A5IPMA76/Yao et al. - 2013 - Answer extraction as sequence tagging with tree edit distance(2).pdf},
  isbn = {978-1-937284-47-3},
  keywords = {u}
}

@inproceedings{yao2014InformationExtractionStructured,
  title = {Information {{Extraction}} over {{Structured Data}}: {{Question Answering}} with {{Freebase}}},
  booktitle = {{{ACL}}},
  author = {Yao, X. and Van Durme, B.},
  date = {2014},
  pages = {956--966},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JVKJ4942/Yao, Van Durme - 2014 - Information Extraction over Structured Data Question Answering with Freebase(2).pdf}
}

@inproceedings{yao2015CompressiveDocumentSummarization,
  title = {Compressive Document Summarization via Sparse Optimization},
  booktitle = {{{IJCAI}}},
  author = {Yao, J. and Wan, X. and Xiao, J.},
  date = {2015},
  pages = {1376--1382},
  issn = {10450823},
  abstract = {In this paper, we formulate a sparse optimization framework for extractive document summarization. The proposed framework has a decomposable con-vex objective function. We derive an efficient ADMM algorithm to solve it. To encourage di-versity in the summaries, we explicitly introduce an additional sentence dissimilarity term in the op-timization framework. We achieve significant im-provement over previous related work under sim-ilar data reconstruction framework. We then gen-eralize our formulation to the case of compressive summarization and derive a block coordinate de-scent algorithm to optimize the objective function. Performance on DUC 2006 and DUC 2007 datasets shows that our compressive summarization results are competitive against the state-of-the-art results while maintaining reasonable readability.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AFSEAEAT/Yao, Wan, Xiao - 2015 - Compressive document summarization via sparse optimization(2).pdf},
  isbn = {978-1-57735-738-4},
  keywords = {u}
}

@inproceedings{yao2017GreedyFlippingConstrained,
  title = {Greedy {{Flipping}} for {{Constrained Word Deletion}}},
  booktitle = {{{AAAI}}},
  author = {Yao, J. and Wan, X.},
  date = {2017},
  pages = {3518--3524},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MFRS2M3P/Yao, Wan - 2017 - Greedy Flipping for Constrained Word Deletion(2).pdf},
  keywords = {u}
}

@inproceedings{yarowsky1995UnsupervisedWordSense,
  title = {Unsupervised Word Sense Disambiguation Rivaling Supervised Methods},
  booktitle = {{{ACL}}},
  author = {Yarowsky, D.},
  date = {1995},
  pages = {189--196},
  issn = {0736587X},
  doi = {10.3115/981658.981684},
  url = {http://portal.acm.org/citation.cfm?doid=981658.981684},
  abstract = {This paper presents an unsupervised learn- ing algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints - that words tend to have one sense per discourse and one sense per collocation - exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96\%.},
  annotation = {2304 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {15003161},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/M4BHV663/Yarowsky - 1995 - Unsupervised word sense disambiguation rivaling supervised methods(2).pdf},
  isbn = {0736-587X}
}

@report{ye2019BPTransformerModellingLongRange,
  title = {{{BP}}-{{Transformer}}: {{Modelling Long}}-{{Range Context}} via {{Binary Partitioning}}},
  shorttitle = {{{BP}}-{{Transformer}}},
  author = {Ye, Zihao and Guo, Qipeng and Gan, Quan and Qiu, Xipeng and Zhang, Zheng},
  date = {2019-11-10},
  url = {http://arxiv.org/abs/1911.04070},
  urldate = {2021-03-28},
  abstract = {The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of self-attention limit its application on long text. In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields \$O(k\textbackslash cdot n\textbackslash log (n/k))\$ connections where \$k\$ is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch.},
  annotation = {15 citations (Semantic Scholar/arXiv) [2021-03-28]},
  archiveprefix = {arXiv},
  eprint = {1911.04070},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/588JPSV6/Ye et al. - 2019 - BP-Transformer Modelling Long-Range Context via B.pdf;/home/hiaoxui/.local/share/zotero_files/storage/YVTDFQGC/1911.html}
}

@inproceedings{yeh2019UnsupervisedSpeechRecognition,
  title = {Unsupervised {{Speech Recognition}} via {{Segmentation Empirical Output Distribution Matching}}},
  booktitle = {{{ICLR}}},
  author = {Yeh, C. and Chen, J. and Yu, C. and Yu, D.},
  date = {2019},
  pages = {1--14},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JHE3JFIX/Yeh et al. - 2019 - Unsupervised Speech Recognition via Segmentation Empirical Output Distribution Matching(2).pdf}
}

@inproceedings{yi2018NeuralSymbolicVQADisentangling,
  title = {Neural-{{Symbolic VQA}}: {{Disentangling Reasoning}} from {{Vision}} and {{Language Understanding}}},
  booktitle = {{{NeurIPS}}},
  author = {Yi, K. and Wu, J. and Gan, C. and Torralba, A. and Kohli, P. and Tenenbaum, J. B.},
  date = {2018},
  issn = {1749-6632},
  doi = {10.1111/j.1749-6632.2009.04729.x},
  url = {http://arxiv.org/abs/1810.02338},
  abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8\% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
  annotation = {7 citations (Semantic Scholar/DOI) [2021-03-26] 157 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {19723035},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/DUND783R/Yi et al. - 2018 - Neural-Symbolic VQA Disentangling Reasoning from Vision and Language Understanding(2).pdf},
  isbn = {978-1-57331-737-5}
}

@inproceedings{yih2015SemanticParsingStaged,
  title = {Semantic {{Parsing}} via {{Staged Query Graph Generation}}: {{Question Answering}} with {{Knowledge Base}}},
  booktitle = {{{ACL}}},
  author = {Yih, W. and Chang, M. and He, X. and Gao, J.},
  date = {2015},
  pages = {1321--1331},
  doi = {10.3115/v1/P15-1128},
  url = {http://aclweb.org/anthology/P15-1128},
  abstract = {We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowl-edge base and can be directly mapped to a logical form. Semantic parsing is re-duced to query graph generation, formu-lated as a staged search problem. Unlike traditional approaches, our method lever-ages the knowledge base in an early stage to prune the search space and thus simpli-fies the semantic matching problem. By applying an advanced entity linking sys-tem and a deep convolutional neural net-work model that matches questions and predicate sequences, our system outper-forms previous methods substantially, and achieves an F 1 measure of 52.5\% on the WEBQUESTIONS dataset.},
  annotation = {438 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/5E79VCFU/Yih et al. - 2015 - Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base(2).pdf},
  isbn = {978-1-941643-72-3},
  keywords = {u}
}

@inproceedings{yin2017SyntacticNeuralModel,
  title = {A {{Syntactic Neural Model}} for {{General}}-{{Purpose Code Generation}}},
  booktitle = {{{ACL}}},
  author = {Yin, P. and Neubig, G.},
  date = {2017},
  pages = {440--450},
  doi = {10.18653/v1/P17-1041},
  url = {http://arxiv.org/abs/1704.01696},
  abstract = {We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.},
  archiveprefix = {arXiv},
  eprint = {1704.01696},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YXS7PPRK/Yin, Neubig - 2017 - A Syntactic Neural Model for General-Purpose Code Generation(2).pdf},
  isbn = {978-1-945626-75-3},
  keywords = {u}
}

@inproceedings{yogatama2017LearningComposeWords,
  title = {Learning to {{Compose Words}} into {{Sentences}} with {{Reinforcement Learning}}},
  booktitle = {{{ICLR}}},
  author = {Yogatama, D. and Blunsom, P. and Dyer, C. and Grefenstette, E. and Ling, W.},
  date = {2017},
  pages = {1--10},
  archiveprefix = {arXiv},
  eprint = {1611.09100v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JP85QGMR/Yogatama et al. - 2017 - Learning to Compose Words into Sentences with Reinforcement Learning(2).pdf},
  keywords = {u}
}

@inproceedings{yu2004IntegrationGroundingLanguage,
  title = {On the {{Integration}} of {{Grounding Language}} and {{Lear}} Ning {{Objects}}},
  booktitle = {{{AAAI}}},
  author = {Yu, C. and Ballard, D.},
  date = {2004},
  pages = {488--493},
  doi = {10.1080/10656210509484979},
  annotation = {28 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/UHCPH4IJ/Yu, Ballard - 2004 - On the Integration of Grounding Language and Lear ning Objects(2).pdf},
  isbn = {0-262-51183-5},
  keywords = {u}
}

@inproceedings{yu2013GroundedLanguageLearning,
  title = {Grounded {{Language Learning}} from {{Video Described}} with {{Sentences}}},
  booktitle = {{{ACL}}},
  author = {Yu, H. and Siskind, J. M.},
  date = {2013},
  pages = {53--63},
  url = {http://www.aclweb.org/anthology/P13-1006},
  abstract = {We present a method that learns repre- sentations for word meanings from short video clips paired with sentences. Un- like prior work on learning language from symbolic input, our input consists of video of people interacting with multiple com- plex objects in outdoor environments. Un- like prior computer-vision approaches that learn from videos with verb labels or im- ages with noun labels, our labels are sen- tences containing nouns, verbs, preposi- tions, adjectives, and adverbs. The cor- respondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts si- multaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video.},
  archiveprefix = {arXiv},
  eprint = {1306.5263},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IJ33Z74H/Yu, Siskind - 2013 - Grounded Language Learning from Video Described with Sentences(2).pdf},
  isbn = {978-1-937284-50-3},
  issue = {August},
  keywords = {u}
}

@report{yu2018LongtermForecastingUsing,
  title = {Long-Term {{Forecasting}} Using {{Tensor}}-{{Train RNNs}}},
  author = {Yu, R. and Zheng, S. and Anandkumar, A. and Yue, Y.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1711.00073v2},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YQMWVYPE/Yu et al. - 2018 - Long-term Forecasting using Tensor-Train RNNs(2).pdf}
}

@report{yu2020FineTuningPretrainedLanguage,
  title = {Fine-{{Tuning Pre}}-Trained {{Language Model}} with {{Weak Supervision}}: {{A Contrastive}}-{{Regularized Self}}-{{Training Approach}}},
  shorttitle = {Fine-{{Tuning Pre}}-Trained {{Language Model}} with {{Weak Supervision}}},
  author = {Yu, Yue and Zuo, Simiao and Jiang, Haoming and Ren, Wendi and Zhao, Tuo and Zhang, Chao},
  date = {2020-10-20},
  url = {http://arxiv.org/abs/2010.07835},
  urldate = {2021-02-24},
  abstract = {Fine-tuned pre-trained language models (LMs) achieve enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the fine-tuning stage. We study the problem of fine-tuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision. To address this problem, we develop a contrastive self-training framework, COSINE, to enable fine-tuning LMs with weak supervision. Underpinned by contrastive regularization and confidence-based reweighting, this contrastive self-training framework can gradually improve model fitting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classification tasks show that our model outperforms the strongest baseline by large margins on 7 benchmarks in 6 tasks, and achieves competitive performance with fully-supervised fine-tuning methods.},
  annotation = {0 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2010.07835},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/N8UE5L7M/Yu et al. - 2020 - Fine-Tuning Pre-trained Language Model with Weak S.pdf;/home/hiaoxui/.local/share/zotero_files/storage/PRWPZX7H/2010.html}
}

@inproceedings{yuan2018MultilingualAnchoringInteractive,
  title = {Multilingual {{Anchoring}}: {{Interactive Topic Modeling}} and {{Alignment Across Languages}}},
  booktitle = {{{NeurIPS}}},
  author = {Yuan, M. and Van Durme, B. and Boyd-Graber, J.},
  date = {2018},
  pages = {8667--8677},
  abstract = {Multilingual topic models can reveal patterns in cross-lingual document collections. However, existing models lack speed and interactivity, which prevents adoption in everyday corpora exploration or quick moving situations (e.g., natural disasters, political instability). First, we propose a multilingual anchoring algorithm that builds an anchor-based topic model for documents in different languages. Then, we incorporate interactivity to develop MTAnchor (Multilingual Topic Anchors), a system that allows users to refine the topic model. We test our algorithms on labeled English, Chinese, and Sinhalese documents. Within minutes, our methods can produce interpretable topics that are useful for specific classification tasks.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9IX6R2FD/Yuan, Van Durme, Boyd-Graber - 2018 - Multilingual Anchoring Interactive Topic Modeling and Alignment Across Languages(2).pdf},
  issue = {NeurIPS},
  keywords = {u}
}

@report{yuan2019InteractiveRefinementCrossLingual,
  title = {Interactive {{Refinement}} of {{Cross}}-{{Lingual Word Embeddings}}},
  author = {Yuan, M. and Zhang, M. and Van Durme, B. and Findlater, L. and Boyd-Graber, J.},
  date = {2019},
  abstract = {Cross-lingual word embeddings transfer knowledge between languages: models trained for a high-resource language can be used in a low-resource language. These embeddings are usually trained on general-purpose corpora but used for a domain-specific task. We introduce CLIME, an interactive system that allows a user to quickly adapt cross-lingual word embeddings for a given classification problem. First, words in the vocabulary are ranked by their salience to the downstream task. Then, salient keywords are displayed on an interface. Users mark the similarity between each keyword and its nearest neighbors in the embedding space. Finally, CLIME updates the embeddings using the annotations. We experiment clime on a cross-lingual text classification benchmark for four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings. CLIME also improves test accuracy faster than an active learning baseline, and a simple combination of CLIME with active learning has the highest test accuracy.},
  archiveprefix = {arXiv},
  eprint = {1911.03070},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/IRPZRN5F/Yuan et al. - 2019 - Interactive Refinement of Cross-Lingual Word Embeddings(2).pdf}
}

@inproceedings{zarriess2016GeneratingColourTerms,
  title = {Towards {{Generating Colour Terms}} for {{Referents}} in {{Photographs}} : {{Prefer}} the {{Expected}} or the {{Unexpected}} ?},
  booktitle = {{{INLG}}},
  author = {Zarrieß, S. and Schlangen, D.},
  date = {2016},
  pages = {246--255},
  abstract = {Colour terms have been a prime phenomenon for studying language grounding, though pre-vious work focussed mostly on descriptions of simple objects or colour swatches. This paper investigates whether colour terms can be learned from more realistic and potentially noisy visual inputs, using a corpus of referring expressions to objects represented as regions in real-world images. We obtain promising re-sults from combining a classifier that grounds colour terms in visual input with a recalibra-tion model that adjusts probability distribu-tions over colour terms according to contex-tual and object-specific preferences.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/KEQ4AWKG/Zarrieß, Schlangen - 2016 - Towards Generating Colour Terms for Referents in Photographs Prefer the Expected or the Unexpected(2).pdf}
}

@inproceedings{zelle1996LearningParseDatabase,
  title = {Learning to {{Parse Database}} Queries Using Inductive Logic Proramming},
  booktitle = {{{AAAI}}},
  author = {Zelle, J. M. and Mooney, R. J.},
  date = {1996},
  pages = {1050--1055},
  abstract = {This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a pre-existing, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AWVMCXVK/Zelle, Mooney - 1996 - Learning to Parse Database queries using inductive logic proramming(2).pdf},
  keywords = {u}
}

@inproceedings{zettlemoyer2007OnlineLearningRelaxed,
  title = {Online Learning of Relaxed {{CCG}} Grammars for Parsing to Logical Form},
  booktitle = {{{EMNLP}}},
  author = {Zettlemoyer, L. S. and Collins, M.},
  date = {2007},
  pages = {678--687},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.71.3926},
  abstract = {We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammarfor example allowing flexible word order, or insertion of lexical items with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86 \% F-measure in recovering fully correct semantic analyses and 95.9\% F-measure by a partial-match criterion, a more than 5 \% improvement over the 90.3\% partial-match figure reported by He and Young (2006).},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VV2MPWNM/Zettlemoyer, Collins - 2007 - Online learning of relaxed CCG grammars for parsing to logical form(2).pdf},
  issue = {June}
}

@inproceedings{zettlemoyer2009LearningContextdependentMappings,
  title = {Learning Context-Dependent Mappings from Sentences to Logical Form},
  booktitle = {{{ACL}}},
  author = {Zettlemoyer, L. S. and Collins, M.},
  date = {2009},
  volume = {2},
  pages = {976},
  doi = {10.3115/1690219.1690283},
  url = {http://portal.acm.org/citation.cfm?doid=1690219.1690283},
  abstract = {We consider the problem of learning context-dependent mappings from sentences to logical form. The training examples are sequences of sentences annotated with lambda-calculus meaning representations. We develop an algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7\% accuracy.},
  annotation = {184 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/LLYAQQ9R/Zettlemoyer, Collins - 2009 - Learning context-dependent mappings from sentences to logical form(2).pdf},
  isbn = {978-1-932432-46-6},
  issue = {August},
  keywords = {u}
}

@inproceedings{zettlemoyer2012LearningMapSentences,
  title = {Learning to {{Map Sentences}} to {{Logical Form}}: {{Structured Classification}} with {{Probabilistic Categorial Grammars}}},
  booktitle = {{{UAI}}},
  author = {Zettlemoyer, L. S. and Collins, M.},
  date = {2012},
  doi = {10.1093/acprof:oso/9780199654680.003.0006},
  url = {http://arxiv.org/abs/1207.1420},
  abstract = {This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.},
  annotation = {772 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1207.1420},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/Q2KLDQ7V/Zettlemoyer, Collins - 2012 - Learning to Map Sentences to Logical Form Structured Classification with Probabilistic Categorial Gramm(2).pdf},
  isbn = {0-9749039-1-4},
  keywords = {u}
}

@report{zettlemoyer2012LearningMapSentencesa,
  title = {Learning to {{Map Sentences}} to {{Logical Form}}: {{Structured Classiﬁcation}} with {{Probabilistic Categorial Grammars}}},
  author = {Zettlemoyer, Luke S and Collins, Michael},
  date = {2012},
  pages = {9},
  abstract = {This paper addresses the problem of mapping natural language sentences to lambda–calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/RB96YR89/Zettlemoyer and Collins - Learning to Map Sentences to Logical Form Structu.pdf},
  keywords = {u},
  langid = {english}
}

@article{zhang2017ConvexifiedConvolutionalNeural,
  title = {Convexified Convolutional Neural Networks},
  author = {Zhang, Y. and Liang, P. and Wainwright, M. J.},
  date = {2017},
  journaltitle = {ICML},
  abstract = {We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented in terms of a low-rank matrix, and the rank constraint can be relaxed so as to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, we find that CCNNs achieve competitive or better performance than CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.},
  archiveprefix = {arXiv},
  eprint = {1609.01000},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZSN72ZAE/Zhang, Liang, Wainwright - 2017 - Convexified convolutional neural networks(2).pdf},
  isbn = {9781510855144},
  keywords = {u}
}

@inproceedings{zhang2017HittingTimeAnalysis,
  title = {A {{Hitting Time Analysis}} of {{Stochastic Gradient Langevin Dynamics}}},
  booktitle = {{{COLT}}},
  author = {Zhang, Y. and Liang, P. and Charikar, M.},
  date = {2017},
  url = {http://arxiv.org/abs/1702.05575},
  abstract = {We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for non-convex optimization. The algorithm performs stochastic gradient descent, where in each step it injects appropriately scaled Gaussian noise to the update. We analyze the algorithm's hitting time to an arbitrary subset of the parameter space. Two results follow from our general theory: First, we prove that for empirical risk minimization, if the empirical risk is point-wise close to the (smooth) population risk, then the algorithm achieves an approximate local minimum of the population risk in polynomial time, escaping suboptimal local minima that only exist in the empirical risk. Second, we show that SGLD improves on one of the best known learnability results for learning linear classifiers under the zero-one loss.},
  annotation = {138 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1702.05575},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YNTLSGT2/Zhang, Liang, Charikar - 2017 - A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics(2).pdf},
  keywords = {u}
}

@inproceedings{zhang2017InterpretableConvolutionalNeural,
  title = {Interpretable {{Convolutional Neural Networks}}},
  booktitle = {{{CVPR}}},
  author = {Zhang, Q. and Wu, Y. N. and Zhu, S.},
  date = {2017},
  url = {http://arxiv.org/abs/1710.00935},
  abstract = {This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of CNNs with different structures. The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision. Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.},
  annotation = {325 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1710.00935},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NCYGGDL7/Zhang, Wu, Zhu - 2017 - Interpretable Convolutional Neural Networks(2).pdf}
}

@inproceedings{zhang2017LearnabilityFullyconnectedNeural,
  title = {On the Learnability of Fully-Connected Neural Networks},
  booktitle = {{{AISTATS}}},
  author = {Zhang, Y. and Lee, J. D. and Wainwright, M. J. and Jordan, M. I.},
  date = {2017},
  abstract = {Despite the empirical success of deep neural networks, there is limited theoretical understanding of the learnability of these models with respect to polynomial-time algorithms. In this paper, we characterize the learnability of fully-connected neural networks via both positive and negative results. We focus on ℓ1-regularized networks, where the ℓ1-norm of the incoming weights of every neuron is assumed to be bounded by a constant B {$>$} 0. Our first result shows that such networks are properly learnable in poly(n, d, exp(1/ϵ2)) time, where n and d are the sample size and the input dimension, and ϵ {$>$} 0 is the gap to optimality. The bound is achieved by repeatedly sampling over a low-dimensional manifold so as to ensure approximate optimality, but avoids the exp(d) cost of exhaustively searching over the parameter space. We also establish a hardness result showing that the exponential dependence on 1/ϵ is unavoidable unless RP = NP. Our second result shows that the exponential dependence on 1/ϵ can be avoided by exploiting the underlying structure of the data distribution. In particular, if the positive and negative examples can be separated with margin γ {$>$} 0 by an unknown neural network, then the network can be learned in poly(n, d, 1/ϵ) time. The bound is achieved by an ensemble method which uses the first algorithm as a weak learner. We further show that the separability assumption can be weakened to tolerate noisy labels. Finally, we show that the exponential dependence on 1/γ is unimprovable under a certain cryptographic assumption.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/7LJYSBCL/Zhang et al. - 2017 - On the learnability of fully-connected neural networks(2).pdf},
  keywords = {u}
}

@inproceedings{zhang2017MacroGrammarsHolistic,
  title = {Macro {{Grammars}} and {{Holistic Triggering}} for {{Efficient Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, Y. and Pasupat, P. and Liang, P.},
  date = {2017},
  url = {http://arxiv.org/abs/1707.07806},
  abstract = {To learn a semantic parser from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing model to improve the state-of-the-art accuracy from 38.7\% to 42.7\%, and then use macro grammars and holistic triggering to achieve an 11x speedup and an accuracy of 43.7\%.},
  annotation = {29 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1707.07806},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GGB5AQDI/Zhang, Pasupat, Liang - 2017 - Macro Grammars and Holistic Triggering for Efficient Semantic Parsing(2).pdf}
}

@article{zhang2017OrdinalCommonsenseInference,
  title = {Ordinal {{Common}}-Sense {{Inference}}},
  author = {Zhang, S. and Rudinger, R. and Duh, K. and Van Durme, B.},
  date = {2017},
  journaltitle = {TACL},
  volume = {5},
  pages = {379--395},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/SH5QPJZD/Zhang et al. - 2017 - Ordinal Common-sense Inference(2).pdf}
}

@inproceedings{zhang2017SemiSupervisedStructuredPrediction,
  title = {Semi-{{Supervised Structured Prediction}} with {{Neural CRF Autoencoder}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, X. and Jiang, Y. and Peng, H. and Tu, K. and Goldwasser, D.},
  date = {2017},
  pages = {1702--1712},
  abstract = {In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning of se-quential structured prediction problems. Our NCRF-AE consists of two parts: an encoder which is a CRF model enhanced by deep neural networks, and a decoder which is a generative model trying to re-construct the input. Our model has a uni-fied structure with different loss functions for labeled and unlabeled data with shared parameters. We developed a variation of the EM algorithm for optimizing both the encoder and the decoder simultaneously by decoupling their parameters. Our ex-perimental results over the Part-of-Speech (POS) tagging task on eight different lan-guages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised sce-narios.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HMYG5DV5/Zhang et al. - 2017 - Semi-Supervised Structured Prediction with Neural CRF Autoencoder(2).pdf},
  keywords = {u}
}

@inproceedings{zhang2017UnderstandingDeepLearning,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  booktitle = {{{ICLR}}},
  author = {Zhang, C. and Bengio, S. and Hardt, M. and Recht, B. and Vinyals, O.},
  date = {2017},
  url = {http://arxiv.org/abs/1611.03530},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  annotation = {2466 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1611.03530},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/JZFMBRHK/Zhang et al. - 2017 - Understanding deep learning requires rethinking generalization(2).pdf},
  keywords = {u}
}

@inproceedings{zhang2018CrosslingualDecompositionalSemantic,
  title = {Cross-Lingual {{Decompositional Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, S. and Ma, X. and Rudinger, R. and Duh, K. and Van Durme, B.},
  date = {2018},
  pages = {1664--1675},
  doi = {10.18653/v1/d18-1194},
  abstract = {We introduce the task of cross-lingual decompositional semantic parsing: mapping content provided in a source language into a decompositional semantic analysis based on a target language. We present: (1) a form of decompositional semantic analysis designed to allow systems to target varying levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score.},
  annotation = {16 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/9X37VHBP/Zhang et al. - 2018 - Cross-lingual Decompositional Semantic Parsing(2).pdf},
  keywords = {u}
}

@inproceedings{zhang2018CrosslingualSemanticParsing,
  title = {Cross-Lingual {{Semantic Parsing}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, S. and Duh, K. and Van Durme, B.},
  date = {2018},
  url = {http://arxiv.org/abs/1804.08037},
  abstract = {We introduce the task of cross-lingual semantic parsing: mapping content provided in a source language into a meaning representation based on a target language. We present: (1) a meaning representation designed to allow systems to target varying levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference meaning representations, (3) an end-to-end model with a novel copy mechanism that supports intrasentential coreference, and (4) an evaluation dataset where experiments show our model outperforms strong baselines by at least 1.18 F1 score.},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1804.08037},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZTJCBQMK/Zhang, Duh, Van Durme - 2018 - Cross-lingual Semantic Parsing(2).pdf}
}

@inproceedings{zhang2019AMRParsingSequencetoGraph,
  title = {{{AMR Parsing}} as {{Sequence}}-to-{{Graph Transduction}}},
  booktitle = {{{ACL}}},
  author = {Zhang, S. and Ma, X. and Duh, K. and Van Durme, B.},
  date = {2019},
  volume = {1},
  pages = {80--94},
  doi = {10.18653/v1/p19-1009},
  annotation = {53 citations (Semantic Scholar/DOI) [2021-03-26]},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GBHMWWBP/Zhang et al. - 2019 - AMR Parsing as Sequence-to-Graph Transduction(2).pdf}
}

@inproceedings{zhang2019BroadCoverageSemanticParsing,
  title = {Broad-{{Coverage Semantic Parsing}} as {{Transduction}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, S. and Ma, X. and Duh, K. and Van Durme, B.},
  date = {2019},
  abstract = {We unify different broad-coverage semantic parsing tasks under a transduction paradigm, and propose an attention-based neural framework that incrementally builds a meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the transducer can be effectively trained without relying on a pre-trained aligner. Experiments conducted on three separate broad-coverage semantic parsing tasks -- AMR, SDP and UCCA -- demonstrate that our attention-based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.},
  archiveprefix = {arXiv},
  eprint = {1909.02607},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/FIEC9233/Zhang et al. - 2019 - Broad-Coverage Semantic Parsing as Transduction(2).pdf}
}

@inproceedings{zhang2019HIBERTDocumentLevel,
  title = {{{HIBERT}}: {{Document Level Pre}}-Training of {{Hierarchical Bidirectional Transformers}} for {{Document Summarization}}},
  shorttitle = {{{HIBERT}}},
  booktitle = {{{ACL}}},
  author = {Zhang, Xingxing and Wei, Furu and Zhou, Ming},
  date = {2019-05-16},
  url = {http://arxiv.org/abs/1905.06566},
  urldate = {2021-03-28},
  abstract = {Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these \textbackslash emph\{inaccurate\} labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders \textbackslash cite\{devlin:2018:arxiv\}, we propose \{\textbackslash sc Hibert\} (as shorthand for \{\textbackslash bf HI\}erachical \{\textbackslash bf B\}idirectional \{\textbackslash bf E\}ncoder \{\textbackslash bf R\}epresentations from \{\textbackslash bf T\}ransformers) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained \{\textbackslash sc Hibert\} to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets.},
  annotation = {98 citations (Semantic Scholar/arXiv) [2021-03-27]},
  archiveprefix = {arXiv},
  eprint = {1905.06566},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/VJXHGKB6/Zhang et al. - 2019 - HIBERT Document Level Pre-training of Hierarchica.pdf;/home/hiaoxui/.local/share/zotero_files/storage/HLKX42B5/1905.html}
}

@inproceedings{zhang2019SyntaxEnhancedSelfAttentionBasedSemantic,
  title = {Syntax-{{Enhanced Self}}-{{Attention}}-{{Based Semantic Role Labeling}}},
  booktitle = {{{EMNLP}}},
  author = {Zhang, Y. and Wang, R. and Si, L.},
  date = {2019},
  pages = {616--626},
  abstract = {As a fundamental NLP task, semantic role labeling (SRL) aims to discover the semantic roles for each predicate within one sentence. This paper investigates how to incorporate syntactic knowledge into the SRL task effectively. We present different approaches of encoding the syntactic information derived from dependency trees of different quality and representations; we propose a syntax-enhanced self-attention model and compare it with other two strong baseline methods; and we conduct experiments with newly published deep contextualized word representations as well. The experiment results demonstrate that with proper incorporation of the high quality syntactic information, our model achieves a new state-of-the-art performance for the Chinese SRL task on the CoNLL-2009 dataset.},
  archiveprefix = {arXiv},
  eprint = {1910.11204},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/E9ACKSBI/Zhang, Wang, Si - 2019 - Syntax-Enhanced Self-Attention-Based Semantic Role Labeling(2).pdf}
}

@report{zhang2020GlobalAttentionName,
  title = {Global {{Attention}} for {{Name Tagging}}},
  author = {Zhang, Boliang and Whitehead, Spencer and Huang, Lifu and Ji, Heng},
  date = {2020-10-19},
  url = {http://arxiv.org/abs/2010.09270},
  urldate = {2020-10-21},
  abstract = {Many name tagging approaches use local contextual information with much success, but fail when the local context is ambiguous or limited. We present a new framework to improve name tagging by utilizing local, documentlevel, and corpus-level contextual information. We retrieve document-level context from other sentences within the same document and corpus-level context from sentences in other topically related documents. We propose a model that learns to incorporate documentlevel and corpus-level contextual information alongside local contextual information via global attentions, which dynamically weight their respective contextual information, and gating mechanisms, which determine the influence of this information. Extensive experiments on benchmark datasets show the effectiveness of our approach, which achieves state-of-the-art results for Dutch, German, and Spanish on the CoNLL-2002 and CoNLL2003 datasets.1.},
  annotation = {11 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2010.09270},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2RJY57EG/Zhang et al. - 2020 - Global Attention for Name Tagging.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{zhang2020TwoStepApproachImplicit,
  title = {A {{Two}}-{{Step Approach}} for {{Implicit Event Argument Detection}}},
  booktitle = {{{ACL}}},
  author = {Zhang, Zhisong and Kong, Xiang and Liu, Zhengzhong and Ma, Xuezhe and Hovy, Eduard},
  date = {2020},
  pages = {7479--7485},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.667},
  url = {https://www.aclweb.org/anthology/2020.acl-main.667},
  urldate = {2021-03-29},
  annotation = {0 citations (Semantic Scholar/DOI) [2021-03-29]},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/GYDIS8UT/Zhang et al. - 2020 - A Two-Step Approach for Implicit Event Argument De.pdf},
  langid = {english}
}

@inproceedings{zhang2021MetaLabelCorrection,
  title = {Meta {{Label Correction}} for {{Noisy Label Learning}}},
  booktitle = {{{AAAI}}},
  author = {Zhang, G. and Awadallah, H. A. and Dumais, S.},
  date = {2021},
  pages = {9},
  abstract = {Leveraging weak or noisy supervision for building effective machine learning models has long been an important research problem. Its importance has further increased recently due to the growing need for large-scale datasets to train deep learning models. Weak or noisy supervision could originate from multiple sources including non-expert annotators or automatic labeling based on heuristics or user interaction signals. There is an extensive amount of previous work focusing on leveraging noisy labels. Most notably, recent work has shown impressive gains by using a meta-learned instance re-weighting approach where a meta-learning framework is used to assign instance weights to noisy labels. In this paper, we extend this approach via posing the problem as a label correction problem within a meta-learning framework. We view the label correction procedure as a meta-process and propose a new meta-learning based framework termed MLC (Meta Label Correction) for learning with noisy labels. Specifically, a label correction network is adopted as a meta-model to produce corrected labels for noisy labels while the main model is trained to leverage the corrected labels. Both models are jointly trained by solving a bi-level optimization problem. We run extensive experiments with different label noise levels and types on both image recognition and text classification tasks. We compare the re-weighing and correction approaches showing that the correction framing addresses some of the limitations of re-weighting. We also show that the proposed MLC approach outperforms previous methods in both image and language tasks.},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/J4HLRI3X/PRELIMINARY VERSION DO NOT CITE.pdf},
  langid = {english}
}

@inproceedings{zhao2014TypeDrivenIncrementalSemantic,
  title = {Type-{{Driven Incremental Semantic Parsing}} with {{Polymorphism}}},
  booktitle = {{{NAACL}}},
  author = {Zhao, K. and Huang, L.},
  date = {2014},
  volume = {0041},
  pages = {1416--1421},
  url = {http://arxiv.org/abs/1411.5379},
  abstract = {Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GeoQuery, Jobs and Atis domains.},
  annotation = {44 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1411.5379},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/S7KYPE36/Zhao, Huang - 2014 - Type-Driven Incremental Semantic Parsing with Polymorphism(2).pdf},
  isbn = {978-1-941643-49-5},
  keywords = {u}
}

@inproceedings{zhao2017MenAlsoShopping,
  title = {Men {{Also Like Shopping}}: {{Reducing Gender Bias Amplification}} Using {{Corpus}}-Level {{Constraints}}},
  booktitle = {{{EMNLP}}},
  author = {Zhao, J. and Wang, T. and Yatskar, M. and Ordonez, V. and Chang, K.},
  date = {2017},
  url = {http://arxiv.org/abs/1707.09457},
  abstract = {Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68\% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5\% and 40.5\% for multilabel classification and visual semantic role labeling, respectively.},
  annotation = {308 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1707.09457},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HXYGM5DU/Zhao et al. - 2017 - Men Also Like Shopping Reducing Gender Bias Amplification using Corpus-level Constraints(2).pdf}
}

@inproceedings{zhao2020ComplexFactoidQuestion,
  title = {Complex {{Factoid Question Answering}} with a {{Free}}-{{Text Knowledge Graph}}},
  booktitle = {{{WWW}}},
  author = {Zhao, C. and Xiong, C. and Qian, X. and Boyd-Graber, J.},
  date = {2020},
  pages = {1205--1216},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/6YQC7EAW/Zhao et al. - 2020 - Complex Factoid Question Answering with a Free-Text Knowledge Graph(2).pdf},
  isbn = {978-1-4503-7023-3},
  keywords = {u}
}

@report{zhao2021CalibrateUseImproving,
  title = {Calibrate {{Before Use}}: {{Improving Few}}-{{Shot Performance}} of {{Language Models}}},
  shorttitle = {Calibrate {{Before Use}}},
  author = {Zhao, Tony Z. and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  date = {2021-02-18},
  url = {http://arxiv.org/abs/2102.09690},
  urldate = {2021-03-03},
  abstract = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0\% absolute) and reduces variance across different choices of the prompt.},
  annotation = {1 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2102.09690},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MVYUZM92/Zhao et al. - 2021 - Calibrate Before Use Improving Few-Shot Performan.pdf;/home/hiaoxui/.local/share/zotero_files/storage/4WP5KJGV/2102.html}
}

@inproceedings{zheng2013DeepLearningChinese,
  title = {Deep {{Learning}} for {{Chinese Word Segmentation}} and {{POS Tagging}}.},
  booktitle = {{{EMNLP}}},
  author = {Zheng, X. and Chen, H. and Xu, T.},
  date = {2013},
  issn = {15324435},
  doi = {10.1162/153244303322533223},
  abstract = {This study explores the feasibility of perform-ing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representa-tion of Chinese characters, and use these im-proved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-the-art performance with minimal computational cost. We also describe a perceptron-style al-gorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.},
  annotation = {4776 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {18244602},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/APJXJ3YD/Zheng, Chen, Xu - 2013 - Deep Learning for Chinese Word Segmentation and POS Tagging(2).pdf},
  isbn = {978-1-937284-97-8}
}

@report{zhong2017Seq2SQLGeneratingStructured,
  title = {{{Seq2SQL}}: {{Generating Structured Queries}} from {{Natural Language}} Using {{Reinforcement Learning}}},
  author = {Zhong, V. and Xiong, C. and Socher, R.},
  date = {2017},
  pages = {1--12},
  url = {http://arxiv.org/abs/1709.00103},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
  annotation = {3 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1709.00103},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/ZLXJ8XVK/Zhong, Xiong, Socher - 2017 - Seq2SQL Generating Structured Queries from Natural Language using Reinforcement Learning(2).pdf}
}

@inproceedings{zhong2018LegalJudgmentPrediction,
  title = {Legal {{Judgment Prediction}} via {{Topological Learning}}},
  booktitle = {{{EMNLP}}},
  author = {Zhong, Haoxi and Guo, Zhipeng and Tu, Cunchao and Xiao, Chaojun and Liu, Zhiyuan and Sun, Maosong},
  date = {2018},
  pages = {3540--3549},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1390},
  url = {http://aclweb.org/anthology/D18-1390},
  urldate = {2020-07-24},
  abstract = {Legal Judgment Prediction (LJP) aims to predict the judgment result based on the facts of a case and becomes a promising application of artificial intelligence techniques in the legal field. In real-world scenarios, legal judgment usually consists of multiple subtasks, such as the decisions of applicable law articles, charges, fines, and the term of penalty. Moreover, there exist topological dependencies among these subtasks. While most existing works only focus on a specific subtask of judgment prediction and ignore the dependencies among subtasks, we formalize the dependencies among subtasks as a Directed Acyclic Graph (DAG) and propose a topological multi-task learning framework, TOPJUDGE, which incorporates multiple subtasks and DAG dependencies into judgment prediction. We conduct experiments on several realworld large-scale datasets of criminal cases in the civil law system. Experimental results show that our model achieves consistent and significant improvements over baselines on all judgment prediction tasks. The source code can be obtained from https://github. com/thunlp/TopJudge.},
  annotation = {70 citations (Semantic Scholar/DOI) [2021-03-26]},
  eventtitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/D6VPDP3I/Zhong et al. - 2018 - Legal Judgment Prediction via Topological Learning.pdf},
  keywords = {u},
  langid = {english}
}

@article{zhong2020GroundedAdaptationZeroshot,
  title = {Grounded {{Adaptation}} for {{Zero}}-Shot {{Executable Semantic Parsing}}},
  author = {Zhong, Victor and Lewis, Mike and Wang, Sida I. and Zettlemoyer, Luke},
  date = {2020-09-16},
  journaltitle = {EMNLP},
  url = {http://arxiv.org/abs/2009.07396},
  urldate = {2020-09-27},
  abstract = {We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycleconsistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose inputoutput consistency are verified. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms dataaugmentation in the training environment, performance increases with the amount of GAZPsynthesized data, and cycle-consistency is central to successful adaptation.},
  annotation = {2 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {2009.07396},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/HMMG6QC5/Zhong et al. - 2020 - Grounded Adaptation for Zero-shot Executable Seman.pdf},
  keywords = {u},
  langid = {english}
}

@inproceedings{zhou2013LearningTriggeringKernels,
  title = {Learning {{Triggering Kernels}} for {{Multi}}-Dimensional {{Hawkes Processes}}},
  booktitle = {{{ICML}}},
  author = {Zhou, K. and Zha, H. and Song, L.},
  date = {2013},
  volume = {28},
  pages = {1301--1309},
  url = {http://jmlr.org/proceedings/papers/v28/zhou13.html},
  abstract = {Abstract How does the activity of one person affect that of another person? Does the strength of influence remain periodic or decay exponentially over time? In this paper, we study these critical questions in social network analysis quantitatively under the framework of multi- ...\textbackslash n},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/YNE99U42/Zhou, Zha, Song - 2013 - Learning Triggering Kernels for Multi-dimensional Hawkes Processes(2).pdf}
}

@inproceedings{zhou2015EndtoendLearningSemantic,
  title = {End-to-End Learning of Semantic Role Labeling Using Recurrent Neural Networks},
  booktitle = {{{ACL}}},
  author = {Zhou, J. and Xu, W.},
  date = {2015},
  pages = {1127--1137},
  issn = {0147-006X},
  doi = {10.3115/v1/P15-1109},
  url = {http://www.aclweb.org/anthology/P15-1109},
  abstract = {Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems. To this date, most of the successful SRL systems were built on top of some form of parsing results (Koomen et al., 2005; Palmer et al., 2010; Pradhan et al., 2013), where pre-defined feature templates over the syntactic structure are used. The attempts of building an end-to-end SRL learning system without using parsing were less successful (Collobert et al., 2011). In this work, we propose to use deep bi-directional recurrent network as an end-to-end system for SRL. We take only original text information as input feature, without using any syntactic knowledge. The proposed algorithm for semantic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F1 score of 81.07. This result outperforms the previous state-of-the-art system from the combination of different parsing trees or models. We also obtained the same conclusion with F1 = 81.27 on CoNLL- 2012 shared task. As a result of simplicity, our model is also computationally efficient that the parsing speed is 6.7k tokens per second. Our analysis shows that our model is better at handling longer sentences than traditional models. And the latent variables of our model implicitly capture the syntactic structure of a sentence.},
  annotation = {273 citations (Semantic Scholar/DOI) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {25246403},
  eprinttype = {pmid},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3P6WMZLM/Zhou, Xu - 2015 - End-to-end learning of semantic role labeling using recurrent neural networks(2).pdf},
  isbn = {978-1-941643-72-3}
}

@inproceedings{zhou2017MultispaceVariationalEncoderDecoders,
  title = {Multi-Space {{Variational Encoder}}-{{Decoders}} for {{Semi}}-Supervised {{Labeled Sequence Transduction}}},
  booktitle = {{{ACL}}},
  author = {Zhou, C. and Neubig, G.},
  date = {2017},
  doi = {10.18653/v1/P17-1029},
  url = {http://arxiv.org/abs/1704.01691},
  abstract = {Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.},
  annotation = {49 citations (Semantic Scholar/DOI) [2021-03-26] 49 citations (Semantic Scholar/arXiv) [2021-03-26]},
  archiveprefix = {arXiv},
  eprint = {1704.01691},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/NYVC59PZ/Zhou, Neubig - 2017 - Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction(2).pdf},
  isbn = {978-1-945626-75-3},
  keywords = {u}
}

@inproceedings{zhou2019GoingVacationTakes,
  title = {"{{Going}} on a Vacation" Takes Longer than "{{Going}} for a Walk": {{A Study}} of {{Temporal Commonsense Understanding}}},
  booktitle = {{{EMNLP}}},
  author = {Zhou, B. and Khashabi, D. and Ning, Q. and Roth, D.},
  date = {2019},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3ETBWGEK/Zhou et al. - 2019 - Going on a vacation takes longer than Going for a walk A Study of Temporal Commonsense Understanding(2).pdf}
}

@report{zhou2020LIMITBERTLinguisticsInformed,
  title = {{{LIMIT}}-{{BERT}} : {{Linguistics Informed Multi}}-{{Task BERT}}},
  author = {Zhou, J. and Zhang, Z. and Zhao, H.},
  date = {2020},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/2ZGHGZPB/Zhou, Zhang, Zhao - 2020 - LIMIT-BERT Linguistics Informed Multi-Task BERT(2).pdf},
  keywords = {review}
}

@inproceedings{zhou2020ParsingAllSyntax,
  title = {Parsing {{All}}: {{Syntax}} and {{Semantics}}, {{Dependencies}} and {{Spans}}},
  shorttitle = {Parsing {{All}}},
  booktitle = {{{EMNLP}}},
  author = {Zhou, Junru and Li, Zuchao and Zhao, Hai},
  date = {2020},
  abstract = {Both syntactic and semantic structures are key linguistic contextual clues, in which parsing the latter has been well shown beneficial from parsing the former. However, few works ever made an attempt to let semantic parsing help syntactic parsing. As linguistic representation formalisms, both syntax and semantics may be represented in either span (constituent/phrase) or dependency, on both of which joint learning was also seldom explored. In this paper, we propose a novel joint model of syntactic and semantic parsing on both span and dependency representations, which incorporates syntactic information effectively in the encoder of neural network and benefits from two representation formalisms in a uniform way. The experiments show that semantics and syntax can benefit each other by optimizing joint objectives. Our single model achieves new state-of-the-art or competitive results on both span and dependency semantic parsing on Propbank benchmarks and both dependency and constituent syntactic parsing on Penn Treebank.},
  archiveprefix = {arXiv},
  eprint = {1908.11522},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/3ZV6637X/Zhou et al. - 2020 - Parsing All Syntax and Semantics, Dependencies an.pdf},
  langid = {english}
}

@inproceedings{zhuang2018NoiseContrastiveEstimation,
  title = {Noise {{Contrastive Estimation}} and {{Negative Sampling}} for {{Conditional Models}}: {{Consistency}} and {{Statistical Efficiency}}},
  booktitle = {{{EMNLP}}},
  author = {Zhuang, M. and Collins, M.},
  date = {2018},
  archiveprefix = {arXiv},
  eprint = {1809.01812v1},
  eprinttype = {arxiv},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/MV8444NA/Zhuang, Collins - 2018 - Noise Contrastive Estimation and Negative Sampling for Conditional Models Consistency and Statistical Effici(2).pdf},
  keywords = {u}
}

@inproceedings{zou2013BilingualWordEmbeddings,
  title = {Bilingual {{Word Embeddings}} for {{Phrase}}-{{Based Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Zou, W. Y. and Socher, R. and Cer, D. and Manning, C. D.},
  date = {2013},
  url = {http://www.aclweb.org/anthology/D13-1141},
  file = {/home/hiaoxui/.local/share/zotero_files/storage/AW27YM53/Zou et al. - 2013 - Bilingual Word Embeddings for Phrase-Based Machine Translation(2).pdf}
}


