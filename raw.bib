@inproceedings{Riloff2013,
abstract = {A common form of sarcasm on Twitter con- sists of a positive sentiment contrasted with a negative situation. For example, many sarcas- tic tweets include a positive sentiment, such as “love” or “enjoy”, followed by an expression that describes an undesirable activity or state (e.g., “taking exams” or “being ignored”).We have developed a sarcasm recognizer to iden- tify this type of sarcasm in tweets. We present a novel bootstrapping algorithmthat automati- cally learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. We show that identifying contrast- ing contexts using the phrases learned through bootstrapping yields improved recall for sar- casm recognition. 1},
author = {Riloff, E. and Qadir, A. and Surve, P. and Silva, L. D. and Gilbert, N. and Huang, R.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Sarcasm as Contrast between a Positive Sentiment and Negative Situation.pdf:pdf},
isbn = {978-193728497-8},
title = {{Sarcasm as Contrast between a Positive Sentiment and Negative Situation}},
url = {papers3://publication/uuid/9E81B37F-37A5-421F-A49F-5F1A107CC460},
year = {2013}
}
@inproceedings{Zheng2013,
abstract = {This study explores the feasibility of perform-ing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representa-tion of Chinese characters, and use these im-proved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-the-art performance with minimal computational cost. We also describe a perceptron-style al-gorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Zheng, X. and Chen, H. and Xu, T.},
booktitle = {EMNLP},
doi = {10.1162/153244303322533223},
eprint = {1301.3781},
file = {:home/hiaoxui/papers/EMNLP/Deep Learning for Chinese Word Segmentation and POS Tagging.pdf:pdf},
isbn = {9781937284978},
issn = {15324435},
pmid = {18244602},
title = {{Deep Learning for Chinese Word Segmentation and POS Tagging.}},
year = {2013}
}
@inproceedings{Zou2013,
author = {Zou, W. Y. and Socher, R. and Cer, D. and Manning, C. D.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Bilingual Word Embeddings for Phrase-Based Machine Translation.pdf:pdf},
title = {{Bilingual Word Embeddings for Phrase-Based Machine Translation}},
url = {http://www.aclweb.org/anthology/D13-1141},
year = {2013}
}
@inproceedings{Kalchbrenner2013,
abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is {\textgreater} 43{\%} lower than that of state-of-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and mean- ing of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
author = {Kalchbrenner, N. and Blunsom, P.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Recurrent Continuous Translation Models.pdf:pdf},
title = {{Recurrent Continuous Translation Models}},
url = {https://www.aclweb.org/anthology/D13-1176},
year = {2013}
}
@inproceedings{Yang2015,
author = {Yang, Y. and Yih, W. and Meek, C.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/WikiQA A Challenge Dataset for Open-Domain Question Answering.pdf:pdf},
title = {{WikiQA : A Challenge Dataset for Open-Domain Question Answering}},
year = {2015}
}
@inproceedings{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, J. and Socher, R. and Manning, C. D.},
booktitle = {EMNLP},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/hiaoxui/papers/EMNLP/GloVe Global Vector for Word Representation.pdf:pdf},
isbn = {9781937284961},
issn = {00047554},
pmid = {1710995},
title = {{GloVe: Global Vector for Word Representation}},
url = {http://nlp.},
year = {2014}
}
@inproceedings{Kong2014,
abstract = {We describe a new dependency parser for English tweets, TWEEBOPARSER. The parser builds on several contributions: new syntactic annotations for a corpus of tweets (TWEEBANK), with conventions informed by the domain; adaptations to a statistical parsing algorithm; and a new approach to exploiting out-of-domain Penn Treebank data. Our experiments show that the parser achieves over 80{\%} unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contribu- tions.},
author = {Kong, L. and Schneider, N. and Swayamdipta, S. and Bhatia, A. and Dyer, C. and Smith, N. A.},
booktitle = {EMNLP},
doi = {10.3115/v1/d14-1108},
file = {:home/hiaoxui/papers/EMNLP/A Dependency Parser for Tweets.pdf:pdf},
title = {{A Dependency Parser for Tweets}},
year = {2014}
}
@inproceedings{Bordes2014,
abstract = {This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a competitive benchmark of the literature.},
archivePrefix = {arXiv},
arxivId = {1406.3676},
author = {Bordes, A. and Chopra, S. and Weston, J.},
booktitle = {EMNLP},
eprint = {1406.3676},
file = {:home/hiaoxui/papers/EMNLP/Question Answering with Subgraph Embeddings.pdf:pdf},
title = {{Question Answering with Subgraph Embeddings}},
url = {http://arxiv.org/abs/1406.3676},
year = {2014}
}
@inproceedings{Cheng2013,
abstract = {Wikification, commonly referred to as Disam- biguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific correspondingWikipedia pages. Previous ap- proaches to D2W focused on the use of lo- cal and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these meth- ods fail (often, embarrassingly), when some level of text understanding is needed to sup- port Wikification. In this paper we introduce a novel approach to Wikification by incorpo- rating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Lin- ear Programming (ILP) formulation of Wik- ification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant im- provements in bothWikification and the TAC Entity Linking task.},
author = {Cheng, X. and Roth, D.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Relational Inference for Wikiﬁcation.pdf:pdf},
title = {{Relational Inference for Wikiﬁcation}},
url = {cogcomp.cs.illinois.edu/papers/ChengRo13.pdf‎},
year = {2013}
}
@inproceedings{Tang2015,
author = {Tang, D. and Qin, B. and Liu, T.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.pdf:pdf},
title = {{Document Modeling with Gated Recurrent Neural Network for Sentiment Classification}},
year = {2015}
}
@inproceedings{Wang2014,
abstract = {We examine the embedding approach to reason new relational facts from a large- scale knowledge graph and a text corpus. We propose a novel method of jointly em- bedding entities and words into the same continuous vector space. The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus. Entity names and Wikipedia an- chors are utilized to align the embeddings of entities and words in the same space. Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text. Particularly, jointly embedding enables the prediction of facts containing entities out of the knowledge graph, which cannot be han- dled by previous embedding methods. At the same time, concerning the quality of the word embeddings, experiments on the analogical reasoning task showthat jointly embedding is comparable to or slightly better than word2vec (Skip-Gram).},
author = {Wang, Z. and Zhang, J. and Feng, J. and Chen, Z.},
booktitle = {EMNLP},
doi = {10.3115/v1/d14-1167},
file = {:home/hiaoxui/papers/EMNLP/Knowledge Graph and Text Jointly Embedding.pdf:pdf},
title = {{Knowledge Graph and Text Jointly Embedding}},
year = {2014}
}
@inproceedings{Neelakantan2014,
abstract = {There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Neelakantan, A. and Shankar, J. and Passos, A. and McCallum, A.},
booktitle = {EMNLP},
eprint = {1504.06654},
file = {:home/hiaoxui/papers/EMNLP/Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space.pdf:pdf},
title = {{Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space}},
url = {http://arxiv.org/abs/1504.06654},
year = {2014}
}
@inproceedings{Iyyer2014,
abstract = {Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations. These methods are ineffective when question text contains very few individual words (e.g., named entities) that are indicative of the answer. We introduce a recursive neural network (rnn) model that can reason over such input by modeling textual compositionality. We apply our model, qanta, to a dataset of questions from a trivia competition called quiz bowl. Unlike previous rnn models, qanta learns word and phrase-level representations that combine across sentences to reason about entities. The model outperforms multiple baselines and, when combined with information retrieval methods, rivals the best human players.},
author = {Iyyer, M. and Boyd-Graber, J. and Claudino, L. and Socher, R. and {Daum{\'{e}} III}, H.},
booktitle = {EMNLP},
doi = {10.3115/v1/d14-1070},
file = {:home/hiaoxui/papers/EMNLP/A Neural Network for Factoid Question Answering over Paragraphs.pdf:pdf},
title = {{A Neural Network for Factoid Question Answering over Paragraphs}},
year = {2014}
}
@inproceedings{Poria2015,
author = {Poria, S. and Cambria, E. and Gelbukh, A.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-Level Multimodal Sentiment Analysis.pdf:pdf},
keywords = {ek laboratory,nanyang technological,singapore,university},
title = {{Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-Level Multimodal Sentiment Analysis}},
year = {2015}
}
@inproceedings{Cho2014a,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, K. and van Merrienboer, B. and Gulcehre, C. and Bahdanau, D. and Bougares, F. and Schwenk, H. and Bengio, Y.},
booktitle = {EMNLP},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:home/hiaoxui/papers/EMNLP/Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@inproceedings{Vaswani2013,
abstract = {We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu.},
author = {Vaswani, A. and Zhao, Y. and Fossum, V. and Chiang, D.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Decoding with Large-Scale Neural Language Models Improves Translation.pdf:pdf},
title = {{Decoding with Large-Scale Neural Language Models Improves Translation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.385.7636{\&}rep=rep1{\&}type=pdf},
year = {2013}
}
@inproceedings{Ballesteros2015,
abstract = {We present extensions to a continuous-state dependency parsing method that makes it applicable to morphologically rich languages. Starting with a high-performance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.},
archivePrefix = {arXiv},
arxivId = {1508.00657},
author = {Ballesteros, M. and Dyer, C. and Smith, N. A.},
booktitle = {EMNLP},
eprint = {1508.00657},
file = {:home/hiaoxui/papers/EMNLP/Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs.pdf:pdf},
title = {{Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs}},
url = {http://arxiv.org/abs/1508.00657},
year = {2015}
}
@inproceedings{Socher2013,
abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Socher, R. and Perelygin, A. and Wu, J. Y. and Chuang, J. and Manning, C. D. and Ng, A. Y. and Potts, C.},
booktitle = {EMNLP},
doi = {10.1371/journal.pone.0073791},
eprint = {1512.03385},
file = {:home/hiaoxui/papers/EMNLP/Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.pdf:pdf},
isbn = {9781937284978},
issn = {1932-6203},
pmid = {24086296},
title = {{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}},
year = {2013}
}
@inproceedings{Bowman2015a,
abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
archivePrefix = {arXiv},
arxivId = {1508.05326},
author = {Bowman, S. R. and Angeli, G. and Potts, C. and Manning, C. D.},
booktitle = {EMNLP},
eprint = {1508.05326},
file = {:home/hiaoxui/papers/EMNLP/A large annotated corpus for learning natural language inference.pdf:pdf},
title = {{A large annotated corpus for learning natural language inference}},
url = {http://arxiv.org/abs/1508.05326},
year = {2015}
}
@inproceedings{Irsoy2014,
abstract = {Recurrent neural networks (RNNs) are con-nectionist models of sequential data that are naturally applicable to the analysis of natural language. Recently, " depth in space " — as an orthogonal notion to " depth in time " — in RNNs has been investigated by stacking mul-tiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architec-ture. In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task. Experimental results show that deep, narrow RNNs outperform traditional shallow, wide RNNs with the same number of parame-ters. Furthermore, our approach outperforms previous CRF-based baselines, including the state-of-the-art semi-Markov CRF model, and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF, as well as without the standard layer-by-layer pre-training typically required of RNN architectures.},
author = {Irsoy, O. and Cardie, C.},
booktitle = {EMNLP},
doi = {10.3115/v1/d14-1080},
file = {:home/hiaoxui/papers/EMNLP/Opinion Mining with Deep Recurrent Neural Networks.pdf:pdf},
title = {{Opinion Mining with Deep Recurrent Neural Networks}},
year = {2014}
}
@inproceedings{Kazemzadeh2014,
abstract = {In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.},
author = {Kazemzadeh, S. and Ordonez, V. and Matten, M. and Berg, T.},
booktitle = {EMNLP},
doi = {10.3115/v1/d14-1086},
file = {:home/hiaoxui/papers/EMNLP/ReferItGame Referring to Objects in Photographs of Natural Scenes.pdf:pdf},
title = {{ReferItGame: Referring to Objects in Photographs of Natural Scenes}},
year = {2014}
}
@inproceedings{Chen2014,
abstract = {Most word representation methods assume that each word owns a single semantic vec-tor. This is usually problematic because lexical ambiguity is ubiquitous, which is also the problem to be resolved by word sense disambiguation. In this paper, we present a unified model for joint word sense representation and disambiguation, which will assign distinct representation-s for each word sense. 1 The basic idea is that both word sense representation (WS-R) and word sense disambiguation (WS-D) will benefit from each other: (1) high-quality WSR will capture rich informa-tion about words and senses, which should be helpful for WSD, and (2) high-quality WSD will provide reliable disambiguat-ed corpora for learning better sense rep-resentations. Experimental results show that, our model improves the performance of contextual word similarity compared to existing WSR methods, outperforms state-of-the-art supervised methods on domain-specific WSD, and achieves competitive performance on coarse-grained all-words WSD.},
author = {Chen, X. and Liu, Z. and Sun, M.},
booktitle = {EMNLP},
doi = {10.3115/v1/d14-1110},
file = {:home/hiaoxui/papers/EMNLP/A Unified Model for Word Sense Representation and Disambiguation.pdf:pdf},
title = {{A Unified Model for Word Sense Representation and Disambiguation}},
year = {2014}
}
@inproceedings{Chiticariu2013,
abstract = {Abstract The rise of “Big Data” analytics over unstructured text has led to renewed interest in information extraction (IE). We surveyed the landscape of IE technologies and identified a major disconnect between industry and academia: while rule - based IE dominates the ...},
author = {Chiticariu, L. and Li, Y. and Reiss, F. R.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Rule-based Information Extraction is Dead! Long Live Rule-based Information Extraction Systems!.pdf:pdf},
title = {{Rule-based Information Extraction is Dead! Long Live Rule-based Information Extraction Systems!}},
url = {https://www.aclweb.org/anthology/D13-1079},
year = {2013}
}
@inproceedings{Kwiatkowski2013,
abstract = {We consider the challenge of learning seman- tic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to rep- resent in a fixed target ontology. For ex- ample, even simple phrases such as ‘daugh- ter' and ‘number of people living in' can- not be directly represented in Freebase, whose ontology instead encodes facts about gen- der, parenthood, and population. In this pa- per, we introduce a new semantic parsing ap- proach that learns to resolve such ontologi- cal mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logical- form meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art per- formance on two benchmark semantic parsing datasets, including a nine point accuracy im- provement on a recent Freebase QA corpus.},
author = {Kwiatkowski, T. and Choi, E. and Artzi, Y. and Zettlemoyer, L. S.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Scaling Semantic Parsers with On-the-fly Ontology Matching.pdf:pdf},
isbn = {9781937284978},
number = {October},
pages = {1545--1556},
title = {{Scaling Semantic Parsers with On-the-fly Ontology Matching}},
url = {http://www.aclweb.org/anthology/D13-1161},
year = {2013}
}
@inproceedings{Schnabel2015,
author = {Schnabel, T. and Labutov, I. and Mimno, D.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Evaluation methods for unsupervised word embeddings.pdf:pdf},
title = {{Evaluation methods for unsupervised word embeddings}},
year = {2015}
}
@misc{Kim2014,
abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
archivePrefix = {arXiv},
arxivId = {1408.5882},
author = {Kim, Y.},
booktitle = {EMNLP},
eprint = {1408.5882},
file = {:home/hiaoxui/papers/EMNLP/Convolutional Neural Networks for Sentence Classification.pdf:pdf},
title = {{Convolutional Neural Networks for Sentence Classification}},
url = {http://arxiv.org/abs/1408.5882},
year = {2014}
}
@inproceedings{Berant2013,
abstract = {In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai andYates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.},
author = {Berant, J. and Chou, A. and Frostig, R. and Liang, P.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Semantic Parsing on Freebase from Question-Answer Pairs.pdf:pdf},
isbn = {9781937284978},
number = {October},
pages = {1533--1544},
pmid = {2216100},
title = {{Semantic Parsing on Freebase from Question-Answer Pairs}},
url = {https://www.aclweb.org/anthology/D/D13/D13-1160.pdf{\%}5Cnhttp://www.samstyle.tk/index.pl/00/http/nlp.stanford.edu/pubs/semparseEMNLP13.pdf},
year = {2013}
}
@inproceedings{Chen2014a,
abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2{\%} improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2{\%} unlabeled attachment score on the English Penn Treebank.},
author = {Chen, D. and Manning, C. D.},
booktitle = {EMNLP},
doi = {10.3115/v1/d14-1082},
file = {:home/hiaoxui/papers/EMNLP/A Fast and Accurate Dependency Parser using Neural Networks.pdf:pdf},
title = {{A Fast and Accurate Dependency Parser using Neural Networks}},
year = {2014}
}
@inproceedings{Mihalcea2007,
abstract = {This paper investigates the problem of automatic humour recognition, and provides and in-depth analysis of two of the most frequently observed features of humorous text: human-centeredness and negative polarity. Through experiments performed on two collections of humorous texts, we show that these properties of verbal humour are consistent across different data sets. Springet-Verlag Berlin Heidelberg 2001.},
author = {Mihalcea, R. and Pulman, S.},
booktitle = {CICLing},
file = {:home/hiaoxui/papers/CICLing/Characterizing Humour An Exploration of Features in Humorous Texts.pdf:pdf},
isbn = {354070938X},
issn = {03029743},
pages = {337--347},
title = {{Characterizing Humour : An Exploration of Features in Humorous Texts}},
year = {2007}
}
@article{Zhang2017,
author = {Zhang, S. and Rudinger, R. and Duh, K. and van Durme, B.},
file = {:home/hiaoxui/papers/TACL/Ordinal Common-sense Inference.pdf:pdf},
journal = {TACL},
pages = {379--395},
title = {{Ordinal Common-sense Inference}},
volume = {5},
year = {2017}
}
@inproceedings{Clark2018,
abstract = {We introduce an approach to neural text gen-eration that explicitly represents entities men-tioned in the text. Entity representations are vectors that are updated as the text proceeds; they are designed specifically for narrative text like fiction or news stories. Our experiments demonstrate that modeling entities offers a benefit in two automatic evaluations: mention generation (in which a model chooses which entity to mention next and which words to use in the mention) and selection between a correct next sentence and a distractor from later in the same story. We also conduct a human evalu-ation on automatically generated text in story contexts; this study supports our emphasis on entities and suggests directions for further re-search.},
author = {Clark, E. and Ji, Y. and Smith, N. A.},
booktitle = {NAACL},
doi = {10.18653/v1/n18-1204},
file = {:home/hiaoxui/papers/NAACL/Neural Text Generation in Stories Using Entity Representations as Context.pdf:pdf},
keywords = {Outstanding},
mendeley-tags = {Outstanding},
pages = {2250--2260},
title = {{Neural Text Generation in Stories Using Entity Representations as Context}},
year = {2018}
}
@inproceedings{Alvarez-Melis2017,
author = {Alvarez-Melis, D. and Jaakkola, T. S.},
booktitle = {ICLR},
file = {:home/hiaoxui/papers/ICLR/Tree-structure decoding with Doubly-Recurrent Neural Networks.pdf:pdf},
title = {{Tree-structure decoding with Doubly-Recurrent Neural Networks}},
year = {2017}
}
@inproceedings{Cai2013,
author = {Cai, Q. and Yates, A.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Large-scale Semantic Parsing via Schema Matching and Lexicon Extension.pdf:pdf},
isbn = {9781937284503},
keywords = {semantic parsing},
mendeley-tags = {semantic parsing},
pages = {423--433},
title = {{Large-scale Semantic Parsing via Schema Matching and Lexicon Extension}},
url = {http://www.aclweb.org/anthology/P13-1042{\%}5Cnhttp://www.cis.temple.edu/{~}yates/papers/textual-schema-matching.pdf},
year = {2013}
}
@inproceedings{Collell2018,
abstract = {Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space. The predicted vectors are then used to perform e.g., retrieval or labeling. Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors. However, whether this is achieved has not been investigated yet. Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue. In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions. Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors. In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.},
archivePrefix = {arXiv},
arxivId = {1805.07616},
author = {Collell, G. and Moens, M.},
booktitle = {ACL},
doi = {arXiv:1805.07616v1},
eprint = {1805.07616},
file = {:home/hiaoxui/papers/ACL/Do Neural Network Cross-Modal Mappings Really Bridge Modalities.pdf:pdf},
pages = {1--7},
title = {{Do Neural Network Cross-Modal Mappings Really Bridge Modalities?}},
url = {http://arxiv.org/abs/1805.07616},
year = {2018}
}
@inproceedings{Kimmig2012,
abstract = {Probabilistic soft logic (PSL) is a framework for collective, probabilistic reasoning in relational domains. PSL uses first order logic rules as a template language for graphical models over random variables with soft truth values from the interval [0, 1]. Inference in this setting is a continuous optimization task, which can be solved efficiently. This paper provides an overview of the PSL language and its techniques for inference and weight learning. An implementation of PSL is available at http://psl.umiacs.umd.edu/.},
author = {Kimmig, A. and Bach, S. H. and Broecheler, M. and Huang, B. and Getoor, L.},
booktitle = {Neurips},
file = {:home/hiaoxui/papers/Neurips/A Short Introduction to Probabilistic Soft Logic.pdf:pdf},
number = {1},
pages = {1--4},
title = {{A Short Introduction to Probabilistic Soft Logic}},
year = {2012}
}
@inproceedings{Berant2014,
abstract = {A central challenge in semantic parsing is handling the myriadways in which knowl- edge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this pa- per, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically gener- ate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best para- phrases the input, and output the corre- sponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves state- of-the-art accuracies on two recently re- leased question-answering datasets. 1},
author = {Berant, J. and Liang, P.},
booktitle = {ACL},
doi = {10.3115/v1/P14-1133},
file = {:home/hiaoxui/papers/ACL/Semantic Parsing via Paraphrasing.pdf:pdf},
isbn = {9781937284725},
issn = {00219258},
pages = {1415--1425},
pmid = {1903399},
title = {{Semantic Parsing via Paraphrasing}},
url = {http://aclweb.org/anthology/P14-1133},
year = {2014}
}
@inproceedings{Shen2018,
abstract = {Semantic hashing has become a power-ful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous tech-niques generally require two-stage train-ing, and the binary constraints are han-dled ad-hoc. In this paper, we present an end-to-end Neural Architecture for Se-mantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli la-tent variables. A neural variational in-ference framework is proposed for train-ing, where gradients are directly back-propagated through the discrete latent variable to optimize the hash function. We also draw connections between pro-posed method and rate-distortion the-ory, which provides a theoretical foun-dation for the effectiveness of the pro-posed framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsuper-vised and supervised scenarios.},
archivePrefix = {arXiv},
arxivId = {1805.05361},
author = {Shen, D. and Su, Q. and Chapfuwa, P. and Wang, W. and Wang, G. and Carin, L. and Henao, R.},
booktitle = {ACL},
eprint = {1805.05361},
file = {:home/hiaoxui/papers/ACL/NASH Toward End-to-End Neural Architecture for Generative Semantic Hashing.pdf:pdf},
pages = {1--10},
title = {{NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing}},
year = {2018}
}
@unpublished{Manning2000,
author = {Manning, C. D.},
file = {:home/hiaoxui/papers/Unknown/An Introduction to Formal Computational Semantics.pdf:pdf},
pages = {1--15},
title = {{An Introduction to Formal Computational Semantics}},
year = {2000}
}
@article{Ransom2003,
abstract = {We describe a new and efficient technique, which we call sideband or phase-modulation searching, that allows one to detect short-period binary pulsars in observations longer than the orbital period. The orbital motion of the pulsar during long observations effectively modulates the phase of the pulsar signal, causing sidebands to appear around the pulsar spin frequency and its harmonics in the Fourier transform. For the majority of binary radio pulsars or low-mass X-ray binaries (LMXBs), large numbers of sidebands are present, allowing efficient searches using Fourier transforms of short portions of the original power spectrum. Analysis of the complex amplitudes and phases of the sidebands can provide enough information to solve for the Keplerian orbital parameters. This technique is particularly applicable to radio pulsar searches in globular clusters and searches for coherent X-ray pulsations from LMXBs and is complementary to more standard "acceleration" searches.},
archivePrefix = {arXiv},
arxivId = {astro-ph/0210010},
author = {Ransom, S. M. and Cordes, J. M. and Eikenberry, S. S.},
doi = {10.1086/374806},
eprint = {0210010},
file = {:home/hiaoxui/papers/The Astrophysical Journal/A New Search Technique for Short Orbital Period Binary Pulsars.pdf:pdf},
issn = {0004-637X},
journal = {The Astrophysical Journal},
number = {2},
pages = {911--920},
primaryClass = {astro-ph},
title = {{A New Search Technique for Short Orbital Period Binary Pulsars}},
url = {http://stacks.iop.org/0004-637X/589/i=2/a=911},
volume = {589},
year = {2003}
}
@inproceedings{Kipf2017,
abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1609.02907},
author = {Kipf, T. N. and Welling, M.},
booktitle = {ICLR},
eprint = {1609.02907},
file = {:home/hiaoxui/papers/ICLR/Semi-Supervised Classification with Graph Convolutional Networks.pdf:pdf},
pages = {1--14},
title = {{Semi-Supervised Classification with Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1609.02907},
year = {2017}
}
@inproceedings{Krifka2007,
abstract = {This paper gives an explanation of the well-known phenomenon that round numbers in measure terms (like one hundred meters) are interpreted in a more approximate way than non-round numbers (like one hundred and three meters). Several possible explanations are considered: First, a preference for short expressions and approximate interpretations; second, a conditional preference for short expressions under approximate interpretations; third, an explanation in terms of strategic communication that makes use of the fact that approximate interpretations, even if not favored initially, turn out to be more likely once the probability of the reported values are factored in. These explanations are shown to be flawed, in particular because the complexity of expressions does not always matter. The theory that is put forward makes use of scales that differ insofar as they are more or less fine-grained, and proposes a principle that a number expression is interpreted on the most coarse-grained scale that it occurs on. This principle can be motivated by strategic communication that factors in the overall likelihood of the message. The emerging theory is refined in various ways. In particular, it will be shown that complexity of expressions is important after all, but mainly on the evolutionary level, where it can be shown to lead to characteristic patterns of language change. The paper ends with the discussion of some surprising facts about the influence that the number system of a language has on which numbers are actually expressed in that language.},
author = {Krifka, M.},
booktitle = {Cognitive foundations of interpretation},
file = {:home/hiaoxui/papers/Cognitive foundations of interpretation/Approximate Interpretations of Number Words A case for strategic communication.pdf:pdf},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {1--16},
title = {{Approximate Interpretations of Number Words: A case for strategic communication}},
url = {http://edoc.hu-berlin.de/oa/bookchapters/reTiXQaSYryIs/PDF/22IE37nuLs8Tg.pdf},
year = {2007}
}
@unpublished{Chen2012,
author = {Chen, D. L.},
file = {:home/hiaoxui/papers/Unknown/Learning language from ambiguous perceptual context.pdf:pdf},
pages = {197},
title = {{Learning language from ambiguous perceptual context}},
year = {2012}
}
@inproceedings{Heafield2013,
abstract = {We present an efficient algorithm to es-timate large modified Kneser-Ney mod-els including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7{\%} of the RAM and 14.0{\%} of the wall time taken by SRILM. The code is open source as part of KenLM.},
author = {Heafield, K. and Pouzyrevsky, I. and Clark, J. H. and Koehn, P.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Scalable Modified Kneser-Ney Language Model Estimation.pdf:pdf},
isbn = {9781937284510},
keywords = {citation},
mendeley-tags = {citation},
pages = {690--696},
title = {{Scalable Modified Kneser-Ney Language Model Estimation}},
url = {https://kheafield.com/papers/edinburgh/estimate{\_}paper.pdf},
year = {2013}
}
@inproceedings{Murdoch2017,
abstract = {Although deep learning models have proven effective at solving problems in natu-ral language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of repre-sentative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.},
archivePrefix = {arXiv},
arxivId = {1702.02540},
author = {Murdoch, J. and Szlam, A.},
booktitle = {ICLR},
doi = {10.5121/ijci.2015.4221},
eprint = {1702.02540},
file = {:home/hiaoxui/papers/ICLR/Automatic rule extraction from long short term memory networks.pdf:pdf},
issn = {23208430},
number = {2},
pages = {221--226},
title = {{Automatic rule extraction from long short term memory networks}},
url = {https://openreview.net/pdf?id=SJvYgH9xe},
volume = {4},
year = {2017}
}
@unpublished{Paige2014,
abstract = {We introduce a new sequential Monte Carlo algorithm we call the particle cascade. The particle cascade is an asynchronous, anytime alternative to traditional particle filtering algorithms. It uses no barrier synchronizations which leads to improved particle throughput and memory efficiency. It is an anytime algorithm in the sense that it can be run forever to emit an unbounded number of particles while keeping within a fixed memory budget. We prove that the particle cascade is an unbiased marginal likelihood estimator which means that it can be straightforwardly plugged into existing pseudomarginal methods.},
archivePrefix = {arXiv},
arxivId = {1407.2864},
author = {Paige, B. and Wood, F. and Doucet, A. and Teh, Y. W.},
eprint = {1407.2864},
file = {:home/hiaoxui/papers/Unknown/Asynchronous Anytime Sequential Monte Carlo.pdf:pdf},
issn = {10495258},
pages = {1--11},
title = {{Asynchronous Anytime Sequential Monte Carlo}},
url = {http://arxiv.org/abs/1407.2864},
year = {2014}
}
@inproceedings{Surdeanu2012,
abstract = {Distant supervision for relation extraction (RE) -- gathering training data by aligning a database of facts with text -- is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains.},
archivePrefix = {arXiv},
arxivId = {1706.05075},
author = {Surdeanu, M. and Tibshirani, J. and Nallapati, R. and Manning, C. D.},
booktitle = {EMNLP},
doi = {10.3115/v1/D14-1200},
eprint = {1706.05075},
file = {:home/hiaoxui/papers/EMNLP/Multi-instance Multi-label Learning for Relation Extraction.pdf:pdf},
isbn = {9781937284435},
keywords = {Relation Extraction},
number = {July},
pages = {455--465},
pmid = {91150},
title = {{Multi-instance Multi-label Learning for Relation Extraction}},
url = {http://dl.acm.org/citation.cfm?id=2390948.2391003},
year = {2012}
}
@inproceedings{Jain2019,
abstract = {Knowledge Bases (KBs) are becoming increasingly large, sparse and probabilistic. These KBs are typically used to perform query inferences and rule mining. But their efficacy is only as high as their completeness. Efficiently utilizing incomplete KBs remains a major challenge as the current KB completion techniques either do not take into account the inherent uncertainty associated with each KB tuple or do not scale to large KBs. Probabilistic rule learning not only considers the probability of every KB tuple but also tackles the problem of KB completion in an explainable way. For any given probabilistic KB, it learns probabilistic first-order rules from its relations to identify interesting patterns. But, the current probabilistic rule learning techniques perform grounding to do probabilistic inference for evaluation of candidate rules. It does not scale well to large KBs as the time complexity of inference using grounding is exponential over the size of the KB. In this paper, we present SafeLearner-a scalable solution to probabilistic KB completion that performs probabilistic rule learning using lifted probabilistic inference-as faster approach instead of grounding. We compared SafeLearner to the state-of-the-art probabilistic rule learner ProbFOIL + and to its deterministic contemporary AMIE+ on standard probabilistic KBs of NELL (Never-Ending Language Learner) and Yago. Our results demonstrate that SafeLearner scales as good as AMIE+ when learning simple rules and is also significantly faster than ProbFOIL + .},
author = {Jain, A. and Friedman, T. and Kuzelka, O. and {Van den Broeck}, G. and Raedt, L. D.},
booktitle = {Automated Knowledge Base Construction},
file = {:home/hiaoxui/papers/Automated Knowledge Base Construction/Scalable Rule Learning in Probabilistic Knowledge Bases.pdf:pdf},
pages = {1--17},
title = {{Scalable Rule Learning in Probabilistic Knowledge Bases}},
year = {2019}
}
@unpublished{Collins2011b,
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/Statistical Machine Translation IBM Models 1 and 2.pdf:pdf},
pages = {1--22},
title = {{Statistical Machine Translation: IBM Models 1 and 2}},
year = {2011}
}
@inproceedings{Winn2018,
author = {Winn, O. and Muresan, S.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/‘ Lighter ' Can Still Be Dark Modeling Comparative Color Descriptions.pdf:pdf},
pages = {1--6},
title = {{‘ Lighter ' Can Still Be Dark : Modeling Comparative Color Descriptions}},
year = {2018}
}
@inproceedings{Wang2015,
abstract = {How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We fur- ther study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.},
author = {Wang, Y. and Berant, J. and Liang, P.},
booktitle = {ACL},
doi = {10.3115/v1/P15-1129},
file = {:home/hiaoxui/papers/ACL/Building a Semantic Parser Overnight.pdf:pdf},
isbn = {9781941643723},
pages = {1332--1342},
pmid = {1684229},
title = {{Building a Semantic Parser Overnight}},
url = {http://aclweb.org/anthology/P15-1129},
year = {2015}
}
@inproceedings{Tai2015,
abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
archivePrefix = {arXiv},
arxivId = {1503.00075},
author = {Tai, K. S. and Socher, R. and Manning, C. D.},
booktitle = {ACL},
eprint = {1503.00075},
file = {:home/hiaoxui/papers/ACL/Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
title = {{Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks}},
url = {http://arxiv.org/abs/1503.00075},
year = {2015}
}
@inproceedings{Krishnamurthy2012a,
abstract = {We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependency- parsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-the- art accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80{\%} precision and 56{\%} recall, despite never having seen an annotated logical form.},
author = {Krishnamurthy, J. and Mitchell, T.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Weakly Supervised Training of Semantic Parsers.pdf:pdf},
isbn = {9781937284435},
number = {July},
pages = {754--765},
title = {{Weakly Supervised Training of Semantic Parsers}},
year = {2012}
}
@inproceedings{Miller1996,
author = {Miller, S. and Stallard, D. and Bobrow, R. and Schwartz, R.},
booktitle = {ACL},
doi = {10.3115/981863.981871},
file = {:home/hiaoxui/papers/ACL/A Fully Statistical Approach to Natural Language Interfaces.pdf:pdf},
pages = {55--61},
title = {{A Fully Statistical Approach to Natural Language Interfaces}},
url = {http://www.aclweb.org/anthology/P96-1008},
year = {1996}
}
@inproceedings{Kim2016,
abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
archivePrefix = {arXiv},
arxivId = {0907.1815},
author = {Kim, Y. and Stratos, K. and Sarikaya, R.},
booktitle = {COLING},
eprint = {0907.1815},
file = {:home/hiaoxui/papers/COLING/Frustratingly Easy Neural Domain Adaptation.pdf:pdf},
pages = {387--396},
title = {{Frustratingly Easy Neural Domain Adaptation}},
url = {http://arxiv.org/abs/0907.1815},
year = {2016}
}
@inproceedings{Listgarten2005,
abstract = {Page 1. of Continuous Jennifer Listgarten † , Radford M. Neal † , Sam T. Roweis † and Andrew Emili ‡},
author = {Listgarten, J. and Neal, R. M. and Roweis, S. T. and Emili, A.},
booktitle = {Neurips},
file = {:home/hiaoxui/papers/Neurips/Multiple Alignment of Continuous Time Series.pdf:pdf},
isbn = {0262195348},
issn = {10495258},
pages = {817--824},
title = {{Multiple Alignment of Continuous Time Series}},
year = {2005}
}
@inproceedings{Hu2017,
abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, much existing work has shown the benefits of enhancing spatial encoding. In this work, we focus on channels and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at slight computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251{\%}, achieving a 25{\%} relative improvement over the winning entry of 2016.},
archivePrefix = {arXiv},
arxivId = {1709.01507},
author = {Hu, J. and Shen, L. and Sun, G.},
booktitle = {ILSVRC},
eprint = {1709.01507},
file = {:home/hiaoxui/papers/ILSVRC/Squeeze-and-Excitation Networks.pdf:pdf},
title = {{Squeeze-and-Excitation Networks}},
url = {http://arxiv.org/abs/1709.01507},
year = {2017}
}
@inproceedings{Rezende2014,
abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
archivePrefix = {arXiv},
arxivId = {1401.4082},
author = {Rezende, D. J. and Mohamed, S. and Wierstra, D.},
booktitle = {ICML},
eprint = {1401.4082},
file = {:home/hiaoxui/papers/ICML/Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf:pdf},
title = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
url = {http://arxiv.org/abs/1401.4082},
year = {2014}
}
@inproceedings{Strubell2018a,
archivePrefix = {arXiv},
arxivId = {arXiv:1804.08199v2},
author = {Strubell, E. and Verga, P. and Andor, D. and Weiss, D. and McCallum, A.},
booktitle = {EMNLP},
eprint = {arXiv:1804.08199v2},
file = {:home/hiaoxui/papers/EMNLP/Linguistically-Informed Self-Attention for Semantic Role Labeling.pdf:pdf},
title = {{Linguistically-Informed Self-Attention for Semantic Role Labeling}},
year = {2018}
}
@inproceedings{Brill2002,
abstract = {We describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy. The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers. Because a wrong an-swer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer.},
author = {Brill, E. and Dumais, S. and Banko, M.},
booktitle = {EMNLP},
doi = {10.3115/1118693.1118726},
file = {:home/hiaoxui/papers/EMNLP/An analysis of the AskMSR question-answering system.pdf:pdf},
number = {July},
pages = {257--264},
title = {{An analysis of the AskMSR question-answering system}},
url = {http://portal.acm.org/citation.cfm?doid=1118693.1118726},
volume = {10},
year = {2002}
}
@inproceedings{Branavan2009,
abstract = {In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.},
author = {Branavan, S. R. K. and Chen, H. and Zettlemoyer, L. S. and Barzilay, R.},
booktitle = {ACL-IJCNLP},
doi = {10.3115/1687878.1687892},
file = {:home/hiaoxui/papers/ACL-IJCNLP/Reinforcement learning for mapping instructions to actions.pdf:pdf},
isbn = {9781932432459},
issn = {1742206X},
number = {August},
pages = {82},
pmid = {18493666},
title = {{Reinforcement learning for mapping instructions to actions}},
url = {http://portal.acm.org/citation.cfm?doid=1687878.1687892},
volume = {1},
year = {2009}
}
@inproceedings{Reddi2018,
abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of ex-ponential moving averages of squared past gradients. It has been empirically ob-served that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous anal-ysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with " long-term memory " of past gradi-ents, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
author = {Al-Shedivat, M. and Bansal, T. and Burda, Y. and Sutskever, I. and Mordatch, I. and Abbeel, P.},
booktitle = {ICLR},
file = {:home/hiaoxui/papers/ICLR/Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments.pdf:pdf},
pages = {1--23},
title = {{Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments}},
url = {https://openreview.net/forum?id=ryQu7f-RZ{\%}0Ahttps://openreview.net/pdf?id=ryQu7f-RZ},
year = {2018}
}
@inproceedings{Zelle1996,
abstract = {This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a pre-existing, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application.},
author = {Zelle, J. M. and Mooney, R. J.},
booktitle = {AAAI},
file = {:home/hiaoxui/papers/AAAI/Learning to Parse Database queries using inductive logic proramming.pdf:pdf},
keywords = {1996,aaai,aaai - 96 proceedings,all rights reserved,copyright,learning to parse database,m,org,queries,using inductive logic programming,www},
number = {August},
pages = {1050--1055},
title = {{Learning to Parse Database queries using inductive logic proramming}},
url = {http://www.aaai.org/Papers/AAAI/1996/AAAI96-156.pdf},
year = {1996}
}
@unpublished{Vashishtha2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1902.01390v1},
author = {Vashishtha, S. and van Durme, B. and White, A. S.},
eprint = {arXiv:1902.01390v1},
file = {:home/hiaoxui/papers/Unknown/Fine-Grained Temporal Relation Extraction.pdf:pdf},
title = {{Fine-Grained Temporal Relation Extraction}},
year = {2019}
}
@inproceedings{Vogel1996,
abstract = {In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.},
author = {Vogel, S. and Ney, H. and Tillmann, C.},
booktitle = {COLING},
doi = {10.3115/993268.993313},
file = {:home/hiaoxui/papers/COLING/HMM-based Word Alignment in Statistical Machine Translation.pdf:pdf},
pages = {836--841},
title = {{HMM-based Word Alignment in Statistical Machine Translation}},
url = {http://portal.acm.org/citation.cfm?doid=993268.993313},
volume = {96pp},
year = {1996}
}
@inproceedings{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, M. E. and Neumann, M. and Iyyer, M. and Gardner, M. and Clark, C. and Lee, K. and Zettlemoyer, L. S.},
booktitle = {NAACL},
eprint = {1802.05365},
file = {:home/hiaoxui/papers/NAACL/Deep contextualized word representations.pdf:pdf},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}
@inproceedings{Rahimi2007,
author = {Rahimi, A. and Recht, B.},
booktitle = {Neurips},
doi = {10.1007/s12204-009-0467-7},
file = {:home/hiaoxui/papers/Neurips/Regularization method and immune genetic algorithm for inverse problems of ship maneuvering.pdf:pdf},
issn = {10071172},
keywords = {Immune,Inverse problem,Regularization method,Ship maneuvering},
title = {{Regularization method and immune genetic algorithm for inverse problems of ship maneuvering}},
year = {2007}
}
@inproceedings{Liang2008,
author = {Liang, P. and Daum, H. and Klein, D.},
booktitle = {ICML},
file = {:home/hiaoxui/papers/ICML/Structure Compilation Trading Structure for Features.pdf:pdf},
pages = {592--599},
title = {{Structure Compilation : Trading Structure for Features}},
year = {2008}
}
@article{Silver2017,
author = {Silver, D. and Huang, A. and Maddison, C. J. and Guez, A. and Sifre, L. and van den Driessche, G. and Schrittwieser, J. and Antonoglou, I. and Panneershelvam, V. and Lanctot, M. and Dieleman, S. and Grewe, D. and Nham, J. and Kalchbrenner, N. and Sutskever, I. and Lillicrap, T. and Leach, M. and Kavukcuoglu, K. and Graepel, T. and Hassabis, D.},
doi = {10.1038/nature16961},
file = {:home/hiaoxui/papers/Nature/Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7585},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2017}
}
@inproceedings{Zhang2017c,
abstract = {This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of CNNs with different structures. The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision. Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.},
archivePrefix = {arXiv},
arxivId = {1710.00935},
author = {Zhang, Q. and Wu, Y. N. and Zhu, S.},
booktitle = {CVPR},
eprint = {1710.00935},
file = {:home/hiaoxui/papers/CVPR/Interpretable Convolutional Neural Networks.pdf:pdf},
title = {{Interpretable Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1710.00935},
year = {2017}
}
@article{Ando2005,
author = {Ando, R. K. and Zhang, T.},
file = {:home/hiaoxui/papers/JMLR/A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data.pdf:pdf},
journal = {JMLR},
pages = {1817--1853},
title = {{A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data}},
volume = {6},
year = {2005}
}
@inproceedings{Jia2017,
abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of {\$}75\backslash{\%}{\$} F1 score to {\$}36\backslash{\%}{\$}; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to {\$}7\backslash{\%}{\$}. We hope our insights will motivate the development of new models that understand language more precisely.},
archivePrefix = {arXiv},
arxivId = {1707.07328},
author = {Jia, R. and Liang, P.},
booktitle = {EMNLP},
eprint = {1707.07328},
file = {:home/hiaoxui/papers/EMNLP/Adversarial Examples for Evaluating Reading Comprehension Systems.pdf:pdf},
pages = {2021--2031},
title = {{Adversarial Examples for Evaluating Reading Comprehension Systems}},
url = {http://arxiv.org/abs/1707.07328},
year = {2017}
}
@article{Marcus1993,
abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.},
author = {Marcus, M. P. and Santorini, B. and Marcinkiewicz, M. A.},
doi = {10.1162/coli.2010.36.1.36100},
file = {:home/hiaoxui/papers/Computational Linguistics/Building a large annotated corpus of English The Penn Treebank.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
keywords = {POS-Tagging},
number = {2},
pages = {313--330},
title = {{Building a large annotated corpus of English: The Penn Treebank}},
url = {http://anthology.aclweb.org/J/J93/J93-2004.pdf},
volume = {19},
year = {1993}
}
@inproceedings{Kottur2017,
abstract = {A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.},
archivePrefix = {arXiv},
arxivId = {1706.08502},
author = {Kottur, S. and Moura, J. M. F. and Lee, S. and Batra, D.},
booktitle = {EMNLP},
eprint = {1706.08502},
file = {:home/hiaoxui/papers/EMNLP/Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog.pdf:pdf},
keywords = {Best},
mendeley-tags = {Best},
title = {{Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog}},
url = {http://arxiv.org/abs/1706.08502},
year = {2017}
}
@inproceedings{Devlin2014,
abstract = {Recent work has shown success in us-ing neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexi-calized and can be integrated into any MT decoder. We also present several varia-tions of the NNJM which provide signif-icant additive improvements. Although the model is quite simple, it yields strong empirical results. On the NIST OpenMT12 Arabic-English condi-tion, the NNJM features produce a gain of +3.0 BLEU on top of a powerful, feature-rich baseline which already includes a target-only NNLM. The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chi-ang's (2007) original Hiero implementa-tion. Additionally, we describe two novel tech-niques for overcoming the historically high cost of using NNLM-style models in MT decoding. These techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM.},
author = {Devlin, J. and Zbib, R. and Huang, Z. and Lamar, T. and Schwartz, R. and Makhoul, J.},
booktitle = {ACL},
doi = {10.3115/v1/p14-1129},
file = {:home/hiaoxui/papers/ACL/Fast and Robust Neural Network Joint Models for Statistical Machine Translation.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
pages = {1370--1380},
title = {{Fast and Robust Neural Network Joint Models for Statistical Machine Translation}},
year = {2014}
}
@unpublished{Collins2013a,
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/Language Modeling.pdf:pdf},
title = {{Language Modeling}},
year = {2013}
}
@inproceedings{Petrov2007,
abstract = {We present several improvements to unlexicalized parsing with hierarchically$\backslash$nstate-split PCFGs. First, we present a novel coarse-to-fine method$\backslash$nin which a grammar's own hierarchical projections are used for incremental$\backslash$npruning, including a method for efficiently computing projections$\backslash$nof a grammar without a treebank. In our experiments, hierarchical$\backslash$npruning greatly accelerates parsing with no loss in empirical accuracy.$\backslash$nSecond, we compare various inference procedures for state-split PCFGs$\backslash$nfrom the standpoint of risk minimization, paying particular attention$\backslash$nto their practical tradeoffs. Finally, we present multilingual experiments$\backslash$nwhich show that parsing with hierarchical state-splitting is fast$\backslash$nand accurate in multiple languages and domains, even without any$\backslash$nlanguage-specific tuning.},
author = {Petrov, S. and Klein, D.},
booktitle = {NAACL-HLT},
doi = {10.3115/1614164},
file = {:home/hiaoxui/papers/NAACL-HLT/Improved Inference for Unlexicalized Parsing.pdf:pdf},
number = {April},
pages = {404--411},
title = {{Improved Inference for Unlexicalized Parsing}},
url = {http://acl.ldc.upenn.edu/N/N07/N07-1051.pdf{\%}5Cnhttp://www.aclweb.org/anthology/N/N07/N07-1051},
year = {2007}
}
@inproceedings{Yi2018a,
abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8{\%} on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
archivePrefix = {arXiv},
arxivId = {1810.02338},
author = {Yi, K. and Wu, J. and Gan, C. and Torralba, A. and Kohli, P. and Tenenbaum, J. B.},
booktitle = {Neurips},
doi = {10.1111/j.1749-6632.2009.04729.x},
eprint = {1810.02338},
file = {:home/hiaoxui/papers/Neurips/Neural-Symbolic VQA Disentangling Reasoning from Vision and Language Understanding.pdf:pdf},
isbn = {9781573317375},
issn = {1749-6632},
pmid = {19723035},
title = {{Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding}},
url = {http://arxiv.org/abs/1810.02338},
year = {2018}
}
@article{Artzi2013,
abstract = {The context in which language is used pro- vides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCGsemantic parsing approach that learns a joint model of mean- ing and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to di- rectly influence learning. It also enables algo- rithms that learn while executing instructions, for example by trying to replicate human ac- tions. Experiments on a benchmark naviga- tional dataset demonstrate strong performance under differing forms of supervision, includ- ing correctly executing 60{\%} more instruction sets relative to the previous state of the art.},
author = {Artzi, Y. and Zettlemoyer, L. S.},
file = {:home/hiaoxui/papers/TACL/Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions.pdf:pdf},
issn = {2307-387X},
journal = {TACL},
pages = {49--62},
title = {{Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions}},
volume = {1},
year = {2013}
}
@inproceedings{Hamilton2017,
abstract = {Graphs (networks) are ubiquitous and allow us to model entities (nodes) and the dependencies (edges) between them. Learning a useful feature representation from graph data lies at the heart and success of many machine learning tasks such as classification, anomaly detection, link prediction, among many others. Many existing techniques use random walks as a basis for learning features or estimating the parameters of a graph model for a downstream prediction task. Examples include recent node embedding methods such as DeepWalk, node2vec, as well as graph-based deep learning algorithms. However, the simple random walk used by these methods is fundamentally tied to the identity of the node. This has three main disadvantages. First, these approaches are inherently transductive and do not generalize to unseen nodes and other graphs. Second, they are not space-efficient as a feature vector is learned for each node which is impractical for large graphs. Third, most of these approaches lack support for attributed graphs. To make these methods more generally applicable, we propose a framework for inductive network representation learning based on the notion of attributed random walk that is not tied to node identity and is instead based on learning a function {\$}\backslashPhi : \backslashmathrm{\{}\backslashrm \backslashbf x{\}} \backslashrightarrow w{\$} that maps a node attribute vector {\$}\backslashmathrm{\{}\backslashrm \backslashbf x{\}}{\$} to a type {\$}w{\$}. This framework serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many other previous methods that leverage traditional random walks.},
archivePrefix = {arXiv},
arxivId = {1710.09471},
author = {Hamilton, W. L. and Ying, R. and Leskovec, J.},
booktitle = {Neurips},
eprint = {1710.09471},
file = {:home/hiaoxui/papers/Neurips/Inductive Representation Learning on Large Graphs.pdf:pdf},
number = {Nips},
pages = {1--11},
title = {{Inductive Representation Learning on Large Graphs}},
url = {http://arxiv.org/abs/1710.09471},
year = {2017}
}
@inproceedings{Cho2014,
abstract = {Neural machine translation is a relatively new approach to statistical machine trans-lation based purely on neural networks. The neural machine translation models of-ten consist of an encoder and a decoder. The encoder extracts a fixed-length repre-sentation from a variable-length input sen-tence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the proper-ties of the neural machine translation us-ing two models; RNN Encoder–Decoder and a newly proposed gated recursive con-volutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance de-grades rapidly as the length of the sentence and the number of unknown words in-crease. Furthermore, we find that the pro-posed gated recursive convolutional net-work learns a grammatical structure of a sentence automatically.},
archivePrefix = {arXiv},
arxivId = {1409.1259},
author = {Cho, K. and van Merrienboer, B. and Bahdanau, D. and Bengio, Y.},
booktitle = {Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
doi = {10.3115/v1/W14-4012},
eprint = {1409.1259},
file = {:home/hiaoxui/papers/Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation/On the Properties of Neural Machine Translation Encoder – Decoder Approaches.pdf:pdf},
isbn = {9781937284961},
pages = {103--111},
title = {{On the Properties of Neural Machine Translation : Encoder – Decoder Approaches}},
year = {2014}
}
@unpublished{Rabanser2017,
abstract = {Tensors are multidimensional arrays of numerical values and therefore generalize matrices to multiple dimensions. While tensors first emerged in the psychometrics community in the {\$}20{\^{}}{\{}\backslashtext{\{}th{\}}{\}}{\$} century, they have since then spread to numerous other disciplines, including machine learning. Tensors and their decompositions are especially beneficial in unsupervised learning settings, but are gaining popularity in other sub-disciplines like temporal and multi-relational data analysis, too. The scope of this paper is to give a broad overview of tensors, their decompositions, and how they are used in machine learning. As part of this, we are going to introduce basic tensor concepts, discuss why tensors can be considered more rigid than matrices with respect to the uniqueness of their decomposition, explain the most important factorization algorithms and their properties, provide concrete examples of tensor decomposition applications in machine learning, conduct a case study on tensor-based estimation of mixture models, talk about the current state of research, and provide references to available software libraries.},
archivePrefix = {arXiv},
arxivId = {1711.10781},
author = {Rabanser, S. and Shchur, O. and G{\"{u}}nnemann, S.},
eprint = {1711.10781},
file = {:home/hiaoxui/papers/Unknown/Introduction to Tensor Decompositions and their Applications in Machine Learning.pdf:pdf},
pages = {1--13},
title = {{Introduction to Tensor Decompositions and their Applications in Machine Learning}},
url = {http://arxiv.org/abs/1711.10781},
year = {2017}
}
@inproceedings{Matuszek2013,
abstract = {As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor envi- ronment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our for- mal representation allows a robot to interpret route instructions online while moving through a previously unknown environment.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Matuszek, C. and Herbst, E. and Zettlemoyer, L. S. and Fox, D.},
booktitle = {ISER},
doi = {10.1007/978-3-319-00065-7_28},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/ISER/Learning to Parse Natural Language Commands to a Robot Control System.pdf:pdf},
isbn = {978-3-319-00064-0},
issn = {21530858},
pages = {403--415},
pmid = {19886812},
title = {{Learning to Parse Natural Language Commands to a Robot Control System}},
url = {http://link.springer.com/10.1007/978-3-319-00065-7{\_}28},
year = {2013}
}
@article{Berger1996,
abstract = {The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.},
author = {Berger, A. L. and Pietra, S. A. D. and Pietra, V. J. D.},
doi = {10.3115/1075812.1075844},
file = {:home/hiaoxui/papers/Computational Linguistics/A maximum entropy approach to natural language processing.pdf:pdf},
isbn = {1558603573},
issn = {08912017},
journal = {Computational Linguistics},
number = {1},
pages = {39--71},
title = {{A maximum entropy approach to natural language processing}},
url = {http://portal.acm.org/citation.cfm?id=234285.234289},
volume = {22},
year = {1996}
}
@inproceedings{Bottou2007,
author = {Bottou, L. and Bousquet, O.},
booktitle = {Neurips},
file = {:home/hiaoxui/papers/Neurips/The Tradeoffs of Large Scale Learning.pdf:pdf},
issn = {03406245},
number = {4},
pages = {1089},
title = {{The Tradeoffs of Large Scale Learning}},
volume = {58},
year = {2007}
}
@inproceedings{Xue2007,
author = {Xue, N.},
booktitle = {Treebanks and Linguistic Theories},
file = {:home/hiaoxui/papers/Treebanks and Linguistic Theories/Tapping the implicit information for the PS to DS conversion of the Chinese Treebank.pdf:pdf},
title = {{Tapping the implicit information for the PS to DS conversion of the Chinese Treebank}},
year = {2007}
}
@inproceedings{Liang2006,
abstract = {We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32{\%} reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29{\%} reduction in AER over symmetrized IBM model 4 predictions.},
author = {Liang, P. and Taskar, B. and Klein, D.},
booktitle = {NAACL},
doi = {10.3115/1220835.1220849},
file = {:home/hiaoxui/papers/NAACL/Alignment by agreement.pdf:pdf},
number = {June},
pages = {104--111},
title = {{Alignment by agreement}},
url = {http://portal.acm.org/citation.cfm?doid=1220835.1220849},
year = {2006}
}
@inproceedings{Jiang2016,
author = {Jiang, Y. and Han, W. and Tu, K.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Unsupervised Neural Dependency Parsing.pdf:pdf},
number = {61503248},
pages = {763--771},
title = {{Unsupervised Neural Dependency Parsing}},
year = {2016}
}
@inproceedings{Choi2018a,
abstract = {We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.},
archivePrefix = {arXiv},
arxivId = {1808.07036},
author = {Choi, E. and He, H. and Iyyer, M. and Yatskar, M. and Yih, W. and Choi, Y. and Liang, P. and Zettlemoyer, L.},
booktitle = {EMNLP},
eprint = {1808.07036},
file = {:home/hiaoxui/papers/EMNLP/QuAC Question Answering in Context.pdf:pdf},
title = {{QuAC: Question Answering in Context}},
url = {http://arxiv.org/abs/1808.07036},
year = {2018}
}
@unpublished{Marcheggiani2017,
abstract = {We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves competitive performance on English, even without any kind of syntactic information and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the English CoNLL-2009 dataset. We also consider Chinese, Czech and Spanish where our approach also achieves competitive results. Syntactic parsers are unreliable on out-of-domain data, so standard (i.e., syntactically-informed) SRL models are hindered when tested in this setting. Our syntax-agnostic model appears more robust, resulting in the best reported results on standard out-of-domain test sets.},
archivePrefix = {arXiv},
arxivId = {1701.02593},
author = {Marcheggiani, D. and Frolov, A. and Titov, I.},
eprint = {1701.02593},
file = {:home/hiaoxui/papers/Unknown/A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling.pdf:pdf},
title = {{A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling}},
url = {http://arxiv.org/abs/1701.02593},
year = {2017}
}
@inproceedings{Bulat2017,
author = {Bulat, L. and Clark, S. and Shutova, E.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Speaking , Seeing , Understanding Correlating semantic models with conceptual representation in the brain.pdf:pdf},
number = {2},
pages = {1081--1091},
title = {{Speaking , Seeing , Understanding : Correlating semantic models with conceptual representation in the brain}},
year = {2017}
}
@inproceedings{Manning2014,
abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Manning, C. D. and Surdeanu, M. and Bauer, J. and Finkel, J. and Bethard, S. and McClosky, D.},
booktitle = {ACL},
doi = {10.3115/v1/P14-5010},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/ACL/The Stanford CoreNLP Natural Language Processing Toolkit.pdf:pdf},
isbn = {9781941643006},
issn = {1098-6596},
pages = {55--60},
pmid = {25246403},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
url = {http://aclweb.org/anthology/P14-5010},
year = {2014}
}
@inproceedings{Kate2005,
abstract = {This paper presents a method for inducing transformation rules that map natural-language sentences into a formal query or command language. The approach assumes a formal gram- mar for the target representation language and learns trans- formation rules that exploit the non-terminal symbols in this grammar. The learned transformation rules incrementally map a natural-language sentence or its syntactic parse tree into a parse-tree for the target formal language. Experimental results are presented for two corpora, one which maps En- glish instructions into an existing formal coaching language for simulated RoboCup soccer agents, and another which maps EnglishU.S.-geography questions into a database query language. We show that our method performs overall better and faster than previous approaches in both domains.},
author = {Kate, R. J. and Wong, Y. W. and Mooney, R. J.},
booktitle = {AAAI},
file = {:home/hiaoxui/papers/AAAI/Learning to transform natural to formal languages.pdf:pdf},
isbn = {1-57735-236-x},
number = {3},
pages = {1062--1068},
title = {{Learning to transform natural to formal languages}},
url = {http://www.aaai.org/Library/AAAI/2005/aaai05-168.php},
volume = {20},
year = {2005}
}
@inproceedings{Bahdanau2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.00781v2},
author = {Bahdanau, D. and Hill, F. and Leike, J. and Hughes, E. and Kohli, P. and Grefenstette, E.},
booktitle = {ICLR},
eprint = {arXiv:1803.00781v2},
file = {:home/hiaoxui/papers/ICLR/Learning to Understand Goal Specifications by Modelling Reward.pdf:pdf},
keywords = {autonomous goal setting,deep neural network,diversity,exploration,learning,unsupervised},
pages = {1--26},
title = {{Learning to Understand Goal Specifications by Modelling Reward}},
year = {2019}
}
@unpublished{Saeed2009,
abstract = {An overview of current multiple alignment systems to date are described.The useful algorithms, the procedures adopted and their limitations are presented.We also present the quality of the alignments obtained and in which cases(kind of alignments, kind of sequences etc) the particular systems are useful.},
archivePrefix = {arXiv},
arxivId = {0901.2747},
author = {Saeed, F. and Khokhar, A.},
eprint = {0901.2747},
file = {:home/hiaoxui/papers/Unknown/An Overview of Multiple Sequence Alignment Systems.pdf:pdf},
pages = {24},
title = {{An Overview of Multiple Sequence Alignment Systems}},
url = {http://arxiv.org/abs/0901.2747},
year = {2009}
}
@inproceedings{Och2003a,
author = {Och, F. J.},
booktitle = {ACL},
doi = {10.3115/1075096.1075117},
file = {:home/hiaoxui/papers/ACL/Minimum Error Rate Training in Statistical Machine Translation.pdf:pdf},
pages = {160--167},
title = {{Minimum Error Rate Training in Statistical Machine Translation}},
year = {2003}
}
@inproceedings{Shang2015,
abstract = {We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75{\%} of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.},
archivePrefix = {arXiv},
arxivId = {1503.02364},
author = {Shang, L. and Lu, Z. and Li, H.},
booktitle = {ACL},
eprint = {1503.02364},
file = {:home/hiaoxui/papers/ACL/Neural Responding Machine for Short-Text Conversation.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
title = {{Neural Responding Machine for Short-Text Conversation}},
url = {http://arxiv.org/abs/1503.02364},
year = {2015}
}
@inproceedings{Roth2004,
abstract = {In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.},
author = {Roth, D. and Yih, W.},
booktitle = {CoNLL},
doi = {10.3115/1690219.1690287},
file = {:home/hiaoxui/papers/CoNLL/A Linear Programming Formulation for Global Inference in Natural Language Tasks.pdf:pdf},
isbn = {9781937284435},
issn = {00010782},
keywords = {Invited Talk,KM,TAL,information extraction,knowledge acquisition,web mining},
number = {July},
pages = {3--10},
title = {{A Linear Programming Formulation for Global Inference in Natural Language Tasks}},
url = {http://www.aclweb.org/anthology/P15-1034{\%}5Cnhttp://anthology.aclweb.org/P/P08/P08-1.pdf{\#}page=72{\%}255Cnhttp://anthology.aclweb.org/P/P08/P08-1.pdf{\#}page=72{\%}255Cnhttp://www.aclweb.org/anthology/P/P08/P08-1004{\%}255Cnhttp://dl.acm.org/citation.cfm?id=2002472.200},
year = {2004}
}
@inproceedings{Banarescu2013,
abstract = {We describe Abstract Meaning Represen- tation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sen- tences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural lan- guage understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.},
author = {Banarescu, L. and Bonial, C. and Cai, S. and Georgescu, M. and Griffitt, K. and Hermjakob, U. and Knight, K. and Koehn, P. and Palmer, M. and Schneider, N.},
booktitle = {Linguistic Annotation Workshop {\&} Interoperability with Discourse},
file = {:home/hiaoxui/papers/Linguistic Annotation Workshop {\&} Interoperability with Discourse/Abstract meaning representation for sembanking.pdf:pdf},
isbn = {1111111111},
pages = {178--186},
title = {{Abstract meaning representation for sembanking}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Abstract+Meaning+Representation+for+Sembanking{\#}0},
year = {2013}
}
@inproceedings{He2015,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224x224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170x faster than the recent leading method R-CNN (and 24-64x faster overall), while achieving better or comparable accuracy on Pascal VOC 2007.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
booktitle = {PAMI},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {1406.4729},
file = {:home/hiaoxui/papers/PAMI/Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition.pdf:pdf},
isbn = {9783319105772},
issn = {01628828},
keywords = {Convolutional Neural Networks,Image Classification,Object Detection,Spatial Pyramid Pooling},
number = {9},
pages = {1904--1916},
pmid = {26353135},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
volume = {37},
year = {2015}
}
@inproceedings{Dai2017,
abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation. The code would be released.},
archivePrefix = {arXiv},
arxivId = {1703.06211},
author = {Dai, J. and Qi, H. and Xiong, Y. and Li, Y. and Zhang, G. and Hu, H. and Wei, Y.},
booktitle = {ICCV},
doi = {10.1051/0004-6361/201527329},
eprint = {1703.06211},
file = {:home/hiaoxui/papers/ICCV/Deformable Convolutional Networks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pmid = {23459267},
title = {{Deformable Convolutional Networks}},
url = {http://arxiv.org/abs/1703.06211},
year = {2017}
}
@inproceedings{Talmor2019,
abstract = {When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.},
archivePrefix = {arXiv},
arxivId = {1811.00937},
author = {Talmor, A. and Herzig, J. and Lourie, N. and Berant, J.},
booktitle = {NAACL-HLT},
eprint = {1811.00937},
file = {:home/hiaoxui/papers/NAACL-HLT/CommonsenseQA A Question Answering Challenge Targeting Commonsense Knowledge.pdf:pdf},
title = {{CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}},
url = {http://arxiv.org/abs/1811.00937},
year = {2019}
}
@inproceedings{Richardson2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.06966v2},
author = {Richardson, K. and Berant, J. and Kuhn, J.},
booktitle = {NAACL},
eprint = {arXiv:1803.06966v2},
file = {:home/hiaoxui/papers/NAACL/Polyglot Semantic Parsing in APIs.pdf:pdf},
title = {{Polyglot Semantic Parsing in APIs}},
year = {2018}
}
@unpublished{Nie2018a,
abstract = {Recent neural models for data-to-document generation have achieved remarkable progress in producing fluent and informative texts. However, large proportions of generated texts do not actually conform to the input data. To address this issue, we propose a new training framework which attempts to verify the consistency between the generated texts and the input data to guide the training process. To measure the consistency, a relation extraction model is applied to check information overlaps between the input data and the generated texts. The non-differentiable consistency signal is optimized via reinforcement learning. Experimental results on a recently released challenging dataset ROTOWIRE show improvements from our framework in various metrics.},
archivePrefix = {arXiv},
arxivId = {1808.05306},
author = {Nie, F. and Chen, H. and Wang, J. and Yao, J. and Lin, C. and Pan, R.},
eprint = {1808.05306},
file = {:home/hiaoxui/papers/Unknown/Incorporating Consistency Verification into Neural Data-to-Document Generation.pdf:pdf},
title = {{Incorporating Consistency Verification into Neural Data-to-Document Generation}},
url = {http://arxiv.org/abs/1808.05306},
year = {2018}
}
@inproceedings{Zhao2017,
abstract = {Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33{\%} more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68{\%} at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5{\%} and 40.5{\%} for multilabel classification and visual semantic role labeling, respectively.},
archivePrefix = {arXiv},
arxivId = {1707.09457},
author = {Zhao, J. and Wang, T. and Yatskar, M. and Ordonez, V. and Chang, K.},
booktitle = {EMNLP},
eprint = {1707.09457},
file = {:home/hiaoxui/papers/EMNLP/Men Also Like Shopping Reducing Gender Bias Amplification using Corpus-level Constraints.pdf:pdf},
keywords = {Best},
mendeley-tags = {Best},
title = {{Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints}},
url = {http://arxiv.org/abs/1707.09457},
year = {2017}
}
@inproceedings{Yu2013,
abstract = {We present a method that learns repre- sentations for word meanings from short video clips paired with sentences. Un- like prior work on learning language from symbolic input, our input consists of video of people interacting with multiple com- plex objects in outdoor environments. Un- like prior computer-vision approaches that learn from videos with verb labels or im- ages with noun labels, our labels are sen- tences containing nouns, verbs, preposi- tions, adjectives, and adverbs. The cor- respondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts si- multaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video.},
archivePrefix = {arXiv},
arxivId = {1306.5263},
author = {Yu, H. and Siskind, J. M.},
booktitle = {ACL},
eprint = {1306.5263},
file = {:home/hiaoxui/papers/ACL/Grounded Language Learning from Video Described with Sentences.pdf:pdf},
isbn = {9781937284503},
number = {August},
pages = {53--63},
title = {{Grounded Language Learning from Video Described with Sentences}},
url = {http://www.aclweb.org/anthology/P13-1006},
year = {2013}
}
@article{Wang2016,
abstract = {We release Galactic Dependencies 1.0---a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format. This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages. Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages. We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages. As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a "nearby" source language. We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.},
archivePrefix = {arXiv},
arxivId = {1710.03838},
author = {Wang, D. and Eisner, J. M.},
eprint = {1710.03838},
file = {:home/hiaoxui/papers/TACL/The Galactic Dependencies Treebanks Getting More Data by Synthesizing New Languages.pdf:pdf},
issn = {2307-387X},
journal = {TACL},
pages = {491--505},
title = {{The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages}},
url = {http://arxiv.org/abs/1710.03838},
volume = {4},
year = {2016}
}
@inproceedings{Ejerhed1988,
abstract = {The Working Group I contribution to the IPCC's Fifth Assessment Report (AR5) considers new evidence of climate change based on many independent scientific analyses from observations of the climate system, paleoclimate archives, theoretical studies of climate processes and simulations using climate models. It builds upon the Working Group I contribution to the IPCC's Fourth Assessment Report (AR4), and incorporates subsequent new findings of research. As a component of the fifth assessment cycle, the IPCC Special Report on Managing the Risks of Extreme Events to Advance Climate Change Adaptation (SREX) is an important basis for information on changing weather and climate extremes. This Summary for Policymakers (SPM) follows the structure of the Working Group I report. The narrative is supported by a series of overarching highlighted conclusions which, taken together, provide a concise summary. Main sections are introduced with a brief paragraph in italics which outlines the methodological basis of the assessment. The degree of certainty in key findings in this assessment is based on the author teams' evaluations of underlying scientific understanding and is expressed as a qualitative level of confidence (from very low to very high) and, when possible, probabilistically with a quantified likelihood (from exceptionally unlikely to virtually certain). Confidence in the validity of a finding is based on the type, amount, quality, and consistency of evidence (e.g., data, mechanistic understanding, theory, models, expert judgment) and the degree of agreement1. Probabilistic estimates of quantified measures of uncertainty in a finding are based on statistical analysis of observations or model results, or both, and expert judgment2. Where appropriate, findings are also formulated as statements of fact without using uncertainty qualifiers. (See Chapter 1 and Box TS.1 for more details about the specific language the IPCC uses to communicate uncertainty) The basis for substantive paragraphs in this Summary for Policymakers can be found in the chapter sections of the underlying report and in the Technical Summary. These references are given},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ejerhed, E. I.},
booktitle = {ANLC},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/ANLC/Finding Clauses In Unrestricted Text By Finitary And Stochastic Methods.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {219--227},
pmid = {25246403},
title = {{Finding Clauses In Unrestricted Text By Finitary And Stochastic Methods}},
url = {https://aclanthology.info/pdf/A/A88/A88-1030.pdf},
year = {1988}
}
@inproceedings{Vaswani2016,
abstract = {In this paper we present new state-of-the-art performance on CCG supertagging and pars-ing. Our model outperforms existing ap-proaches by an absolute gain of 1.5{\%}. We an-alyze the performance of several neural mod-els and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.},
author = {Vaswani, A. and Bisk, Y. and Sagae, K. and Musa, R.},
booktitle = {NAACL},
file = {:home/hiaoxui/papers/NAACL/Supertagging With LSTMs.pdf:pdf},
isbn = {9781941643914},
pages = {232--237},
title = {{Supertagging With LSTMs}},
url = {http://www.aclweb.org/anthology/N16-1027},
year = {2016}
}
@inproceedings{Koh2017,
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
archivePrefix = {arXiv},
arxivId = {1703.04730},
author = {Koh, P. W. and Liang, P.},
booktitle = {ICML},
eprint = {1703.04730},
file = {:home/hiaoxui/papers/ICML/Understanding Black-box Predictions via Influence Functions.pdf:pdf},
issn = {1938-7228},
keywords = {best},
mendeley-tags = {best},
title = {{Understanding Black-box Predictions via Influence Functions}},
url = {http://arxiv.org/abs/1703.04730},
year = {2017}
}
@inproceedings{Graca2008,
abstract = {The expectation maximization (EM) algorithm is a widely used$\backslash$nmaximum likeli- hood estimation procedure for statistical models$\backslash$nwhen the values of some of the variables in the model are not$\backslash$nobserved. Very often, however, our aim is primar- ily to ﬁnd a$\backslash$nmodel that assigns values to the latent variables that have$\backslash$nintended meaning for our data and maximizing expected likelihood$\backslash$nonly sometimes ac- complishes this. Unfortunately, it is$\backslash$ntypically difﬁcult to add even simple a-priori information about$\backslash$nlatent variables in graphical models without making the models$\backslash$noverly complex or intractable. In this paper, we present an$\backslash$nefﬁcient, principled way to inject rich constraints on the$\backslash$nposteriors of latent variables into the EM algorithm. Our method$\backslash$ncan be used to learn tractable graphical models that sat- isfy$\backslash$nadditional, otherwise intractable constraints. Focusing on$\backslash$nclustering and the alignment problem for statistical machine$\backslash$ntranslation, we show that simple, in- tuitive posterior$\backslash$nconstraints can greatly improve the performance over standard$\backslash$nbaselines and be competitive with more complex, intractable$\backslash$nmodels.},
author = {Gra{\c{c}}a, J. V. and Ganchev, K. and Taskar, B.},
booktitle = {Neurips},
file = {:home/hiaoxui/papers/Neurips/Expectation Maximization and Posterior Constraints.pdf:pdf},
isbn = {160560352X},
keywords = {Mendeley Import (Jun 17)},
title = {{Expectation Maximization and Posterior Constraints}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.4545{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@inproceedings{DeNero2009,
abstract = {The minimum Bayes risk (MBR) decoding objective$\backslash$nimproves BLEU scores for machine translation$\backslash$noutput relative to the standard Viterbi objective$\backslash$nof maximizing model score. However,$\backslash$nMBR targeting BLEU is prohibitively slow to optimize$\backslash$nover k-best lists for large k. In this paper,$\backslash$nwe introduce and analyze an alternative to$\backslash$nMBR that is equally effective at improving performance,$\backslash$nyet is asymptotically faster—running$\backslash$n80 times faster than MBR in experiments with$\backslash$n1000-best lists. Furthermore, our fast decoding$\backslash$nprocedure can select output sentences based on$\backslash$ndistributions over entire forests of translations, in$\backslash$naddition to k-best lists. We evaluate our procedure$\backslash$non translation forests from two large-scale,$\backslash$nstate-of-the-art hierarchical machine translation$\backslash$nsystems. Our forest-based decoding objective$\backslash$nconsistently outperforms k-best list MBR, giving$\backslash$nimprovements of up to 1.0 BLEU.$\backslash$n1 Introduction$\backslash$nIn statistical},
author = {DeNero, J. and Chiang, D. and Knight, K.},
booktitle = {ACL-IJCNLP},
doi = {10.1109/MWSYM.2003.1211046},
file = {:home/hiaoxui/papers/ACL-IJCNLP/Fast consensus decoding over translation forests.pdf:pdf},
isbn = {0780376943},
issn = {0149645X},
number = {1},
pages = {1--177},
title = {{Fast consensus decoding over translation forests}},
url = {http://www.isi.edu/natural-language/mt/acl09-consensus.pdf},
volume = {3},
year = {2009}
}
@unpublished{Collins2013g,
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/The Inside-Outside Algorithm.pdf:pdf},
pages = {1--15},
title = {{The Inside-Outside Algorithm}},
year = {2013}
}
@inproceedings{Andreas2017,
abstract = {We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a "syntax" with functional analogues to qualitative properties of natural language.},
archivePrefix = {arXiv},
arxivId = {1707.08139},
author = {Andreas, J. and Klein, D.},
booktitle = {EMNLP},
doi = {10.18653/v1/D17-1311},
eprint = {1707.08139},
file = {:home/hiaoxui/papers/EMNLP/Analogs of Linguistic Structure in Deep Representations.pdf:pdf},
issn = {18734197},
title = {{Analogs of Linguistic Structure in Deep Representations}},
url = {http://arxiv.org/abs/1707.08139},
year = {2017}
}
@article{Hallett2007,
abstract = {This article describes a method for composing fluent and complex natural language questions, while avoiding the standard pitfalls of free text queries. The method, based on Conceptual Authoring, is targeted at question-answering systems where reliability and transparency are critical, and where users cannot be expected to undergo extensive training in question composition. This scenario is found in most corporate domains, especially in applications that are risk-averse. We present a proof-of-concept system we have developed: a question-answering interface to a large repository of medical histories in the area of cancer. We show that the method allows users to successfully and reliably compose complex queries with minimal training.},
author = {Hallett, C. and Scott, D. and Power, R.},
doi = {10.1162/coli.2007.33.1.105},
file = {:home/hiaoxui/papers/Computational Linguistics/Composing Questions through Conceptual Authoring.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
number = {1},
pages = {105--133},
title = {{Composing Questions through Conceptual Authoring}},
url = {http://www.mitpressjournals.org/doi/10.1162/coli.2007.33.1.105},
volume = {33},
year = {2007}
}
@unpublished{Collins2013f,
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/Phrase-Based Translation Models.pdf:pdf},
pages = {1--12},
title = {{Phrase-Based Translation Models}},
year = {2013}
}
@inproceedings{Romanov2019,
abstract = {There is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual's true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals' names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale dataset of online biographies. We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier's overall true positive rate.},
archivePrefix = {arXiv},
arxivId = {1904.05233},
author = {Romanov, A. and De-Arteaga, M. and Wallach, H. and Chayes, J. and Borgs, C. and Chouldechova, A. and Geyik, S. and Kenthapadi, K. and Rumshisky, A. and Kalai, A. T.},
booktitle = {NAACL-HLT},
eprint = {1904.05233},
file = {:home/hiaoxui/papers/NAACL-HLT/What's in a Name Reducing Bias in Bios without Access to Protected Attributes.pdf:pdf},
title = {{What's in a Name? Reducing Bias in Bios without Access to Protected Attributes}},
url = {http://arxiv.org/abs/1904.05233},
year = {2019}
}
@inproceedings{Tellex2011,
abstract = {This paper describes a new model for understanding natural language commands given to autonomous systems that perform navigation and mobile manipulation in semi-structured environments. Previous approaches have used models with fixed structure to infer the likelihood of a sequence of actions given the environment and the command. In contrast, our framework, called Generalized Grounding Graphs (G3), dynamically instantiates a probabilistic graphical model for a particular natural language command according to the command's hierarchical and compositional semantic structure. Our system performs inference in the model to successfully find and execute plans corresponding to natural language commands such as “Put the tire pallet on the truck.” The model is trained using a corpus of commands collected using crowdsourcing. We pair each command with robot actions and use the corpus to learn the parameters of the model. We evaluate the robot's performance by inferring plans from natural language commands, executing each plan in a realistic robot simulator, and asking users to evaluate the system's performance. We demonstrate that our system can successfully follow many natural language commands from the corpus.},
author = {Tellex, S. and Kollar, T. and Dickerson, S.},
booktitle = {AAAI},
file = {:home/hiaoxui/papers/AAAI/Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation.pdf:pdf},
keywords = {Special Track on Physically Grounded Artificial In},
pages = {1507--1514},
title = {{Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation.}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewFile/3623/4113},
year = {2011}
}
@inproceedings{Girshick2014,
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, R. and Donahue, J. and Darrell, T. and Malik, J.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:home/hiaoxui/papers/CVPR/Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
pages = {2--9},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@unpublished{Szegedy2013,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, C. and Zaremba, W. and Sutskever, I. and Bruna, J. and Erhan, D. and Goodfellow, I. and Fergus, R.},
eprint = {1312.6199},
file = {:home/hiaoxui/papers/Unknown/Intriguing properties of neural networks.pdf:pdf},
title = {{Intriguing properties of neural networks}},
url = {http://arxiv.org/abs/1312.6199},
year = {2013}
}
@inproceedings{McDonald2013,
author = {McDonald, R. and Nivre, J. and Quirmbach-Brundage, Y. and Goldberg, Y. and Das, D. and Ganchev, K. and Hall, K. and Petrov, S. and Zhang, H. and T{\"{a}}ckstr{\"{o}}m, O. and Bedini, C. and Castell{\'{o}}, N. B. and Lee, J.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Universal Dependency Annotation for Multilingual Parsing.pdf:pdf},
issn = {00094846},
keywords = {citation},
mendeley-tags = {citation},
number = {4},
pages = {92--97},
title = {{Universal Dependency Annotation for Multilingual Parsing}},
volume = {82},
year = {2013}
}
@inproceedings{Zhou2015,
abstract = {Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems. To this date, most of the successful SRL systems were built on top of some form of parsing results (Koomen et al., 2005; Palmer et al., 2010; Pradhan et al., 2013), where pre-defined feature templates over the syntactic structure are used. The attempts of building an end-to-end SRL learning system without using parsing were less successful (Collobert et al., 2011). In this work, we propose to use deep bi-directional recurrent network as an end-to-end system for SRL. We take only original text information as input feature, without using any syntactic knowledge. The proposed algorithm for semantic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F1 score of 81.07. This result outperforms the previous state-of-the-art system from the combination of different parsing trees or models. We also obtained the same conclusion with F1 = 81.27 on CoNLL- 2012 shared task. As a result of simplicity, our model is also computationally efficient that the parsing speed is 6.7k tokens per second. Our analysis shows that our model is better at handling longer sentences than traditional models. And the latent variables of our model implicitly capture the syntactic structure of a sentence.},
archivePrefix = {arXiv},
arxivId = {1605.07515},
author = {Zhou, J. and Xu, W.},
booktitle = {ACL},
doi = {10.3115/v1/P15-1109},
eprint = {1605.07515},
file = {:home/hiaoxui/papers/ACL/End-to-end learning of semantic role labeling using recurrent neural networks.pdf:pdf},
isbn = {9781941643723},
issn = {0147-006X},
keywords = {CRF,LSTM,NeuroCRF,Semantic Role Labeling},
pages = {1127--1137},
pmid = {25246403},
title = {{End-to-end learning of semantic role labeling using recurrent neural networks}},
url = {http://www.aclweb.org/anthology/P15-1109},
year = {2015}
}
@inproceedings{Li2015,
abstract = {Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization$\backslash$footnote{\{}Code for the three models described in this paper can be found at www.stanford.edu/{\~{}}jiweil/ .}},
archivePrefix = {arXiv},
arxivId = {1506.01057},
author = {Li, J. and Luong, M. and Jurafsky, D.},
booktitle = {ACL},
eprint = {1506.01057},
file = {:home/hiaoxui/papers/ACL/A Hierarchical Neural Autoencoder for Paragraphs and Documents.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
title = {{A Hierarchical Neural Autoencoder for Paragraphs and Documents}},
url = {http://arxiv.org/abs/1506.01057},
year = {2015}
}
@inproceedings{Huang2017,
abstract = {This paper presents a novel template-based method to solve math word prob-lems. This method learns the mappings between math concept phrases in math word problems and their math expressions from training data. For each equation tem-plate, we automatically construct a rich template sketch by aggregating informa-tion from various problems with the same template. Our approach is implemented in a two-stage system. It first retrieves a few relevant equation system templates and aligns numbers in math word problems to those templates for candidate equation generation. It then does a fine-grained in-ference to obtain the final answer. Ex-periment results show that our method achieves an accuracy of 28.4{\%} on the lin-ear Dolphin18K benchmark, which is 10{\%} (54{\%} relative) higher than previous state-of-the-art systems while achieving an ac-curacy increase of 12{\%} (59{\%} relative) on the TS6 benchmark subset.},
author = {Huang, D. and Shi, S. and Yin, J. and Lin, C.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Learning Fine-Grained Expressions to Solve Math Word Problems.pdf:pdf},
pages = {816--825},
title = {{Learning Fine-Grained Expressions to Solve Math Word Problems}},
url = {http://www.aclweb.org/anthology/D17-1084},
year = {2017}
}
@inproceedings{Kingma2014,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.5298v1},
author = {Kingma, D. P. and Rezende, D. J. and Mohamed, S. and Welling, M.},
booktitle = {Neurips},
eprint = {arXiv:1406.5298v1},
file = {:home/hiaoxui/papers/Neurips/Semi-supervised Learning with Deep Generative Models.pdf:pdf},
issn = {10495258},
pages = {1--9},
title = {{Semi-supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
year = {2014}
}
@inproceedings{Gal2016,
author = {Gal, Y. and Ghahramani, Z.},
booktitle = {ICML},
file = {:home/hiaoxui/papers/ICML/Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:pdf},
title = {{Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning}},
volume = {48},
year = {2016}
}
@inproceedings{Wiseman2018,
archivePrefix = {arXiv},
arxivId = {1808.10122},
author = {Wiseman, S. and Shieber, S. M. and Rush, A. M.},
booktitle = {EMNLP},
doi = {arXiv:1808.10122v1},
eprint = {1808.10122},
file = {:home/hiaoxui/papers/EMNLP/Learning Neural Templates for Text Generation.pdf:pdf},
pages = {1--11},
title = {{Learning Neural Templates for Text Generation}},
year = {2018}
}
@article{Liang2016,
abstract = {For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are interested in learning semantic parsers from data, which introduces a new layer of statistical and computational issues. This article lays out the components of a statistical semantic parser, highlighting the key challenges. We will see that semantic parsing is a rich fusion of the logical and the statistical world, and that this fusion will play an integral role in the future of natural language understanding systems.},
archivePrefix = {arXiv},
arxivId = {1603.06677},
author = {Liang, P.},
doi = {10.1145/2866568},
eprint = {1603.06677},
file = {:home/hiaoxui/papers/Communications of the ACM/Learning Executable Semantic Parsers for Natural Language Understanding.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
title = {{Learning Executable Semantic Parsers for Natural Language Understanding}},
url = {http://arxiv.org/abs/1603.06677},
year = {2016}
}
@inproceedings{Kingma2015a,
abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
archivePrefix = {arXiv},
arxivId = {1506.02557},
author = {Kingma, D. P. and Salimans, T. and Welling, M.},
booktitle = {Neurips},
doi = {10.1016/S0733-8619(03)00096-3},
eprint = {1506.02557},
file = {:home/hiaoxui/papers/Neurips/Variational Dropout and the Local Reparameterization Trick.pdf:pdf},
isbn = {1506.02557},
issn = {10495258},
pages = {1--14},
pmid = {15062530},
title = {{Variational Dropout and the Local Reparameterization Trick}},
url = {http://arxiv.org/abs/1506.02557},
year = {2015}
}
@inproceedings{Lipton2018,
archivePrefix = {arXiv},
arxivId = {1807.03341},
author = {Lipton, Z. C. and Steinhardt, J.},
booktitle = {ICML},
eprint = {1807.03341},
file = {:home/hiaoxui/papers/ICML/Troubling Trends in Machine Learning Scholarship.pdf:pdf},
pages = {1--15},
title = {{Troubling Trends in Machine Learning Scholarship}},
year = {2018}
}
@inproceedings{Cianflone2018,
archivePrefix = {arXiv},
arxivId = {1806.04262},
author = {Cianflone, A. and Chi, J. and Cheung, K.},
booktitle = {ACL},
eprint = {1806.04262},
file = {:home/hiaoxui/papers/ACL/Let ' s do it “ again ” A First Computational Approach to Detecting Adverbial Presupposition Triggers.pdf:pdf},
title = {{Let ' s do it “ again ”: A First Computational Approach to Detecting Adverbial Presupposition Triggers}},
year = {2018}
}
@misc{Reiter2017,
author = {Reiter, E.},
title = {{You Need to Understand your Corpora! The Weathergov Example}},
url = {https://ehudreiter.com/2017/05/09/weathergov/},
year = {2017}
}
@inproceedings{Salimans2015,
abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
archivePrefix = {arXiv},
arxivId = {1410.6460},
author = {Salimans, T. and Kingma, D. P. and Welling, M.},
booktitle = {ICML},
eprint = {1410.6460},
file = {:home/hiaoxui/papers/ICML/Markov Chain Monte Carlo and Variational Inference Bridging the Gap.pdf:pdf},
isbn = {1410.6460},
title = {{Markov Chain Monte Carlo and Variational Inference: Bridging the Gap}},
url = {http://arxiv.org/abs/1410.6460},
year = {2015}
}
@inproceedings{Chen2008,
abstract = {We present a novel commentator system that learns language from sportscasts$\backslash$nof simulated soccer games. The system learns to parse and generate$\backslash$ncommentaries without any engineered knowledge about the English language.$\backslash$nTraining is done using only ambiguous supervision in the form of$\backslash$ntextual human commentaries and simulation states of the soccer games.$\backslash$nThe system simultaneously tries to establish correspondences between$\backslash$nthe commentaries and the simulation states as well as build a translation$\backslash$nmodel. We also present a novel algorithm, Iterative Generation Strategy$\backslash$nLearning (IGSL), for deciding which events to comment on. Human evaluations$\backslash$nof the generated commentaries indicate they are of reasonable quality$\backslash$ncompared to human commentaries.},
author = {Chen, D. L. and Mooney, R. J.},
booktitle = {ICML},
doi = {http://doi.acm.org/10.1145/1390156.1390173},
file = {:home/hiaoxui/papers/ICML/Learning to sportscast a test of grounded language acquisition.pdf:pdf},
isbn = {978-1-60558-205-4},
number = {July},
pages = {128--135},
title = {{Learning to sportscast: a test of grounded language acquisition}},
year = {2008}
}
@incollection{Rausch2011,
author = {Rausch, T. and Reinert, K.},
booktitle = {Problem Solving Handbook in Computational Biology and Bioinformatics},
doi = {10.1007/978-0-387-09760-2},
file = {:home/hiaoxui/papers/Problem Solving Handbook in Computational Biology and Bioinformatics/Practical Multiple Sequence Alignment.pdf:pdf},
isbn = {978-0-387-09759-6},
title = {{Practical Multiple Sequence Alignment}},
url = {http://link.springer.com/10.1007/978-0-387-09760-2},
year = {2011}
}
@unpublished{Doersch2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1606.05908v2},
author = {Doersch, C.},
eprint = {arXiv:1606.05908v2},
file = {:home/hiaoxui/papers/Unknown/Tutorial on Variational Autoencoders.pdf:pdf},
keywords = {neural networks,prediction,structured,unsupervised learning,variational autoencoders},
pages = {1--23},
title = {{Tutorial on Variational Autoencoders}},
year = {2016}
}
@inproceedings{Shen2016,
abstract = {We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. Experiments on Chinese-English and English-French translation show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system.},
archivePrefix = {arXiv},
arxivId = {1512.02433},
author = {Shen, S. and Cheng, Y. and He, Z. and He, W. and Wu, H. and Sun, M. and Liu, Y.},
booktitle = {ACL},
eprint = {1512.02433},
file = {:home/hiaoxui/papers/ACL/Minimum Risk Training for Neural Machine Translation.pdf:pdf},
pages = {1--9},
title = {{Minimum Risk Training for Neural Machine Translation}},
url = {http://arxiv.org/abs/1512.02433},
year = {2016}
}
@inproceedings{Kate2007,
abstract = {This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.},
author = {Kate, R. J. and Mooney, R. J.},
booktitle = {AAAI},
file = {:home/hiaoxui/papers/AAAI/Learning language semantics from ambiguous supervision.pdf:pdf},
isbn = {9781577353232},
keywords = {Natural-Language Processing,Technical Papers},
number = {July},
pages = {895--900},
title = {{Learning language semantics from ambiguous supervision}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-142.pdf},
year = {2007}
}
@inproceedings{Nie2018,
abstract = {Recent neural models for data-to-text generation are mostly based on data-driven end-to-end training over encoder-decoder networks. Even though the generated texts are mostly fluent and informative, they often generate descriptions that are not consistent with the input structured data. This is a critical issue especially in domains that require inference or calculations over raw data. In this paper, we attempt to improve the fidelity of neural data-to-text generation by utilizing pre-executed symbolic operations. We propose a framework called Operation-guided Attention-based sequence-to-sequence network (OpAtt), with a specifically designed gating mechanism as well as a quantization module for operation results to utilize information from pre-executed operations. Experiments on two sports datasets show our proposed method clearly improves the fidelity of the generated texts to the input structured data.},
archivePrefix = {arXiv},
arxivId = {1809.02735},
author = {Nie, F. and Wang, J. and Yao, J. and Pan, R. and Lin, C.},
booktitle = {EMNLP},
eprint = {1809.02735},
file = {:home/hiaoxui/papers/EMNLP/Operations-Guided Neural Networks for High Fidelity Data-To-Text Generation.pdf:pdf},
title = {{Operations-Guided Neural Networks for High Fidelity Data-To-Text Generation}},
url = {http://arxiv.org/abs/1809.02735},
year = {2018}
}
@inproceedings{Snyder2007,
abstract = {This paper addresses the task of aligning a database with a corresponding text. The goal is to link individual database entries with sentences that verbalize the same information. By providing explicit semantics-to-text links, these alignments can aid the training of natural language generation and information extraction systems. Beyond these pragmatic benefits, the alignment problem is appealing from a modeling perspective: the mappings between database entries and text sentences exhibit rich structural dependencies, unique to this task. Thus, the key challenge is to make use of as many global dependencies as possible without sacrificing tractability. To this end, we cast text-database alignment as a structured multilabel classification task where each sentence is labeled with a subset of matching database entries. In contrast to existing multilabel classifiers, our approach operates over arbitrary global features of inputs and proposed labels. We compare our model with a baseline classifier that makes locally optimal decisions. Our results show that the proposed model yields a 15{\%} relative reduction in error, and compares favorably with human performance.},
author = {Snyder, B. and Barzilay, R.},
booktitle = {IJCAI},
file = {:home/hiaoxui/papers/IJCAI/Database-text alignment via structured multilabel classification.pdf:pdf},
issn = {10450823},
keywords = {information extraction,natural language processing},
pages = {1713--1718},
title = {{Database-text alignment via structured multilabel classification}},
year = {2007}
}
@inproceedings{Ma2016,
abstract = {State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55$\backslash${\%} accuracy for POS tagging and 91.21$\backslash${\%} F1 for NER.},
archivePrefix = {arXiv},
arxivId = {1603.01354},
author = {Ma, X. and Hovy, E.},
booktitle = {ACL},
doi = {10.18653/v1/P16-1101},
eprint = {1603.01354},
file = {:home/hiaoxui/papers/ACL/End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.pdf:pdf},
isbn = {9781510827585},
issn = {1098-6596},
pmid = {25246403},
title = {{End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF}},
url = {http://arxiv.org/abs/1603.01354},
year = {2016}
}
@inproceedings{Zettlemoyer2009,
abstract = {We consider the problem of learning context-dependent mappings from sentences to logical form. The training examples are sequences of sentences annotated with lambda-calculus meaning representations. We develop an algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7{\%} accuracy.},
author = {Zettlemoyer, L. S. and Collins, M.},
booktitle = {ACL},
doi = {10.3115/1690219.1690283},
file = {:home/hiaoxui/papers/ACL/Learning context-dependent mappings from sentences to logical form.pdf:pdf},
isbn = {9781932432466},
number = {August},
pages = {976},
title = {{Learning context-dependent mappings from sentences to logical form}},
url = {http://portal.acm.org/citation.cfm?doid=1690219.1690283},
volume = {2},
year = {2009}
}
@inproceedings{Liang2009,
abstract = {A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.},
author = {Liang, P. and Jordan, M. I. and Klein, D.},
booktitle = {ACL-IJCNLP},
doi = {10.3115/1687878.1687893},
file = {:home/hiaoxui/papers/ACL-IJCNLP/Learning Semantic Correspondences with Less Supervision.pdf:pdf},
isbn = {978-1-932432-45-9},
number = {August},
pages = {91--99},
title = {{Learning Semantic Correspondences with Less Supervision}},
year = {2009}
}
@article{Feldman2004,
abstract = {In this paper, we outline an explicitly neural theory of language (NTL) that attempts to explain how many brain functions (including emotion and social cognition) work together to understand and learn language. The focus will be on the required representations and computations, although there will be some discussion of results on specific brain structures. In this approach, one does not expect to find brain areas specialized only for language or to find language processing confined to only a few areas.},
author = {Feldman, J. and Narayanan, S.},
doi = {10.1016/S0093-934X(03)00355-9},
file = {:home/hiaoxui/papers/Brain and Language/Embodied meaning in a neural theory of language.pdf:pdf},
isbn = {0093-934X, 0093-934X},
issn = {0093934X},
journal = {Brain and Language},
number = {2},
pages = {385--392},
pmid = {15068922},
title = {{Embodied meaning in a neural theory of language}},
volume = {89},
year = {2004}
}
@inproceedings{Wei2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1801.03265v3},
author = {Wei, C. and Luo, H.},
booktitle = {COLT},
eprint = {arXiv:1801.03265v3},
file = {:home/hiaoxui/papers/COLT/More Adaptive Algorithms for Adversarial Bandits.pdf:pdf},
keywords = {adaptive regret bounds,increasing learning rate,multi-armed bandit,optimistic online mirror de-,scent,semi-bandit},
pages = {1--29},
title = {{More Adaptive Algorithms for Adversarial Bandits}},
volume = {75},
year = {2018}
}
@inproceedings{Klein2002,
author = {Klein, D. and Manning, C. D.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Corpus-Based Induction of Syntactic Structure Models of Dependency and Constituency.pdf:pdf},
title = {{Corpus-Based Induction of Syntactic Structure : Models of Dependency and Constituency}},
year = {2002}
}
@article{Lapata2003,
abstract = {Ordering information is a critical task for natural language generation applications. In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation. We describe a model that learns constraints on sentence order from a corpus of domain-specific texts and an algorithm that yields the most likely order among several alternatives. We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model's task. We also assess the appropriateness of such a model for multidocument summarization.},
author = {Lapata, M.},
doi = {10.3115/1075096.1075165},
file = {:home/hiaoxui/papers/ACL/Probabilistic text structuring experiments with sentence ordering.pdf:pdf},
journal = {ACL},
pages = {545--552},
title = {{Probabilistic text structuring: experiments with sentence ordering}},
url = {http://portal.acm.org/citation.cfm?id=1075165},
year = {2003}
}
@inproceedings{Hu2018,
abstract = {Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. A key subtask is to reliably predict whether the question is unanswerable. In this paper, we propose a unified model, called U-Net, with three important components: answer pointer, no-answer pointer, and answer verifier. We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. The universal node encodes the fused information from both the question and passage, and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U-Net. Different from the state-of-art pipeline models, U-Net can be learned in an end-to-end fashion. The experimental results on the SQuAD 2.0 dataset show that U-Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0.},
archivePrefix = {arXiv},
arxivId = {1810.06638},
author = {Hu, M. and Wei, F. and Peng, Y. and Huang, Z. and Yang, N. and Li, D.},
booktitle = {AAAI},
doi = {10.1002/bit.260440802},
eprint = {1810.06638},
file = {:home/hiaoxui/papers/AAAI/Read Verify Machine Reading Comprehension with Unanswerable Questions.pdf:pdf},
issn = {0006-3592},
number = {1},
pmid = {18618904},
title = {{Read + Verify: Machine Reading Comprehension with Unanswerable Questions}},
url = {http://arxiv.org/abs/1810.06638},
year = {2018}
}
@inproceedings{Wong2007,
abstract = {This paper explores the use of statistical machine translation (SMT) methods for tactical natural language generation. We present results on using phrase-based SMT for learning to map meaning representations to natural language. Improved results are obtained by inverting a semantic parser that uses SMT methods to map sentences into meaning representations. Finally, we show that hybridizing these two approaches results in still more accurate generation systems. Automatic and human evaluation of generated sentences are presented across two domains and four languages. {\textcopyright} 2007 Association for Computational Linguistics.},
author = {Wong, Y. W. and Mooney, R. J.},
booktitle = {NAACL-HLT},
file = {:home/hiaoxui/papers/NAACL-HLT/Generation by Inverting a Semantic Parser That Uses Statistical Machine Translation.pdf:pdf},
number = {April},
pages = {172--179},
title = {{Generation by Inverting a Semantic Parser That Uses Statistical Machine Translation}},
year = {2007}
}
@inproceedings{Yang2018,
author = {Yang, Z. and Dai, Z and Salakhutdinov, R. and Cohen, W. W.},
booktitle = {ICLR},
file = {:home/hiaoxui/papers/ICLR/Breaking the Softmax Bottleneck A High-Rank RNN Language Model.pdf:pdf},
pages = {1--13},
title = {{Breaking the Softmax Bottleneck: A High-Rank RNN Language Model}},
year = {2018}
}
@inproceedings{Dong2015,
abstract = {Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct exten- sive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.},
author = {Dong, L. and Wei, F. and Zhou, M. and Xu, K.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Question Answering over Freebase with Multi-Column Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643723},
keywords = {convolutional,deep learning,question answering},
pages = {260--269},
pmid = {1683577},
title = {{Question Answering over Freebase with Multi-Column Convolutional Neural Networks}},
year = {2015}
}
@inproceedings{Chung2015,
abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
archivePrefix = {arXiv},
arxivId = {1506.02216},
author = {Chung, J. and Kastner, K. and Dinh, L. and Goel, K. and Courville, A. and Bengio, Y.},
booktitle = {Neurips},
eprint = {1506.02216},
file = {:home/hiaoxui/papers/Neurips/A Recurrent Latent Variable Model for Sequential Data.pdf:pdf},
issn = {10495258},
pages = {8},
title = {{A Recurrent Latent Variable Model for Sequential Data}},
url = {http://arxiv.org/abs/1506.02216},
year = {2015}
}
@inproceedings{Dou2018,
author = {Dou, L. and Qin, G. and Wang, J. and Yao, J. and Lin, C.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Data2Text Studio Automated Text Generation from Structured Data.pdf:pdf},
title = {{Data2Text Studio : Automated Text Generation from Structured Data}},
year = {2018}
}
@inproceedings{Raghunathan2018,
abstract = {While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most $\backslash$epsilon = 0.1 can cause more than 35{\%} test error.},
archivePrefix = {arXiv},
arxivId = {1801.09344},
author = {Raghunathan, A. and Steinhardt, J. and Liang, P.},
booktitle = {ICLR},
eprint = {1801.09344},
file = {:home/hiaoxui/papers/ICLR/Certified Defenses against Adversarial Examples.pdf:pdf},
pages = {1--15},
title = {{Certified Defenses against Adversarial Examples}},
url = {http://arxiv.org/abs/1801.09344},
year = {2018}
}
@inproceedings{Tsatsaronis2007,
abstract = {Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data and/or do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks{\^{a}}€™ links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.},
archivePrefix = {arXiv},
arxivId = {1508.01346},
author = {Tsatsaronis, G. and Vazirgiannis, M. and Androutsopoulos, I.},
booktitle = {IJCAI},
doi = {10.1145/1459352.1459355},
eprint = {1508.01346},
file = {:home/hiaoxui/papers/IJCAI/Word sense disambiguation with spreading activation networks generated from thesauri.pdf:pdf},
isbn = {0360-0300},
issn = {10450823},
keywords = {learning,natural language processing},
pages = {1725--1730},
pmid = {18353985},
title = {{Word sense disambiguation with spreading activation networks generated from thesauri}},
year = {2007}
}
@inproceedings{Koehn2003,
author = {Koehn, P. and Och, F. J. and Marcu, D.},
booktitle = {HLT-NAACL},
file = {:home/hiaoxui/papers/NAACL/Statistical phrase-based translation.pdf:pdf},
number = {June},
pages = {48--54},
title = {{Statistical Phrase-Based Translation}},
year = {2003}
}
@inproceedings{Eisenstein2008,
abstract = {This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven by lexical cohesion: the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the Bayesian framework. 1},
author = {Eisenstein, J. and Barzilay, R.},
booktitle = {EMNLP},
doi = {10.3115/1613715.1613760},
file = {:home/hiaoxui/papers/EMNLP/Bayesian unsupervised topic segmentation.pdf:pdf},
number = {October},
pages = {334},
title = {{Bayesian unsupervised topic segmentation}},
url = {http://portal.acm.org/citation.cfm?doid=1613715.1613760},
year = {2008}
}
@inproceedings{Ribeiro2018,
abstract = {Complex machine learning models for NLP are often brittle, making different predic-tions for input instances that are extremely similar semantically. To automatically de-tect this behavior for individual instances, we present semantically equivalent ad-versaries (SEAs) – semantic-preserving perturbations that induce changes in the model's predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, uni-versal replacement rules that induce ad-versaries on many instances. We demon-strate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we gener-ate high-quality local adversaries for more instances than humans, and that SEARs in-duce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models us-ing data augmentation significantly reduces bugs, while maintaining accuracy.},
author = {Ribeiro, M. T. and Singh, S and Guestrin, C.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Semantically Equivalent Adversarial Rules for Debugging NLP Models.pdf:pdf},
pages = {1--9},
title = {{Semantically Equivalent Adversarial Rules for Debugging NLP Models}},
year = {2018}
}
@article{Ramos-Soto2015,
abstract = {We present in this paper an application that automatically generates textual short-term weather forecasts for every municipality in Galicia (NW Spain), using the real data provided by the Galician Meteorology Agency (MeteoGalicia). This solution combines in an innovative way computing with perceptions techniques and strategies for linguistic description of data, together with a natural language generation (NLG) system. The application, which is named GALiWeather, extracts relevant information from weather forecast input data and encodes it into intermediate descriptions using linguistic variables and temporal references. These descriptions are later translated into natural language texts by the NLG system. The obtained forecast results have been thoroughly validated by an expert meteorologist from MeteoGalicia using a quality assessment methodology, which covers two key dimensions of a text: the accuracy of its content and the correctness of its form. Following this validation, GALiWeather will be released as a real service, offering custom forecasts for a wide public.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4925v1},
author = {Ramos-Soto, A. and Bugar{\'{i}}n, A. and Barro, S. and Taboada, J.},
doi = {10.1109/TFUZZ.2014.2328011},
eprint = {arXiv:1411.4925v1},
file = {:home/hiaoxui/papers/TFS/Linguistic descriptions for automatic generation of textual short-term weather forecasts on real prediction data.pdf:pdf},
isbn = {9783642407680},
issn = {10636706},
journal = {TFS},
keywords = {Computing with perceptions (CWPs),Linguistic descriptions of data (LDD),Natural language generation (NLG),Open data},
number = {1},
pages = {44--57},
title = {{Linguistic descriptions for automatic generation of textual short-term weather forecasts on real prediction data}},
volume = {23},
year = {2015}
}
@inproceedings{Cotterell2017,
abstract = {Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.},
archivePrefix = {arXiv},
arxivId = {1705.01684},
author = {Cotterell, R. and Eisner, J. M.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1109},
eprint = {1705.01684},
file = {:home/hiaoxui/papers/ACL/Probabilistic Typology Deep Generative Models of Vowel Inventories.pdf:pdf},
isbn = {9781945626753},
keywords = {best},
mendeley-tags = {best},
title = {{Probabilistic Typology: Deep Generative Models of Vowel Inventories}},
url = {http://arxiv.org/abs/1705.01684},
year = {2017}
}
@inproceedings{Krishnamurthy2013,
abstract = {This paper introduces Logical Semantics with Perception (LSP), a model for grounded lan- guage acquisition that learns to map natu- ral language statements to their referents in a physical environment. For example, given an image, LSP can map the statement “blue mug on the table” to the set of image seg- ments showing blue mugs on tables. LSP learns physical representations for both cate- gorical (“blue,” “mug”) and relational (“on”) language, and also learns to compose these representations to produce the referents of en- tire statements. We further introduce a weakly supervised training procedure that estimates LSP's parameters using annotated referents for entire statements, without annotated ref- erents for individual words or the parse struc- ture of the statement. We perform experiments on two applications: scene understanding and geographical question answering. We find that LSP outperforms existing, less expressive models that cannot represent relational lan- guage. We further find that weakly supervised training is competitive with fully supervised training while requiring significantly less an- notation effort.},
author = {Krishnamurthy, J. and Kollar, T.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Jointly Learning to Parse and Perceive Connecting Natural Language to the Physical World.pdf:pdf},
issn = {2307-387X},
pages = {193--206},
title = {{Jointly Learning to Parse and Perceive: Connecting Natural Language to the Physical World}},
url = {http://rtw.ml.cmu.edu/tacl2013{\%}7B{\_}{\%}7Dlsp/tacl2013-krishnamurthy-kollar.pdf},
volume = {1},
year = {2013}
}
@inproceedings{Yarowsky1995,
abstract = {This paper presents an unsupervised learn- ing algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints - that words tend to have one sense per discourse and one sense per collocation - exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96{\%}.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Yarowsky, D.},
booktitle = {ACL},
doi = {10.3115/981658.981684},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/ACL/Unsupervised word sense disambiguation rivaling supervised methods.pdf:pdf},
isbn = {0736-587X},
issn = {0736587X},
pages = {189--196},
pmid = {15003161},
title = {{Unsupervised word sense disambiguation rivaling supervised methods}},
url = {http://portal.acm.org/citation.cfm?doid=981658.981684},
year = {1995}
}
@article{Agrawal2004,
author = {Agrawal, M. and Kayal, N. and Saxena, N.},
file = {:home/hiaoxui/papers/Annals of mathematics/Primes is in P.pdf:pdf},
journal = {Annals of mathematics},
number = {2},
pages = {781--793},
title = {{Primes is in P}},
volume = {160},
year = {2004}
}
@inproceedings{Tenney2019,
author = {Tenney, I. and Xia, P. and Chen, B. and Wang, A. and Poliak, A. and McCoy, R. T. and Kim, N. and van Durme, B. and Bowman, S. R. and Das, D. and Pavlick, E.},
booktitle = {ICLR},
file = {:home/hiaoxui/papers/ICLR/What Do You Learn from Context Probing for Sentence Structure in Contextualized Word Representations.pdf:pdf},
pages = {1--17},
title = {{What Do You Learn from Context? Probing for Sentence Structure in Contextualized Word Representations}},
year = {2019}
}
@article{Collins2005,
abstract = {We describe a method for incorporating syntactic information in statistical$\backslash$nmachine translation systems. The first step of the method is to parse$\backslash$nthe source language string that is being translated. The second step$\backslash$nis to apply a series of transformations to the parse tree, effectively$\backslash$nreordering the surface string on the source language side of the$\backslash$ntranslation system. The goal of this step is to recover an underlying$\backslash$nword order that is closer to the target language word-order than$\backslash$nthe original string. The reordering approach is applied as a pre-processing$\backslash$nstep in both the training and decoding phases of a phrase-based statistical$\backslash$nMT system. We describe experiments on translation from German to$\backslash$nEnglish, showing an improvement from 25.2{\%} Bleu score for a baseline$\backslash$nsystem to 26.8{\%} Bleu score for the system with reordering, a statistically$\backslash$nsignificant improvement.},
author = {Collins, M. and Koehn, P. and Ku{\v{c}}erov{\'{a}}, I.},
doi = {10.3115/1219840.1219906},
file = {:home/hiaoxui/papers/ACL/Clause restructuring for statistical machine translation.pdf:pdf},
isbn = {1932432515},
journal = {ACL},
pages = {531--540},
title = {{Clause restructuring for statistical machine translation}},
url = {http://portal.acm.org/citation.cfm?doid=1219840.1219906},
year = {2005}
}
@article{Lake2015,
abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation.We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several “visual Turing tests” probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
archivePrefix = {arXiv},
arxivId = {10.1126/science.aab3050},
author = {Lake, B. M. and Salakhutdinov, R. and Tenenbaum, J. B.},
doi = {10.1126/science.aab3050},
eprint = {science.aab3050},
file = {:home/hiaoxui/papers/Science/Human-level concept learning through probabilistic program induction.pdf:pdf},
isbn = {0036-8075},
issn = {10959203},
journal = {Science},
number = {6266},
pages = {1332--1338},
pmid = {26659050},
primaryClass = {10.1126},
title = {{Human-level concept learning through probabilistic program induction}},
volume = {350},
year = {2015}
}
@inproceedings{Konstas2012,
abstract = {This paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. A key insight in our ap-proach is to reduce the tasks of content se-lection (" what to say ") and surface realization (" how to say ") into a common parsing prob-lem. We define a probabilistic context-free grammar that describes the structure of the in-put (a corpus of database records and text de-scribing some of them) and represent it com-pactly as a weighted hypergraph. The hyper-graph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. We propose a novel decoding algorithm for finding the best scoring derivation and generating in this set-ting. Experimental evaluation on the ATIS do-main shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study.},
author = {Konstas, I. and Lapata, M.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Concept-to-text Generation via Discriminative Reranking.pdf:pdf},
isbn = {9781937284244},
number = {July},
pages = {369--378},
title = {{Concept-to-text Generation via Discriminative Reranking}},
year = {2012}
}
@inproceedings{Feng2018,
abstract = {One way to interpret neural model predictions is to highlight the most important input features---for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word's importance is determined by either input perturbation---measuring the decrease in model confidence when that word is removed---or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.},
archivePrefix = {arXiv},
arxivId = {1804.07781},
author = {Feng, S. and Wallace, E. and Grissom, A. and Iyyer, M. and Rodriguez, P. and Boyd-Graber, J.},
booktitle = {EMNLP},
doi = {http://dx.doi.org/10.1016/0255-2701(91)80005-A},
eprint = {1804.07781},
file = {:home/hiaoxui/papers/EMNLP/Pathologies of Neural Models Make Interpretations Difficult.pdf:pdf},
isbn = {0255-2701},
issn = {19342608},
pages = {3719--3728},
title = {{Pathologies of Neural Models Make Interpretations Difficult}},
url = {http://arxiv.org/abs/1804.07781},
year = {2018}
}
@inproceedings{Bengio2007,
abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Y. and Lamblin, P. and Popovici, D. and Larochelle, H.},
booktitle = {Neurips},
doi = {citeulike-article-id:4640046},
eprint = {0500581},
file = {:home/hiaoxui/papers/Neurips/Greedy Layer-Wise Training of Deep Networks.pdf:pdf},
isbn = {0262195682},
issn = {01628828},
number = {1},
pages = {153},
pmid = {19018704},
primaryClass = {submit},
title = {{Greedy Layer-Wise Training of Deep Networks}},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
volume = {19},
year = {2007}
}
@inproceedings{Barzilay2005a,
author = {Barzilay, R. and Lapata, M.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Modeling Local Coherence An entity based approach.pdf:pdf},
keywords = {Discourse,Readability},
title = {{Modeling Local Coherence: An entity based approach}},
year = {2005}
}
@inproceedings{Wong2006,
abstract = {We present a novel statistical approach to semantic parsing, WASP, for construct-ing a complete, formal meaning represen-tation of a sentence. A semantic parser is learned given a set of sentences anno-tated with their correct meaning represen-tations. The main innovation of WASP is its use of state-of-the-art statistical ma-chine translation techniques. A word alignment model is used for lexical acqui-sition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods re-quiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.},
author = {Wong, Y. W. and Mooney, R. J.},
booktitle = {HLT},
doi = {10.3115/1220835.1220891},
file = {:home/hiaoxui/papers/HLT/Learning for Semantic Parsing with Statistical Machine Translation.pdf:pdf},
number = {June},
pages = {439--446},
title = {{Learning for Semantic Parsing with Statistical Machine Translation}},
url = {http://www.aclweb.org/anthology/N/N06/N06-1056},
year = {2006}
}
@unpublished{Yu2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1711.00073v2},
author = {Yu, R. and Zheng, S. and Anandkumar, A. and Yue, Y.},
eprint = {arXiv:1711.00073v2},
file = {:home/hiaoxui/papers/Unknown/Long-term Forecasting using Tensor-Train RNNs.pdf:pdf},
title = {{Long-term Forecasting using Tensor-Train RNNs}},
year = {2018}
}
@inproceedings{Zhou2013,
abstract = {Abstract How does the activity of one person affect that of another person? Does the strength of influence remain periodic or decay exponentially over time? In this paper, we study these critical questions in social network analysis quantitatively under the framework of multi- ...$\backslash$n},
author = {Zhou, K. and Zha, H. and Song, L.},
booktitle = {ICML},
file = {:home/hiaoxui/papers/ICML/Learning Triggering Kernels for Multi-dimensional Hawkes Processes.pdf:pdf},
pages = {1301--1309},
title = {{Learning Triggering Kernels for Multi-dimensional Hawkes Processes}},
url = {http://jmlr.org/proceedings/papers/v28/zhou13.html},
volume = {28},
year = {2013}
}
@article{Kamper2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.08135v2},
author = {Kamper, H. and Livescu, K. and Goldwater, S.},
eprint = {arXiv:1703.08135v2},
file = {:home/hiaoxui/papers/Computational Linguistics/An Embedded Segmental K-Means Model for Unsupervised Segmentation and Clustering of Speech.pdf:pdf},
journal = {Computational Linguistics},
title = {{An Embedded Segmental K-Means Model for Unsupervised Segmentation and Clustering of Speech}},
year = {2017}
}
@phdthesis{Lei2017,
abstract = {Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2017.},
author = {Lei, T.},
booktitle = {Massachusetts Institute of Technology},
file = {:home/hiaoxui/papers/Massachusetts Institute of Technology/Interpretable Neural Models for Natural Language Processing.pdf:pdf},
title = {{Interpretable Neural Models for Natural Language Processing}},
year = {2017}
}
@unpublished{Winata2019,
annote = {Strengths:
1. The authors did a bunch of experiments to support their cliams, which makes their results very convincing.
2.The authors try to address a very meaningful and practical problem: the efficiency of LSTMs.
3. The authors do some thorough thinkings about the results, which lead to some meaningful conclusions.

Weaknesses:
1. I don't think this paper is innovative enough. It's not very tough to get the idea to use matrix factorization to reduce the computational cost of neural networks, and the results are not surprising or heuristic at all. 
2. The paper Grachev et al. 2017 uses three main methods in their experiment, but only the first one is compared in your submission. I want to know the reason why you don't show the results of the other two methods.
3. Some of the conclusions in the analysis are very interesting, but lack of tight connections to the LSTMs itself.


Questions to the authors:
1. You use two methods for matrix factorization, SVD and Semi-NMF. Can you explain the functionality of the non-negativity constraint? And some other methods are not discussed in your submission, why?
2. I notice that you use CPUs instead of GPUs in the experiment. (Plz point it out if it's not true) However, the bigger the matrix is, the more advantage GPUs have over the CPUs. What's more, nowadays more and more people use GPUs for deep learning tasks to get the efficiency. So could you plz show out some results on GPU device, at least for some core experiments?
3.Some of your conclusions, like the relationship between different methods and norms, are very interesting. I'm quite curious that whether these conclusions holds for other matrices, such as randomly generated matrices. I saw some intuitional explanations in your submission, but they are not rigorous enough.


Personal Comments:

1. The authors may be lack of latex experience. E.g., they use {\textless}{\textless} in mathmode instead of $\backslash$ll, and use {\_}$\backslash${\{}dim{\}} for subscript instead of {\_}$\backslash$text{\{}dim{\}}. Also, some variables in the text is surronded by dollar marks, which make me very unconfortable.
2. A huge amount typos, some of them hinger me from understanding the paper. E.g., the first sentence in the last paragraph in page 7.
3.The "Feed-forward Neural Networks" in the bottom of page 1 should be written as "feed-forward neural networks". There is no need to use capital letters here.
4. A small suggestion: In my opinion, the experiment should be done from the vanilla RNNs, instead of LSTMs. The extension from vanilla RNNs to GRUs or LSTMs is very straightforward.
5. The eqn (1) and (6) aren't very rigorous, or even wrong. Only the people who are familiar with LSTMs could understand this formula. I know what you want to express, but could you use some special marks to distinguish the functions and matrics? It's very like a matrix production, and the ICLR readers might be confused about the (4 x 1) x (1 x 2) x (2 x 1) production. Also, I don't know the reason that you (almost) repeat these eqiations twice. You could have used these spaces to write the complete LSTMs formula.
6. I doubt whether SVD is a factorization method, but it's just a terminology problem.},
author = {Winata, G. I. and Madotto, A. and Shin, J. and Barezi, E. J.},
file = {:home/hiaoxui/papers/Unknown/Low-Rank Matrix Factorization of LSTM as Effective Model Compression.pdf:pdf},
pages = {1--25},
title = {{Low-Rank Matrix Factorization of LSTM as Effective Model Compression}},
year = {2019}
}
@inproceedings{Kim2012,
abstract = {“Grounded” language learning employs train- ing data in the form of sentences paired with relevant but ambiguous perceptual contexts. B¨ orschinger et al. (2011) introduced an ap- proach to grounded language learning based on unsupervised PCFG induction. Their ap- proach works well when each sentence po- tentially refers to one of a small set of pos- sible meanings, such as in the sportscasting task. However, it does not scale to prob- lems with a large set of potential meanings for each sentence, such as the navigation in- struction following task studied by Chen and Mooney (2011). This paper presents an en- hancement of the PCFG approach that scales to such problems with highly-ambiguous su- pervision. Experimental results on the naviga- tion task demonstrates the effectiveness of our approach.},
author = {Kim, J. and Mooney, R. J.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision.pdf:pdf},
isbn = {9781937284435},
number = {July},
pages = {433--444},
title = {{Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision}},
url = {http://dl.acm.org/citation.cfm?id=2391001{\%}5Cnhttp://www.cs.utexas.edu/users/ai-lab/pubs/kim.emnlp12.pdf},
year = {2012}
}
@article{Konstas2013,
abstract = {Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. We present a joint model that captures content selection (" what to say ") and surface realization (" how to say ") in an unsupervised domain-independent fashion. Rather than breaking up the generation process into a sequence of local decisions, we define a probabilis-tic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). We recast generation as the task of finding the best derivation tree for a set of database records and describe an algorithm for decoding in this framework that allows to intersect the grammar with additional information capturing fluency and syntactic well-formedness constraints. Experimental evaluation on several domains achieves re-sults competitive with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.},
author = {Konstas, I. and Lapata, M.},
doi = {10.1613/jair.4025},
file = {:home/hiaoxui/papers/JAIR/A global model for concept-to-text generation.pdf:pdf},
issn = {10769757},
journal = {JAIR},
pages = {305--346},
title = {{A global model for concept-to-text generation}},
volume = {48},
year = {2013}
}
@inproceedings{Chen2011,
abstract = {The ability to understand natural-language instructions is crit- ical to building intelligent agents that interact with humans. We present a systemthat learns to transformnatural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environ- ments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation in- structions for these environments is used for training and test- ing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the sys- tem is able to automatically learn to correctly interpret a rea- sonable fraction of the complex instructions in this corpus. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chen, D. L. and Mooney, R. J.},
booktitle = {AAAI},
doi = {10.1.1.221.8069/???},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/AAAI/Learning to Interpret Natural Language Navigation Instructions from Observations.pdf:pdf},
isbn = {9781577355083},
issn = {1938-7228},
keywords = {aa dd sgsafg},
number = {August},
pages = {859--865},
pmid = {23459267},
title = {{Learning to Interpret Natural Language Navigation Instructions from Observations}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/download/3701/3974},
year = {2011}
}
@inproceedings{Ge2006,
abstract = {Semantic parsing is the task of mapping natural language sentences to complete formal meaning representations. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. In this paper, we investigate discriminative reranking upon a baseline semantic parser, SCISSOR, where the composition of meaning representations is guided by syntax. We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics. We report experimental results on two real applications, an interpreter for coaching instructions in robotic soccer and a natural-language database interface. The results show that reranking can improve the performance on the coaching interpreter, but not on the database interface.},
author = {Ge, R. and Mooney, R. J.},
booktitle = {ACL},
doi = {10.3115/1273073.1273107},
file = {:home/hiaoxui/papers/ACL/Discriminative reranking for semantic parsing.pdf:pdf},
number = {July},
pages = {263},
title = {{Discriminative reranking for semantic parsing}},
url = {http://portal.acm.org/citation.cfm?id=1273107},
year = {2006}
}
@inproceedings{Du2019,
abstract = {One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an {\$}m{\$} hidden node shallow neural network with ReLU activation and {\$}n{\$} training data, we show as long as {\$}m{\$} is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function. Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.},
archivePrefix = {arXiv},
arxivId = {1810.02054},
author = {Du, S. S. and Zhai, X. and Poczos, B. and Singh, A.},
booktitle = {ICLR},
eprint = {1810.02054},
file = {:home/hiaoxui/papers/ICLR/Gradient Descent Provably Optimizes Over-parameterized Neural Networks.pdf:pdf},
pages = {1--15},
title = {{Gradient Descent Provably Optimizes Over-parameterized Neural Networks}},
url = {http://arxiv.org/abs/1810.02054},
year = {2019}
}
@inproceedings{Sennrich2016,
abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
archivePrefix = {arXiv},
arxivId = {1508.07909},
author = {Sennrich, R. and Haddow, B. and Birch, A.},
booktitle = {ACL},
eprint = {1508.07909},
file = {:home/hiaoxui/papers/ACL/Neural Machine Translation of Rare Words with Subword Units.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
title = {{Neural Machine Translation of Rare Words with Subword Units}},
url = {http://arxiv.org/abs/1508.07909},
year = {2016}
}
@inproceedings{Barrett2018,
abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.},
archivePrefix = {arXiv},
arxivId = {1807.04225},
author = {Barrett, D. G. T. and Hill, F. and Santoro, A. and Morcos, A. S. and Lillicrap, T.},
booktitle = {ICML},
eprint = {1807.04225},
file = {:home/hiaoxui/papers/ICML/Measuring abstract reasoning in neural networks.pdf:pdf},
issn = {1938-7228},
title = {{Measuring abstract reasoning in neural networks}},
url = {http://arxiv.org/abs/1807.04225},
year = {2018}
}
@unpublished{Oroojlooyjadid2017,
abstract = {The beer game is a widely used in-class game that is played in supply chain management classes to demonstrate the bullwhip effect. The game is a decentralized, multi-agent, cooperative problem that can be modeled as a serial supply chain network in which agents cooperatively attempt to minimize the total cost of the network even though each agent can only observe its own local information. Each agent chooses order quantities to replenish its stock. Under some conditions, a base-stock replenishment policy is known to be optimal. However, in a decentralized supply chain in which some agents (stages) may act irrationally (as they do in the beer game), there is no known optimal policy for an agent wishing to act optimally. We propose a machine learning algorithm, based on deep Q-networks, to optimize the replenishment decisions at a given stage. When playing alongside agents who follow a base-stock policy, our algorithm obtains near-optimal order quantities. It performs much better than a base-stock policy when the other agents use a more realistic model of human ordering behavior. Unlike most other algorithms in the literature, our algorithm does not have any limits on the beer game parameter values. Like any deep learning algorithm, training the algorithm can be computationally intensive, but this can be performed ahead of time; the algorithm executes in real time when the game is played. Moreover, we propose a transfer learning approach so that the training performed for one agent and one set of cost coefficients can be adapted quickly for other agents and costs. Our algorithm can be extended to other decentralized multi-agent cooperative games with partially observed information, which is a common type of situation in real-world supply chain problems.},
archivePrefix = {arXiv},
arxivId = {1708.05924},
author = {Oroojlooyjadid, A. and Nazari, M. and Snyder, L. and Tak{\'{a}}{\v{c}}, M.},
eprint = {1708.05924},
file = {:home/hiaoxui/papers/Unknown/A Deep Q-Network for the Beer Game A Reinforcement Learning algorithm to Solve Inventory Optimization Problems.pdf:pdf},
keywords = {beer game,inventory optimization,reinforcement learning},
pages = {1--38},
title = {{A Deep Q-Network for the Beer Game: A Reinforcement Learning algorithm to Solve Inventory Optimization Problems}},
url = {http://arxiv.org/abs/1708.05924},
year = {2017}
}
@inproceedings{He2017,
abstract = {We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10{\%} relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.},
author = {He, L. and Lee, K. and Lewis, M. and Zettlemoyer, L. S.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1044},
file = {:home/hiaoxui/papers/ACL/Deep Semantic Role Labeling What Works and What's Next.pdf:pdf},
isbn = {9781945626753},
pages = {473--483},
title = {{Deep Semantic Role Labeling: What Works and What's Next}},
url = {http://aclweb.org/anthology/P17-1044},
year = {2017}
}
@article{Grosz1986,
abstract = {In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse. In this theory, discourse structure is composed of three separate but interre- lated components: the structure of the sequence of utterances (called the linguistic structure), a struc- ture of purposes (called the intentional structure), and the state of focus of attention (called the attentional state). The linguistic structure consists of segments of the discourse into which the utter- ances naturally aggregate. The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds. The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse. The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions. The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses. Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored. This theory provides a framework for describing the processing of utterances in a discourse. Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and track- ing the discourse through the operation of the mechanisms associated with attentional state. This processing description specifies in these recognition tasks the role of information from the discourse and from the participants' knowledge of the domain.},
author = {Grosz, B. J. and Sidner, C. L.},
doi = {10.14348/molcells.2014.0104},
file = {:home/hiaoxui/papers/Computational Linguistics/Attention, intentions, and the structure of discourse.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
number = {3},
pages = {175--204},
title = {{Attention, intentions, and the structure of discourse}},
url = {http://dl.acm.org/citation.cfm?id=12458},
volume = {12},
year = {1986}
}
@article{Kennedy2005,
abstract = {In this article we develop a semantic typology of gradable predicates, with special emphasis on deverbal adjectives. We argue for the linguistic relevance of this typology by demonstrating that the distribution and interpretation of degree modifiers is sensitive to its twomajor classificatory parameters: (1) whether a gradable predicate is associated with what we call an open or closed scale, and (2) whether the standard of comparison for the applicability of the predicate is absolute or relative to a context. We further showthat the classification of an important subclass of adjectives within the typology is largely predictable. Specifically, the scale structure of a deverbal gradable adjective correlates either with the algebraic part structure of the event denoted by its source verb or with the part structure of the entities to which the adjective applies. These correla- tions underscore the fact that gradability is characteristic not only of adjectives but also of verbs and nouns, and that scalar properties are shared by categorially distinct but derivationally related expressions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kennedy, C. and McNally, L.},
doi = {10.1353/lan.2005.0071},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/Language/Scale Structure, Degree Modification and the Semantics of Gradable Predicates.pdf:pdf},
isbn = {00978507},
issn = {1535-0665},
journal = {Language},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
number = {2},
pages = {345--381},
pmid = {25246403},
title = {{Scale Structure, Degree Modification and the Semantics of Gradable Predicates}},
url = {http://muse.jhu.edu/content/crossref/journals/language/v081/81.2kennedy.pdf},
volume = {81},
year = {2005}
}
@inproceedings{Jiang2014,
abstract = {Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which for-malizes the preference for both easy and diverse samples into a general regularizer. This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method signifi-cantly outperforms the conventional SPL on three real-world datasets. Specifical-ly, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.},
author = {Jiang, L. and Meng, D. and Yu, S. and Lan, Z. and Shan, S. and Hauptmann, A.},
booktitle = {Neurips},
file = {:home/hiaoxui/papers/Neurips/Self-Paced Learning with Diversity.pdf:pdf},
issn = {10495258},
pages = {2078--2086},
title = {{Self-Paced Learning with Diversity}},
url = {http://papers.nips.cc/paper/5568-self-paced-learning-with-diversity},
year = {2014}
}
@unpublished{Liang2013,
abstract = {Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.},
archivePrefix = {arXiv},
arxivId = {1109.6841v1},
author = {Liang, P.},
doi = {10.1162/COLI_a_00127},
eprint = {1109.6841v1},
file = {:home/hiaoxui/papers/Unknown/Lambda Dependency-Based Compositional Semantics.pdf:pdf},
isbn = {9781932432879},
issn = {0891-2017},
number = {2},
pages = {389--446},
pmid = {25246403},
title = {{Dependency-Based Compositional Semantics}},
url = {http://www.mitpressjournals.org/doi/10.1162/COLI{\_}a{\_}00127},
volume = {39},
year = {2013}
}
@inproceedings{Zhang2017a,
abstract = {In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning of se-quential structured prediction problems. Our NCRF-AE consists of two parts: an encoder which is a CRF model enhanced by deep neural networks, and a decoder which is a generative model trying to re-construct the input. Our model has a uni-fied structure with different loss functions for labeled and unlabeled data with shared parameters. We developed a variation of the EM algorithm for optimizing both the encoder and the decoder simultaneously by decoupling their parameters. Our ex-perimental results over the Part-of-Speech (POS) tagging task on eight different lan-guages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised sce-narios.},
author = {Zhang, X. and Jiang, Y. and Peng, H. and Tu, K. and Goldwasser, D.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Semi-Supervised Structured Prediction with Neural CRF Autoencoder.pdf:pdf},
pages = {1702--1712},
title = {{Semi-Supervised Structured Prediction with Neural CRF Autoencoder}},
url = {http://aclweb.org/anthology/D17-1179{\%}0Ahttp://purduenlp.cs.purdue.edu/pubs/emnlp{\_}2017.pdf},
year = {2017}
}
@article{Shrivastava2011,
abstract = {The flow around an arrangement of two cylinders in tandem exhibits a remarkably complex behaviour that is of interest for many engineering problems, such as environmental flows or structural design. In the present paper, a Large Eddy Simulation using a staggered Cartesian grid has been performed for the flow around two cylinders in tandem of diameter D = 20 mm and height H = 50 mm submerged in an open channel with height h = 60 mm . The two axes have a streamwise spacing of 2D. The Reynolds number is 1500, based on the cylinder diameter and the free-stream velocity u . The results obtained show that no vortex shedding occurs in the gap between the two cylinders where the separated shear layers produced by the upstream cylinder reattach on the surface of the downstream one. The flow separates on the top of the first cylinder with the presence of two spiral nodes known as owl-face configuration. On top of the downstream cylinder, the flow is attached. A complex mean flow develops in the gap and also behind the second cylinder. Comparisons with PIV measurements reveal good general agreement, but there are differences concerning some details of the flow in the gap between the cylinders.},
author = {Shrivastava, A. and Malisiewicz, T. and Gupta, A. and Efros, A. A.},
doi = {10.1145/2070781.2024188},
file = {:home/hiaoxui/papers/ACM Transactions on Graphics/Data-driven visual similarity for cross-domain image matching.pdf:pdf},
isbn = {9781450308076},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {image matching,image re-,paintings,re-photography,saliency,sketches,trieval,visual memex,visual similarity},
number = {6},
pages = {1},
title = {{Data-driven visual similarity for cross-domain image matching}},
url = {http://dl.acm.org/citation.cfm?doid=2070781.2024188},
volume = {30},
year = {2011}
}
@inproceedings{Rudinger2018,
abstract = {We present a model for semantic proto-role labeling (SPRL) using an adapted bidirectional LSTM encoding strategy that we call "Neural-Davidsonian": predicate-argument structure is represented as pairs of hidden states corresponding to predicate and argument head tokens of the input sequence. We demonstrate: (1) state-of-the-art results in SPRL, and (2) that our network naturally shares parameters between attributes, allowing for learning new attribute types with limited added supervision.},
archivePrefix = {arXiv},
arxivId = {1804.07976},
author = {Rudinger, R. and Teichert, A. and Culkin, R. and Zhang, S. and van Durme, B.},
booktitle = {EMNLP},
eprint = {1804.07976},
file = {:home/hiaoxui/papers/EMNLP/Neural-Davidsonian Semantic Proto-role Labeling.pdf:pdf},
pages = {944--955},
title = {{Neural-Davidsonian Semantic Proto-role Labeling}},
url = {http://arxiv.org/abs/1804.07976},
year = {2018}
}
@article{WebberB.2003,
author = {{Webber B.} and {Stone M.} and Joshi, A. and {Knott A}},
file = {:home/hiaoxui/papers/Computational Linguistics/Anaphora and discourse structure.pdf:pdf},
journal = {Computational Linguistics},
pages = {545--587},
title = {{Anaphora and discourse structure }},
volume = {29},
year = {2003}
}
@inproceedings{Arnaud2017,
author = {Arnaud, A. S. and Beck, D.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Identifying Cognate Sets Across Dictionaries of Related Languages.pdf:pdf},
pages = {2519--2528},
title = {{Identifying Cognate Sets Across Dictionaries of Related Languages}},
year = {2017}
}
@inproceedings{Maddison2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.00712v3},
author = {Maddison, C. J. and Mnih, A. and Teh, Y.W.},
booktitle = {ICLR},
eprint = {arXiv:1611.00712v3},
file = {:home/hiaoxui/papers/ICLR/The Concrete Distribution A Continuous Relaxation of Discrete Random Variables.pdf:pdf},
pages = {1--20},
title = {{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
year = {2017}
}
@inproceedings{Jia2016,
abstract = {Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.},
archivePrefix = {arXiv},
arxivId = {1606.03622},
author = {Jia, R. and Liang, P.},
booktitle = {ACL},
doi = {10.18653/v1/P16-1002},
eprint = {1606.03622},
file = {:home/hiaoxui/papers/ACL/Data Recombination for Neural Semantic Parsing.pdf:pdf},
isbn = {9781510827585},
issn = {04194217},
pages = {12--22},
pmid = {22251136},
title = {{Data Recombination for Neural Semantic Parsing}},
url = {http://arxiv.org/abs/1606.03622},
year = {2016}
}
@inproceedings{Krishnamurthy2012,
abstract = {We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependency- parsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-the- art accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80{\%} precision and 56{\%} recall, despite never having seen an annotated logical form.},
author = {Krishnamurthy, J. and Mitchell, T. M.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Weakly Supervised Training of Semantic Parsers.pdf:pdf},
isbn = {9781937284435},
number = {July},
pages = {754--765},
title = {{Weakly Supervised Training of Semantic Parsers}},
year = {2012}
}
@inproceedings{Artzi2015,
abstract = {We propose a grammar induction tech-nique for AMR semantic parsing. While previous grammar induction techniques were designed to re-learn a new parser for each target application, the recently anno-tated AMR Bank provides a unique op-portunity to induce a single model for un-derstanding broad-coverage newswire text and support a wide range of applications. We present a new model that combines CCG parsing to recover compositional aspects of meaning and a factor graph to model non-compositional phenomena, such as anaphoric dependencies. Our ap-proach achieves 66.2 Smatch F1 score on the AMR bank, significantly outperform-ing the previous state of the art.},
author = {Artzi, Y. and Lee, K. and Zettlemoyer, L. S.},
booktitle = {EMNLP},
doi = {10.18653/v1/D15-1198},
file = {:home/hiaoxui/papers/EMNLP/Broad-coverage CCG Semantic Parsing with AMR.pdf:pdf},
isbn = {9781941643327},
number = {September},
pages = {1699--1710},
title = {{Broad-coverage CCG Semantic Parsing with AMR}},
url = {http://aclweb.org/anthology/D15-1198},
year = {2015}
}
@inproceedings{Miao2015,
abstract = {Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.},
archivePrefix = {arXiv},
arxivId = {1511.06038},
author = {Miao, Y. and Yu, L. and Blunsom, P.},
booktitle = {ICML},
eprint = {1511.06038},
file = {:home/hiaoxui/papers/EMNLP/Language as a Latent Variable Discrete Generative Models for Sentence Compression.pdf:pdf},
isbn = {0898716004},
number = {Mcmc},
title = {{Neural Variational Inference for Text Processing}},
url = {http://arxiv.org/abs/1511.06038},
volume = {48},
year = {2015}
}
@unpublished{Mikolov2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, T. and Corrado, G. and Chen, K. and Dean, J.},
eprint = {arXiv:1301.3781v3},
file = {:home/hiaoxui/papers/Unknown/Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}
@inproceedings{Ge2005,
abstract = {We introduce a learning semantic parser, SCISSOR, thatmaps natural-language sentences to a detailed, formal, meaning-representation language. It first uses an integrated statistical parser to produce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We present experimental results demonstrating that SCISSOR produces more accurate semantic representations than several previous approaches.},
author = {Ge, R. and Mooney, R.},
booktitle = {CoNLL},
doi = {10.3115/1706543.1706546},
file = {:home/hiaoxui/papers/CoNLL/A Statistical Semantic Parser that Integrates Syntax and Semantics.pdf:pdf},
number = {June},
pages = {9--16},
title = {{A Statistical Semantic Parser that Integrates Syntax and Semantics}},
year = {2005}
}
@article{Berant2015,
abstract = {Semantic parsers conventionally construct logical forms bottom-up in a fixed order, re- sulting in the generation of many extraneous partial logical forms. In this paper, we com- bine ideas from imitation learning and agenda- based parsing to train a semantic parser that searches partial logical forms in a more strate- gic order. Empirically, our parser reduces the number of constructed partial logical forms by an order of magnitude, and obtains a 6x-9x speedup over fixed-order parsing, while main- taining comparable accuracy.},
author = {Berant, J. and Liang, P.},
file = {:home/hiaoxui/papers/TACL/Imitation Learning of Agenda-based Semantic Parsers.pdf:pdf},
journal = {TACL},
pages = {545--558},
title = {{Imitation Learning of Agenda-based Semantic Parsers}},
volume = {3},
year = {2015}
}
@inproceedings{Edunov2017,
abstract = {There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT'14 German-English translation as well as Gigaword abstractive summarization. On the larger WMT'14 English-French translation task, sequence-level training achieves 41.5 BLEU which is on par with the state of the art.},
archivePrefix = {arXiv},
arxivId = {1711.04956},
author = {Edunov, S. and Ott, M. and Auli, M. and Grangier, D. and Ranzato, M.},
booktitle = {NAACL},
eprint = {1711.04956},
file = {:home/hiaoxui/papers/NAACL/Classical Structured Prediction Losses for Sequence to Sequence Learning.pdf:pdf},
title = {{Classical Structured Prediction Losses for Sequence to Sequence Learning}},
url = {http://arxiv.org/abs/1711.04956},
year = {2017}
}
@inproceedings{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, M. T. and Singh, S. and Guestrin, C.},
booktitle = {KDD},
doi = {10.18653/v1/N16-3020},
eprint = {1602.04938},
file = {:home/hiaoxui/papers/KDD/Why Should I Trust You Explaining the Predictions of Any Classifier.pdf:pdf},
isbn = {9781450321389},
issn = {9781450321389},
pmid = {214160309},
title = {{"Why Should I Trust You?": Explaining the Predictions of Any Classifier}},
url = {http://arxiv.org/abs/1602.04938},
year = {2016}
}
@inproceedings{Hershcovich2017,
abstract = {We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.},
archivePrefix = {arXiv},
arxivId = {1704.00552},
author = {Hershcovich, D. and Abend, O. and Rappoport, A.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1104},
eprint = {1704.00552},
file = {:home/hiaoxui/papers/ACL/A Transition-Based Directed Acyclic Graph Parser for UCCA.pdf:pdf},
isbn = {9781945626753},
pages = {1127--1138},
title = {{A Transition-Based Directed Acyclic Graph Parser for UCCA}},
url = {http://arxiv.org/abs/1704.00552},
year = {2017}
}
@article{Navigli2007,
abstract = {Most word sense disambiguation (WSD) methods require large quantities of manually annotated training data and/or do not exploit fully the semantic relations of thesauri. We propose a new unsupervised WSD algorithm, which is based on generating Spreading Activation Networks (SANs) from the senses of a thesaurus and the relations between them. A new method of assigning weights to the networks{\^{a}}€™ links is also proposed. Experiments show that the algorithm outperforms previous unsupervised approaches to WSD.},
archivePrefix = {arXiv},
arxivId = {1508.01346},
author = {Navigli, R.},
doi = {10.1145/1459352.1459355},
eprint = {1508.01346},
file = {:home/hiaoxui/papers/ACM Computing Surveys/Word Sense Disambiguation A Survey.pdf:pdf},
isbn = {0360-0300},
issn = {10450823},
journal = {ACM Computing Surveys},
number = {2},
pages = {1725--1730},
pmid = {18353985},
title = {{Word Sense Disambiguation: A Survey}},
volume = {41},
year = {2007}
}
@article{Lazer2009,
author = {Lazer, D. and Pentland, A. and Adamic, L. and Aral, S. and Barab{\'{a}}si, A. and Brewer, D. and Christakis, N. and Contractor, N. and Fowler, J. and Gutmann, M. and Jebara, T. and King, G. and Macy, M. and Roy, D. and {Van Alstyne}, M.},
doi = {10.13618/j.issn.1001-5728.2014.06.012},
file = {:home/hiaoxui/papers/Science/Computational Social Science.pdf:pdf},
isbn = {0265-1491},
issn = {10015728},
journal = {Science},
keywords = {Diquat,Forensic toxicological analysis,Spectrophotometry,Urine},
number = {February},
title = {{Computational Social Science}},
volume = {323},
year = {2009}
}
@article{Zettlemoyer2005,
abstract = {This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.},
archivePrefix = {arXiv},
arxivId = {1207.1420},
author = {Zettlemoyer, L. S. and Collins, M.},
doi = {10.1093/acprof:oso/9780199654680.003.0006},
eprint = {1207.1420},
file = {:home/hiaoxui/papers/UAI/Learning to Map Sentences to Logical Form Structured Classification with Probabilistic Categorial Grammars(2).pdf:pdf},
isbn = {0974903914},
journal = {UAI},
title = {{Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars}},
url = {http://arxiv.org/abs/1207.1420},
year = {2005}
}
@inproceedings{Reddi2018a,
abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of ex-ponential moving averages of squared past gradients. It has been empirically ob-served that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous anal-ysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with " long-term memory " of past gradi-ents, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
archivePrefix = {arXiv},
arxivId = {1709.01507},
author = {Reddi, S. J. and Kale, S. and Kumar, S.},
booktitle = {ICLR},
doi = {10.1134/S0001434607010294},
eprint = {1709.01507},
file = {:home/hiaoxui/papers/ICLR/On the convergence of adam and beyond.pdf:pdf},
issn = {08695652},
pages = {1--23},
title = {{On the convergence of adam and beyond}},
url = {https://openreview.net/forum?id=ryQu7f-RZ{\%}0Ahttps://openreview.net/pdf?id=ryQu7f-RZ},
year = {2018}
}
@inproceedings{Liang2011,
abstract = {Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.},
archivePrefix = {arXiv},
arxivId = {1109.6841v1},
author = {Liang, P. and Jordan, M. I. and Klein, D.},
booktitle = {ACL},
doi = {10.1162/COLI_a_00127},
eprint = {1109.6841v1},
file = {:home/hiaoxui/papers/ACL/Learning Dependency-Based Compositional Semantics.pdf:pdf},
isbn = {9781932432879},
issn = {0891-2017},
number = {2},
pages = {389--446},
pmid = {25246403},
title = {{Learning Dependency-Based Compositional Semantics}},
url = {http://www.mitpressjournals.org/doi/10.1162/COLI{\_}a{\_}00127},
volume = {39},
year = {2011}
}
@inproceedings{Salimans2016,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, T. and Goodfellow, I. and Zaremba, W. and Cheung, V. and Radford, A. and Chen, X.},
booktitle = {Neurips},
eprint = {1606.03498},
file = {:home/hiaoxui/papers/Neurips/Improved Techniques for Training GANs.pdf:pdf},
pages = {1--10},
title = {{Improved Techniques for Training GANs}},
url = {http://arxiv.org/abs/1606.03498},
year = {2016}
}
@unpublished{Borman2009,
abstract = {This tutorial discusses the ExpectationMaximization (EM) algorithm of Demp- ster, Laird and Rubin 1. The approach taken follows that of an unpublished note by Stuart Russel, but fleshes out some of the gory details. In order to ensure that the presentation is reasonably self-contained, some of the results on which the derivation of the algorithm is based are presented prior to the main results. The EM algorithm has become a popular tool in statistical estimation problems involving incomplete data, or in problems which can be posed in a sim- ilar form, such as mixture estimation 3, 4. The EM algorithm has also been used in various motion estimation frameworks 5 and variants of it have been used in multiframe superresolution restoration methods which combine motion estimation along the lines of 2.},
author = {Borman, S.},
doi = {10.1097/RLU.0b013e3181b06c41\r00003072-200909000-00002},
file = {:home/hiaoxui/papers/Unknown/The Expectation Maximization Algorithm A short tutorial.pdf:pdf},
isbn = {0387952845},
issn = {15360229},
number = {x},
pages = {1--9},
pmid = {19692813},
title = {{The Expectation Maximization Algorithm A short tutorial}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.150.8193{\&}rep=rep1{\&}type=pdf},
volume = {25},
year = {2009}
}
@inproceedings{Chen2017b,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.06038v3},
author = {Chen, Q. and Wei, S. and Inkpen, D.},
booktitle = {ACL},
eprint = {arXiv:1609.06038v3},
file = {:home/hiaoxui/papers/ACL/Enhanced LSTM for Natural Language Inference.pdf:pdf},
number = {2017},
title = {{Enhanced LSTM for Natural Language Inference}},
year = {2017}
}
@inproceedings{Kate2006,
author = {Kate, R. J. and Mooney, R. J.},
booktitle = {ACL},
doi = {10.3115/1220175.1220290},
file = {:home/hiaoxui/papers/ACL/Using string-kernels for learning semantic parsers.pdf:pdf},
isbn = {1932432655},
number = {July},
pages = {913--920},
title = {{Using string-kernels for learning semantic parsers}},
url = {http://dl.acm.org/citation.cfm?id=1220290},
year = {2006}
}
@inproceedings{Blondel2018,
abstract = {We study in this paper Fenchel-Young losses, a generic way to construct convex loss functions from a convex regularizer. We provide an in-depth study of their properties in a broad setting and show that they unify many well-known loss functions. When constructed from a generalized entropy, which includes well-known entropies such as Shannon and Tsallis entropies, we show that Fenchel-Young losses induce a predictive probability distribution and develop an efficient algorithm to compute that distribution for separable entropies. We derive conditions for generalized entropies to yield a distribution with sparse support and losses with a separation margin. Finally, we present both primal and dual algorithms to learn predictive models with generic Fenchel-Young losses.},
archivePrefix = {arXiv},
arxivId = {1805.09717},
author = {Blondel, M. and Martins, A. F. T. and Niculae, V.},
booktitle = {ICML},
eprint = {1805.09717},
file = {:home/hiaoxui/papers/ICML/Learning Classifiers with Fenchel-Young Losses Generalized Entropies, Margins, and Algorithms.pdf:pdf},
pages = {1--21},
title = {{Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms}},
url = {http://arxiv.org/abs/1805.09717},
year = {2018}
}
@inproceedings{Taketa2004,
abstract = {1. 1. The hemoglobins found in various members of the Felidae have been separated and compared with respect to structure in relation to 2,3-diphosphoglycerate (2,3-DPG) sensitivity. 2. 2. Multiple hemoglobin components are found in the blood of all Felidae, and they are characterized by the presence of one of two types of $\beta$-chains that are common to the members of the family. 3. 3. The two types, A-$\beta$ and B-$\beta$, are distinguished from one another by differences in positions of the $\beta$T-1 and $\beta$T-XIV peptides in fingerprints of tryptic digests. 4. 4. Components that contain the A-$\beta$ type are invariably 2,3-DPG-sensitive whereas those that contain the B-$\beta$ type are insensitive. 5. 5. The A-$\beta$ and B-$\beta$ chains are apparently products of nonallelic genes but are found in widely variable proportions in mixtures of hemoglobins in the blood of different members of the Felidae. {\textcopyright} 1973.},
author = {Taketa, F.},
booktitle = {EMNLP},
doi = {10.1016/0305-0491(73)90144-2},
file = {:home/hiaoxui/papers/EMNLP/TextRank Brining Order into Texts.pdf:pdf},
issn = {03050491},
keywords = {2,3-diphosphoglycerate,Felidae,Hemoglobin,acetylation,electrophoresis,fingerprint,oxygen affinity},
title = {{TextRank: Brining Order into Texts}},
year = {2004}
}
@inproceedings{Lei2015,
abstract = {The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2{\%} accuracy on the fine-grained sentiment classification task.},
archivePrefix = {arXiv},
arxivId = {1508.04112},
author = {Lei, T. and Barzilay, R. and Jaakkola, T. S.},
booktitle = {EMNLP},
eprint = {1508.04112},
file = {:home/hiaoxui/papers/EMNLP/Molding CNNs for text non-linear, non-consecutive convolutions.pdf:pdf},
isbn = {9781941643327},
number = {September},
pages = {1565--1575},
title = {{Molding CNNs for text: non-linear, non-consecutive convolutions}},
url = {http://arxiv.org/abs/1508.04112},
year = {2015}
}
@inproceedings{Tran2016,
abstract = {In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.},
archivePrefix = {arXiv},
arxivId = {1609.09007},
author = {Tran, K. and Bisk, Y. and Vaswani, A. and Marcu, D. and Knight, K.},
booktitle = {EMNLP},
doi = {10.18653/v1/W16-5907},
eprint = {1609.09007},
file = {:home/hiaoxui/papers/EMNLP/Unsupervised Neural Hidden Markov Models.pdf:pdf},
pages = {63--71},
title = {{Unsupervised Neural Hidden Markov Models}},
url = {http://arxiv.org/abs/1609.09007},
year = {2016}
}
@unpublished{Storks2019,
abstract = {Commonsense knowledge and commonsense reasoning are some of the main bottlenecks in machine intelligence. In the NLP community, many benchmark datasets and tasks have been created to address commonsense reasoning for language understanding. These tasks are designed to assess machines' ability to acquire and learn commonsense knowledge in order to reason and understand natural language text. As these tasks become instrumental and a driving force for commonsense research, this paper aims to provide an overview of existing tasks and benchmarks, knowledge resources, and learning and inference approaches toward commonsense reasoning for natural language understanding. Through this, our goal is to support a better understanding of the state of the art, its limitations, and future challenges.},
archivePrefix = {arXiv},
arxivId = {1904.01172},
author = {Storks, S. and Gao, Q. and Chai, J. Y.},
eprint = {1904.01172},
file = {:home/hiaoxui/papers/Unknown/Commonsense Reasoning for Natural Language Understanding A Survey of Benchmarks, Resources, and Approaches.pdf:pdf},
pages = {1--60},
title = {{Commonsense Reasoning for Natural Language Understanding: A Survey of Benchmarks, Resources, and Approaches}},
url = {http://arxiv.org/abs/1904.01172},
year = {2019}
}
@inproceedings{Niculae2018,
abstract = {Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function. SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns probability mass to all structures, including implausible ones. Importantly, SparseMAP can be computed using only calls to a MAP oracle, making it applicable to problems with intractable marginal inference, e.g., linear assignment. Sparsity makes gradient backpropagation efficient regardless of the structure, enabling us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.},
archivePrefix = {arXiv},
arxivId = {1802.04223},
author = {Niculae, V. and Martins, A. F. T. and Blondel, M. and Cardie, C.},
booktitle = {ICML},
eprint = {1802.04223},
file = {:home/hiaoxui/papers/ICML/SparseMAP Differentiable Sparse Structured Inference.pdf:pdf},
title = {{SparseMAP: Differentiable Sparse Structured Inference}},
url = {http://arxiv.org/abs/1802.04223},
year = {2018}
}
@unpublished{Frankle2018,
abstract = {Neural network compression techniques are able to reduce the parameter counts of trained networks by over 90{\%}--decreasing storage requirements and improving inference performance--without compromising accuracy. However, contemporary experience is that it is difficult to train small architectures from scratch, which would similarly improve training performance. We articulate a new conjecture to explain why it is easier to train large networks: the "lottery ticket hypothesis." It states that large networks that train successfully contain subnetworks that--when trained in isolation--converge in a comparable number of iterations to comparable accuracy. These subnetworks, which we term "winning tickets," have won the initialization lottery: their connections have initial weights that make training particularly effective. We find that a standard technique for pruning unnecessary network weights naturally uncovers a subnetwork which, at the start of training, comprised a winning ticket. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis. We consistently find winning tickets that are less than 20{\%} of the size of several fully-connected, convolutional, and residual architectures for MNIST and CIFAR10. Furthermore, winning tickets at moderate levels of pruning (20-50{\%} of the original network size) converge up to 6.7x faster than the original network and exhibit higher test accuracy.},
archivePrefix = {arXiv},
arxivId = {1803.03635},
author = {Frankle, J. and Carbin, M.},
doi = {arXiv:1803.03635v1},
eprint = {1803.03635},
file = {:home/hiaoxui/papers/Unknown/The Lottery Ticket Hypothesis Finding Small, Trainable Neural Networks.pdf:pdf},
title = {{The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks}},
url = {http://arxiv.org/abs/1803.03635},
year = {2018}
}
@article{Edmonds2002,
abstract = {We develop a new computational model for representing the fine-grained meanings of near-synonyms and the differences between them. We also develop a lexical-choice process that can decide which of several near-synonyms is most appropriate in a particular situation. This research has direct applications in machine translation and text generation.We first identify the problems of representing near-synonyms in a computational lexicon and show that no previous model adequately accounts for near-synonymy. We then propose a preliminary theory to account for near-synonymy, relying crucially on the notion of granularity of representation, in which the meaning of a word arises out of a context-dependent combination of a context-independent core meaning and a set of explicit differences to its near-synonyms. That is, near-synonyms cluster together.We then develop a clustered model of lexical knowledge, derived from the conventional ontological model. The model cuts off the ontology at a coarse grain, thus avoiding an awkward proliferation of language-dependent concepts in the ontology, yet maintaining the advantages of efficient computation and reasoning. The model groups near-synonyms into subconceptual clusters that are linked to the ontology. A cluster differentiates near-synonyms in terms of fine-grained aspects of denotation, implication, expressed attitude, and style. The model is general enough to account for other types of variation, for instance, in collocational behavior.An efficient, robust, and flexible fine-grained lexical-choice process is a consequence of a clustered model of lexical knowledge. To make it work, we formalize criteria for lexical choice as preferences to express certain concepts with varying indirectness, to express attitudes, and to establish certain styles. The lexical-choice process itself works on two tiers: between clusters and between near-synonyns of clusters. We describe our prototype implementation of the system, called I-Saurus.},
author = {Edmonds, P. and Hirst, G.},
doi = {10.1162/089120102760173625},
file = {:home/hiaoxui/papers/Computational Linguistics/Near-Synonymy and Lexical Choice.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
number = {2},
pages = {105--144},
title = {{Near-Synonymy and Lexical Choice}},
url = {http://www.mitpressjournals.org/doi/10.1162/089120102760173625},
volume = {28},
year = {2002}
}
@inproceedings{Wang2013,
abstract = {{\textcopyright} 2013 Association for Computational Linguistics. NLP models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting. A recently re-popularized form of regularization is to generate fake training data by repeatedly adding noise to real data. We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data. We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs. We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently. The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transductive extension. Applied to text classification and NER, our method provides a {\textgreater}1{\%} absolute performance gain over use of standard L2 regularization.},
author = {Wang, S. I. and Wang, M. and Wager, S. and Liang, P. and Manning, C. D.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Feature noising for log-linear structured prediction.pdf:pdf},
isbn = {9781937284978},
number = {October},
pages = {1170--1179},
title = {{Feature noising for log-linear structured prediction}},
year = {2013}
}
@inproceedings{Caglayan2019,
abstract = {Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.},
archivePrefix = {arXiv},
arxivId = {1903.08678},
author = {Caglayan, O. and Madhyastha, P. and Specia, L. and Barrault, L.},
booktitle = {NAACL-HLT},
eprint = {1903.08678},
file = {:home/hiaoxui/papers/NAACL-HLT/Probing the Need for Visual Context in Multimodal Machine Translation.pdf:pdf},
number = {i},
title = {{Probing the Need for Visual Context in Multimodal Machine Translation}},
url = {http://arxiv.org/abs/1903.08678},
year = {2019}
}
@inproceedings{He2018,
abstract = {Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the relations between them. The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates.},
archivePrefix = {arXiv},
arxivId = {1805.04787},
author = {He, L. and Lee, K. and Levy, O. and Zettlemoyer, L. S.},
booktitle = {ACL},
eprint = {1805.04787},
file = {:home/hiaoxui/papers/ACL/Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling.pdf:pdf},
pages = {1--6},
title = {{Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling}},
url = {http://arxiv.org/abs/1805.04787},
year = {2018}
}
@inproceedings{Lin2017a,
abstract = {Traditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the KB may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call it as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which can be used as seed mentions to bridge other mentions and the uninformative entities. In this work, we select most linkable mentions as seed mentions and disambiguate other mentions by comparing them with the seed mentions rather than directly with the entities. Our experiments on linking mentions to seven automatically mined lists show promising results and demonstrate the effectiveness of our approach.},
author = {Lin, Y. and Lin, C. and Ji, H.},
booktitle = {ACL},
doi = {10.18653/v1/P17-2085},
file = {:home/hiaoxui/papers/ACL/List-only Entity Linking.pdf:pdf},
pages = {536--541},
title = {{List-only Entity Linking}},
url = {http://aclweb.org/anthology/P17-2085},
year = {2017}
}
@unpublished{Weiss2019,
annote = {Stengths:
1. This paper is well-motivated and properly developed. Begin with some dicussions about the limitations of Hawkes and Cox processes, the authors propose their model, which directly addresses these problems.
2. This paper is technically innovative. It uses wavelet as kernel to both address difficulties and keep the interrogability.
3. The experiments are convincing, with several metrics to make their model tested.
4. Just as mentioned in the title, this work might be very meaningful for clinical domains. The additivity of Hawekes process might cause some trouble in clinical filed, but the the authors bypass this problem with a flexible reduce function, which could be either additive or not. 
5. The model proposed by the authors are very flexible, which could do both forecasting and nowcasting. Some details in the model are carefully designed, like the relative- to absolute-time transformations. 


Weaknesses:
1. For people unfamiliar with wavelet decomposition and reconstruction like me, this paper is very hard to follow. Some terms related to wavelet are lack of explanation, like scale parameters and coefficient. More literature of wavelet should be introduced.
2. Some formulas and notations are not very clear. E.g., in sec 3, the definition of q is very confusing, and S mentioned above is also lack of explanation. The 
$\backslash$bigotimes in the second paragraph of Sec 3 is also undefined, but I guess it's also a Hadamard product. Some notations are not very rigorous, like the defnition of g{\_}{\{}es{\}} in 1st paragraph in Sec 3, where t is a variable but $\backslash$mathcal{\{}R{\}} is a space.
3. The technical part of this paper might be over-complex. The functionalities of some defined functions are rather simple, which coule have been absorbed by other functions.
4. I know the difficulities for hunting datasets, but this work would have more impact if it could be tested on more real world datasets.


Questions:
1. You mentioned appendix twice in your paper, but I didn't find any appendix. Did you forget about that?
2. Could you plz point it our how e and s interact with the function g{\_}{\{}es{\}} more clearly?
3. You emphesize the interrogability of your model, but only two paragraphs are related to this property if I understand right. Could you plz give more details about how and why is interrogable?

Personal Comments:
1. I feel very hard to read this paper, partly because I'm lack of necessary experience in this field.
2. The authors should give more space for the model, which is full of mathematical definitions and some of them are very unclear.},
author = {Weiss, J. C.},
file = {:home/hiaoxui/papers/Unknown/Clinical Risk Wavelet Reconstruction Networks for Marked Point Processes.pdf:pdf},
pages = {1--11},
title = {{Clinical Risk: Wavelet Reconstruction Networks for Marked Point Processes}},
year = {2019}
}
@inproceedings{Pang2002,
abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
author = {Pang, B. and Lee, L. and Vaithyanathan, S.},
booktitle = {EMNLP},
doi = {10.1515/9783110239171.151},
file = {:home/hiaoxui/papers/EMNLP/Thumbs up Sentiment Classification using Machine Learning Techniques.pdf:pdf},
issn = {0003-5696},
title = {{Thumbs up? Sentiment Classification using Machine Learning Techniques}},
year = {2002}
}
@inproceedings{Chen2017,
abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution {\$}p(z){\$} and decoding distribution {\$}p(x|z){\$}, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
archivePrefix = {arXiv},
arxivId = {1611.02731},
author = {Chen, X. and Kingma, D. P. and Salimans, T. and Duan, Y. and Dhariwal, P. and Schulman, J. and Sutskever, I. and Abbeel, P.},
booktitle = {ICLR},
eprint = {1611.02731},
file = {:home/hiaoxui/papers/ICLR/Variational Lossy Autoencoder.pdf:pdf},
pages = {1--17},
title = {{Variational Lossy Autoencoder}},
url = {http://arxiv.org/abs/1611.02731},
year = {2017}
}
@inproceedings{Yang2015,
abstract = {Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge. In this work, we first identify several semantic structures behind humor and design sets of features for each structure, and next employ a com- putational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations.},
author = {Yang, D. and Lavie, A. and Dyer, C. and Hovy, E.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Humor Recognition and Humor Anchor Extraction.pdf:pdf},
isbn = {9781941643327},
number = {September},
pages = {2367--2376},
title = {{Humor Recognition and Humor Anchor Extraction}},
year = {2015}
}
@inproceedings{Grachev2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1708.05963v1},
author = {Grachev, A. M. and Ignatov, D. I. and Savchenko, A. V.},
booktitle = {International Conference on Pattern Recognition and Artificial Intelligence},
eprint = {arXiv:1708.05963v1},
file = {:home/hiaoxui/papers/International Conference on Pattern Recognition and Artificial Intelligence/Neural Networks Compression for Language Modeling.pdf:pdf},
keywords = {language modeling,low-rank factorization,lstm,pruning,quantization,rnn},
title = {{Neural Networks Compression for Language Modeling}},
year = {2017}
}
@inproceedings{Khani2016,
abstract = {Can we train a system that, on any new input, either says "don't know" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100{\%} precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.},
archivePrefix = {arXiv},
arxivId = {1606.06368},
author = {Khani, F. and Rinard, M. and Liang, P.},
booktitle = {ACL},
eprint = {1606.06368},
file = {:home/hiaoxui/papers/ACL/Unanimous Prediction for 100{\%} Precision with Application to Learning Semantic Mappings.pdf:pdf},
isbn = {9781510827585},
title = {{Unanimous Prediction for 100{\%} Precision with Application to Learning Semantic Mappings}},
url = {http://arxiv.org/abs/1606.06368},
year = {2016}
}
@article{Kennedy1999,
author = {Kennedy, C. and McNally, L.},
file = {:home/hiaoxui/papers/SALT/From event structure to scale structure Degree modification in deverbal adjectives.pdf:pdf},
journal = {SALT},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
number = {2},
pages = {163--180},
title = {{From event structure to scale structure: Degree modification in deverbal adjectives}},
url = {http://elanguage.net/journals/salt/article/download/9.163/1681},
year = {1999}
}
@book{Huang1990,
author = {Huang, X. and Ariki, Y. and Jack, M. A.},
publisher = {Edinburgh University Press},
title = {{Hidden Markov Models for Speech Recognition}},
year = {1990}
}
@inproceedings{Ballard2004,
author = {Yu, C. and Ballard, D.},
booktitle = {AAAI},
doi = {10.1080/10656210509484979},
file = {:home/hiaoxui/papers/AAAI/On the Integration of Grounding Language and Lear ning Objects.pdf:pdf},
isbn = {0262511835},
issn = {10656219},
keywords = {Copyright {\textcopyright} 2004 American Association for Artifici},
pages = {488--493},
title = {{On the Integration of Grounding Language and Lear ning Objects}},
url = {http://www.indiana.edu/{~}dll/papers/yu{\_}aaai04.pdf},
year = {2004}
}
@inproceedings{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, R.},
booktitle = {ICCV},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:home/hiaoxui/papers/ICCV/Fast R-CNN.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
pages = {1440--1448},
pmid = {23739795},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}
@article{Ng1997,
abstract = {In recent years, there has been a flurry of research into empirical, corpus-based learning approaches to natural language processing (NLP). Most empir- ical NLP work to date has focused on relatively low-level language processing such as part-of- speech tagging, text segmentation, and syntactic parsing. The success of these approaches has stim- ulated research in using empirical learning tech- niques in other facets of NLP, including semantic analysis—uncovering the meaning of an utter- ance. This article is an introduction to some of the emerging research in the application of corpus- based learning techniques to problems in semantic interpretation. In particular, we focus on two im- portant problems in semantic interpretation, namely, word-sense disambiguation and semantic parsing.},
author = {Ng, H. T. and Zelle, J.},
doi = {10.1609/aimag.v18i4.1321},
file = {:home/hiaoxui/papers/AI Magazine/Corpus-Based Approaches to Semantic Interpretation in Natural Language Processing.pdf:pdf},
issn = {0738-4602},
journal = {AI Magazine},
number = {4},
pages = {45--64},
title = {{Corpus-Based Approaches to Semantic Interpretation in Natural Language Processing}},
volume = {18},
year = {1997}
}
@article{Barzilay2008,
abstract = {This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.},
author = {Barzilay, R. and Lapata, M.},
doi = {10.1162/coli.2008.34.1.1},
file = {:home/hiaoxui/papers/Computational Linguistics/Modeling Local Coherence An Entity-Based Approach.pdf:pdf},
isbn = {1932432515},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {1--34},
title = {{Modeling Local Coherence: An Entity-Based Approach}},
url = {http://www.mitpressjournals.org/doi/10.1162/coli.2008.34.1.1},
volume = {34},
year = {2008}
}
@inproceedings{Heinzerling2017,
abstract = {We introduce automatic verification as a post-processing step for entity linking (EL). The proposed method trusts EL sys-tem results collectively, by assuming en-tity mentions are mostly linked correctly, in order to create a semantic profile of the given text using geospatial and temporal information, as well as fine-grained entity types. This profile is then used to auto-matically verify each linked mention indi-vidually, i.e., to predict whether it has been linked correctly or not. Verification allows leveraging a rich set of global and pairwise features that would be prohibitively expen-sive for EL systems employing global in-ference. Evaluation shows consistent im-provements across datasets and systems. In particular, when applied to state-of-the-art systems, our method yields an abso-lute improvement in linking performance of up to 1.7 F 1 on AIDA/CoNLL'03 and up to 2.4 F 1 on the English TAC KBP 2015 TEDL dataset.},
author = {Heinzerling, B. and Strube, M. and Lin, C.},
booktitle = {EACL},
file = {:home/hiaoxui/papers/EACL/Trust, but Verify! Better Entity Linking through Automatic Verification.pdf:pdf},
isbn = {9781510838604},
pages = {828--838},
title = {{Trust, but Verify! Better Entity Linking through Automatic Verification}},
volume = {1},
year = {2017}
}
@inproceedings{Nenkova2004,
abstract = {We present an empirically grounded method for evaluating content selection in summariza- tion. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative im- portance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus im- proves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.},
author = {Nenkova, A. and Passonneau, R.},
booktitle = {NAACL-HLT},
file = {:home/hiaoxui/papers/NAACL-HLT/Evaluating content selection in summarization The pyramid method.pdf:pdf},
pages = {145--152},
title = {{Evaluating content selection in summarization: The pyramid method}},
url = {papers2://publication/uuid/DC675E84-0A45-48B7-A26C-F08B4B9398D3},
year = {2004}
}
@inproceedings{Bowman2015,
abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
archivePrefix = {arXiv},
arxivId = {1511.06349},
author = {Bowman, S. R. and Vilnis, L. and Vinyals, O. and Dai, A. M. and Jozefowicz, R. and Bengio, S.},
booktitle = {CoNLL},
doi = {10.18653/v1/K16-1002},
eprint = {1511.06349},
file = {:home/hiaoxui/papers/CoNLL/Generating Sentences from a Continuous Space.pdf:pdf},
pages = {10--21},
title = {{Generating Sentences from a Continuous Space}},
url = {http://arxiv.org/abs/1511.06349},
year = {2015}
}
@article{Brown1993,
abstract = {We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.},
author = {Brown, P. F. and Pietra, S. A. D. and Pietra, V. J. D. and Mercer, R. L.},
doi = {10.1080/08839514.2011.559906},
file = {:home/hiaoxui/papers/Computational Linguistics/The mathematics of statistical machine translation Parameter estimation.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
number = {2},
pages = {263--311},
pmid = {3046723},
title = {{The mathematics of statistical machine translation: Parameter estimation}},
url = {http://www.aclweb.org/anthology/J93-2003},
volume = {19},
year = {1993}
}
@inproceedings{Peng2017,
abstract = {Neural attention models have achieved great success in different NLP tasks. How- ever, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we de- scribe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural atten- tion model and our results are also compet- itive against state-of-the-art systems that do not use extra linguistic resources.},
archivePrefix = {arXiv},
arxivId = {1702.05053},
author = {Peng, X. and Wang, C. and Gildea, D. and Xue, N.},
booktitle = {EACL},
eprint = {1702.05053},
file = {:home/hiaoxui/papers/EACL/Addressing the Data Sparsity Issue in Neural AMR Parsing.pdf:pdf},
isbn = {9781510838604},
pages = {366--375},
title = {{Addressing the Data Sparsity Issue in Neural AMR Parsing}},
url = {http://arxiv.org/abs/1702.05053},
volume = {1},
year = {2017}
}
@inproceedings{Andrews2017,
abstract = {Lexical resources such as dictionaries and gazetteers are often used as auxiliary data for tasks such as part-of-speech induction and named-entity recognition. However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the ex-pense of features which generalize better. In this paper, we investigate a more robust approach: we stipulate that the lexicon is the result of an assumed generative process. Practically, this means that we may treat the lexical resources as observations under the proposed generative model. The lexi-cal resources provide training data for the generative model without requiring sepa-rate data to estimate lexical feature weights. We evaluate the proposed approach in two settings: part-of-speech induction and low-resource named-entity recognition.},
author = {Andrews, N. and Dredze, M. and van Durme, B. and Eisner, J. M.},
booktitle = {ACL},
doi = {10.18653/v1/p17-1095},
file = {:home/hiaoxui/papers/ACL/Bayesian Modeling of Lexical Resources for Low-Resource Settings.pdf:pdf},
pages = {1029--1039},
title = {{Bayesian Modeling of Lexical Resources for Low-Resource Settings}},
year = {2017}
}
@inproceedings{Jang2017,
abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
archivePrefix = {arXiv},
arxivId = {1611.01144},
author = {Jang, E. and Gu, S. and Poole, B.},
booktitle = {ICLR},
eprint = {1611.01144},
file = {:home/hiaoxui/papers/ICLR/Categorical Reparameterization with Gumbel-Softmax.pdf:pdf},
issn = {1611.01144},
pages = {1--13},
title = {{Categorical Reparameterization with Gumbel-Softmax}},
url = {http://arxiv.org/abs/1611.01144},
year = {2017}
}
@inproceedings{Hale2018,
archivePrefix = {arXiv},
arxivId = {1806.04127},
author = {Hale, J. and Dyer, C. and Kuncoro, A. and Brennan, J. R.},
booktitle = {ACL},
eprint = {1806.04127},
file = {:home/hiaoxui/papers/ACL/Finding Syntax in Human Encephalography with Beam Search.pdf:pdf},
number = {2014},
title = {{Finding Syntax in Human Encephalography with Beam Search}},
year = {2018}
}
@inproceedings{Wong2008,
abstract = {We formulate semantic parsing as a parsing problem on a synchronous context free grammar (SCFG) which is automatically built on the corpus of natural language sentences and the representation of semantic outputs. We then present an online learning fr...},
author = {Wong, Y. W. and Mooney, R. J.},
booktitle = {ICTAI},
doi = {10.1109/ICTAI.2008.96},
file = {:home/hiaoxui/papers/ICTAI/Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus.pdf:pdf},
isbn = {9780769534404},
issn = {10823409},
number = {June},
pages = {135--142},
title = {{Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus}},
volume = {2},
year = {2008}
}
@article{Vieira2017,
abstract = {Pruning hypotheses during dynamic program- ming is commonly used to speed up inference in settings such as parsing. Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy. This poses a difficult machine learning problem, which we tackle with the LOLS algorithm. LOLS training must continually compute the ef- fects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime.},
author = {Vieira, T. and Eisner, J. M.},
file = {:home/hiaoxui/papers/TACL/Learning to Prune Exploring the Frontier of Fast and Accurate Parsing.pdf:pdf},
issn = {2307-387X},
journal = {TACL},
number = {2011},
pages = {263--278},
title = {{Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing}},
volume = {5},
year = {2017}
}
@inproceedings{Eisner2016,
abstract = {A probabilistic or weighted grammar implies a posterior probability distribution over possi-ble parses of a given input sentence. One often needs to extract information from this distri-bution, by computing the expected counts (in the unknown parse) of various grammar rules, constituents, transitions, or states. This re-quires an algorithm such as inside-outside or forward-backward that is tailored to the gram-mar formalism. Conveniently, each such al-gorithm can be obtained by automatically dif-ferentiating an " inside " algorithm that merely computes the log-probability of the evidence (the sentence). This mechanical procedure produces correct and efficient code. As for any other instance of back-propagation, it can be carried out manually or by software. This pedagogical paper carefully spells out the con-struction and relates it to traditional and non-traditional views of these algorithms.},
author = {Eisner, J. M.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Inside-Outside and Forward-Backward Algorithms Are Just Backprop.pdf:pdf},
pages = {1--17},
title = {{Inside-Outside and Forward-Backward Algorithms Are Just Backprop}},
year = {2016}
}
@inproceedings{Kipf2016,
abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {1611.07308},
author = {Kipf, T. N. and Welling, M.},
booktitle = {Neurips},
eprint = {1611.07308},
file = {:home/hiaoxui/papers/Neurips/Variational Graph Auto-Encoders.pdf:pdf},
number = {2},
pages = {1--3},
title = {{Variational Graph Auto-Encoders}},
url = {http://arxiv.org/abs/1611.07308},
year = {2016}
}
@article{Wang2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.03601v4},
author = {Wang, M. and Deng, W.},
eprint = {arXiv:1802.03601v4},
file = {:home/hiaoxui/papers/Neurocomputing/Deep Visual Domain Adaptation A Survey.pdf:pdf},
journal = {Neurocomputing},
pages = {1--20},
title = {{Deep Visual Domain Adaptation: A Survey}},
year = {2018}
}
@inproceedings{Zettlemoyer2007,
abstract = {We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammarfor example allowing flexible word order, or insertion of lexical items with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86 {\%} F-measure in recovering fully correct semantic analyses and 95.9{\%} F-measure by a partial-match criterion, a more than 5 {\%} improvement over the 90.3{\%} partial-match figure reported by He and Young (2006).},
author = {Zettlemoyer, L. S. and Collins, M.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Online learning of relaxed CCG grammars for parsing to logical form.pdf:pdf},
number = {June},
pages = {678--687},
title = {{Online learning of relaxed CCG grammars for parsing to logical form}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.71.3926},
year = {2007}
}
@inproceedings{Sebastian2011,
author = {Sebastian, N. and Rother, C. and Bagon, S. and Sharp, T. and Yao, B. and Kohli, P.},
booktitle = {ICCV},
file = {:home/hiaoxui/papers/ICCV/Decision Tree Fields.pdf:pdf},
title = {{Decision Tree Fields}},
year = {2011}
}
@inproceedings{Levy2014,
author = {Levy, O. and Goldberg, Y.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Dependency-Based Word Embeddings.pdf:pdf},
keywords = {Mentoplasty,Microgenia,Retrogenia,Submental incision,citation},
mendeley-tags = {citation},
pages = {302--308},
title = {{Dependency-Based Word Embeddings}},
year = {2014}
}
@unpublished{Jing2017,
abstract = {The recent work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNN) in creating artistic fantastic imagery by separating and recombing the image content and style. This process of using CNN to migrate the semantic content of one image to different styles is referred to as Neural Style Transfer. Since then, Neural Style Transfer has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention from computer vision researchers and several methods are proposed to either improve or extend the original neural algorithm proposed by Gatys et al. However, there is no comprehensive survey presenting and summarizing recent Neural Style Transfer literature. This review aims to provide an overview of the current progress towards Neural Style Transfer, as well as discussing its various applications and open problems for future research.},
archivePrefix = {arXiv},
arxivId = {1705.04058},
author = {Jing, Y. and Yang, Y. and Feng, Z. and Ye, J. and Song, M.},
eprint = {1705.04058},
file = {:home/hiaoxui/papers/Unknown/Neural Style Transfer A Review.pdf:pdf},
title = {{Neural Style Transfer: A Review}},
url = {http://arxiv.org/abs/1705.04058},
year = {2017}
}
@inproceedings{Andor2016,
abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.},
archivePrefix = {arXiv},
arxivId = {1603.06042},
author = {Andor, D. and Alberti, C. and Weiss, D. and Severyn, A. and Presta, A. and Ganchev, K. and Petrov, S. and Collins, M.},
booktitle = {ACL},
eprint = {1603.06042},
file = {:home/hiaoxui/papers/ACL/Globally Normalized Transition-Based Neural Networks.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
title = {{Globally Normalized Transition-Based Neural Networks}},
url = {http://arxiv.org/abs/1603.06042},
year = {2016}
}
@inproceedings{Peng2018,
abstract = {We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence 1 (Egidi and Gerrig, 2009) and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower per-plexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.},
author = {Peng, N. and Ghazvininejad, M. and May, J. and Knight, K.},
booktitle = {NAACL},
file = {:home/hiaoxui/papers/NAACL/Towards Controllable Story Generation.pdf:pdf},
pages = {43--49},
title = {{Towards Controllable Story Generation}},
url = {http://visionandlanguage.net/workshop2018/cdrom/pdf/W18-1505.pdf},
year = {2018}
}
@unpublished{Anonymous2019,
annote = {What's this paper about?

This paper proposes a dataset (TACOQA) for temporal commonsense reasoning, which consists of 13 k QA pairs. It covers 5 aspects of temporal commonsense phenomena, thus it could serve as a metric and quantify the performance of models in these aspects. As experiment, the authors select 4 popular models and test them on TACOQA and shows that temporal commonsense reasoning is still challenging for these models.

Contribution:

1. It proposes a dataset (TACOQA) to evaluate the models' ability of temporal commonsense reasoning in 5 seperate aspects.
2. It evaluates multiple popular systems on TACOQA.

Strengths:

1.

Weakness:

1. Human evaluation is small and not convincing
2. The proposed baseline model is too naive.
3. A masked baseline model is needed.

Questions to the authors:

1. How do you treat the numerical values?
2. What is unit normalization
3. Do you rule out the sentences that is irrelevant with temporal commonsense?

Typos:

1. L128 Upper case.
2. L098 Should use $\backslash$citet.},
author = {Anonymous},
file = {:home/hiaoxui/papers/Unknown/Going on a vacation takes longer than Going for a walk A Study of Temporal Commonsense Understanding.pdf:pdf;:home/hiaoxui/papers/Unknown/Going on a vacation takes longer than Going for a walk A Study of Temporal Commonsense Understanding(2).pdf:pdf},
pages = {1--6},
title = {{"Going on a vacation" takes longer than "Going for a walk": A Study of Temporal Commonsense Understanding}},
year = {2019}
}
@inproceedings{White2018,
abstract = {We investigate neural models' ability to capture lexicosyntactic inferences: inferences triggered by the interaction of lexical and syntactic information. We take the task of event factuality prediction as a case study and build a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts. We use this dataset, which we make publicly available, to probe the behavior of current state-of-the-art neural systems, showing that these systems make certain systematic errors that are clearly visible through the lens of factuality prediction.},
archivePrefix = {arXiv},
arxivId = {1808.06232},
author = {White, A. S. and Rudinger, R. and Rawlins, K. and van Durme, B.},
booktitle = {EMNLP},
eprint = {1808.06232},
file = {:home/hiaoxui/papers/EMNLP/Lexicosyntactic Inference in Neural Models.pdf:pdf},
number = {i},
pages = {4717--4724},
title = {{Lexicosyntactic Inference in Neural Models}},
url = {http://arxiv.org/abs/1808.06232},
year = {2018}
}
@inproceedings{Stede2000,
author = {Stede, M.},
booktitle = {INLG},
file = {:home/hiaoxui/papers/INLG/The hyperonym problem revisited Conceptual and lexical hierarchies in language.pdf:pdf},
keywords = {Lexical choice,lexical choice},
mendeley-tags = {lexical choice},
pages = {93--99},
title = {{The hyperonym problem revisited: Conceptual and lexical hierarchies in language}},
year = {2000}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, D. P. and Ba, J. L.},
booktitle = {ICLR},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
file = {:home/hiaoxui/papers/ICLR/Adam A Method for Stochastic Optimization.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
pages = {1--15},
pmid = {172668},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@unpublished{Collins2011,
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/Probabilistic Context-Free Grammars (PCFGs).pdf:pdf},
issn = {0387307680},
pages = {1--18},
title = {{Probabilistic Context-Free Grammars (PCFGs)}},
url = {http://www.cs.columbia.edu/{~}mcollins/courses/nlp2011/notes/pcfgs.pdf{\%}5Cnpapers2://publication/uuid/2C19D3AA-F64C-49AE-91D6-33EB72F49683},
year = {2011}
}
@inproceedings{Martins2016,
abstract = {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.},
archivePrefix = {arXiv},
arxivId = {1602.02068},
author = {Martins, A. F. T. and Astudillo, R. F.},
booktitle = {ICML},
doi = {10.1109/72.279181},
eprint = {1602.02068},
file = {:home/hiaoxui/papers/ICML/From Softmax to Sparsemax A Sparse Model of Attention and Multi-Label Classification.pdf:pdf},
isbn = {9781510829008},
issn = {19410093},
pmid = {18267787},
title = {{From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification}},
url = {http://arxiv.org/abs/1602.02068},
volume = {48},
year = {2016}
}
@inproceedings{Berg-kirkpatrick2010,
abstract = {We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.},
author = {Berg-kirkpatrick, T. and Bouchard-C{\^{o}}t{\'{e}}, A. and DeNero, J. and Klein, D.},
booktitle = {NAACL},
file = {:home/hiaoxui/papers/NAACL/Painless unsupervised learning with features.pdf:pdf},
isbn = {1-932432-65-5},
keywords = {Best,EM},
mendeley-tags = {Best,EM},
number = {June},
pages = {582----590},
title = {{Painless unsupervised learning with features}},
url = {http://portal.acm.org/citation.cfm?id=1858082},
year = {2010}
}
@inproceedings{Lample2018,
abstract = {Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT'14 English-French and WMT'16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.},
archivePrefix = {arXiv},
arxivId = {1804.07755},
author = {Lample, G. and Ott, M. and Conneau, A. and Denoyer, L. and Ranzato, M.},
booktitle = {EMNLP},
doi = {10.1053/j.jvca.2010.06.032},
eprint = {1804.07755},
file = {:home/hiaoxui/papers/EMNLP/Phrase-Based {\&} Neural Unsupervised Machine Translation.pdf:pdf},
isbn = {0892-0915 (Print)$\backslash$r0892-0915 (Linking)},
issn = {1532-8422},
pmid = {20829068},
title = {{Phrase-Based {\&} Neural Unsupervised Machine Translation}},
url = {http://arxiv.org/abs/1804.07755},
year = {2018}
}
@incollection{Pierrehumberl1990,
author = {Pierrehumberl, J. and Hirschberg, J.},
booktitle = {Intentions in Communication},
file = {:home/hiaoxui/papers/Intentions in Communication/The Meaning of Intonational Contours in the Interpretation of Discourse.pdf:pdf},
title = {{The Meaning of Intonational Contours in the Interpretation of Discourse}},
year = {1990}
}
@inproceedings{Malisiewicz2011,
abstract = {This paper proposes a conceptually simple but surpris- ingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspon- dence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar- SVMs is thus defined by a single positive instance and mil- lions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generaliza- tion. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computa- tional cost increase. But the central benefit of our approach is that it creates an explicit association between each de- tection and a single training exemplar. Because most de- tections show good alignment to their associated exemplar, it is possible to transfer any available exemplar meta-data (segmentation, geometric structure, 3D model, etc.) directly onto the detections, which can then be used as part of over- all scene understanding.},
author = {Malisiewicz, T. and Gupta, A. and Efros, A. A.},
booktitle = {ICCV},
file = {:home/hiaoxui/papers/ICCV/Ensemble of Exemplar SVMs for Object Detection and Beyond.pdf:pdf},
isbn = {9781457711022},
pages = {89--96},
title = {{Ensemble of Exemplar SVMs for Object Detection and Beyond}},
year = {2011}
}
@inproceedings{Wilson2005,
author = {Wilson, T. and Wiebe, J and Hoffmann, P.},
booktitle = {EMNLP-HLT},
file = {:home/hiaoxui/papers/EMNLP-HLT/Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.pdf:pdf},
title = {{Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis}},
year = {2005}
}
@inproceedings{Bollacker2008,
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
author = {Bollacker, K. and Evans, C. and Paritosh, P. and Sturge, T. and Taylor, J.},
booktitle = {International Conference on Management of Data (SIGMOD)},
doi = {10.1145/1376616.1376746},
file = {:home/hiaoxui/papers/International Conference on Management of Data (SIGMOD)/Freebase a collaboratively created graph database for structuring human knowledge.pdf:pdf},
isbn = {9781605581026},
issn = {07308078},
pages = {1247--1250},
pmid = {3105260},
title = {{Freebase: a collaboratively created graph database for structuring human knowledge}},
url = {http://doi.acm.org/10.1145/1376616.1376746},
year = {2008}
}
@inproceedings{Ritter2010,
author = {Ritter, A. and Cherry, C. and Dolan, W. B.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Data-Driven Response Generation in Social Media.pdf:pdf},
title = {{Data-Driven Response Generation in Social Media}},
year = {2010}
}
@unpublished{Sripada,
author = {Sripada, S. G. and Reiter, E. and Hunter, J. and Yu, J.},
file = {:home/hiaoxui/papers/Unknown/SumTime-Meteo Parallel Corpus of Naturally Occurring Forecast Texts and Weather Data.pdf:pdf},
pages = {1--13},
title = {{SumTime-Meteo: Parallel Corpus of Naturally Occurring Forecast Texts and Weather Data}},
year = {2003}
}
@inproceedings{Jean2015,
abstract = {Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English-{\textgreater}German translation and almost as high performance as state-of-the-art English-{\textgreater}French translation system.},
archivePrefix = {arXiv},
arxivId = {1412.2007},
author = {Jean, S. and Cho, K. and Memisevic, R. and Bengio, Y.},
booktitle = {ACL},
eprint = {1412.2007},
file = {:home/hiaoxui/papers/ACL/On Using Very Large Target Vocabulary for Neural Machine Translation.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
title = {{On Using Very Large Target Vocabulary for Neural Machine Translation}},
url = {http://arxiv.org/abs/1412.2007},
volume = {000},
year = {2015}
}
@inproceedings{Collins2002,
abstract = {New algorithms for training tagging models, as an alternative to MEM and CRF models},
author = {Collins, M.},
booktitle = {EMNLP},
doi = {10.3115/1118693.1118694},
file = {:home/hiaoxui/papers/EMNLP/Discrimative Training Methods for Hidden Markov Models Theory and Experiments with Perceptron Algorithms.pdf:pdf},
number = {July},
pages = {1--8},
title = {{Discrimative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms}},
url = {http://www.aclweb.org/anthology/W02-1001},
year = {2002}
}
@unpublished{Zhong2017,
abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9{\%} to 59.4{\%} and logical form accuracy from 23.4{\%} to 48.3{\%}.},
archivePrefix = {arXiv},
arxivId = {1709.00103},
author = {Zhong, V. and Xiong, C. and Socher, R.},
eprint = {1709.00103},
file = {:home/hiaoxui/papers/Unknown/Seq2SQL Generating Structured Queries from Natural Language using Reinforcement Learning.pdf:pdf},
pages = {1--12},
title = {{Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning}},
url = {http://arxiv.org/abs/1709.00103},
year = {2017}
}
@article{Och2003,
abstract = {We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.},
author = {Och, F. J. and Ney, H.},
doi = {10.1162/089120103321337421},
file = {:home/hiaoxui/papers/Computational Linguistics/A Systematic Comparison of Various Statistical Alignment Models.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {19--51},
title = {{A Systematic Comparison of Various Statistical Alignment Models}},
url = {http://www.mitpressjournals.org/doi/10.1162/089120103321337421},
volume = {29},
year = {2003}
}
@inproceedings{Cai2017,
author = {Cai, J. and Jiang, Y. and Tu, K.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/CRF Autoencoder for Unsupervised Dependency Parsing.pdf:pdf},
pages = {1638--1643},
title = {{CRF Autoencoder for Unsupervised Dependency Parsing}},
year = {2017}
}
@inproceedings{Rajpurkar2016,
abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0{\%}, a significant improvement over a simple baseline (20{\%}). However, human performance (86.8{\%}) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
archivePrefix = {arXiv},
arxivId = {1606.05250},
author = {Rajpurkar, P. and Zhang, J. and Lopyrev, K. and Liang, P.},
booktitle = {EMNLP},
doi = {10.18653/v1/D16-1264},
eprint = {1606.05250},
file = {:home/hiaoxui/papers/EMNLP/SQuAD 100,000 Questions for Machine Comprehension of Text.pdf:pdf},
isbn = {9781941643327},
issn = {9781941643327},
keywords = {outstanding},
mendeley-tags = {outstanding},
number = {ii},
pmid = {299497},
title = {{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},
url = {http://arxiv.org/abs/1606.05250},
year = {2016}
}
@inproceedings{Richardson2017,
author = {Richardson, K. and Kuhn, J.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Learning Semantic Correspondences in Technical Documentation.pdf:pdf},
pages = {1612--1622},
title = {{Learning Semantic Correspondences in Technical Documentation}},
year = {2017}
}
@inproceedings{Lin2015,
abstract = {Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsupervised problems has been less thoroughly explored. In this paper, we show that embeddings can likewise add value to the problem of unsupervised POS induction. In two representative models of POS induction, we replace multinomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages. We also analyze the effect of various choices while inducing word embeddings on "downstream" POS induction results.},
archivePrefix = {arXiv},
arxivId = {1503.06760},
author = {Lin, C. and Ammar, W. and Dyer, C. and Levin, L.},
booktitle = {NAACL},
eprint = {1503.06760},
file = {:home/hiaoxui/papers/NAACL/Unsupervised POS Induction with Word Embeddings.pdf:pdf},
isbn = {9781941643495},
pages = {1311--1316},
title = {{Unsupervised POS Induction with Word Embeddings}},
url = {http://arxiv.org/abs/1503.06760},
year = {2015}
}
@inproceedings{Chen2018,
abstract = {We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete and APX-hard. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.},
archivePrefix = {arXiv},
arxivId = {1711.05408},
author = {Chen, Y. and Gilroy, S. and Maletti, A. and May, J. and Knight, K.},
booktitle = {NAACL},
eprint = {1711.05408},
file = {:home/hiaoxui/papers/NAACL/Recurrent Neural Networks as Weighted Language Recognizers.pdf:pdf},
keywords = {Outstanding},
mendeley-tags = {Outstanding},
title = {{Recurrent Neural Networks as Weighted Language Recognizers}},
url = {http://arxiv.org/abs/1711.05408},
year = {2018}
}
@inproceedings{Kaushik2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1808.04926v2},
author = {Kaushik, D. and Lipton, Z. C.},
booktitle = {EMNLP},
eprint = {arXiv:1808.04926v2},
file = {:home/hiaoxui/papers/EMNLP/How Much Reading Does Reading Comprehension Require.pdf:pdf},
title = {{How Much Reading Does Reading Comprehension Require ?}},
year = {2018}
}
@inproceedings{Zhuang2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.01812v1},
author = {Zhuang, M. and Collins, M.},
booktitle = {EMNLP},
eprint = {arXiv:1809.01812v1},
file = {:home/hiaoxui/papers/EMNLP/Noise Contrastive Estimation and Negative Sampling for Conditional Models Consistency and Statistical Efficiency.pdf:pdf},
title = {{Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency}},
year = {2018}
}
@article{V.Graca2010,
abstract = {Word-level alignment of bilingual text is a critical resource for a growing variety of tasks. Proba- bilistic models for word alignment present a fundamental trade-off between richness of captured constraints and correlations versus efficiency and tractability of inference. In this article, we use the Posterior Regularization framework (Gra¸ ca, Ganchev, and Taskar 2007) to incorporate complex constraints into probabilistic models during learning without changing the efficiency of the underlying model. We focus on the simple and tractable hidden Markov model, and present an efficient learning algorithm for incorporating approximate bijectivity and symmetry constraints. Models estimated with these constraints produce a significant boost in performance as measured by both precision and recall of manually annotated alignments for six language pairs. We also report experiments on two different tasks where word alignments are required: phrase-based machine translation and syntax transfer, and show promising improvements over standard methods},
author = {Gra{\c{c}}a, J. V. and Ganchev, K. and Taskar, B.},
doi = {10.1162/coli_a_00007},
file = {:home/hiaoxui/papers/Computational Linguistics/Learning Tractable Word Alignment Models with Complex Constraints.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {3},
pages = {481--504},
title = {{Learning Tractable Word Alignment Models with Complex Constraints}},
volume = {36},
year = {2010}
}
@inproceedings{DaumeIII2004,
abstract = {We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical sum-marization systems that can be trained on large cor-pora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outper-forms both the Cut {\&} Paste alignment model (Jing, 2002) and models developed in the context of ma-chine translation (Brown et al., 1993).},
author = {{Daum{\'{e}} III}, H. and Marcu, D.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/A phrase-based HMM approach to documentabstract alignment.pdf:pdf},
pages = {119--126},
title = {{A phrase-based HMM approach to document/abstract alignment.}},
year = {2004}
}
@inproceedings{Mei2019,
author = {Mei, H. and Qin, G. and Eisner, J. M.},
booktitle = {ICML},
file = {:home/hiaoxui/papers/ICML/Imputing Missing Events in Continuous-Time Event Streams.pdf:pdf;:home/hiaoxui/papers/ICML/Imputing Missing Events in Continuous-Time Event Streams(2).pdf:pdf},
title = {{Imputing Missing Events in Continuous-Time Event Streams}},
year = {2019}
}
@inproceedings{Yogatama2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.09100v1},
author = {Yogatama, D. and Blunsom, P. and Dyer, C. and Grefenstette, E. and Ling, W.},
booktitle = {ICLR},
eprint = {arXiv:1611.09100v1},
file = {:home/hiaoxui/papers/ICLR/Learning to Compose Words into Sentences with Reinforcement Learning.pdf:pdf},
pages = {1--10},
title = {{Learning to Compose Words into Sentences with Reinforcement Learning}},
year = {2017}
}
@inproceedings{Sabour2017,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, S. and Frosst, N. and Hinton, G. E.},
booktitle = {Neurips},
eprint = {1710.09829},
file = {:home/hiaoxui/papers/Neurips/Dynamic Routing Between Capsules.pdf:pdf},
number = {Nips},
title = {{Dynamic Routing Between Capsules}},
url = {http://arxiv.org/abs/1710.09829},
year = {2017}
}
@article{Copestake2005,
abstract = {Minimal recursion semantics (MRS) is a framework for computational semantics that is suitable for parsing and generation and that can be implemented in typed feature structure formalisms. We discuss why, in general, a semantic representation with minimal structure is desirable and illustrate how a descriptively adequate representation with a nonrecursive structure may be achieved. MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition. We have integrated MRS with a broad-coverage HPSG grammar.},
author = {Copestake, A. and Flickinger, D. and Pollard, C. and Sag, I. A.},
doi = {10.1007/s11168-006-6327-9},
file = {:home/hiaoxui/papers/Research on Language and Computation/Minimal recursion semantics An introduction.pdf:pdf},
issn = {15707075},
journal = {Research on Language and Computation},
keywords = {Computational semantics,Flat semantics,Grammar implementation,Semantic composition},
number = {4},
pages = {281--332},
title = {{Minimal recursion semantics: An introduction}},
volume = {3},
year = {2005}
}
@incollection{Neal1998,
abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Neal, R. M. and Hinton, G. E.},
booktitle = {Learning in Graphical Models},
doi = {10.1007/978-94-011-5014-9_12},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/Learning in Graphical Models/A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants.pdf:pdf},
isbn = {0262600323},
issn = {978-1-932432-41-1},
pages = {355--368},
pmid = {15991970},
title = {{A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants}},
url = {http://link.springer.com/10.1007/978-94-011-5014-9{\_}12},
year = {1998}
}
@inproceedings{Tu2017,
abstract = {OBJECTIVE To evaluate the effect of an intensivist-model of critical care delivery on the risk of death following injury. SUMMARY BACKGROUND DATA An intensivist-model of ICU care is associated with improved outcomes and less resource utilization in mixed medical and surgical ICUs. The process of trauma center verification assures a relatively high standard of care and quality assurance; thus, it is unclear what the effect of a specific model of ICU care delivery might have on trauma-related mortality. METHODS Using data from a large multicenter (68 centers) prospective cohort study, we evaluated the relationship between the model of ICU care (open vs. intensivist-model) and in-hospital mortality following severe injury. An intensivist-model was defined as an ICU where critically ill trauma patients were either on a distinct ICU service (led by an intensivist) or were comanaged with an intensivist (a physician board-certified in critical care). RESULTS After adjusting for differences in baseline characteristics, the relative risk of death in intensivist-model ICUs was 0.78 (0.58-1.04) compared with an open ICU model. The effect was greatest in the elderly [RR, 0.55 (0.39-0.77)], in units led by surgical intensivists [RR, 0.67 (0.50-0.90)], and in designated trauma centers 0.64 (0.46-0.88). CONCLUSIONS Care in an intensivist-model ICU is associated with a large reduction in in-hospital mortality following trauma, particularly in elderly patients who might have limited physiologic reserve and extensive comorbidity. That the effect is greatest in trauma centers and in units led by surgical intensivists suggests the importance of content expertise in the care of the critically injured. Injured patients are best cared for using an intensivist-model of dedicated critical care delivery, a criterion that should be considered in the verification of trauma centers.},
author = {Tu, C. and Liu, H. and Liu, Z. and Sun, M.},
booktitle = {ACL},
doi = {10.18653/v1/p17-1158},
file = {:home/hiaoxui/papers/ACL/CANE Context-Aware Network Embedding for Relation Modeling.pdf:pdf},
pages = {1722--1731},
title = {{CANE: Context-Aware Network Embedding for Relation Modeling}},
year = {2017}
}
@article{Xue2005,
abstract = {With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with different segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are difficult. As a first step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The first two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking efforts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality.},
author = {Xue, N. and Xia, F. and Chiou, F. D. and Palmer, M.},
doi = {10.1017/S135132490400364X},
file = {:home/hiaoxui/papers/Natural Language Engineering/The Penn Chinese TreeBank Phrase structure annotation of a large corpus.pdf:pdf},
isbn = {1469-8110},
issn = {13513249},
journal = {Natural Language Engineering},
number = {2},
pages = {207--238},
title = {{The Penn Chinese TreeBank: Phrase structure annotation of a large corpus}},
volume = {11},
year = {2005}
}
@inproceedings{Chen2018b,
abstract = {We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete and APX-hard. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.},
archivePrefix = {arXiv},
arxivId = {1711.05408},
author = {Chen, Y. and Gilroy, S. and Maletti, A. and May, J. and Knight, K.},
booktitle = {NAACL-HLT},
doi = {10.18653/v1/N18-1205},
eprint = {1711.05408},
file = {:home/hiaoxui/papers/NAACL-HLT/Recurrent Neural Networks as Weighted Language Recognizers.pdf:pdf},
pages = {2261--2271},
title = {{Recurrent Neural Networks as Weighted Language Recognizers}},
url = {http://arxiv.org/abs/1711.05408},
year = {2018}
}
@unpublished{Chen2018a,
archivePrefix = {arXiv},
arxivId = {1806.07366},
author = {Chen, T. Q. and Rubanova, Y. and Bettencourt, J. and Duvenaud, D.},
eprint = {1806.07366},
file = {:home/hiaoxui/papers/Unknown/Neural Ordinary Differential Equations.pdf:pdf},
pages = {1--19},
title = {{Neural Ordinary Differential Equations}},
year = {2018}
}
@inproceedings{Kumar2002,
author = {Kumar, S. and Byrne, W.},
booktitle = {HLT-NAACL},
file = {:home/hiaoxui/papers/HLT-NAACL/Minimum Bayes-Risk Decoding for Statistical Machine Translation.pdf:pdf},
isbn = {0001401106},
number = {0121285},
title = {{Minimum Bayes-Risk Decoding for Statistical Machine Translation}},
year = {2002}
}
@inproceedings{Yih2015,
abstract = {We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowl-edge base and can be directly mapped to a logical form. Semantic parsing is re-duced to query graph generation, formu-lated as a staged search problem. Unlike traditional approaches, our method lever-ages the knowledge base in an early stage to prune the search space and thus simpli-fies the semantic matching problem. By applying an advanced entity linking sys-tem and a deep convolutional neural net-work model that matches questions and predicate sequences, our system outper-forms previous methods substantially, and achieves an F 1 measure of 52.5{\%} on the WEBQUESTIONS dataset.},
author = {Yih, W. and Chang, M. and He, X. and Gao, J.},
booktitle = {ACL},
doi = {10.3115/v1/P15-1128},
file = {:home/hiaoxui/papers/ACL/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base.pdf:pdf},
isbn = {9781941643723},
pages = {1321--1331},
title = {{Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base}},
url = {http://aclweb.org/anthology/P15-1128},
year = {2015}
}
@inproceedings{Hajishirzi2012,
abstract = {This paper presents an approach for learning to translate simple narratives, i.e., texts (sequences of sentences) describing dynamic systems, into coherent sequences of events without the need for labeled training data. Our approach incorporates domain knowledge in the form of preconditions and effects of events, and we show that it outperforms state-of-the-art supervised learning systems on the task of reconstructing RoboCup soccer games from their commentaries.},
archivePrefix = {arXiv},
arxivId = {1202.3728},
author = {Hajishirzi, H. and Hockenmaier, J. and Mueller, E. T. and Amir, E.},
booktitle = {UAI},
eprint = {1202.3728},
file = {:home/hiaoxui/papers/UAI/Reasoning about RoboCup Soccer Narratives.pdf:pdf},
isbn = {978-0-9749039-7-2},
title = {{Reasoning about RoboCup Soccer Narratives}},
url = {http://arxiv.org/abs/1202.3728},
year = {2011}
}
@article{Reddy2014,
abstract = {In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the FREE 917 and WEBQUESTIONS benchmark datasets show our semantic parser improves over the state of the art.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Reddy, S. and Lapata, M. and Steedman, M.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/TACL/Large-scale Semantic Parsing without Question-Answer Pairs.pdf:pdf},
isbn = {9782951740877},
issn = {2307-387X},
journal = {TACL},
pages = {377--392},
pmid = {25810777},
title = {{Large-scale Semantic Parsing without Question-Answer Pairs}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/398},
volume = {2},
year = {2014}
}
@inproceedings{Vogel2010,
abstract = {We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.},
author = {Vogel, A. and Jurafsky, D.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Learning to Follow Navigational Directions.pdf:pdf},
isbn = {9781617388088},
number = {July},
pages = {806--814},
title = {{Learning to Follow Navigational Directions}},
year = {2010}
}
@inproceedings{Tang2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tang, D. and Wei, F. and Yang, N. and Zhou, M. and Liu, T. and Qin, B.},
booktitle = {ACL},
doi = {10.1561/2200000006},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/ACL/Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification.pdf:pdf},
isbn = {9781937284725},
issn = {03029743},
keywords = {Natural Language Processing,Special Track on Machine Learning,and semantic role labeling,be applied,by trying,chunking,citation,learning algorithm that can,named entity recognition,natural language processing,neural network architecture and,neural networks,part-of-speech tagging,processing tasks including,this versatility is achieved,to various natural language,we propose a unified},
mendeley-tags = {citation},
pages = {1--54},
pmid = {18487783},
title = {{Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification}},
volume = {2},
year = {2014}
}
@inproceedings{Li2015a,
abstract = {Soybean kernels of cultivars Bosa and ZPS 015 were used in the experiment. The contents of available lysine as well as water and salt soluble proteins, were analysed in fresh soybean kernels, soybean products made after the processes of dry extrusion, micronisation, microwave toasting and autoclaving. Utilizing a technological procedure of processing, kernels were exposed to temperatures from 57 to 150°C. The duration of exposure of the soybean kernels to the increased temperatures, ranged from 25-30 seconds in dry extrusion to 30 minutes in autoclaving. All treatments were subjected to different sources of heat, causing different thermodynamic processes to take place in kernels and change their chemical composition; i.e. nutritive quality. The content of water and salt soluble proteins decreased under the influence of higher temperatures in the course of all treatments of processing. The drop of solubility already was drastically effected by temperatures of 100°C in dry extrusion, while there was a gradual decrease in other treatments. The content of available lysine was determined by the modified Carpenter methods with DNFB. The processes of micronisation and microwave toasting showed the greatest effect on the reduction of lysine availability. Dry extrusion and autoclaving, performed within closed systems — in which the increased moisture content has a special effect — resulted in significantly smaller changes of the available lysine content.},
author = {Li, J. J. and Nenkova, A.},
booktitle = {AAAI},
doi = {10.2478/s11535-006-0039-x},
file = {:home/hiaoxui/papers/AAAI/Fast and Accurate Prediction of Sentence Specificity.pdf:pdf},
isbn = {9781577357018},
issn = {1895104X},
keywords = {NLP and Machine Learning Track},
pages = {2281--2287},
title = {{Fast and Accurate Prediction of Sentence Specificity}},
year = {2015}
}
@inproceedings{Johansson2007,
abstract = {We describe a new method to convert En- glish constituent trees using the Penn Tree- bank annotation style into dependency trees. The new format was inspired by annota- tion practices used in other dependency tree- banks with the intention to produce a better interface to further semantic processing than existing methods. In particular, we used a richer set of edge labels and introduced links to handle long-distance phenomena such as wh-movement and topicalization. The resulting trees generally have a more complex dependency structure. For exam- ple, 6{\%}of the trees contain at least one non- projective link, which is difficult for many parsing algorithms. As can be expected, the more complex structure and the enriched set of edge labels make the trees more difficult to predict, and we observed a decrease in parsing accuracy when applying two depen- dency parsers to the new corpus. However, the richer information contained in the new trees resulted in a 23{\%} error reduction in a baseline FrameNet semantic role labeler that relied on dependency arc labels only.},
author = {Johansson, R. and Nugues, P.},
booktitle = {NODALIDA},
file = {:home/hiaoxui/papers/NODALIDA/Extended constituent-to-dependency conversion for English.pdf:pdf},
isbn = {978-9985-4-0514-7},
pages = {105--112},
title = {{Extended constituent-to-dependency conversion for English}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.5018{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@inproceedings{Yuan2018,
abstract = {Multilingual topic models can reveal patterns in cross-lingual document collections. However, existing models lack speed and interactivity, which prevents adoption in everyday corpora exploration or quick moving situations (e.g., natural disasters, political instability). First, we propose a multilingual anchoring algorithm that builds an anchor-based topic model for documents in different languages. Then, we incorporate interactivity to develop MTAnchor (Multilingual Topic Anchors), a system that allows users to refine the topic model. We test our algorithms on labeled English, Chinese, and Sinhalese documents. Within minutes, our methods can produce interpretable topics that are useful for specific classification tasks.},
author = {Yuan, M. and van Durme, B. and Boyd-Graber, J.},
booktitle = {Neurips},
file = {:home/hiaoxui/papers/Neurips/Multilingual Anchoring Interactive Topic Modeling and Alignment Across Languages.pdf:pdf},
number = {NeurIPS},
pages = {8667--8677},
title = {{Multilingual Anchoring: Interactive Topic Modeling and Alignment Across Languages}},
url = {http://papers.nips.cc/paper/8083-multilingual-anchoring-interactive-topic-modeling-and-alignment-across-languages.pdf},
year = {2018}
}
@inproceedings{Lin2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.07889v3},
author = {Lin, T. and RoyChowdhury, A. and Maji, S.},
booktitle = {ICCV},
eprint = {arXiv:1504.07889v3},
file = {:home/hiaoxui/papers/ICCV/Bilinear CNN Models for Fine-grained Visual Recognition.pdf:pdf},
title = {{Bilinear CNN Models for Fine-grained Visual Recognition}},
year = {2017}
}
@inproceedings{Hu2016,
abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
archivePrefix = {arXiv},
arxivId = {1603.06318},
author = {Hu, Z. and Ma, X. and Liu, Z. and Hovy, E. and Xing, E. P.},
booktitle = {ACL},
doi = {10.18653/v1/P16-1228},
eprint = {1603.06318},
file = {:home/hiaoxui/papers/ACL/Harnessing Deep Neural Networks with Logic Rules.pdf:pdf},
isbn = {9781510827585},
issn = {1541-1672},
pages = {2410--2420},
pmid = {18925972},
title = {{Harnessing Deep Neural Networks with Logic Rules}},
url = {http://arxiv.org/abs/1603.06318},
year = {2016}
}
@inproceedings{Narisawa2013,
author = {Narisawa, K. and Watanabe, Y. and Mizuno, J. and Okazaki, N. and Inui, K.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Is a 204 cm Man Tall or Small Acquisition of Numerical Common Sense from the Web.pdf:pdf},
isbn = {9781937284503},
pages = {382--391},
title = {{Is a 204 cm Man Tall or Small? Acquisition of Numerical Common Sense from the Web.}},
url = {http://cse.iitk.ac.in/users/cs671/2013/hw3/narisawa-watanabe-13{\_}tall-or-small-acquiring-numerical-sense-from-web.pdf},
year = {2013}
}
@inproceedings{Tromble2008,
abstract = {I address the commentators' calls for clarification of theoretical terms, discussion of similarities to other proposals, and extension of the ideas. In doing so, I keep the focus on the purpose of memory: enabling the organism to make sense of its environment so that it can take action appropriate to constraints resulting from the physical, personal, social, and cultural situations.},
author = {Tromble, R. W. and Kumar, S. and Och, F. and Macherey, W.},
booktitle = {EMNLP},
doi = {10.3115/1613715.1613792},
file = {:home/hiaoxui/papers/EMNLP/Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation.pdf:pdf},
isbn = {0001401106},
number = {1},
pages = {64--69},
title = {{Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation}},
volume = {17},
year = {2008}
}
@inproceedings{Louizos2017,
abstract = {Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1705.08665},
author = {Louizos, C. and Ullrich, K. and Welling, M.},
booktitle = {Neurips},
eprint = {1705.08665},
file = {:home/hiaoxui/papers/Neurips/Bayesian Compression for Deep Learning.pdf:pdf},
title = {{Bayesian Compression for Deep Learning}},
url = {http://arxiv.org/abs/1705.08665},
year = {2017}
}
@inproceedings{Hu2017a,
abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
archivePrefix = {arXiv},
arxivId = {1703.00955},
author = {Hu, Z. and Yang, Z. and Liang, X. and Salakhutdinov, R. and Xing, E. P.},
booktitle = {ICML},
doi = {arXiv:1},
eprint = {1703.00955},
file = {:home/hiaoxui/papers/ICML/Toward Controlled Generation of Text.pdf:pdf},
title = {{Toward Controlled Generation of Text}},
url = {http://arxiv.org/abs/1703.00955},
year = {2017}
}
@inproceedings{Lu2008,
abstract = {In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models.},
author = {Lu, W. and Ng, H. T. and Lee, W. S. and Zettlemoyer, L. S.},
booktitle = {EMNLP},
doi = {10.3115/1613715.1613815},
file = {:home/hiaoxui/papers/EMNLP/A generative model for parsing natural language to meaning representations.pdf:pdf},
number = {October},
pages = {782--791},
title = {{A generative model for parsing natural language to meaning representations}},
url = {http://dl.acm.org/citation.cfm?id=1613815},
year = {2008}
}
@inproceedings{Socher2013,
abstract = {Natural language parsing has typically been done with small sets of discrete categories such as {\{}NP{\}} and {\{}VP{\}}, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar ({\{}CVG){\}}, which combines {\{}PCFGs{\}} with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The {\{}CVG{\}} improves the {\{}PCFG{\}} of the Stanford Parser by 3.8 {\%} to obtain an F1 score of 90.4{\%}. It is fast to train and implemented approximately as an efficient reranker it is about 20 {\%} faster than the current Stanford factored parser. The {\{}CVG{\}} learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as {\{}PP{\}} attachments. 1},
author = {Socher, R. and Bauer, J. and Manning, C. D. and Ng, A. Y.},
booktitle = {ACL},
doi = {10.1021/ja01552a021},
file = {:home/hiaoxui/papers/ACL/Parsing with Compositional Vector Grammars.pdf:pdf},
issn = {15205126},
keywords = {citation},
mendeley-tags = {citation},
number = {19},
pages = {5080--5083},
title = {{Parsing with Compositional Vector Grammars}},
volume = {80},
year = {2013}
}
@article{Gildea2000,
abstract = {We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gildea, D. and Jurafsky, D.},
doi = {10.3115/1075218.1075283},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/Computational Linguistics/Automatic labeling of semantic roles.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1972},
pages = {512--520},
pmid = {25246403},
title = {{Automatic labeling of semantic roles}},
url = {http://portal.acm.org/citation.cfm?doid=1075218.1075283},
year = {2000}
}
@phdthesis{Collins1999,
author = {Collins, M.},
booktitle = {UPenn},
file = {:home/hiaoxui/papers/UPenn/Head-Driven Statistical Models for Natural Language Parsing.pdf:pdf},
title = {{Head-Driven Statistical Models for Natural Language Parsing}},
year = {1999}
}
@inproceedings{He2017b,
abstract = {We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.},
archivePrefix = {arXiv},
arxivId = {1704.07130},
author = {He, H. and Balakrishnan, A. and Eric, M. and Liang, P.},
booktitle = {ACL},
doi = {https://doi.org/10.18653/v1/P17-1162},
eprint = {1704.07130},
file = {:home/hiaoxui/papers/ACL/Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings.pdf:pdf},
isbn = {9781945626753},
title = {{Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings}},
url = {http://arxiv.org/abs/1704.07130},
year = {2017}
}
@inproceedings{Grenager2005,
abstract = {The applicability of many current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.},
author = {Grenager, T. and Klein, D. and Manning, C. D.},
booktitle = {ACL},
doi = {10.3115/1219840.1219886},
file = {:home/hiaoxui/papers/ACL/Unsupervised Learning of Field Segmentation Models for Information Extraction.pdf:pdf},
isbn = {1932432515},
number = {June},
pages = {371--378},
title = {{Unsupervised Learning of Field Segmentation Models for Information Extraction}},
url = {http://portal.acm.org/citation.cfm?id=1219886{\&}dl=},
year = {2005}
}
@inproceedings{Bosnjak2017,
abstract = {Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.},
archivePrefix = {arXiv},
arxivId = {1605.06640},
author = {Bo{\v{s}}njak, M. and Rockt{\"{a}}schel, T. and Naradowsky, J. and Riedel, S.},
booktitle = {ICML},
eprint = {1605.06640},
file = {:home/hiaoxui/papers/ICML/Programming with a Differentiable Forth Interpreter.pdf:pdf},
title = {{Programming with a Differentiable Forth Interpreter}},
url = {http://arxiv.org/abs/1605.06640},
year = {2017}
}
@inproceedings{Gururangan2018,
abstract = {Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67{\%} of SNLI (Bowman et. al, 2015) and 53{\%} of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.},
archivePrefix = {arXiv},
arxivId = {1803.02324},
author = {Gururangan, S. and Swayamdipta, S. and Levy, O. and Schwartz, R. and Bowman, S. R. and Smith, N. A.},
booktitle = {NAACL},
doi = {10.18653/v1/N18-2017},
eprint = {1803.02324},
file = {:home/hiaoxui/papers/NAACL/Annotation Artifacts in Natural Language Inference Data.pdf:pdf},
isbn = {9781510827585},
issn = {1702.00887},
title = {{Annotation Artifacts in Natural Language Inference Data}},
url = {http://arxiv.org/abs/1803.02324},
year = {2018}
}
@inproceedings{Kushman2013,
abstract = {We consider the problem of translating natural language text queries into regular expres- sions which represent their meaning. The mis- match in the level of abstraction between the natural language representation and the regu- lar expression representation make this a novel and challenging problem. However, a given regular expression can be written in many se- mantically equivalent forms, and we exploit this flexibility to facilitate translation by find- ing a form which more directly corresponds to the natural language. We evaluate our tech- nique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a state- of-the-art semantic parsing baseline, yielding a 29{\%} absolute improvement in accuracy.},
author = {Kushman, N. and Barzilay, R.},
booktitle = {NAACL-HLT},
file = {:home/hiaoxui/papers/NAACL-HLT/Using Semantic Unification to Generate Regular Expressions from Natural Language.pdf:pdf},
isbn = {9781937284473},
number = {June},
pages = {826--836},
title = {{Using Semantic Unification to Generate Regular Expressions from Natural Language}},
url = {http://www.aclweb.org/anthology/N13-1103},
year = {2013}
}
@inproceedings{Liu2017,
abstract = {Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels relational facts with knowledge bases. Previous sentence level denoise models don't achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training. To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During training, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms other state-of-the-art systems. Author{\{}4{\}}{\{}Affiliation{\}}},
author = {Liu, T. and Wang, K. and Chang, B. and Sui, Z.},
booktitle = {EMNLP},
doi = {10.18653/v1/D17-1189},
file = {:home/hiaoxui/papers/EMNLP/A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction.pdf:pdf},
pages = {1790--1795},
title = {{A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction}},
url = {http://aclweb.org/anthology/D17-1189},
year = {2017}
}
@article{Knuth1972,
author = {Knuth, D. E.},
file = {:home/hiaoxui/papers/Information Processing/Mathematical Analysis of Algorithms.pdf:pdf},
journal = {Information Processing},
title = {{Mathematical Analysis of Algorithms}},
year = {1972}
}
@inproceedings{Valmadre2017,
abstract = {The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.},
archivePrefix = {arXiv},
arxivId = {1704.06036},
author = {Valmadre, J. and Bertinetto, L. and Henriques, J. F. and Vedaldi, A. and Torr, P H. S.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2017.531},
eprint = {1704.06036},
file = {:home/hiaoxui/papers/CVPR/End-to-end representation learning for Correlation Filter based tracking.pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
title = {{End-to-end representation learning for Correlation Filter based tracking}},
url = {http://arxiv.org/abs/1704.06036},
year = {2017}
}
@inproceedings{Korattikara2015,
author = {Korattikara, A. and Rathod, V. and Murphy, K. and Welling, M.},
booktitle = {Neurips},
file = {:home/hiaoxui/papers/Neurips/Bayesian Dark Knowledge.pdf:pdf},
pages = {1--9},
title = {{Bayesian Dark Knowledge}},
year = {2015}
}
@article{Wang2017,
abstract = {We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction. While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data. The supervised learner must identify surface features of a language's POS sequence (hand-engineered or neural features) that correlate with the language's deeper structure (latent trees). In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar induction baseline by a large margin.},
archivePrefix = {arXiv},
arxivId = {1710.03877},
author = {Wang, D. and Eisner, J. M.},
eprint = {1710.03877},
file = {:home/hiaoxui/papers/TACL/Fine-Grained Prediction of Syntactic Typology Discovering Latent Structure with Supervised Learning.pdf:pdf},
journal = {TACL},
number = {2016},
pages = {147--161},
title = {{Fine-Grained Prediction of Syntactic Typology: Discovering Latent Structure with Supervised Learning}},
url = {http://arxiv.org/abs/1710.03877},
volume = {5},
year = {2017}
}
@inproceedings{Lakkaraju2016,
abstract = {One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Lakkaraju, H. and Bach, S. H. and Jure, L.},
booktitle = {KDD},
doi = {10.1145/2939672.2939874},
eprint = {15334406},
file = {:home/hiaoxui/papers/KDD/Interpretable Decision Sets A Joint Framework for Description and Prediction.pdf:pdf},
isbn = {2154817X (Linking)},
issn = {2154-817X},
keywords = {Joint,KDD Stanford University,Prediction important obstacles,Stanford University jure,University bach cs,art,average rule,balances accuracy interpretability,choice questions boundaries,edu Recent,edu Stanford University,fact,interpretability rules,interpretable accurate classification,interpretable formalize,interpretable sets,interpretable write descriptions,learning,methods,objective non monotone,optimal rules Experiments,overlapping rules,predictive models,simple concise,small important classes,space pay attention,stanford edu Stanford,state,submodular function,systems,techniques,variables important model},
pages = {1675--1684},
pmid = {27853627},
title = {{Interpretable Decision Sets: A Joint Framework for Description and Prediction}},
volume = {1},
year = {2016}
}
@inproceedings{Bahdanau2015,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, D. and Cho, K. and Bengio, Y.},
booktitle = {ICLR},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:home/hiaoxui/papers/ICLR/Neural Machine Translation by Jointly Learning to Align and Translate.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
pages = {1--15},
pmid = {14527267},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2015}
}
@inproceedings{Srivastava2017,
abstract = {Natural language constitutes a predomi-nant medium for much of human learn-ing and pedagogy. We consider the prob-lem of concept learning from natural lan-guage explanations, and a small number of labeled examples of the concept. For example, in learning the concept of a phish-ing email, one might say 'this is a phishing email because it asks for your bank account number'. Solving this problem involves both learning to interpret open-ended nat-ural language statements, as well as learn-ing the concept itself. We present a joint model for (1) language interpretation (se-mantic parsing) and (2) concept learning (classification) that does not require label-ing statements with logical forms. Instead, the model prefers discriminative interpre-tations of statements in context of observ-able features of the data as a weak signal for parsing. On a dataset of email-related concepts, this approach yields across-the-board improvements in classification per-formance, with a 30{\%} relative improve-ment in F1 score over competitive classifi-cation methods in the low data regime.},
author = {Srivastava, S. and Labutov, S. and Mitchell, T.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Joint Concept Learning and Semantic Parsing from Natural Language Explanations.pdf:pdf},
pages = {1527--1536},
title = {{Joint Concept Learning and Semantic Parsing from Natural Language Explanations}},
url = {http://aclweb.org/anthology/D17-1161},
year = {2017}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, J. and Hazan, E. and Singer, Y.},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:home/hiaoxui/papers/JMLR/Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {JMLR},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
pmid = {2868127},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@inproceedings{Pitler2008,
abstract = {Knott (1996) provides an extensive of connectives and their properties13 It is important to select a word with some syntactic mo- tivation to an argument span, but due to the lack of consistent alignment between syntax and , we must},
author = {Pitler, E. and Raghupathy, M. and Mehta, H. and Nenkova, A. and Lee, A. and Joshi, A.},
booktitle = {COLING},
file = {:home/hiaoxui/papers/COLING/Easily Identifiable Discourse Relations.pdf:pdf},
isbn = {9781905593446},
number = {June},
pages = {87--90},
title = {{Easily Identifiable Discourse Relations}},
url = {http://www.aclweb.org/anthology/C08-2022},
year = {2008}
}
@inproceedings{Bengio2014,
abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.},
archivePrefix = {arXiv},
arxivId = {1306.1091},
author = {Bengio, Y. and Thibodeau-Laufer, {\'{E}}. and Alain, G. and Yosinski, J.},
booktitle = {ICML},
eprint = {1306.1091},
file = {:home/hiaoxui/papers/ICML/Deep Generative Stochastic Networks Trainable by Backprop.pdf:pdf},
title = {{Deep Generative Stochastic Networks Trainable by Backprop}},
url = {http://arxiv.org/abs/1306.1091},
year = {2014}
}
@inproceedings{Church1988,
author = {Church, K. W.},
booktitle = {ANLC},
file = {:home/hiaoxui/papers/ANLC/A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text.pdf:pdf},
pages = {136--143},
title = {{A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text}},
url = {http://www.aclweb.org/anthology/A88-1019},
year = {1988}
}
@inproceedings{Havrylov2017,
abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
archivePrefix = {arXiv},
arxivId = {1705.11192},
author = {Havrylov, S. and Titov, I.},
booktitle = {Neurips},
eprint = {1705.11192},
file = {:home/hiaoxui/papers/Neurips/Emergence of Language with Multi-agent Games Learning to Communicate with Sequences of Symbols.pdf:pdf},
title = {{Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols}},
url = {http://arxiv.org/abs/1705.11192},
year = {2017}
}
@unpublished{Collins2000,
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/MEMMs ( Log-Linear Tagging Models ).pdf:pdf},
pages = {1--12},
title = {{MEMMs ( Log-Linear Tagging Models )}},
year = {2000}
}
@inproceedings{Pasupat2015,
abstract = {Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.},
archivePrefix = {arXiv},
arxivId = {1508.00305},
author = {Pasupat, P. and Liang, P.},
booktitle = {ACL},
doi = {10.3115/v1/P15-1142},
eprint = {1508.00305},
file = {:home/hiaoxui/papers/ACL/Compositional Semantic Parsing on Semi-Structured Tables.pdf:pdf},
isbn = {9781941643723},
title = {{Compositional Semantic Parsing on Semi-Structured Tables}},
url = {http://arxiv.org/abs/1508.00305},
year = {2015}
}
@inproceedings{Yao2017,
author = {Yao, J. and Wan, X.},
booktitle = {AAAI},
file = {:home/hiaoxui/papers/AAAI/Greedy Flipping for Constrained Word Deletion.pdf:pdf},
keywords = {Natural Language Processing and Text Mining},
number = {1},
pages = {3518--3524},
title = {{Greedy Flipping for Constrained Word Deletion}},
year = {2017}
}
@inproceedings{Du2016,
abstract = {Large volumes of event data are becoming increasingly available in a wide variety of applications, such as healthcare analytics, smart cities and social network analysis. The precise time interval or the exact distance between two events carries a great deal of information about the dynamics of the underlying systems. These characteristics make such data fundamentally different from independently and identically distributed data and time-series data where time and space are treated as indexes rather than random variables. Marked temporal point processes are the mathematical framework for modeling event data with covariates. However, typical point process models often make strong assumptions about the generative processes of the event data, which may or may not reflect the reality, and the specifically fixed parametric assumptions also have restricted the expressive power of the respective processes. Can we obtain a more expressive model of marked temporal point processes? How can we learn such a model from massive data? In this paper, we propose the Recurrent Marked Temporal Point Process (RMTPP) to simultaneously model the event timings and the markers. The key idea of our approach is to view the intensity function of a temporal point process as a nonlinear function of the history, and use a recurrent neural network to automatically learn a representation of influences from the event history. We develop an efficient stochastic gradient algorithm for learning the model parameters which can readily scale up to millions of events. Using both synthetic and real world datasets, we show that, in the case where the true models have parametric specifications, RMTPP can learn the dynamics of such models without the need to know the actual parametric forms; and in the case where the true models are unknown, RMTPP can also learn the dynamics and achieve better predictive performance than other parametric alternatives based on particular prior assumptions.},
author = {Du, N. and Tech, G. and Gomez-rodriguez, M. and Tech, G.},
booktitle = {KDD},
doi = {10.1145/2939672.2939875},
file = {:home/hiaoxui/papers/KDD/Recurrent Marked Temporal Point Processes Embedding Event History to Vector.pdf:pdf},
isbn = {9781450342322},
keywords = {marked temporal point process,recur-,rent neural network,stochastic process},
pages = {1555--1564},
title = {{Recurrent Marked Temporal Point Processes : Embedding Event History to Vector}},
url = {http://dl.acm.org/citation.cfm?doid=2939672.2939875},
year = {2016}
}
@inproceedings{Dyer2015,
abstract = {We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.},
archivePrefix = {arXiv},
arxivId = {1505.08075},
author = {Dyer, C. and Ballesteros, M. and Ling, W. and Matthews, A. and Smith, N. A.},
booktitle = {ACL},
eprint = {1505.08075},
file = {:home/hiaoxui/papers/ACL/Transition-Based Dependency Parsing with Stack Long Short-Term Memory.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
title = {{Transition-Based Dependency Parsing with Stack Long Short-Term Memory}},
url = {http://arxiv.org/abs/1505.08075},
year = {2015}
}
@inproceedings{Pasupat2014,
abstract = {In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction sys-tems rely on seed examples or redundancy across multiple web pages. In this paper, we consider a new zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page. Our approach de-fines a log-linear model over latent extrac-tion predicates, which select lists of enti-ties from the web page. The main chal-lenge is to define features on widely vary-ing candidate entity lists. We tackle this by abstracting list elements and using aggre-gate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline.},
author = {Pasupat, P. and Liang, P.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Zero-shot Entity Extraction from Web Pages.pdf:pdf},
isbn = {9781937284725},
pages = {391--401},
title = {{Zero-shot Entity Extraction from Web Pages}},
year = {2014}
}
@inproceedings{Xiao2017a,
author = {Xiao, S. and Xu, H. and Yan, J. and Farajtabar, M. and Yang, X. and Song, L. and Zha, H.},
booktitle = {AAAI},
file = {:home/hiaoxui/papers/AAAI/Learning Conditional Generative Models for Temporal Point Processes.pdf:pdf},
keywords = {Planning and Scheduling Track},
pages = {6302--6309},
title = {{Learning Conditional Generative Models for Temporal Point Processes}},
year = {2017}
}
@unpublished{Andersen2018,
abstract = {While binary pulsar systems are fantastic laboratories for a wide array of astrophysics, they are particularly difficult to detect. The orbital motion of the pulsar changes its apparent spin frequency over the course of an observation, essentially "smearing" the response of the time series in the Fourier domain. We review the Fourier domain acceleration search (FDAS), which uses a matched filtering algorithm to correct for this smearing by assuming constant acceleration for a small enough portion of the orbit. We discuss the theory and implementation of a Fourier domain "jerk" search, developed as part of the $\backslash$textsc{\{}PRESTO{\}} software package, which extends the FDAS to account for a linearly changing acceleration, or constant orbital jerk, of the pulsar. We test the performance of our algorithm on archival Green Bank Telescope observations of the globular cluster Terzan{\~{}}5, and show that while the jerk search has a significantly longer runtime, it improves search sensitivity to binaries when the observation duration is {\$}5{\$} to {\$}15\backslash{\%}{\$} of the orbital period. Finally, we present the jerk-search-enabled detection of Ter5am (PSR{\~{}}J1748{\$}-{\$}2446am), a new highly-accelerated pulsar in a compact, eccentric, and relativistic orbit, with a likely pulsar mass of 1.649{\$}{\^{}}{\{}+0.037{\}}{\_}{\{}-0.11{\}}{\$}$\backslash$,$\backslash$msun.},
archivePrefix = {arXiv},
arxivId = {1807.07900},
author = {Andersen, B. C. and Ransom, S. M.},
doi = {10.3847/2041-8213/aad59f},
eprint = {1807.07900},
file = {:home/hiaoxui/papers/Unknown/A Fourier Domain Jerk Search for Binary Pulsars.pdf:pdf},
issn = {20418213},
keywords = {2446am,binaries,general,individual,j1748,neutron,pulsars,stars},
title = {{A Fourier Domain "Jerk" Search for Binary Pulsars}},
url = {http://arxiv.org/abs/1807.07900},
year = {2018}
}
@inproceedings{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. N. and Kaiser, L. and Polosukhin, I.},
booktitle = {Neurips},
doi = {10.1017/S0140525X16001837},
eprint = {1706.03762},
file = {:home/hiaoxui/papers/Neurips/Attention Is All You Need.pdf:pdf},
isbn = {9781577357384},
issn = {0140-525X},
pmid = {1000303116},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}
@inproceedings{Kuru2016,
abstract = {We describe and evaluate a character-level tagger for language-independent Named Entity Recognition (NER). Instead of words, a sentence is represented as a sequence of characters. The model consists of stacked bidirectional LSTMs which inputs characters and outputs tag probabilities for each character. These probabilities are then converted to consistent word level named entity tags using a Viterbi decoder. We are able to achieve close to state-of-the-art NER performance in seven languages with the same basic model using only labeled NER data and no hand-engineered features or other external resources like syntactic taggers or Gazetteers.},
author = {Kuru, O. and Can, O. A. and Deniz, Y.},
booktitle = {COLING},
file = {:home/hiaoxui/papers/COLING/CharNER Character-Level Named Entity Recognition.pdf:pdf},
pages = {911--921},
title = {{CharNER : Character-Level Named Entity Recognition}},
year = {2016}
}
@inproceedings{Sakaguchi2017,
abstract = {We propose a new dependency pars-ing scheme which jointly parses a sen-tence and repairs grammatical errors by extending the non-directional transition-based formalism of Goldberg and El-hadad (2010) with three additional ac-tions: SUBSTITUTE, DELETE, INSERT. Be-cause these actions may cause an infinite loop in derivation, we also introduce sim-ple constraints that ensure the parser ter-mination. We evaluate our model with re-spect to dependency accuracy and gram-maticality improvements for ungrammat-ical sentences, demonstrating the robust-ness and applicability of our scheme.},
author = {Sakaguchi, K. and Post, M. and van Durme, B.},
booktitle = {ACL},
doi = {10.18653/v1/p17-2030},
file = {:home/hiaoxui/papers/ACL/Error-repair Dependency Parsing for Ungrammatical Texts.pdf:pdf},
pages = {189--195},
title = {{Error-repair Dependency Parsing for Ungrammatical Texts}},
year = {2017}
}
@inproceedings{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
booktitle = {Neurips},
doi = {10.1017/CBO9781139058452},
eprint = {arXiv:1406.2661v1},
file = {:home/hiaoxui/papers/Neurips/Generative Adversarial Nets.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
pages = {2672--2680},
pmid = {1000183096},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@inproceedings{Poon2009,
abstract = {We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.},
author = {Poon, H. and Domingos, P.},
booktitle = {EMNLP},
doi = {10.3115/1699510.1699512},
file = {:home/hiaoxui/papers/EMNLP/Unsupervised semantic parsing.pdf:pdf},
isbn = {9781932432596},
number = {August},
pages = {1--10},
title = {{Unsupervised semantic parsing}},
url = {http://portal.acm.org/citation.cfm?doid=1699510.1699512{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1699512},
volume = {1},
year = {2009}
}
@inproceedings{Arthur2016,
abstract = {Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.},
archivePrefix = {arXiv},
arxivId = {1606.02006},
author = {Arthur, P. and Neubig, G. and Nakamura, S.},
booktitle = {EMNLP},
eprint = {1606.02006},
file = {:home/hiaoxui/papers/EMNLP/Incorporating Discrete Translation Lexicons into Neural Machine Translation.pdf:pdf},
pages = {1557--1567},
title = {{Incorporating Discrete Translation Lexicons into Neural Machine Translation}},
url = {http://arxiv.org/abs/1606.02006},
year = {2016}
}
@inproceedings{Lee2004,
author = {Lee, Y. K. and Ng, H. T. and Chia, T. K.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Supervised word sense disambiguation with support vector machines and multiple knowledge sources.pdf:pdf},
number = {July},
pages = {137--140},
title = {{Supervised word sense disambiguation with support vector machines and multiple knowledge sources}},
year = {2004}
}
@unpublished{Li2017,
abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comp arisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
archivePrefix = {arXiv},
arxivId = {1712.09913},
author = {Li, H. and Xu, Z. and Taylor, G. and Studer, C. and Goldstein, T.},
eprint = {1712.09913},
file = {:home/hiaoxui/papers/Unknown/Visualizing the Loss Landscape of Neural Nets.pdf:pdf},
title = {{Visualizing the Loss Landscape of Neural Nets}},
url = {http://arxiv.org/abs/1712.09913},
year = {2017}
}
@inproceedings{Wu2017,
abstract = {Combinatory Category Grammar (CCG) supertag-ging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. How-ever, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window ap-proach, which can be treated as an attention mech-anism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words di-rectly, which is superior to the regular dropout on word embeddings. We use this approach to demon-strate the state-of-the-art CCG supertagging perfor-mance on the standard test set.},
archivePrefix = {arXiv},
arxivId = {1610.02749},
author = {Wu, H. and Zhang, J. and Zong, C.},
booktitle = {AAAI},
eprint = {1610.02749},
file = {:home/hiaoxui/papers/AAAI/A Dynamic Window Neural Network for CCG Supertagging.pdf:pdf},
keywords = {Natural Language Processing and Machine Learning},
pages = {3337--3343},
title = {{A Dynamic Window Neural Network for CCG Supertagging}},
year = {2017}
}
@inproceedings{Cohen2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1801.10130v1},
author = {Cohen, T. S. and Geiger, M. and K{\"{o}}hler, J. and Welling, M.},
booktitle = {ICLR},
eprint = {arXiv:1801.10130v1},
file = {:home/hiaoxui/papers/ICLR/Spherical CNNs.pdf:pdf},
number = {3},
pages = {1--15},
title = {{Spherical CNNs}},
year = {2018}
}
@inproceedings{Lebret2016,
abstract = {This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.},
archivePrefix = {arXiv},
arxivId = {1603.07771},
author = {Lebret, R. and Grangier, D. and Auli, M.},
booktitle = {EMNLP},
doi = {10.18653/v1/D16-1128},
eprint = {1603.07771},
file = {:home/hiaoxui/papers/EMNLP/Neural Text Generation from Structured Data with Application to the Biography Domain.pdf:pdf},
title = {{Neural Text Generation from Structured Data with Application to the Biography Domain}},
url = {http://arxiv.org/abs/1603.07771},
year = {2016}
}
@inproceedings{Kim2010,
author = {Kim, J. and Mooney, R. J.},
booktitle = {COLING},
file = {:home/hiaoxui/papers/COLING/Generative alignment and semantic parsing for learning from ambiguous supervision.pdf:pdf},
number = {2008},
pages = {543--551},
title = {{Generative alignment and semantic parsing for learning from ambiguous supervision}},
url = {http://dl.acm.org/citation.cfm?id=1944628{\%}5Cnpapers3://publication/uuid/EEC4CDC8-A12E-4837-8EF4-57020ABF3026},
year = {2010}
}
@inproceedings{Guu2017,
abstract = {Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.},
archivePrefix = {arXiv},
arxivId = {1704.07926},
author = {Guu, K. and Pasupat, P. and Liu, E. Z. and Liang, P.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1097},
eprint = {1704.07926},
file = {:home/hiaoxui/papers/ACL/From Language to Programs Bridging Reinforcement Learning and Maximum Marginal Likelihood.pdf:pdf},
isbn = {9781945626753},
pages = {1051--1062},
title = {{From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood}},
url = {http://arxiv.org/abs/1704.07926},
year = {2017}
}
@incollection{Doucet2011,
abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented},
author = {Doucet, A. and Johansen, A. M.},
booktitle = {Handbook of Nonlinear Filtering},
doi = {10.1.1.157.772},
file = {:home/hiaoxui/papers/Handbook of Nonlinear Filtering/A tutorial on particle filtering and smoothing fifteen years later.pdf:pdf},
isbn = {978-0199532902},
issn = {01677152},
title = {{A tutorial on particle filtering and smoothing: fifteen years later}},
url = {http://automatica.dei.unipd.it/tl{\_}files/utenti/lucaschenato/Classes/PSC10{\_}11/Tutorial{\_}PF{\_}doucet{\_}johansen.pdf},
year = {2009}
}
@inproceedings{Mooney2011,
author = {Chen, D. L. and Mooney, R. J.},
booktitle = {MLSLP},
file = {:home/hiaoxui/papers/MLSLP/Panning for gold finding relevant semantic content for grounded language learning.pdf:pdf},
number = {June},
title = {{Panning for gold: finding relevant semantic content for grounded language learning}},
year = {2011}
}
@article{Salakhutdinov2007,
author = {Salakhutdinov, R. and Hinton, G. E.},
file = {:home/hiaoxui/papers/JMLR/Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure.pdf:pdf},
journal = {JMLR},
number = {1},
title = {{Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure}},
year = {2007}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Y. and Courville, A. and Vincent, P.},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {:home/hiaoxui/papers/PAMI/Representation learning A review and new perspectives.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {01628828},
journal = {PAMI},
keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
number = {8},
pages = {1798--1828},
pmid = {23459267},
title = {{Representation learning: A review and new perspectives}},
volume = {35},
year = {2013}
}
@article{Giles1992,
abstract = {We show that a recurrent, second-order neural network using a real-time, forward training algorithm readily learns to infer small regular grammars from positive and negative string training samples. We present simulations that show the effect of initial conditions, training set size and order, and neural network architecture. All simulations were performed with random initial weight strengths and usually converge after approximately a hundred epochs of training. We discuss a quantization algorithm for dynamically extracting finite state automata during and after training. For a well-trained neural net, the extracted automata constitute an equivalence class of state machines that are reducible to the minimal machine of the inferred grammar. We then show through simulations that many of the neural net state machines are dynamically stable, that is, they correctly classify many long unseen strings. In addition, some of these extracted automata actually outperform the trained neural network for classification...},
author = {Giles, C. L. and Miller, C. B. and Chen, D. and Chen, H. H. and Sun, G. Z. and Lee, Y. C.},
doi = {10.1162/neco.1992.4.3.393},
file = {:home/hiaoxui/papers/Neural Computation/Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
number = {3},
pages = {393--405},
title = {{Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.3.393{\#}.Vr3oBfnhAuU},
volume = {4},
year = {1992}
}
@unpublished{Huang1989,
author = {Huang, X.},
file = {:home/hiaoxui/papers/Unknown/Semi-Continuous Hidden Markov Models.pdf:pdf},
title = {{Semi-Continuous Hidden Markov Models}},
year = {1989}
}
@article{Liang2015,
abstract = {Computational semantics has long been considered a field divided between logical and statistical approaches, but this divide is rapidly eroding with the development of statistical models that learn compositional semantic theories from corpora and databases. This review presents a simple discriminative learning framework for defining such models and relating them to logical theories. Within this framework, we discuss the task of learning to map utterances to logical forms (semantic parsing) and the task of learning from denotations with logical forms as latent variables. We also consider models that use distributed (e.g., vector) representations rather than logical ones, showing that these can be considered part of the same overall framework for understanding meaning and structural complexity.},
author = {Liang, P. and Potts, C.},
doi = {10.1146/annurev-linguist-030514-125312},
file = {:home/hiaoxui/papers/Annual Reviews of Linguistics/Bringing Machine Learning and Compositional Semantics Together.pdf:pdf},
isbn = {2333-9683},
issn = {2333-9683},
journal = {Annual Reviews of Linguistics},
keywords = {compositionality,discriminative learning,distributed representations,logical forms,mantic parsing,recursive neural networks,se-},
number = {1},
pages = {355--376},
title = {{Bringing Machine Learning and Compositional Semantics Together}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-linguist-030514-125312},
volume = {1},
year = {2015}
}
@inproceedings{Wang2018a,
abstract = {To approximately parse an unfamiliar language , it helps to have a treebank of a similar language. But what if the closest available treebank still has the wrong word order? We show how to (stochastically) permute the constituents of an existing dependency tree-bank so that its surface part-of-speech statistics approximately match those of the target language. The parameters of the permutation model can be evaluated for quality by dynamic programming and tuned by gradient descent (up to a local optimum). This optimization procedure yields trees for a new artificial language that resembles the target language. We show that delexicalized parsers for the target language can be successfully trained using such "made to order" artificial languages.},
author = {Wang, D. and Eisner, J. M.},
booktitle = {EMNLP},
doi = {10.1109/IEEESTD.1998.87897},
file = {:home/hiaoxui/papers/EMNLP/Synthetic Data Made to Order The Case of Parsing.pdf:pdf},
title = {{Synthetic Data Made to Order: The Case of Parsing}},
url = {https://www.cs.jhu.edu/{~}wdd/pub/wang+eisner.emnlp18.pdf},
year = {2018}
}
@unpublished{Collins2013b,
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/Lexicalized Probabilistic Context-Free Grammars Weaknesses of PCFGs as Parsing Models.pdf:pdf},
number = {1},
pages = {1--22},
title = {{Lexicalized Probabilistic Context-Free Grammars Weaknesses of PCFGs as Parsing Models}},
year = {2013}
}
@inproceedings{Lin2018,
abstract = {We introduce neural particle smoothing, a sequential Monte Carlo method for sampling annotations of an input string from a given probability model. In contrast to conventional particle filtering algorithms, we train a proposal distribution that looks ahead to the end of the input string by means of a right-to-left LSTM. We demonstrate that this innovation can improve the quality of the sample. To motivate our formal choices, we explain how our neural model and neural sampler can be viewed as low-dimensional but nonlinear approximations to working with HMMs over very large state spaces.},
archivePrefix = {arXiv},
arxivId = {1804.10747},
author = {Lin, C. and Eisner, J. M.},
booktitle = {NAACL},
eprint = {1804.10747},
file = {:home/hiaoxui/papers/NAACL/Neural Particle Smoothing for Sampling from Conditional Sequence Models.pdf:pdf},
number = {3},
title = {{Neural Particle Smoothing for Sampling from Conditional Sequence Models}},
url = {http://arxiv.org/abs/1804.10747},
volume = {21218},
year = {2018}
}
@inproceedings{Piech2015,
abstract = {Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University's CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.},
archivePrefix = {arXiv},
arxivId = {1505.05969},
author = {Piech, C. and Huang, J. and Nguyen, A. and Phulsuksombati, M. and Sahami, M. and Guibas, L.},
booktitle = {ICML},
eprint = {1505.05969},
file = {:home/hiaoxui/papers/ICML/Learning Program Embeddings to Propagate Feedback on Student Code.pdf:pdf},
isbn = {9781510810587 (ISBN)},
title = {{Learning Program Embeddings to Propagate Feedback on Student Code}},
url = {http://arxiv.org/abs/1505.05969},
volume = {37},
year = {2015}
}
@inproceedings{Cocos2017,
abstract = {The role of word sense disambiguation in lexical substitution has been questioned due to the high performance of vector space models which propose good sub-stitutes without explicitly accounting for sense. We show that a filtering mecha-nism based on a sense inventory optimized for substitutability can improve the results of these models. Our sense inventory is constructed using a clustering method which generates paraphrase clusters that are congruent with lexical substitution an-notations in a development set. The re-sults show that lexical substitution can still benefit from senses which can improve the output of vector space paraphrase ranking models.},
author = {Cocos, A. and Apidianaki, M. and Callison-Burch, C.},
booktitle = {Workshop on Sense, Concept and Entity Representations and their Applications},
doi = {http://dx.doi.org/10.1097/00002093-199700112-00003},
file = {:home/hiaoxui/papers/Workshop on Sense, Concept and Entity Representations and their Applications/Word Sense Filtering Improves Embedding-Based Lexical Substitution.pdf:pdf},
issn = {0893-0341, 0893-0341},
pages = {110--119},
title = {{Word Sense Filtering Improves Embedding-Based Lexical Substitution}},
url = {http://aclweb.org/anthology//W/W17/W17-1914.pdf},
volume = {April},
year = {2017}
}
@inproceedings{Devlin2018,
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, J. and Chang, M. and Lee, K. and Toutanova, K.},
booktitle = {EMNLP},
eprint = {1810.04805},
file = {:home/hiaoxui/papers/EMNLP/BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}
@inproceedings{Gao2015,
abstract = {Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.},
archivePrefix = {arXiv},
arxivId = {1511.06062},
author = {Gao, Y. and Beijbom, O. and Zhang, N. and Darrell, T.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2016.41},
eprint = {1511.06062},
file = {:home/hiaoxui/papers/CVPR/Compact Bilinear Pooling.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
number = {2},
pages = {317--326},
title = {{Compact Bilinear Pooling}},
url = {http://arxiv.org/abs/1511.06062},
year = {2015}
}
@inproceedings{Angeli2010,
abstract = {We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework. In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domainsRobocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-of-the-art domain-specific systems both in terms of BLEU scores and human evaluation.},
author = {Angeli, G. and Liang, P. and Klein, D.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/A Simple Domain-Independent Probabilistic Approach to Generation.pdf:pdf},
isbn = {1932432868},
number = {October},
pages = {502--512},
title = {{A Simple Domain-Independent Probabilistic Approach to Generation}},
url = {http://www.aclweb.org/anthology/D10-1049},
year = {2010}
}
@inproceedings{Deutsch2018,
author = {Deutsch, D. and Hewitt, J. and Roth, D.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/A Distributional and Orthographic Aggregation Model for English Derivational Morphology.pdf:pdf},
pages = {1--10},
title = {{A Distributional and Orthographic Aggregation Model for English Derivational Morphology}},
year = {2018}
}
@inproceedings{Levesque2013,
author = {Levesque, H. J.},
booktitle = {IJCAI},
file = {:home/hiaoxui/papers/IJCAI/On our best behaviour.pdf:pdf},
issn = {0035-8797},
keywords = {Behavioral Sciences,Behavioral Sciences: education,Education,England,Family,Medical,Occupations,Physician-Patient Relations,Physicians},
pmid = {966205},
title = {{On our best behaviour.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23850475},
year = {2013}
}
@inproceedings{Kuncoro2017,
author = {Kuncoro, A. and Ballesteros, M. and Kong, L. and Dyer, C. and Neubig, G. and Smith, N. A.},
booktitle = {EACL},
file = {:home/hiaoxui/papers/EACL/What Do Recurrent Neural Network Grammars Learn About Syntax.pdf:pdf},
pages = {1249--1258},
title = {{What Do Recurrent Neural Network Grammars Learn About Syntax ?}},
volume = {1},
year = {2017}
}
@article{Moro2014,
abstract = {Entity Linking (EL) and Word Sense Disambiguation (WSD) both address the lexical ambiguity of language. But while the two tasks are pretty similar, they differ in a fundamental respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while in WSD there is a perfect match between the word form (better, its lemma) and a suitable word sense. In this paper we present Babelfy, a unified graph-based approach to EL and WSD based on a loose identification of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence semantic interpretations. Our experiments show state-of- the-art performances on both tasks on 6 different datasets, including a multilingual setting. Babelfy is online at http://babelfy.org},
author = {Moro, A. and Raganato, A. and Navigli, R.},
file = {:home/hiaoxui/papers/TACL/Entity Linking meets Word Sense Disambiguation a Unified Approach.pdf:pdf},
issn = {2307-387X},
journal = {TACL},
number = {0},
pages = {231--244},
title = {{Entity Linking meets Word Sense Disambiguation: a Unified Approach}},
volume = {2},
year = {2014}
}
@inproceedings{Quirk2015,
abstract = {Using natural language to write programs is a touchstone problem for computational linguistics. We present an approach that learns to map natural-language descrip-tions of simple " if-then " rules to executable code. By training and testing on a large cor-pus of naturally-occurring programs (called " recipes ") and their natural language de-scriptions, we demonstrate the ability to effectively map language to code. We compare a number of semantic parsing ap-proaches on the highly noisy training data collected from ordinary users, and find that loosely synchronous systems perform best.},
author = {Quirk, C. and Mooney, R. and Galley, M.},
booktitle = {ACL-IJCNLP},
doi = {10.3115/v1/P15-1085},
file = {:home/hiaoxui/papers/ACL-IJCNLP/Language to Code Learning Semantic Parsers for If-This-Then-That Recipes.pdf:pdf},
isbn = {9781941643723},
pages = {878--888},
title = {{Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes}},
year = {2015}
}
@inproceedings{Reddy2017,
abstract = {Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions. Our code and data can be downloaded at https://github.com/sivareddyg/udeplambda.},
archivePrefix = {arXiv},
arxivId = {1702.03196},
author = {Reddy, S. and T{\"{a}}ckstr{\"{o}}m, O. and Petrov, S. and Steedman, M. and Lapata, M.},
booktitle = {EMNLP},
eprint = {1702.03196},
file = {:home/hiaoxui/papers/EMNLP/Universal Semantic Parsing.pdf:pdf},
pages = {89--101},
title = {{Universal Semantic Parsing}},
url = {http://arxiv.org/abs/1702.03196},
year = {2017}
}
@inproceedings{Ferreira2016,
abstract = {In this study, we introduce a nondeterministic method for referring expression generation. We describe two models that account for individual variation in the choice of referential form in automatically generated text: a Naive Bayes model and a Recurrent Neural Network. Both are evaluated using the VaREG corpus. Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those produced by a random baseline model.},
author = {Ferreira, T. C. and Krahmer, E. and Wubben, S.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Towards more variation in text generation Developing and evaluating variation models for choice of referential form.pdf:pdf},
isbn = {9781510827585},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {568--577},
title = {{Towards more variation in text generation: Developing and evaluating variation models for choice of referential form}},
url = {http://www.aclweb.org/anthology/P16-1054},
year = {2016}
}
@inproceedings{Sarawagi2005,
abstract = {We describe semi-Markov conditional random fields (semi-CRFs), a con-ditionally trained version of semi-Markov chains. Intuitively, a semi-CRF on an input sequence x outputs a " segmentation " of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements x i of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time—often only a small constant factor slower than conventional CRFs. In experiments on five named entity recognition problems, semi-CRFs generally outper-form conventional CRFs.},
author = {Sarawagi, S. and Cohen, W. W.},
booktitle = {Neurips},
doi = {10.1.1.128.3524},
file = {:home/hiaoxui/papers/Neurips/Semi-Markov Conditional Random Fields for Information Extraction.pdf:pdf},
isbn = {0262195348},
issn = {10495258},
pages = {1185--1192},
title = {{Semi-Markov Conditional Random Fields for Information Extraction}},
url = {http://papers.nips.cc/paper/2648-semi-markov-conditional-random-fields-for-information-extraction},
year = {2005}
}
@inproceedings{DaumeIII2007,
abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
annote = {Reading group paper},
archivePrefix = {arXiv},
arxivId = {0907.1815},
author = {{Daum{\'{e}} III}, H.},
booktitle = {ACL},
eprint = {0907.1815},
file = {:home/hiaoxui/papers/ACL/Frustratingly Easy Domain Adaptation.pdf:pdf},
number = {June},
pages = {256--263},
title = {{Frustratingly Easy Domain Adaptation}},
url = {http://arxiv.org/abs/0907.1815},
year = {2007}
}
@unpublished{Collins2013e,
abstract = {Throughout this note I'll use underline to denote vectors. For example, w ∈ R d will be a vector with components w 1 , w 2 , . . . w d . We use exp(x) for the exponential function, i.e., exp(x) = e x . 2 Log-linear models We have sets X and Y: we will assume that Y is a finite set. Our goal is to build a model that estimates the conditional probability p(y|x) of a label y ∈ Y given an input x ∈ X . For example, x might be a word, and y might be a candidate part-of-speech (noun, verb, preposition etc.) for that word. We have a feature-vector definition $\phi$ : X × Y → R d . We also assume a parameter vector w ∈ R d . Given these definitions, log-linear models take the following form: p(y|x; w) =},
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/Log-Linear Models, MEMMs, and CRFs.pdf:pdf},
pages = {1--11},
title = {{Log-Linear Models, MEMMs, and CRFs}},
year = {2013}
}
@article{Schroedl2005,
abstract = {Multiple sequence alignment (MSA) is a ubiquitous problem in computational biology. Although it is NP-hard to find an optimal solution for an arbitrary number of sequences, due to the importance of this problem researchers are trying to push the limits of exact algorithms further. Since MSA can be cast as a classical path finding problem, it is attracting a growing number of AI researchers interested in heuristic search algorithms as a challenge with actual practical relevance. In this paper, we first review two previous, complementary lines of research. Based on Hirschberg's algorithm, Dynamic Programming needs O(kN{\^{}}(k-1)) space to store both the search frontier and the nodes needed to reconstruct the solution path, for k sequences of length N. Best first search, on the other hand, has the advantage of bounding the search space that has to be explored using a heuristic. However, it is necessary to maintain all explored nodes up to the final solution in order to prevent the search from re-expanding them at higher cost. Earlier approaches to reduce the Closed list are either incompatible with pruning methods for the Open list, or must retain at least the boundary of the Closed list. In this article, we present an algorithm that attempts at combining the respective advantages; like A* it uses a heuristic for pruning the search space, but reduces both the maximum Open and Closed size to O(kN{\^{}}(k-1)), as in Dynamic Programming. The underlying idea is to conduct a series of searches with successively increasing upper bounds, but using the DP ordering as the key for the Open priority queue. With a suitable choice of thresholds, in practice, a running time below four times that of A* can be expected. In our experiments we show that our algorithm outperforms one of the currently most successful algorithms for optimal multiple sequence alignments, Partial Expansion A*, both in time and memory. Moreover, we apply a refined heuristic based on optimal alignments not only of pairs of sequences, but of larger subsets. This idea is not new; however, to make it practically relevant we show that it is equally important to bound the heuristic computation appropriately, or the overhead can obliterate any possible gain. Furthermore, we discuss a number of improvements in time and space efficiency with regard to practical implementations. Our algorithm, used in conjunction with higher-dimensional heuristics, is able to calculate for the first time the optimal alignment for almost all of the problems in Reference 1 of the benchmark database BAliBASE.},
author = {Schroedl, S.},
doi = {10.1613/jair.1534},
file = {:home/hiaoxui/papers/JAIR/An improved search algorithm for optimal multiple-sequence alignment.pdf:pdf},
issn = {10769757},
journal = {JAIR},
pages = {587--623},
title = {{An improved search algorithm for optimal multiple-sequence alignment}},
volume = {23},
year = {2005}
}
@article{Chen2010,
abstract = {We present a novel framework for learning to interpret and generate language using only per-ceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descrip-tive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.},
author = {Chen, D. L. and Kim, J. and Mooney, R. J.},
doi = {10.1613/jair.2962},
file = {:home/hiaoxui/papers/JAIR/Training a multilingual sportscaster Using perceptual context to learn language.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {JAIR},
pages = {397--435},
title = {{Training a multilingual sportscaster: Using perceptual context to learn language}},
volume = {37},
year = {2010}
}
@inproceedings{Peng2018a,
abstract = {We introduce the structured projection of intermediate gradients optimization technique (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks (Kim et al., 2017) and some reinforcement learning-inspired solutions (Yogatama et al., 2017). Like so-called straight-through estimators (Hinton, 2012), SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT's proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.},
archivePrefix = {arXiv},
arxivId = {1805.04658},
author = {Peng, H. and Thomson, S. and Smith, N. A.},
booktitle = {ACL},
eprint = {1805.04658},
file = {:home/hiaoxui/papers/ACL/Backpropagating through Structured Argmax using a SPIGOT.pdf:pdf},
pages = {1--11},
title = {{Backpropagating through Structured Argmax using a SPIGOT}},
url = {http://arxiv.org/abs/1805.04658},
year = {2018}
}
@inproceedings{Clarke2010,
abstract = {Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers},
annote = {Treat semantic parsing as a binary classification problem.},
author = {Clarke, J. and Goldwasser, D. and Chang, M. and Roth, D.},
booktitle = {CoNLL},
file = {:home/hiaoxui/papers/CoNLL/Driving Semantic Parsing from the World's Response.pdf:pdf},
isbn = {9781932432831},
keywords = {semantic parsing,weakly supervised},
mendeley-tags = {semantic parsing,weakly supervised},
number = {July},
pages = {18--27},
title = {{Driving Semantic Parsing from the World's Response}},
year = {2010}
}
@article{Gutmann2012,
abstract = {We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, themodel is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective func- tion for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially gener- ated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to be- have like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimationmethods for unnormalizedmodels. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.},
author = {Gutmann, M. U. and Hyv{\"{a}}rine, A.},
file = {:home/hiaoxui/papers/JMLR/Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {JMLR},
keywords = {computation,estimation,natural image statistics,partition function,unnormalized models},
pages = {307--361},
title = {{Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics}},
url = {http://www.jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf},
volume = {13},
year = {2012}
}
@unpublished{Gkatzia2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1610.08375v1},
author = {Gkatzia, D.},
eprint = {arXiv:1610.08375v1},
file = {:home/hiaoxui/papers/Unknown/Content Selection in Data-to-Text Systems A Survey.pdf:pdf},
title = {{Content Selection in Data-to-Text Systems: A Survey}},
year = {2016}
}
@inproceedings{Taghipour2015,
abstract = {One of the weaknesses of current supervised word sense disambiguation (WSD) systems is that they only treat a word as a discrete en-tity. However, a continuous-space represen-tation of words (word embeddings) can pro-vide valuable information and thus improve generalization accuracy. Since word embed-dings are typically obtained from unlabeled data using unsupervised methods, this method can be seen as a semi-supervised word sense disambiguation approach. This paper investi-gates two ways of incorporating word embed-dings in a word sense disambiguation setting and evaluates these two methods on some Sen-sEval/SemEval lexical sample and all-words tasks and also a domain-specific lexical sam-ple task. The obtained results show that such representations consistently improve the ac-curacy of the selected supervised WSD sys-tem. Moreover, our experiments on a domain-specific dataset show that our supervised base-line system beats the best knowledge-based systems by a large margin.},
author = {Taghipour, K.},
booktitle = {HLT},
doi = {10.3115/v1/N15-1035},
file = {:home/hiaoxui/papers/HLT/Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains.pdf:pdf},
pages = {314--323},
title = {{Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains}},
year = {2015}
}
@inproceedings{Poliak2018,
abstract = {We propose a process for investigating the extent to which sentence representations arising from neural machine translation (NMT) systems encode distinct semantic phenomena. We use these representations as features to train a natural language inference (NLI) classifier based on datasets recast from existing semantic annotations. In applying this process to a representative NMT system, we find its encoder appears most suited to supporting inferences at the syntax-semantics interface, as compared to anaphora resolution requiring world-knowledge. We conclude with a discussion on the merits and potential deficiencies of the existing process, and how it may be improved and extended as a broader framework for evaluating semantic coverage.},
archivePrefix = {arXiv},
arxivId = {1804.09779},
author = {Poliak, A. and Belinkov, Y. and Glass, J. and van Durme, B.},
booktitle = {NAACL-HLT},
eprint = {1804.09779},
file = {:home/hiaoxui/papers/NAACL-HLT/On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference.pdf:pdf},
pages = {513--523},
title = {{On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference}},
url = {http://arxiv.org/abs/1804.09779},
year = {2018}
}
@inproceedings{Zettlemoyer2012,
abstract = {This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.},
archivePrefix = {arXiv},
arxivId = {1207.1420},
author = {Zettlemoyer, L. S. and Collins, M.},
booktitle = {UAI},
doi = {10.1093/acprof:oso/9780199654680.003.0006},
eprint = {1207.1420},
file = {:home/hiaoxui/papers/UAI/Learning to Map Sentences to Logical Form Structured Classification with Probabilistic Categorial Grammars.pdf:pdf},
isbn = {0974903914},
title = {{Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars}},
url = {http://arxiv.org/abs/1207.1420},
year = {2012}
}
@inproceedings{Louis2012,
abstract = {We introduce a model of coherence which captures the intentional discourse structure in text. Our work is based on the hypothesis that syntax provides a proxy for the communica- tive goal of a sentence and therefore the se- quence of sentences in a coherent discourse should exhibit detectable structural patterns. Results show that our method has high dis- criminating power for separating out coherent and incoherent news articles reaching accura- cies of up to 90{\%}. We also show that our syn- tactic patterns are correlated with manual an- notations of intentional structure for academic conference articles and can successfully pre- dict the coherence of abstract, introduction and related work sections of these articles. 1},
author = {Louis, A. and Nenkova, A.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/A Coherence Model Based on Syntactic Patterns.pdf:pdf},
isbn = {9781937284435},
number = {July},
pages = {1157--1168},
title = {{A Coherence Model Based on Syntactic Patterns}},
url = {http://www.aclweb.org/anthology-new/D/D12/D12-1106.bib{\%}5Cnhttp://www.aclweb.org/anthology-new/D/D12/D12-1106.pdf},
year = {2012}
}
@inproceedings{Smith2005,
abstract = {Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.},
author = {Smith, N. A. and Eisner, J. M.},
booktitle = {ACL},
doi = {10.3115/1219840.1219884},
file = {:home/hiaoxui/papers/ACL/Contrastive estimation Training Log-Linear Models on Unlabeled Data.pdf:pdf},
isbn = {1932432515},
number = {June},
pages = {354--362},
title = {{Contrastive estimation: Training Log-Linear Models on Unlabeled Data}},
url = {http://www.aclweb.org/anthology/P/P05/P05-1044.pdf},
year = {2005}
}
@inproceedings{Wang2013a,
abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.},
author = {Wang, S. and Manning, C. D.},
booktitle = {ICML},
file = {:home/hiaoxui/papers/ICML/Fast dropout training.pdf:pdf},
keywords = {I,boring formatting information,machine learning},
pages = {118--126},
title = {{Fast dropout training}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/wang13a},
volume = {28},
year = {2013}
}
@inproceedings{Salakhutdinov2003,
abstract = {We show a close relationship between the Expectation- Maximization (EM) algorithm and direct optimization algorithms such as gradientbased methods for parameter learning. We identify analytic conditions under which EM exhibits Newton-like behavior, and conditions under which it possesses poor, first-order convergence. Based on this analysis, we propose two novel algorithms for maximum likelihood estimation of latent variable models, and report empirical results showing that, as predicted by theory, the proposed new algorithms can substantially outperform standard EM in terms of speed of convergence in certain cases. 1.},
author = {Salakhutdinov, R. and Roweis, S. and Ghahramani, Z.},
booktitle = {ICML},
doi = {10.1145/ 1273496.1273497},
file = {:home/hiaoxui/papers/ICML/Optimization with EM and expectation-conjugate-gradient.pdf:pdf},
isbn = {1-57735-189-4},
keywords = {EM},
mendeley-tags = {EM},
number = {2},
pages = {672},
title = {{Optimization with EM and expectation-conjugate-gradient}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-088.pdf},
volume = {20},
year = {2003}
}
@inproceedings{Pauls2009,
author = {Pauls, A. and DeNero, J. and Klein, D.},
booktitle = {EMNLP},
doi = {10.3115/1699648.1699688},
file = {:home/hiaoxui/papers/EMNLP/Consensus Training for Consensus Decoding in Machine Translation.pdf:pdf},
isbn = {9781932432633},
keywords = {consensus-decoding,emnlp2009,smt},
title = {{Consensus Training for Consensus Decoding in Machine Translation}},
url = {http://www.aclweb.org/anthology-new/D/D09/D09-1147.bib{\%}5Cnhttp://www.aclweb.org/anthology-new/D/D09/D09-1147.pdf},
year = {2009}
}
@inproceedings{Hashimoto2018,
abstract = {Machine learning models (e.g., speech recognizers) are usually trained to minimize average loss, which results in representation disparity---minority groups (e.g., non-native speakers) contribute less to the training objective and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even make initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.},
archivePrefix = {arXiv},
arxivId = {1806.08010},
author = {Hashimoto, T. B. and Srivastava, M. and Namkoong, H. and Liang, P.},
booktitle = {ICML},
eprint = {1806.08010},
file = {:home/hiaoxui/papers/ICML/Fairness Without Demographics in Repeated Loss Minimization.pdf:pdf},
keywords = {Best},
mendeley-tags = {Best},
title = {{Fairness Without Demographics in Repeated Loss Minimization}},
url = {http://arxiv.org/abs/1806.08010},
year = {2018}
}
@article{Reiter2005,
abstract = {One of the main challenges in automatically generating textual weather forecasts is choosing appropriate English words to communicate numeric weather data. A corpus-based analysis of how humans write forecasts showed that there were major differences in how individual writers performed this task, that is, in how they translated data into words. These differences included both different preferences between potential near-synonyms that could be used to express information, and also differences in the meanings that individual writers associated with specific words. Because we thought these differences could confuse readers, we built our SumTime-Mousam weather-forecast generator to use consistent data-to-word rules, which avoided words which were only used by a few people, and words which were interpreted differently by different people. An evaluation by forecast users suggested that they preferred SumTime-Mousam's texts to human-generated texts, in part because of better word choice; this may be the first time that an evaluation has shown that nlg texts are better than human-authored texts. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Reiter, E. and Sripada, S. and Hunter, J. and Yu, J. and Davy, I.},
doi = {10.1016/j.artint.2005.06.006},
file = {:home/hiaoxui/papers/AI/Choosing words in computer-generated weather forecasts.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {AI},
keywords = {Idiolect,Information presentation,Language and the word,Lexical choice,Natural language generation,Natural language processing,Weather forecasts,lexical choice},
mendeley-tags = {lexical choice},
number = {1-2},
pages = {137--169},
title = {{Choosing words in computer-generated weather forecasts}},
volume = {167},
year = {2005}
}
@inproceedings{Weston2015,
abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term mem- ory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chainingmultiple supporting sentences to an- swer questions that require understanding the intension of verbs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.3916v10},
author = {Weston, J. and Chopra, S. and Bordes, A.},
booktitle = {ICLR},
doi = {v0},
eprint = {arXiv:1410.3916v10},
file = {:home/hiaoxui/papers/ICLR/Memory networks.pdf:pdf},
isbn = {9781424469178},
issn = {1098-7576},
pages = {1--15},
pmid = {9377276},
title = {{Memory networks}},
year = {2015}
}
@inproceedings{Johnson2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.06277v5},
author = {Johnson, M. J. and Duvenaud, D. and Wiltschko, A. B. and Datta, S. R. and Adams, R. P.},
booktitle = {Neurips},
eprint = {arXiv:1603.06277v5},
file = {:home/hiaoxui/papers/Neurips/Composing graphical models with neural networks for structured representations and fast inference.pdf:pdf},
title = {{Composing graphical models with neural networks for structured representations and fast inference}},
year = {2016}
}
@article{Williams1999,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.01121v2},
author = {Williams, A. and Drozdov, A. and Bowman, S. R.},
eprint = {arXiv:1709.01121v2},
file = {:home/hiaoxui/papers/TACL/Do latent tree learning models identify meaningful structure in sentences.pdf:pdf},
journal = {TACL},
title = {{Do latent tree learning models identify meaningful structure in sentences ?}},
year = {1999}
}
@inproceedings{Boyd1998,
abstract = {| A system is described that integrates knowledge-based signal processing and natural lan-guage processing to automatically generate descrip-tions of time-series data. These descriptions are based on short and long-term trends in the data which are detected using wavelet analysis. The basic architec-ture of the system is presented and some experimental results are shown for weather data.},
author = {Boyd, S.},
booktitle = {ICIPS},
doi = {10.1.1.57.3705},
file = {:home/hiaoxui/papers/ICIPS/TREND A System for Generating Intelligent Descriptions of Time-Series Data.pdf:pdf},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {1--5},
title = {{TREND: A System for Generating Intelligent Descriptions of Time-Series Data}},
year = {1998}
}
@inproceedings{Rajpurkar2018,
abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD 1.1 achieves only 66{\%} F1 on SQuAD 2.0.},
archivePrefix = {arXiv},
arxivId = {1806.03822},
author = {Rajpurkar, P. and Jia, R. and Liang, P.},
booktitle = {ACL},
eprint = {1806.03822},
file = {:home/hiaoxui/papers/ACL/Know What You Don't Know Unanswerable Questions for SQuAD.pdf:pdf},
pages = {1--6},
title = {{Know What You Don't Know: Unanswerable Questions for SQuAD}},
url = {http://arxiv.org/abs/1806.03822},
year = {2018}
}
@article{Theune2001,
abstract = {We present a data-to-speech system called D2S, which can be used for the creation of data-to-speech systems in different languages {\&} domains. The most important characteristic of a data-to-speech system is that it combines language {\&} speech generation: language generation is used to produce a natural language text expressing the system's input data, {\&} speech generation is used to make this text audible. In D2S, this combination is exploited by using linguistic information available in the language generation module for the computation of prosody. This allows us to achieve a better prosodic output quality than can be achieved in a plain text-to-speech system. For language generation in D2S, the use of syntactically enriched templates is guided by knowledge of the discourse context, while for speech generation pre-recorded phrases are combined in a prosodically sophisticated manner. This combination of techniques makes it possible to create linguistically sound but efficient systems with a high quality language {\&} speech output. 1 Table, 16 Figures, 69 References. Adapted from the source document},
author = {Theune, M. and Klabbers, E. and Odijk, J. and {De Pijper}, J. R. and Krahmer, E.},
file = {:home/hiaoxui/papers/Natural Language Engineering/From Data to Speech A General Approach.pdf:pdf},
isbn = {1469-8110},
issn = {1351-3249, 1351-3249},
journal = {Natural Language Engineering},
keywords = {*Natural Language Processing (56550),*Speech Synthesis (82900),*Suprasegmentals (85750),*Syntactic Processing (86760),5113: descriptive linguistics,6111: phonetics,article,computational and mathematical linguistics,lexical choice,speech synthesis/recognition},
mendeley-tags = {lexical choice},
number = {1},
pages = {47--86},
title = {{From Data to Speech: A General Approach}},
url = {http://findit.lib.cuhk.edu.hk/852cuhk/?url{\_}ver=Z39.88-2004{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal{\&}genre=article{\&}sid=ProQ:ProQ{\%}3Allbashell{\&}atitle=From+Data+to+Speech{\%}3A+A+General+Approach{\&}title=Natural+Language+Engineering{\&}issn=13513249{\&}date=2001-03-},
volume = {7},
year = {2001}
}
@inproceedings{Srivastava2015,
abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
archivePrefix = {arXiv},
arxivId = {1505.00387},
author = {Srivastava, R. K. and Greff, K. and Schmidhuber, J.},
booktitle = {ICML},
eprint = {1505.00387},
file = {:home/hiaoxui/papers/ICML/Highway Networks.pdf:pdf},
title = {{Highway Networks}},
url = {http://arxiv.org/abs/1505.00387},
year = {2015}
}
@inproceedings{Mikolov2013a,
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, T. and Chen, K. and Corrado, G. and Dean, J.},
booktitle = {Neurips},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:home/hiaoxui/papers/Neurips/Distributed Representations of Words and Phrases and Their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and Their Compositionality}},
year = {2013}
}
@inproceedings{Buys2017,
abstract = {Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focused almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69{\%} Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.},
archivePrefix = {arXiv},
arxivId = {1704.07092},
author = {Buys, J. and Blunsom, P.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1112},
eprint = {1704.07092},
file = {:home/hiaoxui/papers/ACL/Robust Incremental Neural Semantic Graph Parsing.pdf:pdf},
isbn = {9781945626753},
pages = {1215--1226},
title = {{Robust Incremental Neural Semantic Graph Parsing}},
url = {http://arxiv.org/abs/1704.07092},
year = {2017}
}
@inproceedings{Deng2017,
abstract = {—With the development of anonymous communi-cation technology, it has led to the fact that the network monitoring is becoming more and more difficult. If the anonymous traffic can be effectively identified, the abuse of such technology can be prevented. Since the study of machine learning is rapidly developing these years, this paper applies the Random Forest Algorithm ---a semi-supervised learning method ---into the traffic detection of Shadowsocks. We can get over 85{\%} detection accuracy rate in our experiments after applying Random Forest Algorithm by collecting train set, gathering features, training models and predicting results. With the scale of train set and test set increase, the detection accuracy rate gradually increases until it becomes constant. We will make several adjustments on train set, test set and feature set to reduce the false alarm rate and false rate when detecting.},
author = {Deng, Z. and Liu, Z. and Chen, Z. and Guo, Y.},
booktitle = {IHMSC},
doi = {10.1109/IHMSC.2017.132},
file = {:home/hiaoxui/papers/IHMSC/The Random Forest Based Detection of Shadowsock's Traffic.pdf:pdf},
isbn = {978-1-5386-3021-1},
keywords = {- detection,random forest algorithm,shadowsocks},
pages = {75--78},
title = {{The Random Forest Based Detection of Shadowsock's Traffic}},
url = {http://ieeexplore.ieee.org/document/8048116/},
year = {2017}
}
@inproceedings{Falke2017,
abstract = {Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.},
archivePrefix = {arXiv},
arxivId = {1704.04452},
author = {Falke, T. and Gurevych, I.},
booktitle = {EMNLP},
eprint = {1704.04452},
file = {:home/hiaoxui/papers/EMNLP/Bringing Structure into Summaries Crowdsourcing a Benchmark Corpus of Concept Maps.pdf:pdf},
pages = {2951--2961},
title = {{Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps}},
url = {http://arxiv.org/abs/1704.04452},
year = {2017}
}
@inproceedings{Strubell2018,
abstract = {Do unsupervised methods for learning rich, contextualized token representations obviate the need for explicit modeling of linguistic structure in neural network models for semantic role labeling (SRL)? We address this question by incorporating the massively successful ELMo em-beddings (Peters et al., 2018) into LISA (Strubell and McCallum, 2018), a strong, linguistically-informed neural network architecture for SRL. In experiments on the CoNLL-2005 shared task we find that though ELMo out-performs typical word embeddings, beginning to close the gap in F1 between LISA with predicted and gold syntactic parses, syntactically-informed models still out-perform syntax-free models when both use ELMo, especially on out-of-domain data. Our results suggest that linguistic structures are indeed still relevant in this golden age of deep learning for NLP.},
author = {Strubell, E. and Mccallum, A.},
booktitle = {Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP},
file = {:home/hiaoxui/papers/Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP/Syntax Helps ELMo Understand Semantics Is Syntax Still Relevant in a Deep Neural Architecture for SRL.pdf:pdf},
pages = {19--27},
title = {{Syntax Helps ELMo Understand Semantics: Is Syntax Still Relevant in a Deep Neural Architecture for SRL?}},
url = {https://nlp.stanford.edu/projects/},
year = {2018}
}
@article{Power2012,
abstract = {We describe a computational model for planning phrases like “more than a quarter” and “25.9 per cent” which describe proportions at different levels of precision. The model lays out the key choices in planning a numerical description, using formal definitions of mathematical form (e.g., the distinction between fractions and percentages) and roundness adapted from earlier studies. The task is modeled as a constraint satisfaction problem, with solutions subsequently ranked by preferences (e.g., for roundness). Detailed constraints are based on a corpus of numerical expressions collected in the NUMGEN project, and evaluated through empirical studies in which subjects were asked to produce (or complete) numerical expressions in specified contexts.},
author = {Power, R. and Williams, S.},
doi = {10.1162/COLI_a_00086},
file = {:home/hiaoxui/papers/Computational Linguistics/Generating Numerical Approximations.pdf:pdf},
issn = {0891-2017},
journal = {Computational Linguistics},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
number = {1},
pages = {113--134},
title = {{Generating Numerical Approximations}},
url = {http://www.mitpressjournals.org/doi/10.1162/COLI{\_}a{\_}00086},
volume = {38},
year = {2012}
}
@inproceedings{Papineni2002,
abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
archivePrefix = {arXiv},
arxivId = {1702.00764},
author = {Papineni, K. and Roukos, S. and Ward, T. and Zhu, W.},
booktitle = {ACL},
doi = {10.3115/1073083.1073135},
eprint = {1702.00764},
file = {:home/hiaoxui/papers/ACL/BLEU a method for automatic evaluation of machine translation.pdf:pdf},
isbn = {1-55860-883-4},
issn = {00134686},
number = {July},
pages = {311--318},
title = {{BLEU: a method for automatic evaluation of machine translation}},
url = {http://dl.acm.org/citation.cfm?id=1073135},
year = {2002}
}
@inproceedings{Miao2016,
abstract = {In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.},
archivePrefix = {arXiv},
arxivId = {1609.07317},
author = {Miao, Y. and Blunsom, P.},
booktitle = {EMNLP},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1609.07317},
file = {:home/hiaoxui/papers/EMNLP/Language as a Latent Variable Discrete Generative Models for Sentence Compression.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
pages = {319--328},
pmid = {26353135},
title = {{Language as a Latent Variable: Discrete Generative Models for Sentence Compression}},
url = {http://arxiv.org/abs/1609.07317},
year = {2016}
}
@article{Povey2011,
abstract = {We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Povey, D. and Ghoshal, A. and Boulianne, G. and Burget, L. and Glembek, O. and Goel, N. and Hannemann, M. and Motlicek, P. and Qian, Y. and Schwarz, P. and Silovsky, J. and Stemmer, G. and Vesely, K.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/ASRU/The Kaldi speech recognition toolkit.pdf:pdf},
isbn = {978-1-4673-0366-8},
issn = {1098-6596},
journal = {ASRU},
pages = {1--4},
pmid = {25246403},
title = {{The Kaldi speech recognition toolkit}},
year = {2011}
}
@inproceedings{Kim2017,
abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
archivePrefix = {arXiv},
arxivId = {1702.00887},
author = {Kim, Y. and Denton, C. and Hoang, L. and Rush, A. M.},
booktitle = {ICLR},
doi = {10.1007/978-1-4615-5533-9_4},
eprint = {1702.00887},
file = {:home/hiaoxui/papers/ICLR/Structured Attention Networks.pdf:pdf},
isbn = {978-1-4613-7529-6},
issn = {0271-678X},
pages = {1--21},
pmid = {2830291},
title = {{Structured Attention Networks}},
url = {http://arxiv.org/abs/1702.00887},
year = {2017}
}
@inproceedings{Sharan2017,
abstract = {We study the problem of learning overcomplete HMMs---those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results---both positive and negative---which help define the boundaries between the tractable and intractable settings. Specifically, we show positive results for a large subclass of HMMs whose transition matrices are sparse, well-conditioned, and have small probability mass on short cycles. On the other hand, we show that learning is impossible given only a polynomial number of samples for HMMs with a small output alphabet and whose transition matrices are random regular graphs with large degree. We also discuss these results in the context of learning HMMs which can capture long-term dependencies.},
archivePrefix = {arXiv},
arxivId = {1711.02309},
author = {Sharan, V. and Kakade, S. and Liang, P. and Valiant, G.},
booktitle = {Neurips},
eprint = {1711.02309},
file = {:home/hiaoxui/papers/Neurips/Learning Overcomplete HMMs.pdf:pdf},
pages = {1--10},
title = {{Learning Overcomplete HMMs}},
url = {http://arxiv.org/abs/1711.02309},
year = {2017}
}
@inproceedings{Mei2016,
abstract = {Many events occur in the world. Some event types are stochastically excited or inhibited---in the sense of having their probabilities elevated or decreased---by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.},
archivePrefix = {arXiv},
arxivId = {1612.09328},
author = {Mei, H. and Eisner, J. M.},
booktitle = {Neurips},
eprint = {1612.09328},
file = {:home/hiaoxui/papers/Neurips/The Neural Hawkes Process A Neurally Self-Modulating Multivariate Point Process.pdf:pdf},
number = {Nips},
title = {{The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process}},
url = {http://arxiv.org/abs/1612.09328},
year = {2016}
}
@article{Weston2013,
abstract = {This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on two scoring functions that operate by learning low-dimensional embeddings of words and of entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over existing methods that rely on text features alone.},
archivePrefix = {arXiv},
arxivId = {1307.7973},
author = {Weston, J. and Bordes, A. and Yakhnenko, O. and Usunier, N.},
eprint = {1307.7973},
file = {:home/hiaoxui/papers/Computational Linguistics/Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction.pdf:pdf},
isbn = {9781937284978},
journal = {Computational Linguistics},
title = {{Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction}},
url = {http://arxiv.org/abs/1307.7973},
year = {2013}
}
@inproceedings{Zarrieß2016,
abstract = {Colour terms have been a prime phenomenon for studying language grounding, though pre-vious work focussed mostly on descriptions of simple objects or colour swatches. This paper investigates whether colour terms can be learned from more realistic and potentially noisy visual inputs, using a corpus of referring expressions to objects represented as regions in real-world images. We obtain promising re-sults from combining a classifier that grounds colour terms in visual input with a recalibra-tion model that adjusts probability distribu-tions over colour terms according to contex-tual and object-specific preferences.},
author = {Zarrie{\ss}, S. and Schlangen, D.},
booktitle = {INLG},
file = {:home/hiaoxui/papers/INLG/Towards Generating Colour Terms for Referents in Photographs Prefer the Expected or the Unexpected.pdf:pdf},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {246--255},
title = {{Towards Generating Colour Terms for Referents in Photographs : Prefer the Expected or the Unexpected ?}},
year = {2016}
}
@inproceedings{Snow2008,
abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We ex-plore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: af-fect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechani-cal Turk non-expert annotations and existing gold standard labels provided by expert label-ers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effec-tive as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annota-tion quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
archivePrefix = {arXiv},
arxivId = {1601.06610},
author = {Snow, R. and O'Connor, B. and Jurafsky, D. and Ng, A. Y.},
booktitle = {EMNLP},
doi = {10.1.1.142.8286},
eprint = {1601.06610},
file = {:home/hiaoxui/papers/EMNLP/Cheap and Fast - But is it Good Evaluating Non-Expert Annotations for Natural Language Tasks.pdf:pdf},
isbn = {9781450329224},
issn = {09246495},
pages = {254--263},
pmid = {23259955},
title = {{Cheap and Fast - But is it Good ? Evaluating Non-Expert Annotations for Natural Language Tasks}},
year = {2008}
}
@unpublished{Rojas1997,
abstract = {This paper is a short and painless introduction to the $\lambda$ calculus. Originally developed in order to study some mathematical properties of effectively com- putable functions, this formalism has provided a strong theoretical foundation for the family of functional programming languages. We show how to perform some arithmetical computations using the $\lambda$ calculus and how to define recur- sive functions, even though functions in $\lambda$ calculus are not given names and thus cannot refer explicitly to themselves.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.09060v1},
author = {Rojas, R.},
doi = {10.1006/anbe.1999.1219},
eprint = {arXiv:1503.09060v1},
file = {:home/hiaoxui/papers/Unknown/A Tutorial Introduction to the Lambda Calculus.pdf:pdf},
issn = {00033472},
pages = {1--9},
pmid = {10512656},
title = {{A Tutorial Introduction to the Lambda Calculus}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.136.4173{\&}rep=rep1{\&}type=pdf},
volume = {58},
year = {1997}
}
@unpublished{Huang2015,
abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
archivePrefix = {arXiv},
arxivId = {1508.01991},
author = {Huang, Z. and Xu, W. and Yu, K.},
doi = {10.18653/v1/P16-1101},
eprint = {1508.01991},
file = {:home/hiaoxui/papers/Unknown/Bidirectional LSTM-CRF Models for Sequence Tagging.pdf:pdf},
isbn = {9781510827585},
issn = {1098-6596},
pmid = {25246403},
title = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
url = {http://arxiv.org/abs/1508.01991},
year = {2015}
}
@inproceedings{Hu2016a,
abstract = {Regulating deep neural networks (DNNs) with human structured knowledge has shown to be of great benefit for improved accuracy and in-terpretability. We develop a general framework that enables learning knowledge and its confidence jointly with the DNNs, so that the vast amount of fuzzy knowledge can be incorporated and automatically optimized with little manual efforts. We apply the framework to sentence sentiment analysis, augmenting a DNN with massive linguistic constraints on discourse and polarity structures. Our model substantially enhances the performance using less training data, and shows improved inter-pretability. The principled framework can also be applied to posterior regularization for regulating other statistical models.},
author = {Hu, Z. and Yang, Z. and Salakhutdinov, R. and Xing, E. P.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Deep Neural Networks with Massive Learned Knowledge.pdf:pdf},
issn = {00192120},
title = {{Deep Neural Networks with Massive Learned Knowledge}},
year = {2016}
}
@unpublished{Lee2019,
abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
archivePrefix = {arXiv},
arxivId = {1902.06720},
author = {Lee, J. and Xiao, L. and Schoenholz, S. S. and Bahri, Y. and Sohl-Dickstein, J. and Pennington, J.},
eprint = {1902.06720},
file = {:home/hiaoxui/papers/Unknown/Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent.pdf:pdf},
title = {{Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent}},
url = {http://arxiv.org/abs/1902.06720},
year = {2019}
}
@inproceedings{Smiley2016,
abstract = {For data-to-text tasks in Natural Language Generation (NLG), researchers are often faced with choices about the right words to express phenomena seen in the data. One common phenomenon centers around the description of trends between two data points and selecting the appropriate verb to express both the di-rection and intensity of movement. Our re-search shows that rather than simply select-ing the same verbs again and again, variation and naturalness can be achieved by quantify-ing writers' patterns of usage around verbs.},
author = {Smiley, C. and Plachouras, V. and Schilder, F. and Bretz, H. and Leidner, J. L. and Song, D.},
booktitle = {INLG},
file = {:home/hiaoxui/papers/INLG/When to Plummet and When to Soar Corpus Based Verb Selection for Natural Language Generation.pdf:pdf},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {36--39},
title = {{When to Plummet and When to Soar: Corpus Based Verb Selection for Natural Language Generation}},
year = {2016}
}
@inproceedings{Qin2018,
author = {Qin, G. and Yao, J. and Wang, X. and Wang, J. and Lin, C.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data.pdf:pdf},
title = {{Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data}},
year = {2018}
}
@unpublished{Collins2011a,
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/Tagging Problems, and Hidden Markov Models.pdf:pdf},
number = {1},
pages = {1--22},
title = {{Tagging Problems, and Hidden Markov Models}},
volume = {1},
year = {2011}
}
@inproceedings{Fan2018,
abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
archivePrefix = {arXiv},
arxivId = {1805.04833},
author = {Fan, A. and Lewis, M. and Dauphin, Y.},
booktitle = {ACL},
eprint = {1805.04833},
file = {:home/hiaoxui/papers/ACL/Hierarchical Neural Story Generation.pdf:pdf},
pages = {1--10},
title = {{Hierarchical Neural Story Generation}},
url = {http://arxiv.org/abs/1805.04833},
year = {2018}
}
@unpublished{Rong2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.2738v4},
author = {Rong, X.},
eprint = {arXiv:1411.2738v4},
file = {:home/hiaoxui/papers/Unknown/word2vec Parameter Learning Explained.pdf:pdf},
pages = {1--21},
title = {{word2vec Parameter Learning Explained}},
year = {2014}
}
@inproceedings{Fleischman2002,
author = {Fleischman, M. and Hovy, E.},
booktitle = {INLG},
file = {:home/hiaoxui/papers/INLG/Towards emotional variation in natural language generation.pdf:pdf},
keywords = {lexical choice},
mendeley-tags = {lexical choice},
pages = {1--8},
title = {{Towards emotional variation in natural language generation}},
year = {2002}
}
@inproceedings{Xie2013,
abstract = {Semantic frames are a rich linguistic re-source. There has been much work on semantic frame parsers, but less that applies them to general NLP problems. We address a task to predict change in stock price from financial news. Seman-tic frames help to generalize from spe-cific sentences to scenarios, and to de-tect the (positive or negative) roles of spe-cific companies. We introduce a novel tree representation, and use it to train predic-tive models with tree kernels using sup-port vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that fea-tures derived from semantic frame pars-ing have significantly better performance across years on the polarity task.},
author = {Xie, B. and Passonneau, R. J. and Wu, L.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Semantic Frames to Predict Stock Price Movement.pdf:pdf},
isbn = {9781937284503},
pages = {873--883},
title = {{Semantic Frames to Predict Stock Price Movement}},
year = {2013}
}
@inproceedings{Xiao2017,
abstract = {Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.},
archivePrefix = {arXiv},
arxivId = {1705.08051},
author = {Xiao, S. and Farajtabar, M. and Ye, X. and Yan, J. and Song, L. and Zha, H.},
booktitle = {Neurips},
eprint = {1705.08051},
file = {:home/hiaoxui/papers/Neurips/Wasserstein Learning of Deep Generative Point Process Models.pdf:pdf},
issn = {10495258},
title = {{Wasserstein Learning of Deep Generative Point Process Models}},
url = {http://arxiv.org/abs/1705.08051},
year = {2017}
}
@article{Ransom2002,
abstract = {We present an assortment of both standard and advanced Fourier techniques that are useful in the analysis of astrophysical time series of very long duration-where the observation time is much greater than the time resolution of the individual data points. We begin by reviewing the operational characteristics of Fourier transforms of time-series data, including power-spectral statistics, discussing some of the differences between analyses of binned data, sampled data, and event data, and we briefly discuss algorithms for calculating discrete Fourier transforms (DFTs) of very long time series. We then discuss the response of DFTs to periodic signals and present techniques to recover Fourier amplitude ``lost'' during simple traditional analyses if the periodicities change frequency during the observation. These techniques include Fourier interpolation, which allows us to correct the response for signals that occur between Fourier frequency bins. We then present techniques for estimating additional signal properties such as the signal's centroid and duration in time, the first and second derivatives of the frequency, the pulsed fraction, and an overall estimate of the significance of a detection. Finally, we present a recipe for a basic but thorough Fourier analysis of a time series for well-behaved pulsations.},
archivePrefix = {arXiv},
arxivId = {astro-ph/0204349},
author = {Ransom, S. M. and Eikenberry, S. S. and Middleditch, J.},
doi = {10.1086/342285},
eprint = {0204349},
file = {:home/hiaoxui/papers/The Astronomical Journal/Fourier Techniques for Very Long Astrophysical Time Series Analysis.pdf:pdf},
issn = {1538-3881},
journal = {The Astronomical Journal},
keywords = {data analysis,general,methods,pulsars},
number = {3},
pages = {38},
primaryClass = {astro-ph},
title = {{Fourier Techniques for Very Long Astrophysical Time Series Analysis}},
url = {http://arxiv.org/abs/astro-ph/0204349},
volume = {124},
year = {2002}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, S. and Szegedy, C.},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:home/hiaoxui/papers/JMLR/Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {JMLR},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@inproceedings{Strapparava2005,
author = {Strapparava, C.},
booktitle = {EMNLP-HLT},
file = {:home/hiaoxui/papers/EMNLP-HLT/Making Computers Laugh Investigations in Automatic Humor Recognition.pdf:pdf},
number = {October},
pages = {531--538},
title = {{Making Computers Laugh : Investigations in Automatic Humor Recognition}},
year = {2005}
}
@inproceedings{Moore2004,
abstract = {We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1. We demonstrate reduction in alignment error rate of approximately 30{\%} resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.},
author = {Moore, R. C.},
booktitle = {ACL},
doi = {10.3115/1218955.1219021},
file = {:home/hiaoxui/papers/ACL/Improving IBM word-alignment model 1.pdf:pdf},
pages = {518--es},
title = {{Improving IBM word-alignment model 1}},
url = {http://portal.acm.org/citation.cfm?doid=1218955.1219021},
year = {2004}
}
@inproceedings{Murakami2017,
abstract = {This paper presents a novel encoder-decoder model for automatically generat-ing market comments from stock prices. The model first encodes both short-and long-term series of stock prices so that it can mention short-and long-term changes in stock prices. In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic op-eration such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best model generates market comments at the fluency and the informativeness approach-ing human-generated reference texts.},
author = {Murakami, S. and Watanabe, A. and Miyazawa, A. and Goshima, K. and Yanase, T. and Takamura, H. and Miyao, Y.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1126},
file = {:home/hiaoxui/papers/ACL/Learning to Generate Market Comments from Stock Prices.pdf:pdf},
isbn = {9781945626753},
pages = {1374--1384},
title = {{Learning to Generate Market Comments from Stock Prices}},
url = {https://doi.org/10.18653/v1/P17-1126},
year = {2017}
}
@inproceedings{Barzilay2005,
abstract = {A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efficient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods.},
author = {Barzilay, R. and Lapata, M.},
booktitle = {EMNLP},
doi = {http://dx.doi.org/10.3115/1220575.1220617},
file = {:home/hiaoxui/papers/EMNLP/Collective content selection for concept-to-text generation.pdf:pdf},
number = {October},
pages = {331--338},
title = {{Collective content selection for concept-to-text generation}},
url = {http://dl.acm.org/citation.cfm?id=1220617},
year = {2005}
}
@unpublished{Etesami2016,
abstract = {Learning the influence structure of multiple time series data is of great interest to many disciplines. This paper studies the problem of recovering the causal structure in network of multivariate linear Hawkes processes. In such processes, the occurrence of an event in one process affects the probability of occurrence of new events in some other processes. Thus, a natural notion of causality exists between such processes captured by the support of the excitation matrix. We show that the resulting causal influence network is equivalent to the Directed Information graph (DIG) of the processes, which encodes the causal factorization of the joint distribution of the processes. Furthermore, we present an algorithm for learning the support of excitation matrix (or equivalently the DIG). The performance of the algorithm is evaluated on synthesized multivariate Hawkes networks as well as a stock market and MemeTracker real-world dataset.},
archivePrefix = {arXiv},
arxivId = {1603.04319},
author = {Etesami, J. and Kiyavash, N. and Zhang, K. and Singhal, K.},
eprint = {1603.04319},
file = {:home/hiaoxui/papers/Unknown/Learning Network of Multivariate Hawkes Processes A Time Series Approach.pdf:pdf},
isbn = {9781510827806},
pages = {1--14},
title = {{Learning Network of Multivariate Hawkes Processes: A Time Series Approach}},
url = {http://arxiv.org/abs/1603.04319},
year = {2016}
}
@inproceedings{Yi2018,
abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8{\%} on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
archivePrefix = {arXiv},
arxivId = {1810.02338},
author = {Yi, K. and Wu, J. and Gan, C. and Torralba, A. and Kohli, P. and Tenenbaum, J. B.},
booktitle = {Neurips},
doi = {10.1111/j.1749-6632.2009.04729.x},
eprint = {1810.02338},
file = {:home/hiaoxui/papers/Neurips/Neural-Symbolic VQA Disentangling Reasoning from Vision and Language Understanding(2).pdf:pdf},
isbn = {9781573317375},
issn = {1749-6632},
number = {NeurIPS},
pmid = {19723035},
title = {{Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding}},
url = {http://arxiv.org/abs/1810.02338},
year = {2018}
}
@inproceedings{Koehn2003a,
author = {Koehn, P. and Och, F. J. and Marcu, D.},
booktitle = {NAACL},
file = {:home/hiaoxui/papers/NAACL/Statistical phrase-based translation.pdf:pdf},
pages = {48--54},
title = {{Statistical phrase-based translation}},
year = {2003}
}
@unpublished{Liang2013a,
abstract = {This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.},
archivePrefix = {arXiv},
arxivId = {1309.4408},
author = {Liang, P.},
doi = {10.1162/COLI},
eprint = {1309.4408},
file = {:home/hiaoxui/papers/Unknown/Lambda Dependency-Based Compositional Semantics.pdf:pdf},
isbn = {9781608459858},
issn = {04194217},
pages = {1--7},
pmid = {22251136},
title = {{Lambda Dependency-Based Compositional Semantics}},
url = {http://arxiv.org/abs/1309.4408},
year = {2013}
}
@inproceedings{Dai2015,
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
archivePrefix = {arXiv},
arxivId = {1511.01432},
author = {Dai, A. M. and Le, Q. V.},
booktitle = {Neurips},
eprint = {1511.01432},
file = {:home/hiaoxui/papers/Neurips/Semi-supervised Sequence Learning.pdf:pdf},
issn = {10495258},
pages = {1--10},
pmid = {414454},
title = {{Semi-supervised Sequence Learning}},
url = {http://arxiv.org/abs/1511.01432},
year = {2015}
}
@inproceedings{Stent2004,
abstract = {A challenging problem for spoken dialog systems is the design of utterance generation modules that are fast, flexible and general, yet produce high quality output in particular domains. A promising approach is trainable generation, which uses general-purpose linguistic knowledge automatically adapted to the application domain. This paper presents a trainable sentence planner for the MATCH dialog system. We show that trainable sentence planning can produce output comparable to that of MATCH's template-based generator even for quite complex information presentations.},
author = {Stent, A. and Prasad, R. and Walker, M.},
booktitle = {ACL},
doi = {10.3115/1218955.1218966},
file = {:home/hiaoxui/papers/ACL/Trainable Sentence Planning for Complex Information Presentation in Spoken Dialog Systems.pdf:pdf},
pages = {79--es},
title = {{Trainable Sentence Planning for Complex Information Presentation in Spoken Dialog Systems}},
year = {2004}
}
@inproceedings{Zhou2017,
abstract = {Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.},
archivePrefix = {arXiv},
arxivId = {1704.01691},
author = {Zhou, C. and Neubig, G.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1029},
eprint = {1704.01691},
file = {:home/hiaoxui/papers/ACL/Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction.pdf:pdf},
isbn = {9781945626753},
title = {{Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction}},
url = {http://arxiv.org/abs/1704.01691},
year = {2017}
}
@unpublished{Gatt2017,
abstract = {This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artificial intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of Natural Language Processing, with an emphasis on different evaluation methods and the relationships between them.},
archivePrefix = {arXiv},
arxivId = {1703.09902},
author = {Gatt, A. and Krahmer, E.},
eprint = {1703.09902},
file = {:home/hiaoxui/papers/Unknown/Survey of the State of the Art in Natural Language Generation Core tasks, applications and evaluation.pdf:pdf},
number = {c},
pages = {1--111},
title = {{Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation}},
url = {http://arxiv.org/abs/1703.09902},
year = {2017}
}
@inproceedings{Fitzgerald2013,
abstract = {We present a new approach to referring expression generation, casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world. Despite an extremely large space of possible expressions, we demonstrate effective learning of a globally normalized log-linear distribution. This learning is enabled by a new, multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms. We train and evaluate the approach on a new corpus of references to sets of visual objects. Experiments show the approach is able to learn accurate models, which generate over 87{\%} of the expressions people used. Additionally, on the previously studied special case of single object reference, we show a 35{\%} relative error reduction over previous state of the art.},
author = {FitzGerald, N. and Artzi, Y. and Zettlemoyer, L. S.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Learning Distributions over Logical Forms for Referring Expression Generation.pdf:pdf},
isbn = {9781937284978},
number = {October},
pages = {1914--1925},
title = {{Learning Distributions over Logical Forms for Referring Expression Generation}},
year = {2013}
}
@article{Bagchi2013,
abstract = {By generalizing earlier work of Johnston {\&} Kulkarni, we present a detailed description of the reduction in the signal-to-noise ratio for observations of binary pulsars. We present analytical expressions, and provide software, to calculate the sensitivity reduction for orbits of arbitrary eccentricity. We find that this reduction can be quite significant, especially in the case of a massive companion like another neutron star or a black hole. On the other hand, the reduction is less for highly eccentric orbits. We also demonstrate that this loss of sensitivity can be recovered by employing "acceleration search" or "acceleration-jerk search" algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1302.4914v2},
author = {Bagchi, M. and Lorimer, D. R. and Wolfe, S.},
doi = {10.1093/mnras/stt559},
eprint = {arXiv:1302.4914v2},
file = {:home/hiaoxui/papers/Monthly Notices of the Royal Astronomical Society/On the detectability of eccentric binary pulsars.pdf:pdf},
issn = {00358711},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Binaries: general,Methods: analytical,Methods: numerical,Pulsars: general,Stars: neutron},
number = {2},
pages = {1303--1314},
title = {{On the detectability of eccentric binary pulsars}},
volume = {432},
year = {2013}
}
@inproceedings{Talmor2018,
archivePrefix = {arXiv},
arxivId = {1803.06643},
author = {Talmor, A. and Berant, J.},
booktitle = {NAACL},
eprint = {1803.06643},
file = {:home/hiaoxui/papers/NAACL/The Web as a Knowledge-base for Answering Complex Questions.pdf:pdf},
pages = {1--10},
title = {{The Web as a Knowledge-base for Answering Complex Questions}},
year = {2018}
}
@inproceedings{Chen2017a,
abstract = {We propose a framework for discriminative IR atop linguistic features, trained to improve the recall of answer candidate pas-sage retrieval, the initial step in text-based question answering. We formalize this as an instance of linear feature-based IR, demonstrating a 34{\%} -43{\%} improvement in recall for candidate triage for QA.},
author = {Chen, T. and van Durme, B.},
booktitle = {EACL},
doi = {10.18653/v1/e17-2114},
file = {:home/hiaoxui/papers/EACL/Discriminative Information Retrieval for Question Answering Sentence Selection.pdf:pdf},
pages = {719--725},
title = {{Discriminative Information Retrieval for Question Answering Sentence Selection}},
volume = {2},
year = {2017}
}
@inproceedings{Poon2013,
abstract = {We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM. To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84{\%}, effectively tying with the best published results by supervised approaches.},
author = {Poon, H.},
booktitle = {ACL},
doi = {10.3115/1699510.1699512},
file = {:home/hiaoxui/papers/ACL/Grounded Unsupervised Semantic Parsing.pdf:pdf},
isbn = {9781932432596},
number = {2010},
pages = {933--943},
title = {{Grounded Unsupervised Semantic Parsing}},
year = {2013}
}
@inproceedings{Bertero2016,
abstract = {We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the au- dience would react. We model the setup- punchline relation of conversational humor with a Long Short-Term Memory, with utter- ance encodings obtained from a Convolutional Neural Network. Out neural network frame- work is able to improve the F-score of8{\%}over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to un- derstand jokes.},
author = {Bertero, D. and Fung, P.},
booktitle = {NAACL-HLT},
file = {:home/hiaoxui/papers/NAACL-HLT/A Long Short-Term Memory Framework for Predicting Humor in Dialogues.pdf:pdf},
isbn = {9781941643914},
pages = {130--135},
title = {{A Long Short-Term Memory Framework for Predicting Humor in Dialogues}},
year = {2016}
}
@article{Rabiner1989,
abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rabiner, L. R.},
doi = {10.1109/5.18626},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/IEEE/A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {IEEE},
number = {2},
pages = {257--286},
pmid = {18626},
title = {{A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition}},
url = {http://ieeexplore.ieee.org/ielx5/5/698/00018626.pdf?tp={\&}arnumber=18626{\&}isnumber=698{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=18626{\&}tag=1{\%}0Ahttp://ieeexplore.ieee.org/document/18626/},
volume = {77},
year = {1989}
}
@inproceedings{Chaganty2016,
abstract = {How much is 131 million US dollars? To help readers put such numbers in con-text, we propose a new task of automati-cally generating short descriptions known as perspectives, e.g. " {\$}131 million is about the cost to employ everyone in Texas over a lunch period " . First, we collect a dataset of numeric mentions in news arti-cles, where each mention is labeled with a set of rated perspectives. We then pro-pose a system to generate these descrip-tions consisting of two steps: formula con-struction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on fa-miliarity, numeric proximity and seman-tic compatibility. In generation, we con-vert a formula into natural language us-ing a sequence-to-sequence recurrent neu-ral network. Our system obtains a 15.2{\%} F 1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.},
archivePrefix = {arXiv},
arxivId = {1609.00070},
author = {Chaganty, A. and Liang, P.},
booktitle = {ACL},
doi = {10.18653/v1/P16-1055},
eprint = {1609.00070},
file = {:home/hiaoxui/papers/ACL/How Much is 131 Million Dollars Putting Numbers in Perspective with Compositional Descriptions.pdf:pdf},
isbn = {9781510827585},
pages = {578--587},
title = {{How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions}},
url = {http://aclweb.org/anthology/P16-1055},
year = {2016}
}
@inproceedings{Abend2013,
abstract = {Syntactic structures, by their nature, re-flect first and foremost the formal con-structions used for expressing meanings. This renders them sensitive to formal vari-ation both within and across languages, and limits their value to semantic ap-plications. We present UCCA, a novel multi-layered framework for semantic rep-resentation that aims to accommodate the semantic distinctions expressed through linguistic utterances. We demonstrate UCCA's portability across domains and languages, and its relative insensitivity to meaning-preserving syntactic variation. We also show that UCCA can be ef-fectively and quickly learned by annota-tors with no linguistic background, and describe the compilation of a UCCA-annotated corpus.},
author = {Abend, O. and Rappoport, A.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Universal Conceptual Cognitive Annotation ( UCCA ).pdf:pdf},
isbn = {978-1-937284-50-3},
number = {Section 2},
pages = {228--238},
title = {{Universal Conceptual Cognitive Annotation ( UCCA )}},
url = {http://www.aclweb.org/anthology/P13-1023},
year = {2013}
}
@article{Ganchev2010,
abstract = {We present Posterior Regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efficiently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior Regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efficiency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efficient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment.},
author = {Ganchev, K. and Gra{\c{c}}a, J. V. and Gillenwater, J. and Taskar, B.},
file = {:home/hiaoxui/papers/JMLR/Posterior Regularization for Structured Latent Variable Models.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {JMLR},
keywords = {latent variables models,natural language processing,posterior regularization framework,prior knowledge,unsupervised learning},
number = {MS-CIS-09-16},
pages = {2001--2049},
title = {{Posterior Regularization for Structured Latent Variable Models}},
url = {http://dl.acm.org/citation.cfm?id=1756006.1859918},
volume = {11},
year = {2010}
}
@inproceedings{Dong2018,
abstract = {Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.},
archivePrefix = {arXiv},
arxivId = {1805.04793},
author = {Dong, L. and Lapata, M.},
booktitle = {ACL},
eprint = {1805.04793},
file = {:home/hiaoxui/papers/ACL/Coarse-to-Fine Decoding for Neural Semantic Parsing.pdf:pdf},
pages = {1--12},
title = {{Coarse-to-Fine Decoding for Neural Semantic Parsing}},
url = {http://arxiv.org/abs/1805.04793},
year = {2018}
}
@inproceedings{Diab2002,
abstract = {With an increasing number of languages making their way to our desktops everyday via the Internet, researchers have come to realize the lack of linguistic knowledge resources for scarcely represented/studied languages. In an attempt to bootstrap some of the required linguistic resources for some of those languages, this paper presents an unsupervised method for automatic multilingual word sense tagging using parallel corpora. The method is evaluated on the English Brown corpus and its translation into three different languages: French, German and Spanish. A preliminary evaluation of the proposed method yielded results of up to 79{\%} accuracy rate for the English data on 81.8{\%} of the SemCor manually tagged data.},
author = {Diab, M. and Resnik, P.},
booktitle = {ACL},
doi = {10.3115/1073083.1073126},
file = {:home/hiaoxui/papers/ACL/An unsupervised method for word sense tagging using parallel corpora.pdf:pdf},
title = {{An unsupervised method for word sense tagging using parallel corpora}},
year = {2002}
}
@inproceedings{Bengio2009,
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illus- trates gradually more concepts, and gradu- ally more complex ones. Here, we formal- ize such training strategies in the context of machine learning, and call them “curricu- lum learning”. In the context of recent re- search studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neu- ral networks), we explore curriculum learn- ing in various set-ups. The experiments show that significant improvements in generaliza- tion can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bengio, Y. and Louradour, J. and Collobert, R. and Weston, J.},
booktitle = {ICML},
doi = {10.1145/1553374.1553380},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/ICML/Curriculum learning.pdf:pdf},
isbn = {9781605585161},
issn = {0022-5193},
pages = {41--48},
pmid = {5414602},
title = {{Curriculum learning}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
year = {2009}
}
@inproceedings{Rao2018,
abstract = {Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. In this work, we build a neural network model for the task of ranking clarification questions. Our model is inspired by the idea of expected value of perfect information: a good question is one whose expected answer will be useful. We study this problem using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We create a dataset of clarification questions consisting of {\~{}}77K posts paired with a clarification question (and answer) from three domains of StackExchange: askubuntu, unix and superuser. We evaluate our model on 500 samples of this dataset against expert human judgments and demonstrate significant improvements over controlled baselines.},
archivePrefix = {arXiv},
arxivId = {1805.04655},
author = {Rao, S. and {Daum{\'{e}} III}, H.},
booktitle = {ACL},
eprint = {1805.04655},
file = {:home/hiaoxui/papers/ACL/Learning to Ask Good Questions Ranking Clarification Questions using Neural Expected Value of Perfect Information.pdf:pdf},
title = {{Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information}},
url = {http://arxiv.org/abs/1805.04655},
year = {2018}
}
@unpublished{Trask2018,
abstract = {Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.},
archivePrefix = {arXiv},
arxivId = {arXiv:1808.00508v1},
author = {Trask, A. and Hill, F. and Reed, S. and Rae, J. and Dyer, C. and Blunsom, P.},
eprint = {arXiv:1808.00508v1},
file = {:home/hiaoxui/papers/Unknown/Neural Arithmetic Logic Units.pdf:pdf},
title = {{Neural Arithmetic Logic Units}},
url = {https://arxiv.org/pdf/1808.00508.pdf},
year = {2018}
}
@inproceedings{Schlangen2009,
author = {Schlangen, D. and Skantze, G.},
booktitle = {EACL},
doi = {10.5087/dad.2011.105},
file = {:home/hiaoxui/papers/EACL/A General, Abstract Model of Incremental Dialogue Processing.pdf:pdf},
title = {{A General, Abstract Model of Incremental Dialogue Processing}},
year = {2009}
}
@inproceedings{Xu2018,
abstract = {Stock movement prediction is a challeng-ing problem: the market is highly stochas-tic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly ex-ploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with tempo-ral auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement predic-tion dataset which we collected. 1},
author = {Xu, Y. and Cohen, S. B.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Stock Movement Prediction from Tweets and Historical Prices.pdf:pdf},
title = {{Stock Movement Prediction from Tweets and Historical Prices}},
url = {https://yumoxu.github.io/res/acl18-stocknet.pdf},
year = {2018}
}
@inproceedings{Li2016,
abstract = {We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.},
archivePrefix = {arXiv},
arxivId = {1603.06155},
author = {Li, J. and Galley, M. and Brockett, C. and Spithourakis, G. P. and Gao, J. and Dolan, B.},
booktitle = {ACL},
eprint = {1603.06155},
file = {:home/hiaoxui/papers/ACL/A Persona-Based Neural Conversation Model.pdf:pdf},
isbn = {9781510827585},
title = {{A Persona-Based Neural Conversation Model}},
url = {http://arxiv.org/abs/1603.06155},
year = {2016}
}
@inproceedings{Baroni2014,
author = {Baroni, M. and Dinuand, G. and Kruszewski, G.},
booktitle = {ACL},
file = {:home/hiaoxui/papers/ACL/Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
pages = {238--247},
title = {{Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors}},
year = {2014}
}
@inproceedings{Bowman2016,
abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
archivePrefix = {arXiv},
arxivId = {1511.06349},
author = {Bowman, S. R. and Vilnis, L. and Vinyals, O. and Dai, A. M. and Jozefowicz, R. and Bengio, S.},
booktitle = {CoNLL},
doi = {10.18653/v1/K16-1002},
eprint = {1511.06349},
file = {:home/hiaoxui/papers/CoNLL/Generating Sentences from a Continuous Space(2).pdf:pdf},
pmid = {387149},
title = {{Generating Sentences from a Continuous Space}},
url = {http://arxiv.org/abs/1511.06349},
year = {2016}
}
@inproceedings{Zhao2014,
abstract = {Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GeoQuery, Jobs and Atis domains.},
archivePrefix = {arXiv},
arxivId = {1411.5379},
author = {Zhao, K. and Huang, L.},
booktitle = {NAACL},
eprint = {1411.5379},
file = {:home/hiaoxui/papers/NAACL/Type-Driven Incremental Semantic Parsing with Polymorphism.pdf:pdf},
isbn = {9781941643495},
pages = {1416--1421},
title = {{Type-Driven Incremental Semantic Parsing with Polymorphism}},
url = {http://arxiv.org/abs/1411.5379},
volume = {0041},
year = {2014}
}
@article{Arulampalam2002,
abstract = {Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or "particle") representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Arulampalam, M. S. and Maskell, S. and Gordon, N. and Clapp, T.},
doi = {10.1109/9780470544198.ch73},
eprint = {arXiv:1011.1669v3},
file = {:home/hiaoxui/papers/TSP/A tutorial on particle filters for online nonlinearnongaussian bayesian tracking.pdf:pdf},
isbn = {9780470544198},
issn = {1053587X},
journal = {TSP},
keywords = {Approximation algorithms,Approximation methods,Bayesian methods,Filtering algorithms,Particle filters},
number = {2},
pages = {723--737},
pmid = {978374},
title = {{A tutorial on particle filters for online nonlinear/nongaussian bayesian tracking}},
volume = {50},
year = {2002}
}
@inproceedings{Welling2011,
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an in-built protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
archivePrefix = {arXiv},
arxivId = {arXiv:1203.5753v5},
author = {Welling, M. and Teh, Y. W.},
booktitle = {ICML},
doi = {10.1515/jip-2012-0071},
eprint = {arXiv:1203.5753v5},
file = {:home/hiaoxui/papers/ICML/Bayesian Learning via Stochastic Gradient Langevin Dynamics.pdf:pdf},
isbn = {978-1-4503-0619-5},
issn = {10495258},
keywords = {Bayesian learning,ICML,machine learning,online learning},
pages = {681--688},
title = {{Bayesian Learning via Stochastic Gradient Langevin Dynamics}},
year = {2011}
}
@unpublished{Singh2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.02383v1},
author = {Singh, S.},
eprint = {arXiv:1807.02383v1},
file = {:home/hiaoxui/papers/Unknown/Natural Language Processing for Information Extraction.pdf:pdf},
pages = {1--24},
title = {{Natural Language Processing for Information Extraction}},
year = {2018}
}
@inproceedings{Gardner2018,
author = {Gardner, M. and Dasigi, P. and Iyer, S. and Suhr, A. and Zettlemoyer, L. S.},
booktitle = {ACL},
doi = {10.1523/JNEUROSCI.5952-09.2010.Orbitofrontal},
file = {:home/hiaoxui/papers/ACL/Neural Semantic Parsing.pdf:pdf},
pages = {17--18},
pmid = {6142362},
title = {{Neural Semantic Parsing}},
year = {2018}
}
@inproceedings{Suhr2018,
abstract = {We propose a context-dependent model to map utterances within an interaction to executable formal queries. To incorporate interaction his-tory, the model maintains an interaction-level encoder that updates after each turn, and can copy sub-sequences of previously predicted queries during generation. Our approach com-bines implicit and explicit modeling of refer-ences between utterances. We evaluate our model on the ATIS flight planning interac-tions, and demonstrate the benefits of model-ing context and explicit references.},
author = {Suhr, A. and Iyer, S. and Artzi, Yoav},
booktitle = {NAACL},
doi = {10.18653/v1/n18-1203},
file = {:home/hiaoxui/papers/NAACL/Learning to Map Context-Dependent Sentences to Executable Formal Queries.pdf:pdf},
keywords = {Outstanding},
mendeley-tags = {Outstanding},
pages = {2238--2249},
title = {{Learning to Map Context-Dependent Sentences to Executable Formal Queries}},
year = {2018}
}
@inproceedings{Roth2005,
author = {Roth, D. and Yih, W.},
booktitle = {ICML},
file = {:home/hiaoxui/papers/ICML/Integer Linear Programming Inference for Conditional Random Fields.pdf:pdf},
title = {{Integer Linear Programming Inference for Conditional Random Fields}},
year = {2005}
}
@inproceedings{Athalye2018,
abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. For each of the three types of obfuscated gradients we discover, we describe characteristic behaviors of defenses exhibiting this effect and develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 8 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely and 1 partially.},
archivePrefix = {arXiv},
arxivId = {1802.00420},
author = {Athalye, A. and Carlini, N. and Wagner, D.},
booktitle = {ICML},
eprint = {1802.00420},
file = {:home/hiaoxui/papers/ICML/Obfuscated Gradients Give a False Sense of Security Circumventing Defenses to Adversarial Examples.pdf:pdf},
title = {{Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples}},
url = {http://arxiv.org/abs/1802.00420},
year = {2018}
}
@inproceedings{Yang2015a,
abstract = {Representation learning has shown its effectiveness in many tasks such as image classification and text mining. Network representation learning aims at learning distributed vector representation for each vertex in a network, which is also increasingly recognized as an important aspect for network analysis. Most network representation learning methods investigate network structures for learning. In reality, network vertices contain rich information (such as text), which cannot be well applied with algorithmic frameworks of typical representation learning methods. By proving that DeepWalk, a state-of-the-art network representation method, is actually equivalent to matrix factorization (MF), we propose text-associated DeepWalk (TADW). TADW incorporates text features of vertices into network representation learning under the framework of matrix factorization. We evaluate our method and various baseline methods by applying them to the task of multi-class classification of vertices. The experimental results show that, our method outperforms other baselines on all three datasets, especially when networks are noisy and training ratio is small. The source code of this paper can be obtained from https://github.com/ albertyang33/TADW},
author = {Yang, C. and Liu, Z. and Zhao, D. and Sun, M. and Chang, E. Y.},
booktitle = {IJCAI},
file = {:home/hiaoxui/papers/IJCAI/Network representation learning with rich text information.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
keywords = {Technical Papers — Social Networks},
pages = {2111--2117},
title = {{Network representation learning with rich text information}},
year = {2015}
}
@inproceedings{Blum1992,
abstract = {We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for this network so that it produces output consistent with a given set of training examples. We extend the result to other simple networks. We also present a network for which training is hard but where switching to a more powerful representation makes training easier. These results suggest that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. They also suggest the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with nonlinear functions such as sigmoids. {\textcopyright} 1992 Pergamon Press plc.},
author = {Blum, A. L. and Rivest, R. L.},
booktitle = {Neurips},
doi = {10.1016/S0893-6080(05)80010-3},
file = {:home/hiaoxui/papers/Neurips/Training a 3-node neural network is NP-complete.pdf:pdf},
isbn = {0-55869-019-5},
issn = {08936080},
keywords = {Computational complexity,Intractability,Learning,Multilayer perceptron,NP-completeness,Neural networks,Representation,Training},
number = {1},
pages = {117--127},
title = {{Training a 3-node neural network is NP-complete}},
url = {http://papers.nips.cc/paper/125-training-a-3-node-neural-network-is-np-complete.pdf},
volume = {5},
year = {1992}
}
@unpublished{Collins2013d,
abstract = {Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers a few seconds beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger. Anticipation requires modeling the driver's action space, events inside the vehicle such as their head movements, and also the outside environment. Performing this joint modeling makes anticipation a challenging problem. In this work we anticipate driving maneuvers a few seconds before they occur. For this purpose we equip a car with cameras and a computing device to capture the context from both inside and outside of the car. We represent the context with expressive features and propose an Autoregressive Input-Output HMM to model the contextual information. We evaluate our approach on a diverse data set with 1180 miles of natural freeway and city driving and show that we can anticipate maneuvers 3.5 seconds before they occur with over 80{\%} F1-score. Our computation time during inference is under 3.6 milliseconds.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.02789v1},
author = {Collins, M.},
doi = {10.1007/978-1-4614-2299-0},
eprint = {arXiv:1504.02789v1},
file = {:home/hiaoxui/papers/Unknown/Log-Linear Models.pdf:pdf},
isbn = {9781461422990},
keywords = {Statistical model,log-linear models},
number = {1},
pages = {1--20},
title = {{Log-Linear Models}},
volume = {1},
year = {2013}
}
@inproceedings{Tang2015,
abstract = {This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the "LINE," which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.},
archivePrefix = {arXiv},
arxivId = {1503.03578},
author = {Tang, J. and Qu, M. and Wang, M. and Zhang, M. and Yan, J. and Mei, Q.},
booktitle = {WWW},
doi = {10.1145/2736277.2741093},
eprint = {1503.03578},
file = {:home/hiaoxui/papers/WWW/LINE Large-scale Information Network Embedding.pdf:pdf},
isbn = {9781450334693},
keywords = {dimension reduction,feature learn-,information network embedding,ing,scalability},
title = {{LINE: Large-scale Information Network Embedding}},
url = {http://arxiv.org/abs/1503.03578{\%}0Ahttp://dx.doi.org/10.1145/2736277.2741093},
year = {2015}
}
@inproceedings{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, G. E. and Vinyals, O. and Dean, J.},
booktitle = {Neurips},
doi = {10.1063/1.4931082},
eprint = {1503.02531},
file = {:home/hiaoxui/papers/Neurips/Distilling the Knowledge in a Neural Network.pdf:pdf},
isbn = {3531207857},
issn = {0022-2488},
pages = {1--9},
pmid = {18249735},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{Reiter2009,
abstract = {There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies.},
author = {Reiter, E. and Belz, A.},
doi = {10.1162/coli.2009.35.4.35405},
file = {:home/hiaoxui/papers/Computational Linguistics/An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems.pdf:pdf},
issn = {08912017},
journal = {Computational Linguistics},
number = {4},
pages = {529--558},
title = {{An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems}},
volume = {35},
year = {2009}
}
@inproceedings{Luong2015,
abstract = {Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task.},
archivePrefix = {arXiv},
arxivId = {1410.8206},
author = {Luong, M. and Sutskever, I. and Le, Q. V. and Vinyals, O. and Zaremba, W.},
booktitle = {ACL},
eprint = {1410.8206},
file = {:home/hiaoxui/papers/ACL/Addressing the Rare Word Problem in Neural Machine Translation.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
title = {{Addressing the Rare Word Problem in Neural Machine Translation}},
url = {http://arxiv.org/abs/1410.8206},
year = {2015}
}
@inproceedings{Wiseman2017,
abstract = {Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.},
archivePrefix = {arXiv},
arxivId = {1707.08052},
author = {Wiseman, S. and Shieber, S. M. and Rush, A. M.},
booktitle = {EMNLP},
eprint = {1707.08052},
file = {:home/hiaoxui/papers/EMNLP/Challenges in Data-to-Document Generation.pdf:pdf},
title = {{Challenges in Data-to-Document Generation}},
url = {http://arxiv.org/abs/1707.08052},
year = {2017}
}
@inproceedings{Bansal2014,
abstract = {Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the use of continuous word representations as features for dependency parsing. We com- pare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of oth- ers. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representa- tions achieves the best results, suggesting their complementarity.},
author = {Bansal, M. and Gimpel, K. and Livescu, K.},
booktitle = {ACL},
doi = {10.3115/v1/p14-2131},
file = {:home/hiaoxui/papers/ACL/Tailoring Continuous Word Representations for Dependency Parsing.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
pages = {809--815},
title = {{Tailoring Continuous Word Representations for Dependency Parsing}},
year = {2014}
}
@inproceedings{Hajishirzi2014,
author = {Koncel-Kedziorski, R. and Hajishirzi, H. and Farhadi, A.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Multi-Resolution Language Grounding with Weak Supervision.pdf:pdf},
isbn = {9781937284961},
pages = {386--396},
title = {{Multi-Resolution Language Grounding with Weak Supervision}},
year = {2014}
}
@inproceedings{Lin2003,
abstract = {Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.},
author = {Lin, C. and Hovy, E.},
booktitle = {NAACL},
doi = {10.3115/1073445.1073465},
file = {:home/hiaoxui/papers/NAACL/Automatic evaluation of summaries using N-gram co-occurrence statistics.pdf:pdf},
keywords = {Human Language Technology,NAACL '03},
number = {June},
pages = {71--78},
title = {{Automatic evaluation of summaries using N-gram co-occurrence statistics}},
url = {http://portal.acm.org/citation.cfm?doid=1073445.1073465},
volume = {2003},
year = {2003}
}
@article{Reiter2018,
author = {Reiter, E.},
file = {:home/hiaoxui/papers/Computational Linguistics/BLEU Structured Review A Structured Review of the Validity of BLEU.pdf:pdf},
journal = {Computational Linguistics},
number = {August 2017},
title = {{BLEU Structured Review: A Structured Review of the Validity of BLEU}},
year = {2018}
}
@unpublished{Collins2013,
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/The Forward-Backward Algorithm.pdf:pdf},
pages = {1--4},
title = {{The Forward-Backward Algorithm}},
url = {http://www.work.caltech.edu/{~}ling/webs/EE127/EE127C/handout/FBA.pdf},
year = {2013}
}
@inproceedings{Yao2015,
abstract = {In this paper, we formulate a sparse optimization framework for extractive document summarization. The proposed framework has a decomposable con-vex objective function. We derive an efficient ADMM algorithm to solve it. To encourage di-versity in the summaries, we explicitly introduce an additional sentence dissimilarity term in the op-timization framework. We achieve significant im-provement over previous related work under sim-ilar data reconstruction framework. We then gen-eralize our formulation to the case of compressive summarization and derive a block coordinate de-scent algorithm to optimize the objective function. Performance on DUC 2006 and DUC 2007 datasets shows that our compressive summarization results are competitive against the state-of-the-art results while maintaining reasonable readability.},
author = {Yao, J. and Wan, X. and Xiao, J.},
booktitle = {IJCAI},
file = {:home/hiaoxui/papers/IJCAI/Compressive document summarization via sparse optimization.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
keywords = {Technical Papers — Natural Language Processing},
pages = {1376--1382},
title = {{Compressive document summarization via sparse optimization}},
year = {2015}
}
@article{Manning2013,
abstract = {This paper evaluates the investment performance of Malaysian-based international equity funds. The results on the overall fund performance using Jensen's (1968) model indicate that, on average, international funds have significant negative risk-adjusted returns over the study period from 2008-2010. Since the model ignores market timing activity, it implicitly attributes the overall negative return to manager's poor stock selection ability. However, the performance breakdown results on managerial expertise using the models of Treynor and Mazuy (1966) and Henriksson and Merton (1981) show evidence of positive selectivity and negative market timing returns. Taken together, the highly significant negative timing returns suggest that, on average, international fund managers have perverse market timing ability. The paper finds little evidence that Malaysian investors achieve diversification benefits from investing in overseas equity markets.},
archivePrefix = {arXiv},
arxivId = {1604.07370},
author = {Manning, C. D.},
doi = {10.1162/COLI},
eprint = {1604.07370},
file = {:home/hiaoxui/papers/COLING/Computational Linguistics and Deep Learning.pdf:pdf},
isbn = {9781608459858},
issn = {01272713},
journal = {COLING},
keywords = {Fund performance,International equity funds,Market timing,Security selection},
pages = {41--51},
pmid = {22251136},
title = {{Computational Linguistics and Deep Learning}},
volume = {38},
year = {2013}
}
@inproceedings{Choi2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.02786v4},
author = {Choi, J. and Yoo, K. M. and Lee, S.},
booktitle = {AAAI},
eprint = {arXiv:1707.02786v4},
file = {:home/hiaoxui/papers/AAAI/Learning to Compose Task-Specific Tree Structures.pdf:pdf},
title = {{Learning to Compose Task-Specific Tree Structures}},
year = {2018}
}
@inproceedings{Kingma2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6114v10},
author = {Kingma, D. P. and Welling, M.},
booktitle = {ICLR},
eprint = {arXiv:1312.6114v10},
file = {:home/hiaoxui/papers/ICLR/Auto-Encoding Variational Bayes.pdf:pdf},
pages = {1--14},
title = {{Auto-Encoding Variational Bayes}},
year = {2014}
}
@inproceedings{Kwiatkowski2010,
abstract = {This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously es- timating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 1},
author = {Kwiatkowski, T. and Zettlemoyer, L. S. and Goldwater, S. and Steedman, M.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification.pdf:pdf},
isbn = {1932432868},
number = {October},
pages = {1223--1233},
title = {{Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification}},
year = {2010}
}
@inproceedings{Misra2017,
abstract = {We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent's exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.},
archivePrefix = {arXiv},
arxivId = {1704.08795},
author = {Misra, D. and Langford, J. and Artzi, Y.},
booktitle = {EMNLP},
eprint = {1704.08795},
file = {:home/hiaoxui/papers/EMNLP/Mapping Instructions and Visual Observations to Actions with Reinforcement Learning.pdf:pdf},
title = {{Mapping Instructions and Visual Observations to Actions with Reinforcement Learning}},
url = {http://arxiv.org/abs/1704.08795},
year = {2017}
}
@unpublished{Graves2017,
abstract = {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.},
archivePrefix = {arXiv},
arxivId = {1704.03003},
author = {Graves, A. and Bellemare, M. G. and Menick, J. and Munos, R. and Kavukcuoglu, K.},
eprint = {1704.03003},
file = {:home/hiaoxui/papers/Unknown/Automated Curriculum Learning for Neural Networks.pdf:pdf},
isbn = {9781510855144},
issn = {1938-7228},
title = {{Automated Curriculum Learning for Neural Networks}},
url = {http://arxiv.org/abs/1704.03003},
year = {2017}
}
@inproceedings{Zhang2017b,
abstract = {To learn a semantic parser from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing model to improve the state-of-the-art accuracy from 38.7{\%} to 42.7{\%}, and then use macro grammars and holistic triggering to achieve an 11x speedup and an accuracy of 43.7{\%}.},
archivePrefix = {arXiv},
arxivId = {1707.07806},
author = {Zhang, Y. and Pasupat, P. and Liang, P.},
booktitle = {EMNLP},
eprint = {1707.07806},
file = {:home/hiaoxui/papers/EMNLP/Macro Grammars and Holistic Triggering for Efficient Semantic Parsing.pdf:pdf},
title = {{Macro Grammars and Holistic Triggering for Efficient Semantic Parsing}},
url = {http://arxiv.org/abs/1707.07806},
year = {2017}
}
@inproceedings{Perez-Beltrachini2018,
abstract = {A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and associated texts. In this paper we aim to bootstrap generators from large scale datasets where the data (e.g., DBPedia facts) and related texts (e.g., Wikipedia abstracts) are loosely aligned. We tackle this challenging task by introducing a special-purpose content selection mechanism. We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention.},
archivePrefix = {arXiv},
arxivId = {1804.06385},
author = {Perez-Beltrachini, L. and Lapata, M.},
booktitle = {NAACL-HLT},
eprint = {1804.06385},
file = {:home/hiaoxui/papers/NAACL-HLT/Bootstrapping Generators from Noisy Data.pdf:pdf},
pages = {1516--1527},
title = {{Bootstrapping Generators from Noisy Data}},
url = {http://arxiv.org/abs/1804.06385},
year = {2018}
}
@inproceedings{Valera2017,
abstract = {Circular variables arise in a multitude of data-modelling contexts ranging from robotics to the social sciences, but they have been largely overlooked by the machine learning community. This paper partially redresses this imbalance by extending some standard probabilistic modelling tools to the circular domain. First we introduce a new multivariate distribution over circular variables, called the multivariate Generalised von Mises (mGvM) distribution. This distribution can be constructed by restricting and renormalising a general multivariate Gaussian distribution to the unit hyper-torus. Previously proposed multivariate circular distributions are shown to be special cases of this construction. Second, we introduce a new probabilistic model for circular regression, that is inspired by Gaussian Processes, and a method for probabilistic principal component analysis with circular hidden variables. These models can leverage standard modelling tools (e.g. covariance functions and methods for automatic relevance determination). Third, we show that the posterior distribution in these models is a mGvM distribution which enables development of an efficient variational free-energy scheme for performing approximate inference and approximate maximum-likelihood learning.},
archivePrefix = {arXiv},
arxivId = {1602.05003},
author = {Valera, I. and Ghahramani, Z.},
booktitle = {ICML},
eprint = {1602.05003},
file = {:home/hiaoxui/papers/ICML/Automatic Discovery of the Statistical Types of Variables in a Dataset.pdf:pdf},
pages = {4--5},
title = {{Automatic Discovery of the Statistical Types of Variables in a Dataset}},
url = {http://arxiv.org/abs/1602.05003},
year = {2017}
}
@inproceedings{Creutz2002,
abstract = {We present two methods for unsupervised segmentation of words into morpheme-like units. The model utilized is especially suited for languages with a rich morphology, such as Finnish. The first method is based on the Minimum Description Length (MDL) principle and works online. In the second method, Maximum Likelihood (ML) optimization is used. The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis. Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current state-of-the-art system.},
archivePrefix = {arXiv},
arxivId = {cs/0205057},
author = {Creutz, M. and Lagus, K.},
booktitle = {Workshop of the ACL Special Interest Group in Computational Phonology},
eprint = {0205057},
file = {:home/hiaoxui/papers/Workshop of the ACL Special Interest Group in Computational Phonology/Unsupervised Discovery of Morphemes.pdf:pdf},
number = {July},
pages = {21--30},
primaryClass = {cs},
title = {{Unsupervised Discovery of Morphemes}},
url = {http://arxiv.org/abs/cs/0205057},
year = {2002}
}
@inproceedings{Dong2016,
abstract = {Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.},
archivePrefix = {arXiv},
arxivId = {1601.01280},
author = {Dong, L. and Lapata, M.},
booktitle = {ACL},
doi = {10.18653/v1/P16-1004},
eprint = {1601.01280},
file = {:home/hiaoxui/papers/ACL/Language to Logical Form with Neural Attention.pdf:pdf},
isbn = {9781510827585},
pages = {33--43},
title = {{Language to Logical Form with Neural Attention}},
url = {http://arxiv.org/abs/1601.01280},
year = {2016}
}
@inproceedings{Yin2017,
abstract = {We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.},
archivePrefix = {arXiv},
arxivId = {1704.01696},
author = {Yin, P. and Neubig, G.},
booktitle = {ACL},
doi = {10.18653/v1/P17-1041},
eprint = {1704.01696},
file = {:home/hiaoxui/papers/ACL/A Syntactic Neural Model for General-Purpose Code Generation.pdf:pdf},
isbn = {9781945626753},
pages = {440--450},
title = {{A Syntactic Neural Model for General-Purpose Code Generation}},
url = {http://arxiv.org/abs/1704.01696},
year = {2017}
}
@inproceedings{Kalchbrenner2014,
abstract = {The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25{\%} error reduction in the last task with respect to the strongest baseline.},
archivePrefix = {arXiv},
arxivId = {1404.2188},
author = {Kalchbrenner, N. and Grefenstette, E. and Blunsom, P.},
booktitle = {ACL},
eprint = {1404.2188},
file = {:home/hiaoxui/papers/ACL/A Convolutional Neural Network for Modelling Sentences.pdf:pdf},
keywords = {citation},
mendeley-tags = {citation},
title = {{A Convolutional Neural Network for Modelling Sentences}},
url = {http://arxiv.org/abs/1404.2188},
year = {2014}
}
@inproceedings{Lu2011,
abstract = {"This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation."},
author = {Lu, W. and Ng, H. T.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions.pdf:pdf},
isbn = {1937284115},
pages = {1611--1622},
title = {{A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions}},
url = {http://www.aclweb.org/anthology/D11-1149{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2145605},
year = {2011}
}
@inproceedings{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, I. and Vinyals, O. and Le, Q. V.},
booktitle = {Neurips},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:home/hiaoxui/papers/Neurips/Sequence to sequence learning with neural networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@inproceedings{Bordes2010,
abstract = {This paper studies the problem of learning from ambiguous supervision, focusing on the task of learning semantic correspondences. A learning problem is said to be ambiguously supervised when, for a given training input, a set of output candidates is provided with no prior of which one is correct. We propose to tackle this problem by solving a related unambiguous task with a label ranking ap- proach and show how and why this performs well on the original task, via the method of task-transfer. We apply it to learning to match natural language sentences to a struc- tured representation of their meaning and empirically demonstrate that this competes with the state-of-the-art on two benchmarks.},
author = {Bordes, A. and Usunier, N. and Weston, J. and Kennedy, P.},
booktitle = {ICML},
file = {:home/hiaoxui/papers/ICML/Label Ranking under Ambiguous Supervision for Learning Semantic Correspondences.pdf:pdf},
isbn = {9781605589077},
number = {Icml},
pages = {103--110},
title = {{Label Ranking under Ambiguous Supervision for Learning Semantic Correspondences}},
url = {http://www.icml2010.org/papers/331.pdf},
year = {2010}
}
@unpublished{Collins2013c,
author = {Collins, M.},
file = {:home/hiaoxui/papers/Unknown/The Naive Bayes Model, Maximum-Likelihood Estimation, and the EM Algorithm.pdf:pdf},
pages = {1--21},
title = {{The Naive Bayes Model, Maximum-Likelihood Estimation, and the EM Algorithm}},
year = {2013}
}
@inproceedings{Ammar2014,
abstract = {We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observable data using a feature-rich conditional random field. Then a reconstruction of the input is (re)generated, conditional on the latent structure, using models for which maximum likelihood estimation has a closed-form. Our autoencoder formulation enables efficient learning without making unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. We show competitive results with instantiations of the model for two canonical NLP tasks: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.},
archivePrefix = {arXiv},
arxivId = {1411.1147},
author = {Ammar, W. and Dyer, C. and Smith, N. A.},
booktitle = {Neurips},
eprint = {1411.1147},
file = {:home/hiaoxui/papers/Neurips/Conditional Random Field Autoencoders for Unsupervised Structured Prediction.pdf:pdf},
issn = {10495258},
title = {{Conditional Random Field Autoencoders for Unsupervised Structured Prediction}},
url = {http://arxiv.org/abs/1411.1147},
year = {2014}
}
@inproceedings{Li2019,
abstract = {This paper seeks to model human language by the mathematical framework of quantum physics. With the well-designed mathematical formulations in quantum physics, this framework unifies different linguistic units in a single complex-valued vector space, e.g. words as particles in quantum states and sentences as mixed systems. A complex-valued network is built to implement this framework for semantic matching. With well-constrained complex-valued components, the network admits interpretations to explicit physical meanings. The proposed complex-valued network for matching (CNM) achieves comparable performances to strong CNN and RNN baselines on two benchmarking question answering (QA) datasets.},
archivePrefix = {arXiv},
arxivId = {1904.05298},
author = {Li, Q. and Wang, B. and Melucci, M.},
booktitle = {NAACL-HLT},
eprint = {1904.05298},
file = {:home/hiaoxui/papers/NAACL-HLT/CNM An Interpretable Complex-valued Network for Matching.pdf:pdf},
title = {{CNM: An Interpretable Complex-valued Network for Matching}},
url = {http://arxiv.org/abs/1904.05298},
year = {2019}
}
@article{Karras2012,
abstract = {A number of methods for constructing bounding volume hierarchies and point-based octrees on the GPU are based on the idea of ordering primitives along a space-filling curve. A major shortcoming with these methods is that they construct levels of the tree sequentially, which limits the amount of parallelism that they can achieve. We present a novel approach that improves scalability by constructing the entire tree in parallel. Our main contribution is an in-place algorithm for constructing binary radix trees, which we use as a building block for other types of trees.},
archivePrefix = {arXiv},
arxivId = {cs/9903011},
author = {Karras, T.},
doi = {10.2312/EGGH/HPG12/033-037},
eprint = {9903011},
file = {:home/hiaoxui/papers/High Performance Graphics/Maximizing Parallelism in the Construction of BVHs , Octrees , and k-d Trees.pdf:pdf},
isbn = {978-3-905674-41-5},
issn = {00043702},
journal = {High Performance Graphics},
pages = {33--37},
primaryClass = {cs},
title = {{Maximizing Parallelism in the Construction of BVHs , Octrees , and k-d Trees}},
year = {2012}
}
@inproceedings{Ling2016,
abstract = {Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.},
archivePrefix = {arXiv},
arxivId = {1603.06744},
author = {Ling, W. and Grefenstette, E. and Hermann, K. M. and Ko{\v{c}}isk{\'{y}}, T. and Senior, A. and Wang, F. and Blunsom, P.},
booktitle = {ACL},
eprint = {1603.06744},
file = {:home/hiaoxui/papers/ACL/Latent Predictor Networks for Code Generation.pdf:pdf},
isbn = {9781510827585},
pages = {599--609},
title = {{Latent Predictor Networks for Code Generation}},
url = {http://arxiv.org/abs/1603.06744},
year = {2016}
}
@phdthesis{S.M.Ransom2001,
author = {{Ransom. S. M.}},
file = {:home/hiaoxui/papers/Unknown/New Search Techniques for Binary Pulsars.pdf:pdf},
title = {{New Search Techniques for Binary Pulsars}},
year = {2001}
}
@article{Siskind1996,
abstract = {This paper presents a computational study of part of the lexical-acquisition task faced by children, namely the acquisition of word-to-meaning mappings. It first approximates this task as a formal mathematical problem. It then presents an implemented algorithm for solving this problem, illustrating its operation on a small example. This algorithm offers one precise interpretation of the intuitive notions of cross-situational learning and the principle of contrast applied between words in an utterance. It robustly learns a homonymous lexicon despite noisy multi-word input, in the presence of referential uncertainty, with no prior knowledge that is specific to the language being learned. Computational simulations demonstrate the robustness of this algorithm and illustrate how algorithms based on cross-situational learning and the principle of contrast might be able to solve lexical-acquisition problems of the size faced by children, under weak, worst-case assumptions about the type and quantity of data available.},
author = {Siskind, J. M.},
doi = {10.1016/S0010-0277(96)00728-7},
file = {:home/hiaoxui/papers/Cognition/A computational study of cross-situational techniques for learning word-to-meaning mappings.pdf:pdf},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
number = {1-2},
pages = {39--91},
pmid = {8990968},
title = {{A computational study of cross-situational techniques for learning word-to-meaning mappings}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027796007287},
volume = {61},
year = {1996}
}
@inproceedings{Mei2015,
abstract = {We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59{\%} relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.},
archivePrefix = {arXiv},
arxivId = {1509.00838},
author = {Mei, H. and Bansal, M. and Walter, M. R.},
booktitle = {NAACL-HLT},
doi = {10.18653/v1/N16-1086},
eprint = {1509.00838},
file = {:home/hiaoxui/papers/NAACL-HLT/What to talk about and how Selective Generation using LSTMs with Coarse-to-Fine Alignment.pdf:pdf},
isbn = {9781941643914},
pages = {720--730},
title = {{What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment}},
url = {http://arxiv.org/abs/1509.00838},
year = {2016}
}
@article{Boito2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.05631v2},
author = {Boito, M. Z. and Alexandre, B. and Villavicencio, A. and Besacier, L. and Dev, F.},
eprint = {arXiv:1709.05631v2},
file = {:home/hiaoxui/papers/ASRU/Unwritten Languages Demand Attention Too! Word Discovery with Encoder-Decoder Models.pdf:pdf},
journal = {ASRU},
title = {{Unwritten Languages Demand Attention Too! Word Discovery with Encoder-Decoder Models}},
year = {2017}
}
@article{Liang2014,
abstract = {Intended for a wide circle of specialists in automated systems. Above all, however, it is intended for those who work on systems for communicating with machines.},
author = {Liang, P.},
doi = {10.1145/2659831},
file = {:home/hiaoxui/papers/Crossroads/Talking to computers in natural language.pdf:pdf},
issn = {15284972},
journal = {Crossroads},
number = {1},
pages = {18--21},
title = {{Talking to computers in natural language}},
url = {http://dl.acm.org/citation.cfm?doid=2677339.2659831},
volume = {21},
year = {2014}
}
@inproceedings{Andreas2016,
abstract = {We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.},
archivePrefix = {arXiv},
arxivId = {1601.01705},
author = {Andreas, J. and Rohrbach, M. and Darrell, T. and Klein, D.},
booktitle = {NAACL},
doi = {10.18653/v1/N16-1181},
eprint = {1601.01705},
file = {:home/hiaoxui/papers/NAACL/Learning to Compose Neural Networks for Question Answering.pdf:pdf},
isbn = {9781941643914},
title = {{Learning to Compose Neural Networks for Question Answering}},
url = {http://arxiv.org/abs/1601.01705},
year = {2016}
}
@inproceedings{Petrov2006,
abstract = {We present an automatic approach to tree annota- tion in which basic nonterminal symbols are alter- nately split and merged to maximize the likelihood of a training treebank. Starting with a simple X- bar grammar, we learn a new grammar whose non- terminals are subsymbols of the original nontermi- nals. In contrast with previous work, we are able to split various terminals to different degrees, as ap- propriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more ac- curate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2{\%} on the Penn Treebank, higher than fully lexicalized systems.},
author = {Petrov, S. and Barrett, L. and Thibaux, R. and Klein, D.},
booktitle = {ACL},
doi = {10.3115/1220175.1220230},
file = {:home/hiaoxui/papers/ACL/Learning accurate, compact, and interpretable tree annotation.pdf:pdf},
isbn = {1932432655},
number = {July},
pages = {433--440},
title = {{Learning accurate, compact, and interpretable tree annotation}},
url = {http://portal.acm.org/citation.cfm?doid=1220175.1220230},
year = {2006}
}
@article{Belz2008,
abstract = {Two important recent trends in natural language generation are (i) probabilistic techniques and (ii) comprehensive approaches that move away from traditional strictly modular and sequential models. This paper reports experiments in which pCRU a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space was used to semi-automatically create five different versions of a weather forecast generator. The generators were evaluated in terms of output quality, development time and computational efficiency against (i) human forecasters, (ii) a traditional handcrafted pipelined NLG system and (iii) a HALOGEN-style statistical generator. The most striking result is that despite acquiring all decision-making abilities automatically, the best pCRU generators produce outputs of high enough quality to be scored more highly by human judges than forecasts written by experts.},
author = {Belz, A.},
doi = {10.1017/S1351324907004664},
file = {:home/hiaoxui/papers/Natural Language Engineering/Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.pdf:pdf},
isbn = {1469-8110},
issn = {1351-3249},
journal = {Natural Language Engineering},
number = {04},
pages = {1--26},
title = {{Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models}},
url = {http://www.journals.cambridge.org/abstract{\_}S1351324907004664},
volume = {14},
year = {2008}
}
@inproceedings{Soricut2003,
abstract = {We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8{\%} over a state-ofthe- art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.},
author = {Soricut, R. and Marcu, D.},
booktitle = {NAACL-HLT},
doi = {10.3115/1073445.1073475},
file = {:home/hiaoxui/papers/NAACL-HLT/Sentence level discourse parsing using syntactic and lexical information.pdf:pdf},
title = {{Sentence level discourse parsing using syntactic and lexical information}},
year = {2003}
}
@inproceedings{Steinhardt2017,
abstract = {Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12{\%} to 23{\%} test error by adding only 3{\%} poisoned data.},
archivePrefix = {arXiv},
arxivId = {1706.03691},
author = {Steinhardt, J. and Koh, P. W. and Liang, P.},
booktitle = {Neurips},
eprint = {1706.03691},
file = {:home/hiaoxui/papers/Neurips/Certified Defenses for Data Poisoning Attacks.pdf:pdf},
number = {i},
title = {{Certified Defenses for Data Poisoning Attacks}},
url = {http://arxiv.org/abs/1706.03691},
year = {2017}
}
@article{Gorniak2007,
abstract = {We introduce a computational theory of situated language understanding in which the meaning of words and utterances depends on the physical environment and the goals and plans of communication partners. According to the theory, concepts that ground linguistic meaning are neither internal nor external to language users, but instead span the objective-subjective boundary. To model the possible interactions between subject and object, the theory relies on the notion of perceived affordances: structured units of interaction that can be used for prediction at multiple levels of abstraction. Language understanding is treated as a process of filtering perceived affordances. The theory accounts for many aspects of the situated nature of human language use and provides a unified solution to a number of demands on any theory of language understanding including conceptual combination, prototypicality effects, and the generative nature of lexical items. To support the theory, we describe an implemented system that understands verbal commands situated in a virtual gaming environment. The implementation uses probabilistic hierarchical plan recognition to generate perceived affordances. The system has been evaluated on its ability to correctly interpret free-form spontaneous verbal commands recorded from unrehearsed game play between human players. The system is able to "step into the shoes" of human players and correctly respond to a broad range of verbal commands in which linguistic meaning depends on social and physical context. We quantitatively compare the system's predictions in response to direct player commands with the actions taken by human players and show generalization to unseen data across a range of situations and verbal constructions.},
author = {Gorniak, P. and Roy, D.},
doi = {10.1080/15326900701221199},
file = {:home/hiaoxui/papers/Cognitive Science/Situated language understanding as filtering perceived affordances.pdf:pdf},
isbn = {0364-0213 1551-6709},
issn = {0364-0213},
journal = {Cognitive Science},
keywords = {Games,Grounding,Language Understanding,Situated Speech,Theories of Concepts},
number = {2},
pages = {197--231},
pmid = {21635295},
title = {{Situated language understanding as filtering perceived affordances.}},
volume = {31},
year = {2007}
}
@inproceedings{He2017a,
abstract = {We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 23.36.},
archivePrefix = {arXiv},
arxivId = {1703.10722},
author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2016.90},
eprint = {1703.10722},
file = {:home/hiaoxui/papers/CVPR/Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {1664-1078},
pages = {770--778},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1703.10722},
year = {2017}
}
@inproceedings{Hinton1983,
author = {Hinton, G. E. and Sejnowski, T. J.},
booktitle = {CVPR},
file = {:home/hiaoxui/papers/CVPR/Optimal Perceptual Inference.pdf:pdf},
title = {{Optimal Perceptual Inference}},
year = {1983}
}
@inproceedings{Hinton1994,
abstract = {An autoencoder network uses a aset of recognition weights to convert the input veecotre into a code vectore. It then uses set of generative weights to convert the code vector inot an approximate reconstruction of the input vector. We derive and objective function for training autoencoderss based on the minimum descrption length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconsrtuction error. WE show that this information is minimzed by choosing code vectors stochastiacally according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approixmation gives an upper bound on the description length. Even hen this bound is poor, it can be used a Lyapuov function for learning both the generative and recognition weights. We demonstrate that this approach can be used to learn factorial codes.},
author = {Hinton, G. E. and Zemel., R. S.},
booktitle = {Neurips},
doi = {10.1021/jp906511z},
file = {:home/hiaoxui/papers/Neurips/Autoencoders, Minimum Description Length and Helmholtz free Energy.pdf:pdf},
isbn = {1049-5258},
issn = {15205207},
pmid = {20148535},
title = {{Autoencoders, Minimum Description Length and Helmholtz free Energy}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/cvq.pdf},
year = {1994}
}
@inproceedings{Salakhutdinov2009,
author = {Salakhutdinov, R. and Hinton, G.},
booktitle = {AISTATS},
file = {:home/hiaoxui/papers/AISTATS/Deep Boltzmann Machines.pdf:pdf},
number = {3},
pages = {448--455},
title = {{Deep Boltzmann Machines}},
url = {http://jmlr.org/proceedings/papers/v5/salakhutdinov09a/salakhutdinov09a.pdf},
year = {2009}
}
@article{Hinton2006,
abstract = {We show how to use complementary priors to eliminate the explaining- away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa- tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive ver- sion of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribu- tion of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning al- gorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, G. E. and Osindero, S. and Teh, Y.},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:home/hiaoxui/papers/Neural Computation/A Fast Learning Algorithm for Deep Belief Nets.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
pages = {1527--1554},
pmid = {16764513},
title = {{A Fast Learning Algorithm for Deep Belief Nets}},
volume = {18},
year = {2006}
}
@article{Hinton2006a,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G. E. and Salakhutdinov, R. R.},
doi = {10.1126/science.1127647},
file = {:home/hiaoxui/papers/Science/Reducing the dimensionality of data with neural networks.pdf:pdf},
issn = {00368075},
journal = {Science},
number = {5786},
pages = {504--507},
title = {{Reducing the dimensionality of data with neural networks}},
volume = {313},
year = {2006}
}
@inproceedings{Cheng2016,
abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
archivePrefix = {arXiv},
arxivId = {1601.06733},
author = {Cheng, J. and Dong, L. and Lapata, M.},
booktitle = {EMNLP},
eprint = {1601.06733},
file = {:home/hiaoxui/papers/EMNLP/Long Short-Term Memory-Networks for Machine Reading.pdf:pdf;:home/hiaoxui/papers/EMNLP/Long Short-Term Memory-Networks for Machine Reading(2).pdf:pdf},
title = {{Long Short-Term Memory-Networks for Machine Reading}},
url = {http://arxiv.org/abs/1601.06733},
year = {2016}
}
@inproceedings{Li2016,
abstract = {Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.},
archivePrefix = {arXiv},
arxivId = {1606.01541},
author = {Li, J. and Monroe, W. and Ritter, A. and Galley, M. and Gao, J. and Jurafsky, D.},
booktitle = {EMNLP},
eprint = {1606.01541},
file = {:home/hiaoxui/papers/EMNLP/Deep Reinforcement Learning for Dialogue Generation.pdf:pdf},
number = {4},
title = {{Deep Reinforcement Learning for Dialogue Generation}},
url = {http://arxiv.org/abs/1606.01541},
year = {2016}
}
@inproceedings{Richardson2013,
abstract = {We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.},
author = {Richardson, M. and Burges, C. J. C. and Renshaw, E.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/MCTest A Challenge Dataset for the Open-Domain Machine Comprehension of Text.pdf:pdf},
title = {{MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text}},
url = {http://research.microsoft.com/en-us/um/redmond/projects/mctest/MCTest{\_}EMNLP2013.pdf},
year = {2013}
}
@inproceedings{Parikh2016,
abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
archivePrefix = {arXiv},
arxivId = {1606.01933},
author = {Parikh, A. P. and T{\"{a}}ckstr{\"{o}}m, O. and Das, D. and Uszkoreit, J.},
booktitle = {EMNLP},
eprint = {1606.01933},
file = {:home/hiaoxui/papers/EMNLP/A Decomposable Attention Model for Natural Language Inference.pdf:pdf},
title = {{A Decomposable Attention Model for Natural Language Inference}},
url = {http://arxiv.org/abs/1606.01933},
year = {2016}
}
@inproceedings{Fukui2016,
abstract = {Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.},
archivePrefix = {arXiv},
arxivId = {1606.01847},
author = {Fukui, A. and Park, D. H. and Yang, D. and Rohrbach, A. and Darrell, T. and Rohrbach, M.},
booktitle = {EMNLP},
eprint = {1606.01847},
file = {:home/hiaoxui/papers/EMNLP/Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.pdf:pdf},
title = {{Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding}},
url = {http://arxiv.org/abs/1606.01847},
year = {2016}
}
@inproceedings{Liu2016,
abstract = {We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.},
archivePrefix = {arXiv},
arxivId = {1603.08023},
author = {Liu, C. and Lowe, R. and Serban, I. V. and Noseworthy, M. and Charlin, L. and Pineau, J.},
booktitle = {EMNLP},
eprint = {1603.08023},
file = {:home/hiaoxui/papers/EMNLP/How NOT To Evaluate Your Dialogue System An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation.pdf:pdf},
title = {{How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation}},
url = {http://arxiv.org/abs/1603.08023},
year = {2016}
}
@inproceedings{Auli2013,
author = {Auli, M. and Galley, M. and Quirk, C. and Zweig, G.},
booktitle = {EMNLP},
file = {:home/hiaoxui/papers/EMNLP/Joint Language and Translation Modeling with Recurrent Neural Networks.pdf:pdf},
title = {{Joint Language and Translation Modeling with Recurrent Neural Networks}},
year = {2013}
}
@inproceedings{Rush2015,
abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
archivePrefix = {arXiv},
arxivId = {1509.00685},
author = {Rush, A. M. and Chopra, S. and Weston, J.},
booktitle = {EMNLP},
eprint = {1509.00685},
file = {:home/hiaoxui/papers/EMNLP/A Neural Attention Model for Abstractive Sentence Summarization.pdf:pdf},
title = {{A Neural Attention Model for Abstractive Sentence Summarization}},
url = {http://arxiv.org/abs/1509.00685},
year = {2015}
}
@inproceedings{Wen2015,
abstract = {Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.},
archivePrefix = {arXiv},
arxivId = {1508.01745},
author = {Wen, T. and Gasic, M. and Mrksic, N. and Su, P. and Vandyke, D. and Young, S.},
booktitle = {EMNLP},
eprint = {1508.01745},
file = {:home/hiaoxui/papers/EMNLP/Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems.pdf:pdf},
title = {{Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems}},
url = {http://arxiv.org/abs/1508.01745},
year = {2015}
}
@inproceedings{Luong2015,
abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
archivePrefix = {arXiv},
arxivId = {1508.04025},
author = {Luong, M. and Pham, H. and Manning, C. D.},
booktitle = {EMNLP},
eprint = {1508.04025},
file = {:home/hiaoxui/papers/EMNLP/Effective Approaches to Attention-based Neural Machine Translation.pdf:pdf},
title = {{Effective Approaches to Attention-based Neural Machine Translation}},
url = {http://arxiv.org/abs/1508.04025},
year = {2015}
}
@inproceedings{Levy2018,
abstract = {We present an alternate view to explain the success of LSTMs: the gates themselves are powerful recurrent models that provide more representational power than previ-ously appreciated. We do this by showing that much of the LSTM's architecture can be removed, producing a restricted class of RNNs where the main recurrence computes an element-wise weighted sum of context-independent functions of the inputs. Experiments on a range of challenging NLP problems demonstrate that the simplified models work as well as the original LSTMs, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients.},
archivePrefix = {arXiv},
arxivId = {1805.03716},
author = {Levy, O. and Lee, K and FitzGerald, N. and Zettlemoyer, L. S.},
booktitle = {ACL},
eprint = {1805.03716},
file = {:home/hiaoxui/papers/ACL/Long Short-Term Memory As a Dynamically Computed Element-Wise Weighted Sum.pdf:pdf},
pages = {1--9},
title = {{Long Short-Term Memory As a Dynamically Computed Element-Wise Weighted Sum}},
year = {2018}
}
@inproceedings{Ling2015,
abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
archivePrefix = {arXiv},
arxivId = {1508.02096},
author = {Ling, W. and Lu{\'{i}}s, T. and Marujo, L. and Astudillo, R. F. and Amir, S. and Dyer, C. and Black, A. W. and Trancoso, I.},
booktitle = {EMNLP},
eprint = {1508.02096},
file = {:home/hiaoxui/papers/EMNLP/Finding Function in Form Compositional Character Models for Open Vocabulary Word Representation.pdf:pdf},
title = {{Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation}},
url = {http://arxiv.org/abs/1508.02096},
year = {2015}
}
@inproceedings{DeNero2008,
abstract = {We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previ- ous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrase- based translation systems.},
author = {DeNero, J. and Bouchard-Cote, A. and Klein, D.},
booktitle = {EMNLP},
doi = {10.3115/1613715.1613758},
file = {:home/hiaoxui/papers/EMNLP/Sampling alignment structure under a Bayesian translation model.pdf:pdf},
number = {October},
pages = {314--323},
title = {{Sampling alignment structure under a Bayesian translation model}},
url = {http://dl.acm.org/citation.cfm?id=1613758},
year = {2008}
}
@inproceedings{Matuszek2012,
abstract = {As robots become more ubiquitous and ca- pable, it becomes ever more important for untrained users to easily interact with them. Recently, this has led to study of the lan- guage grounding problem, where the goal is to extract representations of the mean- ings of natural language tied to the physi- cal world. We present an approach for joint learning of language and perception models for grounded attribute induction. The per- ception model includes classifiers for phys- ical characteristics and a language model based on a probabilistic categorial grammar that enables the construction of composi- tional meaning representations. We evaluate on the task of interpreting sentences that de- scribe sets of objects in a physical workspace, and demonstrate accurate task performance and effective latent-variable concept induc- tion in physical grounded scenes. 1.},
archivePrefix = {arXiv},
arxivId = {1206.6423},
author = {Matuszek, C. and FitzGerald, N. and Zettlemoyer, L. S. and Liefeng, B. and Fox, D.},
booktitle = {ICML},
eprint = {1206.6423},
file = {:home/hiaoxui/papers/ICML/A Joint Model of Language and Perception for Grounded Attribute Learning.pdf:pdf},
isbn = {978-1-4503-1285-1},
pages = {1671--1678},
title = {{A Joint Model of Language and Perception for Grounded Attribute Learning}},
url = {http://arxiv.org/abs/1206.6423},
year = {2012}
}
@inproceedings{FitzGerald2018,
archivePrefix = {arXiv},
arxivId = {1805.05377},
author = {FitzGerald, N. and Michael, J. and He, L. and Zettlemoyer, L. S.},
booktitle = {ACL},
eprint = {1805.05377},
file = {:home/hiaoxui/papers/ACL/Large-Scale QA-SRL Parsing.pdf:pdf},
pages = {1--10},
title = {{Large-Scale QA-SRL Parsing}},
year = {2018}
}
